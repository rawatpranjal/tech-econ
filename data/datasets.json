[
  {
    "name": "JD.com 2020 (MSOM-20)",
    "description": "2.5M customers (457k purchasers) and 31,868 SKUs from JD.com",
    "category": "E-Commerce",
    "url": "https://huggingface.co/datasets/a6687543/MSOM_Data_Driven_Challenge_2020",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "customers",
      "SKUs",
      "INFORMS",
      "operations"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The JD.com 2020 dataset contains information on 2.5 million customers, including 457,000 purchasers, and 31,868 SKUs. This dataset can be used to analyze consumer purchasing behavior, SKU performance, and operational efficiencies within the e-commerce sector.",
    "use_cases": [
      "Analyzing consumer purchasing behavior",
      "Evaluating SKU performance",
      "Identifying operational efficiencies",
      "Segmenting customers based on purchasing patterns"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What insights can be gained from JD.com customer data?",
      "How do purchasing patterns vary among different SKUs?",
      "What operational efficiencies can be identified from the dataset?",
      "How can customer segmentation be performed using this dataset?",
      "What are the trends in consumer behavior on JD.com?",
      "How can regression analysis be applied to this dataset?",
      "What factors influence SKU sales on JD.com?",
      "How can this dataset be used for pricing optimization?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2020",
    "size_category": "medium",
    "model_score": 0.0305,
    "image_url": "/images/datasets/jdcom-2020-msom-20.png",
    "embedding_text": "The JD.com 2020 dataset, known as MSOM-20, provides a comprehensive view of consumer behavior in the e-commerce sector, specifically focusing on the Chinese market. It includes data on 2.5 million customers, of which 457,000 are identified as purchasers, and encompasses 31,868 stock-keeping units (SKUs). The dataset is structured in a tabular format, with rows representing individual customers and columns capturing various attributes such as customer demographics, purchasing history, and SKU details. This rich dataset allows researchers and analysts to explore a multitude of research questions related to consumer behavior, operational efficiencies, and SKU performance. The data collection methodology likely involved tracking customer interactions on the JD.com platform, capturing transactional data, and possibly integrating customer demographic information from other sources. However, specific details about the data collection process are not provided. Key variables in the dataset include customer ID, SKU ID, purchase frequency, and total spend, which measure customer engagement and SKU sales performance. While the dataset is extensive, potential limitations may include data quality issues related to missing values or inaccuracies in customer demographic information. Common preprocessing steps may involve cleaning the data, handling missing values, and transforming variables for analysis. Researchers can utilize this dataset to conduct various types of analyses, including regression analysis to identify factors influencing purchasing decisions, machine learning algorithms for customer segmentation, and descriptive statistics to summarize customer behavior trends. Overall, the JD.com 2020 dataset serves as a valuable resource for understanding e-commerce dynamics and consumer behavior in a rapidly evolving market.",
    "tfidf_keywords": [
      "customer-segmentation",
      "SKU-performance",
      "purchasing-patterns",
      "operational-efficiencies",
      "e-commerce-analysis",
      "consumer-behavior",
      "regression-analysis",
      "data-cleaning",
      "data-preprocessing",
      "transactional-data"
    ],
    "semantic_cluster": "e-commerce-consumer-behavior",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "consumer-behavior",
      "data-preprocessing",
      "regression-analysis",
      "market-analysis",
      "e-commerce-strategies"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "marketplaces",
      "pricing",
      "data-engineering",
      "econometrics"
    ]
  },
  {
    "name": "Retail Rocket",
    "description": "2.76M events (views, carts, purchases) from 1.4M visitors",
    "category": "E-Commerce",
    "url": "https://www.kaggle.com/datasets/retailrocket/ecommerce-dataset",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "user events",
      "conversions",
      "Kaggle"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "user-events"
    ],
    "summary": "The Retail Rocket dataset consists of 2.76 million events, including views, carts, and purchases, collected from 1.4 million unique visitors. This dataset can be utilized to analyze consumer behavior, conversion rates, and the effectiveness of marketing strategies in the e-commerce sector.",
    "use_cases": [
      "Analyzing user conversion rates from views to purchases.",
      "Identifying patterns in consumer behavior across different events.",
      "Evaluating the impact of marketing campaigns on user engagement.",
      "Segmenting visitors based on their interaction with the e-commerce platform."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the user events in the Retail Rocket dataset?",
      "How can I analyze conversion rates using Retail Rocket data?",
      "What insights can be gained from 2.76M e-commerce events?",
      "How does visitor behavior change across different events in Retail Rocket?",
      "What are the key metrics derived from the Retail Rocket dataset?",
      "How can I visualize user interactions from the Retail Rocket data?",
      "What machine learning models can be applied to Retail Rocket events?",
      "How does the Retail Rocket dataset compare to other e-commerce datasets?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0299,
    "image_url": "/images/datasets/retail-rocket.png",
    "embedding_text": "The Retail Rocket dataset is a comprehensive collection of 2.76 million events, including views, carts, and purchases, sourced from 1.4 million unique visitors to an e-commerce platform. This dataset is structured in a tabular format, with each row representing an individual event and columns capturing key variables such as event type, timestamp, user ID, and product details. The collection methodology involves tracking user interactions on the platform, providing a rich source of data for understanding consumer behavior in the digital retail space. While the dataset does not specify temporal or geographic coverage, it offers valuable insights into user engagement and conversion patterns. Key variables include event type, which measures user interactions, and user ID, which allows for tracking individual behavior over time. Researchers can employ various preprocessing steps, such as data cleaning, normalization, and feature engineering, to prepare the dataset for analysis. The dataset supports a range of analytical approaches, including regression analysis, machine learning models, and descriptive statistics, enabling researchers to address questions related to consumer behavior, marketing effectiveness, and sales forecasting. Common research questions include understanding the factors that influence conversion rates, identifying trends in user engagement, and evaluating the success of promotional campaigns. Overall, the Retail Rocket dataset serves as a valuable resource for data scientists and researchers aiming to explore the dynamics of e-commerce and consumer interactions.",
    "tfidf_keywords": [
      "user-events",
      "conversion-rates",
      "consumer-behavior",
      "e-commerce",
      "data-analytics",
      "event-tracking",
      "marketing-strategies",
      "data-preprocessing",
      "machine-learning",
      "user-segmentation",
      "sales-forecasting",
      "data-visualization",
      "behavioral-analysis"
    ],
    "semantic_cluster": "e-commerce-analytics",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "data-analytics",
      "marketing-strategies",
      "user-experience",
      "sales-analysis"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "data-engineering",
      "machine-learning",
      "product-analytics",
      "experimentation"
    ]
  },
  {
    "name": "BestBuy",
    "description": "Mobile website clicks (~42k) for Xbox games from BestBuy",
    "category": "E-Commerce",
    "url": "https://www.kaggle.com/competitions/acm-sf-chapter-hackathon-big",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "clicks",
      "mobile",
      "gaming",
      "hackathon"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "gaming"
    ],
    "summary": "The BestBuy dataset contains approximately 42,000 mobile website clicks specifically for Xbox games. This dataset can be utilized to analyze consumer behavior in the gaming sector, particularly how mobile interactions influence purchasing decisions.",
    "use_cases": [
      "Analyzing consumer engagement with Xbox games",
      "Evaluating the effectiveness of mobile marketing strategies",
      "Understanding click behavior in e-commerce for gaming",
      "Comparing mobile clicks to sales conversions for Xbox games"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the click patterns for Xbox games on mobile devices?",
      "How do mobile clicks for Xbox games compare to other gaming platforms?",
      "What demographic factors influence mobile clicks for Xbox games?",
      "What time of day do users most frequently click on Xbox game listings?",
      "How does the click-through rate vary across different Xbox game titles?",
      "What trends can be observed in mobile clicks for Xbox games over time?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0232,
    "image_url": "/images/datasets/bestbuy.png",
    "embedding_text": "The BestBuy dataset comprises mobile website clicks for Xbox games, totaling around 42,000 interactions. This dataset is structured in a tabular format, with rows representing individual clicks and columns detailing various attributes such as timestamp, game title, and user engagement metrics. The collection methodology likely involves tracking user interactions on the BestBuy mobile website, capturing valuable data on consumer behavior in the gaming sector. The key variables in this dataset include the game title, click timestamp, and possibly user identifiers or session data, which measure how users interact with Xbox game listings on mobile devices. While the dataset provides insights into mobile engagement, it may have limitations regarding data quality, such as potential biases in user demographics or incomplete tracking of all interactions. Common preprocessing steps may include cleaning the data to remove duplicates, normalizing timestamps, and categorizing games for more straightforward analysis. Researchers can leverage this dataset to address various research questions, such as identifying peak engagement times, understanding user preferences for specific game titles, and evaluating the impact of mobile marketing campaigns on click-through rates. The dataset supports various types of analyses, including regression analysis to explore relationships between click behavior and game attributes, machine learning models for predicting user engagement, and descriptive statistics to summarize overall click trends. Typically, researchers utilize this dataset in studies focused on consumer behavior, e-commerce strategies, and the effectiveness of digital marketing in the gaming industry.",
    "tfidf_keywords": [
      "mobile-clicks",
      "consumer-engagement",
      "e-commerce",
      "gaming-behavior",
      "click-through-rate",
      "user-interaction",
      "digital-marketing",
      "mobile-strategies",
      "Xbox-games",
      "data-analysis"
    ],
    "semantic_cluster": "e-commerce-gaming-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "digital-marketing",
      "e-commerce-strategies",
      "user-engagement",
      "gaming-industry"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "experimentations",
      "data-engineering"
    ]
  },
  {
    "name": "LMSYS-Chat-1M",
    "description": "1M real-world conversations with 25 state-of-the-art LLMs spanning 154 languages",
    "category": "AI & LLM",
    "url": "https://huggingface.co/datasets/lmsys/lmsys-chat-1m",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "LLM",
      "conversations",
      "multilingual",
      "chatbot"
    ],
    "best_for": "Learning LLM evaluation, chatbot quality assessment, and dialogue systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "AI",
      "LLM",
      "multilingual",
      "chatbot"
    ],
    "summary": "The LMSYS-Chat-1M dataset consists of 1 million real-world conversations generated by 25 state-of-the-art large language models (LLMs) across 154 languages. This dataset can be utilized for training, evaluating, and fine-tuning chatbots and other conversational AI systems, providing insights into multilingual interactions and model performance.",
    "use_cases": [
      "Training chatbots for multilingual support",
      "Evaluating the performance of LLMs in diverse languages",
      "Analyzing conversational patterns across different cultures",
      "Improving natural language understanding in AI systems"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the LMSYS-Chat-1M dataset?",
      "How can I access the LMSYS-Chat-1M dataset?",
      "What types of conversations are included in the LMSYS-Chat-1M dataset?",
      "Which languages are covered in the LMSYS-Chat-1M dataset?",
      "What are the applications of the LMSYS-Chat-1M dataset?",
      "How many conversations are in the LMSYS-Chat-1M dataset?",
      "What models were used to generate the LMSYS-Chat-1M dataset?",
      "What can researchers learn from the LMSYS-Chat-1M dataset?"
    ],
    "domain_tags": [
      "technology",
      "AI"
    ],
    "data_modality": "text",
    "size_category": "medium",
    "model_score": 0.021,
    "image_url": "/images/datasets/lmsys-chat-1m.png",
    "embedding_text": "The LMSYS-Chat-1M dataset is a comprehensive collection of 1 million real-world conversations generated by 25 advanced large language models (LLMs), covering a remarkable 154 languages. This dataset is structured as a text corpus, where each entry represents a conversation between users and the LLMs, allowing researchers and developers to explore a wide range of conversational dynamics. The collection methodology involves leveraging state-of-the-art LLMs to simulate realistic interactions, ensuring that the conversations reflect diverse linguistic and cultural contexts. The key variables in this dataset include the conversation text, the language of the conversation, and metadata related to the LLM used for generation. While the dataset offers a rich resource for understanding multilingual interactions, it is important to note that the quality of the conversations may vary based on the capabilities of the underlying models. Common preprocessing steps may include tokenization, language detection, and filtering for quality assurance. Researchers can utilize this dataset to address various research questions, such as how different LLMs perform in generating contextually relevant responses across languages, or how cultural nuances affect conversational flow. The dataset supports a variety of analyses, including descriptive statistics, machine learning model training, and comparative studies of LLM performance. Typically, researchers use this dataset to enhance the capabilities of conversational agents, improve natural language understanding, and explore the implications of multilingual AI systems in real-world applications.",
    "tfidf_keywords": [
      "large language models",
      "conversational AI",
      "multilingual conversations",
      "chatbot training",
      "natural language understanding",
      "AI evaluation",
      "text corpus",
      "cultural context",
      "language diversity",
      "conversation dynamics"
    ],
    "semantic_cluster": "multilingual-conversational-ai",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "natural-language-processing",
      "machine-learning",
      "AI ethics",
      "conversational agents",
      "language modeling"
    ],
    "canonical_topics": [
      "natural-language-processing",
      "machine-learning",
      "consumer-behavior"
    ]
  },
  {
    "name": "Open E-Commerce 1.0 (MIT)",
    "description": "1.8M Amazon purchases with demographics (age, gender, location). Real household e-commerce behavior at scale",
    "category": "E-Commerce",
    "url": "https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/YGAVK9",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Amazon",
      "demographics",
      "purchases",
      "households",
      "MIT"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "demographics"
    ],
    "summary": "The Open E-Commerce 1.0 dataset provides insights into 1.8 million Amazon purchases, enriched with demographic information such as age, gender, and location. This dataset allows researchers to analyze real household e-commerce behavior at scale, facilitating studies on consumer purchasing patterns and demographic influences on buying decisions.",
    "use_cases": [
      "Analyzing the impact of demographic factors on purchasing behavior.",
      "Examining trends in e-commerce over time.",
      "Studying the relationship between location and purchasing patterns."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the demographics of Amazon purchasers?",
      "How does age affect e-commerce purchasing behavior?",
      "What trends can be observed in household e-commerce data?",
      "How do gender differences manifest in online shopping?",
      "What is the scale of Amazon purchases captured in this dataset?",
      "How can demographic data enhance e-commerce analysis?",
      "What insights can be drawn from analyzing 1.8M Amazon purchases?",
      "What variables are included in the Open E-Commerce 1.0 dataset?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0188,
    "image_url": "/images/logos/harvard.png",
    "embedding_text": "The Open E-Commerce 1.0 dataset is a comprehensive collection of 1.8 million Amazon purchases, enriched with demographic data that includes age, gender, and location of the purchasers. This dataset is structured in a tabular format, with rows representing individual purchases and columns containing variables such as purchase amount, product category, and demographic attributes. The collection methodology involves aggregating data from Amazon's transaction records, ensuring a robust representation of household e-commerce behavior at scale. While the dataset does not specify temporal or geographic coverage, it provides a rich source of information for analyzing consumer behavior across various demographics. Key variables within the dataset measure aspects such as purchase frequency, average spending, and demographic distributions, allowing researchers to explore how these factors influence e-commerce trends. However, researchers should be aware of potential limitations, such as data quality issues related to missing or incomplete demographic information. Common preprocessing steps may include data cleaning, normalization, and feature engineering to prepare the dataset for analysis. This dataset supports a variety of analytical approaches, including regression analysis, machine learning models, and descriptive statistics, making it a versatile tool for researchers investigating e-commerce dynamics. Typical research questions addressed using this dataset include the effects of demographic characteristics on purchasing decisions, the identification of consumer segments, and the exploration of trends in online shopping behavior. Researchers often leverage this dataset to gain insights into market dynamics, inform business strategies, and contribute to the academic understanding of consumer behavior in the digital age.",
    "tfidf_keywords": [
      "e-commerce",
      "demographics",
      "consumer-behavior",
      "purchase-patterns",
      "household-data",
      "Amazon-purchases",
      "data-analysis",
      "regression-models",
      "machine-learning",
      "data-preprocessing",
      "demographic-influence"
    ],
    "semantic_cluster": "e-commerce-consumer-behavior",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "consumer-behavior",
      "demographic-analysis",
      "market-research",
      "data-mining",
      "behavioral-economics"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "econometrics",
      "machine-learning"
    ]
  },
  {
    "name": "DiQAD",
    "description": "100K real-world user dialogues with comprehensive 6-dimension quality assessment",
    "category": "AI & LLM",
    "url": "https://github.com/yukunZhao/Dataset_Dialogue_quality_evaluation",
    "docs_url": null,
    "github_url": "https://github.com/yukunZhao/Dataset_Dialogue_quality_evaluation",
    "tags": [
      "dialogue",
      "quality assessment",
      "NLP"
    ],
    "best_for": "Learning LLM evaluation, chatbot quality assessment, and dialogue systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "DiQAD is a dataset containing 100,000 real-world user dialogues, each assessed across six dimensions of quality. This dataset can be utilized for training and evaluating natural language processing models, particularly in dialogue systems.",
    "use_cases": [
      "Training dialogue systems",
      "Evaluating NLP model performance",
      "Conducting quality assessments of dialogues"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the DiQAD dataset?",
      "How can I use DiQAD for NLP tasks?",
      "What are the dimensions of quality assessed in DiQAD?",
      "Where can I find user dialogue datasets?",
      "What types of analyses can be performed with DiQAD?",
      "How does DiQAD contribute to dialogue system research?"
    ],
    "domain_tags": [
      "AI",
      "NLP"
    ],
    "data_modality": "text",
    "size_category": "medium",
    "model_score": 0.0168,
    "image_url": "/images/datasets/diqad.png",
    "embedding_text": "The DiQAD dataset is a comprehensive collection of 100,000 real-world user dialogues, meticulously curated to facilitate research in natural language processing (NLP) and artificial intelligence (AI). Each dialogue in the dataset is evaluated across six distinct dimensions of quality, providing a robust framework for assessing the performance of dialogue systems. The data structure consists of a series of dialogues, each represented as a sequence of text exchanges between users, with accompanying quality assessments that measure various aspects such as coherence, relevance, and engagement. The collection methodology involves gathering dialogues from diverse sources, ensuring a rich representation of conversational patterns and styles. While the dataset does not specify temporal or geographic coverage, it is designed to reflect a wide range of user interactions, making it applicable to various research contexts. Key variables within the dataset include the dialogue text, user identifiers, and the six quality dimensions, which collectively enable researchers to explore questions related to user engagement, dialogue effectiveness, and the impact of quality on user satisfaction. Researchers typically preprocess the data by cleaning the text, normalizing dialogue formats, and potentially segmenting dialogues for more granular analysis. The dataset supports various types of analyses, including regression, machine learning, and descriptive statistics, allowing for a multifaceted exploration of dialogue quality and its implications in AI applications. Common research questions addressed using DiQAD include how different quality dimensions correlate with user satisfaction and how dialogue characteristics influence the effectiveness of conversational agents. Overall, DiQAD serves as a valuable resource for advancing the field of dialogue systems and enhancing the understanding of user interactions in AI-driven environments.",
    "tfidf_keywords": [
      "dialogue",
      "quality assessment",
      "NLP",
      "user interactions",
      "conversational AI",
      "text analysis",
      "machine learning",
      "data preprocessing",
      "user engagement",
      "dialogue effectiveness"
    ],
    "semantic_cluster": "nlp-for-economics",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "natural-language-processing",
      "dialogue-systems",
      "user-experience",
      "text-mining",
      "machine-learning"
    ],
    "canonical_topics": [
      "natural-language-processing",
      "machine-learning",
      "consumer-behavior"
    ]
  },
  {
    "name": "Instacart",
    "description": "3.4M orders, 206k+ users, 49k+ products with reorder behavior",
    "category": "Food & Delivery",
    "url": "https://www.kaggle.com/datasets/psparks/instacart-market-basket-analysis",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "grocery",
      "orders",
      "reorder prediction"
    ],
    "best_for": "Learning food & delivery analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Instacart dataset consists of 3.4 million orders placed by over 206,000 users, featuring more than 49,000 products. This dataset allows for analysis of reorder behavior, enabling researchers to explore consumer purchasing patterns and improve predictive models for grocery delivery services.",
    "use_cases": [
      "Predicting reorder behavior of grocery items",
      "Analyzing consumer purchasing trends over time",
      "Evaluating the effectiveness of promotional strategies",
      "Segmenting users based on purchasing habits"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the reorder behavior of users in the Instacart dataset?",
      "How many unique products are available in the Instacart dataset?",
      "What insights can be derived from analyzing 3.4 million orders?",
      "How can machine learning be applied to predict reorder behavior?",
      "What are the demographics of users in the Instacart dataset?",
      "How does product variety impact order frequency in grocery delivery?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0154,
    "image_url": "/images/datasets/instacart.png",
    "embedding_text": "The Instacart dataset is a comprehensive collection of grocery delivery orders, comprising 3.4 million transactions from over 206,000 users and featuring more than 49,000 distinct products. The data is structured in a tabular format, with rows representing individual orders and columns capturing various attributes such as user ID, product ID, order timestamp, and reorder behavior. This dataset is particularly valuable for researchers and practitioners interested in understanding consumer behavior in the context of e-commerce and grocery delivery services. The collection methodology involves aggregating real-world transaction data from Instacart's platform, ensuring a rich representation of user interactions with grocery products. Key variables include order frequency, product categories, and user demographics, which can be leveraged to analyze patterns in purchasing behavior. However, the dataset may have limitations related to data quality, such as missing values or biases in user representation, which should be addressed during preprocessing. Common preprocessing steps may include data cleaning, normalization, and feature engineering to prepare the data for analysis. Researchers can utilize this dataset to address various research questions, such as identifying factors that influence reorder behavior, evaluating the impact of promotions on sales, and segmenting users based on their shopping habits. The dataset supports a range of analytical techniques, including regression analysis, machine learning models, and descriptive statistics, making it a versatile resource for studies in consumer behavior and e-commerce optimization. Overall, the Instacart dataset serves as a valuable tool for exploring the dynamics of grocery shopping in a digital context, offering insights that can inform business strategies and enhance customer experiences.",
    "tfidf_keywords": [
      "reorder behavior",
      "grocery delivery",
      "consumer purchasing patterns",
      "e-commerce analysis",
      "transaction data",
      "user segmentation",
      "promotional effectiveness",
      "machine learning",
      "feature engineering",
      "data preprocessing"
    ],
    "semantic_cluster": "consumer-behavior-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "e-commerce",
      "data-analysis",
      "predictive-modeling",
      "user-segmentation"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "machine-learning",
      "pricing",
      "recommendation-systems"
    ]
  },
  {
    "name": "Athey's Course Datasets",
    "description": "Datasets related to causal inference and experimental design from Susan Athey",
    "category": "Education",
    "url": "https://github.com/itamarcaspi/experimentdatar",
    "docs_url": null,
    "github_url": "https://github.com/itamarcaspi/experimentdatar",
    "tags": [
      "causal inference",
      "experiments",
      "research"
    ],
    "best_for": "Learning education analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "causal inference",
      "experimental design",
      "research methodology"
    ],
    "summary": "Athey's Course Datasets provide a collection of datasets focused on causal inference and experimental design, primarily sourced from Susan Athey's academic work. Researchers can utilize these datasets to explore various experimental approaches and analyze causal relationships in different contexts.",
    "use_cases": [
      "Analyzing causal relationships in economic experiments",
      "Evaluating the effectiveness of different experimental designs",
      "Conducting regression analyses on experimental data"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What are Athey's Course Datasets?",
      "How can I use Athey's datasets for causal inference?",
      "What types of experiments are included in Athey's datasets?",
      "Where can I find datasets related to experimental design?",
      "What research questions can be addressed using Athey's datasets?",
      "How do I analyze data from Athey's Course Datasets?"
    ],
    "domain_tags": [
      "education",
      "research"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0154,
    "image_url": "/images/datasets/atheys-course-datasets.png",
    "embedding_text": "Athey's Course Datasets encompass a variety of datasets that are integral to the study of causal inference and experimental design, reflecting the methodologies developed by Susan Athey. These datasets are structured in a tabular format, typically consisting of rows representing individual observations and columns that capture various variables relevant to the experiments conducted. The collection methodology involves rigorous academic standards, ensuring that the datasets are suitable for research purposes. While specific temporal and geographic coverage details are not provided, the datasets are designed to support a wide range of experimental scenarios, making them versatile for researchers in economics and related fields. Key variables within these datasets often measure treatment effects, control variables, and outcomes of interest, allowing for comprehensive analyses. However, users should be aware of potential limitations regarding data quality, such as missing values or biases inherent in the experimental design. Common preprocessing steps may include data cleaning, normalization, and transformation to prepare the datasets for analysis. Researchers typically employ these datasets to address critical research questions related to causal inference, such as evaluating the impact of interventions or understanding consumer behavior in experimental settings. The datasets support various types of analyses, including regression models, machine learning techniques, and descriptive statistics, making them valuable resources for both academic and applied research. Overall, Athey's Course Datasets serve as a foundational tool for those looking to delve into the intricacies of causal inference and experimental design.",
    "tfidf_keywords": [
      "causal-inference",
      "experimental-design",
      "treatment-effects",
      "regression-analysis",
      "data-collection-methodology",
      "data-quality",
      "preprocessing-steps",
      "research-questions",
      "econometrics",
      "analysis-scenarios"
    ],
    "semantic_cluster": "causal-inference-methods",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "causal-inference",
      "experimental-design",
      "treatment-effects",
      "econometrics",
      "data-analysis"
    ],
    "canonical_topics": [
      "causal-inference",
      "experimentation",
      "econometrics"
    ]
  },
  {
    "name": "Amazon Sessions (KDD Cup 23)",
    "description": "Sessions from 6 locales with 40k-500k products per locale",
    "category": "E-Commerce",
    "url": "https://www.aicrowd.com/challenges/amazon-kdd-cup-23-multilingual-recommendation-challenge",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Amazon",
      "sessions",
      "multilingual",
      "KDD"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Amazon Sessions dataset from KDD Cup 23 contains session data from six locales, featuring between 40,000 to 500,000 products per locale. This dataset can be utilized to analyze consumer behavior, product interactions, and multilingual e-commerce dynamics.",
    "use_cases": [
      "Analyzing consumer behavior across different locales",
      "Studying the impact of product variety on session duration",
      "Exploring multilingual interactions in e-commerce",
      "Identifying trends in product engagement over time"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the session patterns in Amazon across different locales?",
      "How do product interactions vary in multilingual settings?",
      "What insights can be derived from consumer behavior in Amazon sessions?",
      "How many products are available in each locale in the Amazon Sessions dataset?",
      "What is the distribution of session lengths in the dataset?",
      "How can we analyze the impact of product availability on session behavior?",
      "What trends can be identified in consumer interactions with products?",
      "How does session data differ across the six locales?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.011,
    "image_url": "/images/datasets/amazon-sessions-kdd-cup-23.jpg",
    "embedding_text": "The Amazon Sessions dataset from KDD Cup 23 provides a rich source of session data collected from six different locales, each containing between 40,000 to 500,000 products. This dataset is structured in a tabular format, with rows representing individual sessions and columns capturing various attributes such as session ID, product ID, locale, and timestamps. The collection methodology involves aggregating session data from Amazon's e-commerce platform, ensuring a diverse representation of consumer interactions across different geographical and linguistic contexts. The key variables in this dataset include session length, product interactions, and user engagement metrics, which collectively measure how consumers navigate and interact with products on Amazon. While the dataset offers valuable insights, researchers should be aware of potential limitations, such as data sparsity in certain locales or variations in product availability. Common preprocessing steps may include cleaning session data, normalizing product IDs, and handling missing values. Researchers can leverage this dataset to address various research questions, such as understanding consumer behavior patterns, analyzing the effects of product diversity on session length, and exploring the dynamics of multilingual e-commerce interactions. The dataset supports a range of analyses, including regression modeling, machine learning applications, and descriptive statistics. Typically, researchers use this dataset to gain insights into consumer preferences, optimize product offerings, and enhance user experiences on e-commerce platforms.",
    "tfidf_keywords": [
      "session-data",
      "consumer-interaction",
      "multilingual-e-commerce",
      "product-engagement",
      "session-length",
      "locale-analysis",
      "user-behavior",
      "data-collection-methodology",
      "e-commerce-trends",
      "product-availability"
    ],
    "semantic_cluster": "e-commerce-consumer-behavior",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "e-commerce",
      "data-analysis",
      "multilingual-marketing",
      "product-analytics"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "marketplaces",
      "data-engineering"
    ]
  },
  {
    "name": "BLP US Car Data",
    "description": "Classic dataset (1971-1990) for demand model estimation",
    "category": "Automotive",
    "url": "https://pyblp.readthedocs.io/en/stable/_notebooks/tutorial/blp.html",
    "docs_url": "https://pyblp.readthedocs.io/en/stable/_notebooks/tutorial/blp.html",
    "github_url": null,
    "tags": [
      "demand estimation",
      "BLP",
      "research",
      "classic"
    ],
    "best_for": "Learning automotive analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The BLP US Car Data is a classic dataset spanning from 1971 to 1990, used primarily for demand model estimation in the automotive sector. Researchers can utilize this dataset to analyze consumer preferences and pricing strategies, making it a valuable resource for both academic and practical applications in economic research.",
    "use_cases": [
      "Estimating demand for different car models",
      "Analyzing the impact of pricing on consumer choice",
      "Evaluating market trends in the automotive industry"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What is the BLP US Car Data?",
      "How can I use the BLP dataset for demand estimation?",
      "What variables are included in the BLP US Car Data?",
      "What time period does the BLP US Car Data cover?",
      "How does the BLP US Car Data help in understanding consumer behavior?",
      "What methodologies can be applied to analyze the BLP US Car Data?",
      "What are the limitations of the BLP US Car Data?",
      "How can I preprocess the BLP US Car Data for analysis?"
    ],
    "domain_tags": [
      "automotive"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1971-1990",
    "size_category": "medium",
    "model_score": 0.0077,
    "image_url": "/images/logos/readthedocs.png",
    "embedding_text": "The BLP US Car Data is a seminal dataset that has been widely used in the field of econometrics, particularly for demand estimation in the automotive sector. This dataset encompasses a range of variables that capture consumer preferences and choices regarding car purchases during the years 1971 to 1990. It is structured in a tabular format, consisting of rows that represent individual observations and columns that include key variables such as car attributes, prices, and market shares. The collection methodology for this dataset involved aggregating data from various sources, including sales records and consumer surveys, which provides a comprehensive view of the automotive market during this period. Researchers often utilize this dataset to address critical research questions related to consumer behavior, such as how price changes affect demand and how different car features influence purchasing decisions. Common preprocessing steps include handling missing values, normalizing data, and transforming categorical variables into a suitable format for analysis. The dataset supports various types of analyses, including regression models, machine learning applications, and descriptive statistics, making it a versatile tool for both theoretical and applied research. However, users should be aware of potential limitations, such as the historical context of the data, which may not fully capture contemporary consumer preferences or market dynamics. Overall, the BLP US Car Data serves as a foundational resource for economists and data scientists interested in understanding demand patterns and pricing strategies in the automotive industry.",
    "geographic_scope": "US",
    "benchmark_usage": [
      "Demand estimation using BLP methodology"
    ],
    "tfidf_keywords": [
      "demand-estimation",
      "BLP-model",
      "consumer-preferences",
      "price-elasticity",
      "market-share",
      "car-attributes",
      "econometrics",
      "regression-analysis",
      "automotive-market",
      "historical-data"
    ],
    "semantic_cluster": "demand-estimation-methods",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "econometrics",
      "consumer-behavior",
      "pricing-strategy",
      "market-analysis",
      "regression-modeling"
    ],
    "canonical_topics": [
      "econometrics",
      "consumer-behavior",
      "pricing"
    ]
  },
  {
    "name": "Uber Movement",
    "description": "Zone-to-zone travel times and street speeds for 50+ cities worldwide. Congestion patterns from actual Uber rides",
    "category": "Transportation & Mobility",
    "url": "https://www.kaggle.com/datasets/ishandutta/uber-travel-movement-data-2-billion-trips",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Uber",
      "travel times",
      "congestion",
      "cities",
      "transportation"
    ],
    "best_for": "Learning transportation & mobility analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "transportation",
      "mobility",
      "urban planning"
    ],
    "summary": "Uber Movement provides zone-to-zone travel times and street speeds for over 50 cities worldwide, derived from actual Uber rides. This dataset can be utilized to analyze congestion patterns, improve urban mobility strategies, and inform transportation planning.",
    "use_cases": [
      "Analyzing traffic congestion trends",
      "Improving urban transportation planning",
      "Evaluating the impact of ride-sharing on traffic patterns"
    ],
    "audience": [
      "Curious-browser",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What are the travel times between different zones in major cities?",
      "How does congestion vary across different times of the day?",
      "What impact do Uber rides have on urban traffic patterns?",
      "How can Uber Movement data inform city planning?",
      "What are the average street speeds in various cities?",
      "How does traffic congestion differ between weekdays and weekends?"
    ],
    "domain_tags": [
      "transportation"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0077,
    "image_url": "/images/datasets/uber-movement.jpg",
    "embedding_text": "Uber Movement is a comprehensive dataset that captures zone-to-zone travel times and street speeds for more than 50 cities around the globe. The data is sourced from actual Uber rides, providing a unique perspective on urban mobility and congestion patterns. The dataset is structured in a tabular format, with rows representing individual trips and columns detailing key variables such as start and end zones, travel times, and street speeds. This structure allows for easy manipulation and analysis using common data analysis tools. The collection methodology involves aggregating data from Uber's ride-sharing service, ensuring that the information reflects real-world travel experiences. While the dataset covers a wide range of cities, it is important to note that the data quality may vary based on the volume of Uber rides in each area and the time of day. Researchers can leverage this dataset to address various research questions, such as understanding peak congestion times, evaluating the effectiveness of transportation policies, or analyzing the relationship between ride-sharing services and urban traffic patterns. Typical analyses supported by this dataset include regression analysis, machine learning models, and descriptive statistics. Common preprocessing steps may involve cleaning the data, handling missing values, and normalizing travel times for comparative analysis. Overall, Uber Movement serves as a valuable resource for urban planners, transportation analysts, and researchers interested in the dynamics of city mobility.",
    "tfidf_keywords": [
      "travel times",
      "street speeds",
      "congestion patterns",
      "urban mobility",
      "ride-sharing",
      "data aggregation",
      "traffic analysis",
      "urban planning",
      "data quality",
      "preprocessing"
    ],
    "semantic_cluster": "urban-mobility-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "urban planning",
      "transportation analysis",
      "congestion management",
      "ride-sharing impact",
      "data visualization"
    ],
    "canonical_topics": [
      "transportation",
      "consumer-behavior",
      "policy-evaluation"
    ],
    "geographic_scope": "global"
  },
  {
    "name": "IEEE-CIS Fraud Detection",
    "description": "590K card-not-present transactions with 393 features from Vesta Corp. Real messy fraud data (3.5% fraud rate)",
    "category": "Financial Services",
    "url": "https://www.kaggle.com/competitions/ieee-fraud-detection",
    "docs_url": null,
    "github_url": "https://github.com/amazon-science/fraud-dataset-benchmark",
    "tags": [
      "fraud detection",
      "transactions",
      "large-scale",
      "messy data",
      "fintech",
      "competition"
    ],
    "best_for": "Learning financial services analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis",
      "machine-learning"
    ],
    "topic_tags": [
      "e-commerce",
      "fraud detection",
      "financial services"
    ],
    "summary": "The IEEE-CIS Fraud Detection dataset contains 590,000 card-not-present transactions with 393 features, sourced from Vesta Corp. It provides a rich yet messy dataset for analyzing fraud detection in financial transactions, allowing researchers to develop and test various machine learning models.",
    "use_cases": [
      "Building predictive models for fraud detection",
      "Analyzing transaction patterns to identify fraudulent behavior",
      "Testing machine learning algorithms on real-world financial data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the IEEE-CIS Fraud Detection dataset?",
      "How can I use the IEEE-CIS dataset for fraud detection?",
      "What features are included in the IEEE-CIS Fraud Detection dataset?",
      "What is the fraud rate in the IEEE-CIS dataset?",
      "How to preprocess the IEEE-CIS Fraud Detection dataset?",
      "What machine learning techniques can be applied to the IEEE-CIS dataset?",
      "Where can I find the IEEE-CIS Fraud Detection dataset?",
      "What are the challenges in analyzing the IEEE-CIS dataset?"
    ],
    "domain_tags": [
      "fintech"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0077,
    "image_url": "/images/datasets/ieee-cis-fraud-detection.jpg",
    "embedding_text": "The IEEE-CIS Fraud Detection dataset is a comprehensive collection of 590,000 card-not-present transactions, featuring 393 variables that capture various aspects of each transaction. This dataset is sourced from Vesta Corp, a company specializing in fraud detection solutions. The data is characterized by a 3.5% fraud rate, making it a valuable resource for researchers and practitioners interested in developing and testing fraud detection algorithms. The structure of the dataset is tabular, with each row representing a transaction and each column corresponding to a specific feature, such as transaction amount, time of transaction, and user characteristics. Due to the nature of the data, it is considered messy, which poses challenges in data cleaning and preprocessing. Researchers typically need to address issues such as missing values, outliers, and feature scaling before applying machine learning techniques. Common preprocessing steps include normalization of numerical features, encoding of categorical variables, and handling of imbalanced classes. The dataset supports various types of analyses, including regression analysis, classification, and machine learning model training. Researchers can explore questions related to transaction behavior, identify patterns associated with fraudulent transactions, and evaluate the effectiveness of different fraud detection models. The dataset's rich feature set allows for in-depth analysis of consumer behavior and transaction dynamics, making it a crucial tool for advancing knowledge in the field of financial services and fraud detection.",
    "tfidf_keywords": [
      "card-not-present",
      "fraud detection",
      "machine learning",
      "feature engineering",
      "data preprocessing",
      "transaction analysis",
      "classification",
      "predictive modeling",
      "data quality",
      "feature selection"
    ],
    "semantic_cluster": "fraud-detection-methods",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "machine-learning",
      "data-preprocessing",
      "predictive-modeling",
      "classification",
      "feature-engineering"
    ],
    "canonical_topics": [
      "machine-learning",
      "finance",
      "consumer-behavior"
    ]
  },
  {
    "name": "Chicago TNC Trips",
    "description": "100M+ rideshare trips with fares (unlike NYC which lacks fare data). Trip-level pricing for Uber/Lyft economic analysis",
    "category": "Transportation & Mobility",
    "url": "https://data.cityofchicago.org/Transportation/Transportation-Network-Providers-Trips/m6dm-c72p",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "rideshare",
      "fares",
      "Uber",
      "Lyft",
      "Chicago",
      "pricing"
    ],
    "best_for": "Learning transportation & mobility analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "pricing",
      "consumer-behavior",
      "rideshare"
    ],
    "summary": "The Chicago TNC Trips dataset contains over 100 million rideshare trips, providing detailed trip-level fare data for Uber and Lyft in Chicago. This dataset enables economic analysis of rideshare pricing and consumer behavior in urban transportation.",
    "use_cases": [
      "Analyzing fare fluctuations during peak hours",
      "Comparing pricing strategies between Uber and Lyft",
      "Studying the impact of rideshare services on public transportation usage",
      "Evaluating consumer behavior trends in rideshare usage"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the fare structure for Uber and Lyft in Chicago?",
      "How do rideshare prices vary by time of day in Chicago?",
      "What are the trends in rideshare usage in Chicago over the years?",
      "How do Uber and Lyft fares compare in Chicago?",
      "What factors influence the pricing of rideshare trips in Chicago?",
      "How can I analyze the economic impact of rideshare services in urban areas?",
      "What are the common patterns in rideshare trip durations and distances?",
      "How does demand for rideshare services fluctuate during events in Chicago?"
    ],
    "domain_tags": [
      "transportation",
      "urban-planning"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Chicago",
    "size_category": "massive",
    "model_score": 0.0077,
    "image_url": "/images/logos/cityofchicago.png",
    "embedding_text": "The Chicago TNC Trips dataset is a comprehensive collection of over 100 million rideshare trips, specifically focusing on Uber and Lyft services in the city of Chicago. This dataset is structured in a tabular format, with each row representing a unique trip and various columns detailing trip attributes such as fare amounts, trip duration, distance, pickup and drop-off locations, and timestamps. The data is invaluable for researchers and analysts interested in understanding the dynamics of rideshare pricing and consumer behavior in urban settings. The collection methodology involves aggregating trip data from rideshare companies, ensuring a rich dataset that captures a wide range of trip scenarios and fare structures. While the dataset offers extensive coverage of rideshare activities in Chicago, it is important to note that data quality may vary, and researchers should be aware of potential limitations such as missing values or discrepancies in fare reporting. Common preprocessing steps may include cleaning the data, handling missing values, and transforming variables for analysis. Researchers can leverage this dataset to address various research questions, such as analyzing fare variations by time of day, understanding the impact of events on rideshare demand, and comparing pricing strategies between different rideshare providers. The dataset supports a range of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it a versatile resource for studies in transportation economics and consumer behavior. Overall, the Chicago TNC Trips dataset serves as a crucial tool for understanding the evolving landscape of urban mobility and the economic implications of rideshare services.",
    "benchmark_usage": [
      "Economic analysis of rideshare pricing",
      "Consumer behavior studies in urban transportation"
    ],
    "tfidf_keywords": [
      "rideshare",
      "fare analysis",
      "urban transportation",
      "consumer behavior",
      "pricing strategies",
      "trip duration",
      "demand fluctuations",
      "economic impact",
      "data preprocessing",
      "Chicago"
    ],
    "semantic_cluster": "rideshare-economics",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "urban-mobility",
      "transportation-economics",
      "consumer-behavior",
      "pricing-models",
      "data-analysis"
    ],
    "canonical_topics": [
      "econometrics",
      "pricing",
      "consumer-behavior",
      "transportation"
    ]
  },
  {
    "name": "OPTN Organ Transplant",
    "description": "Complete US organ donation records since 1987. Waiting lists, donor-recipient matches, outcomes. Market design and matching research",
    "category": "Healthcare",
    "url": "https://optn.transplant.hrsa.gov/data/",
    "docs_url": "https://optn.transplant.hrsa.gov/data/about-data/",
    "github_url": null,
    "tags": [
      "organ transplant",
      "matching",
      "market design",
      "healthcare",
      "waitlists"
    ],
    "best_for": "Understanding healthcare analytics, patient outcomes, and clinical predictions",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The OPTN Organ Transplant dataset contains comprehensive records of organ donations in the United States since 1987, including waiting lists, donor-recipient matches, and outcomes. This dataset can be utilized for market design and matching research, providing insights into the efficiency of organ allocation and the dynamics of transplant outcomes.",
    "use_cases": [
      "Analyzing the effectiveness of different matching algorithms for organ transplants.",
      "Studying the impact of socioeconomic factors on waiting times for organ transplants.",
      "Evaluating the outcomes of organ transplants based on donor-recipient characteristics."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the trends in organ donation rates since 1987?",
      "How do donor-recipient matches affect transplant outcomes?",
      "What factors influence waiting list times for organ transplants?",
      "How can market design improve organ allocation?",
      "What are the demographics of organ donors and recipients?",
      "What is the impact of policy changes on organ transplant outcomes?"
    ],
    "domain_tags": [
      "healthcare"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1987-present",
    "geographic_scope": "United States",
    "size_category": "medium",
    "model_score": 0.0077,
    "image_url": "/images/logos/hrsa.png",
    "embedding_text": "The OPTN Organ Transplant dataset is a rich source of information that encompasses complete records of organ donations in the United States since 1987. It includes various dimensions such as waiting lists, donor-recipient matches, and transplant outcomes, making it a valuable resource for researchers interested in healthcare economics and market design. The dataset is structured in a tabular format, with rows representing individual records of organ donations and columns capturing key variables such as donor demographics, recipient demographics, match characteristics, and outcomes. The collection methodology involves systematic data gathering from organ procurement organizations and transplant centers across the country, ensuring comprehensive coverage of the organ transplant landscape. Key variables in the dataset include donor age, organ type, recipient wait time, and transplant success rates, which allow for nuanced analysis of the factors influencing organ donation and transplantation. However, researchers should be aware of potential limitations in data quality, such as missing values or variations in reporting standards across different states. Common preprocessing steps may include data cleaning, normalization, and handling of missing data to prepare the dataset for analysis. The dataset supports a variety of research questions, including the evaluation of matching algorithms, the impact of demographic factors on transplant success, and the effectiveness of policy interventions in improving organ donation rates. Researchers typically employ statistical methods such as regression analysis, machine learning techniques, and descriptive statistics to derive insights from the data. Overall, the OPTN Organ Transplant dataset serves as a foundational resource for advancing knowledge in organ allocation strategies and improving patient outcomes in the field of transplantation.",
    "tfidf_keywords": [
      "organ donation",
      "transplant outcomes",
      "waiting lists",
      "market design",
      "donor-recipient matches",
      "socioeconomic factors",
      "transplant success rates",
      "data quality",
      "matching algorithms",
      "policy interventions",
      "healthcare economics"
    ],
    "semantic_cluster": "healthcare-market-design",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "causal-inference",
      "market-design",
      "healthcare-policy",
      "data-analysis",
      "transplantation-research"
    ],
    "canonical_topics": [
      "healthcare",
      "policy-evaluation",
      "causal-inference"
    ]
  },
  {
    "name": "CMS Hospital Price Transparency",
    "description": "Hospital pricing data mandated since 2021. Negotiated rates, chargemaster prices across 6,000+ hospitals. Healthcare pricing research",
    "category": "Healthcare",
    "url": "https://www.cms.gov/hospital-price-transparency",
    "docs_url": "https://www.cms.gov/hospital-price-transparency/resources",
    "github_url": null,
    "tags": [
      "healthcare",
      "hospital pricing",
      "transparency",
      "CMS",
      "insurance"
    ],
    "best_for": "Understanding healthcare analytics, patient outcomes, and clinical predictions",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "healthcare",
      "pricing",
      "transparency"
    ],
    "summary": "The CMS Hospital Price Transparency dataset contains hospital pricing data mandated since 2021, including negotiated rates and chargemaster prices from over 6,000 hospitals. Researchers can use this dataset to analyze healthcare pricing trends, compare costs across hospitals, and evaluate the impact of transparency on healthcare costs.",
    "use_cases": [
      "Comparative analysis of hospital pricing across different regions",
      "Evaluating the effectiveness of price transparency policies on healthcare costs",
      "Investigating the relationship between hospital pricing and patient outcomes",
      "Analyzing trends in negotiated rates over time"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the negotiated rates for hospitals in the CMS dataset?",
      "How do chargemaster prices vary across different hospitals?",
      "What insights can be drawn from the CMS Hospital Price Transparency data?",
      "How has hospital pricing changed since the implementation of price transparency?",
      "What are the implications of hospital pricing transparency for insurance companies?",
      "How can I access the CMS Hospital Price Transparency dataset?",
      "What variables are included in the CMS Hospital Price Transparency dataset?",
      "What research has been conducted using the CMS Hospital Price Transparency data?"
    ],
    "domain_tags": [
      "healthcare"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2021",
    "size_category": "medium",
    "model_score": 0.0077,
    "image_url": "/images/logos/cms.png",
    "embedding_text": "The CMS Hospital Price Transparency dataset provides a comprehensive view of hospital pricing data that has been mandated since 2021. This dataset includes negotiated rates and chargemaster prices from over 6,000 hospitals across the United States, offering a unique opportunity for researchers and analysts to delve into the complexities of healthcare pricing. The data is structured in a tabular format, with rows representing individual hospitals and columns detailing various pricing metrics. Key variables include the negotiated rates for specific medical services, chargemaster prices, and potentially other relevant pricing information. The collection methodology for this dataset is based on regulatory requirements set forth by the Centers for Medicare & Medicaid Services (CMS), ensuring that the data is both reliable and standardized across participating hospitals. However, researchers should be aware of potential limitations in data quality, such as discrepancies in how different hospitals report their prices or variations in service definitions. Common preprocessing steps may involve cleaning the data to handle missing values, normalizing prices for comparison, and aggregating data for regional analyses. The dataset supports a variety of analytical approaches, including regression analysis, machine learning techniques, and descriptive statistics. Researchers typically utilize this dataset to address critical questions regarding healthcare pricing dynamics, the impact of transparency on consumer behavior, and the overall efficiency of healthcare delivery systems. By leveraging this dataset, analysts can uncover insights that inform policy decisions and contribute to a better understanding of the healthcare market.",
    "tfidf_keywords": [
      "hospital pricing",
      "negotiated rates",
      "chargemaster prices",
      "price transparency",
      "healthcare costs",
      "CMS",
      "insurance",
      "data analysis",
      "healthcare policy",
      "cost comparison",
      "pricing trends",
      "healthcare research",
      "data collection",
      "healthcare economics"
    ],
    "semantic_cluster": "healthcare-pricing-transparency",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "healthcare economics",
      "policy evaluation",
      "consumer behavior",
      "data analysis",
      "pricing strategies"
    ],
    "canonical_topics": [
      "pricing",
      "healthcare",
      "policy-evaluation"
    ]
  },
  {
    "name": "Medicare Provider Utilization",
    "description": "All Medicare providers with service utilization and payment data. CMS public use files for healthcare analytics",
    "category": "Healthcare",
    "url": "https://data.cms.gov/provider-summary-by-type-of-service",
    "docs_url": "https://data.cms.gov/",
    "github_url": null,
    "tags": [
      "Medicare",
      "healthcare",
      "providers",
      "payments",
      "CMS"
    ],
    "best_for": "Understanding healthcare analytics, patient outcomes, and clinical predictions",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Medicare Provider Utilization dataset includes comprehensive information on all Medicare providers, detailing their service utilization and payment data. This dataset can be utilized for various healthcare analytics, enabling researchers and analysts to explore provider performance, payment trends, and service delivery patterns.",
    "use_cases": [
      "Analyzing trends in Medicare provider payments over time.",
      "Evaluating the relationship between provider characteristics and service utilization.",
      "Comparing service delivery patterns across different regions or provider types."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Medicare Provider Utilization dataset?",
      "How can I analyze Medicare provider payment data?",
      "What insights can be gained from Medicare service utilization data?",
      "Where can I find CMS public use files for healthcare analytics?",
      "What are the trends in Medicare provider payments?",
      "How do Medicare providers vary in service utilization?"
    ],
    "domain_tags": [
      "healthcare"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0077,
    "image_url": "/images/logos/cms.png",
    "embedding_text": "The Medicare Provider Utilization dataset is a rich resource for healthcare analytics, encompassing all Medicare providers along with their service utilization and payment data. This dataset is derived from the Centers for Medicare & Medicaid Services (CMS) public use files, which are designed to facilitate healthcare research and policy evaluation. The data structure typically includes rows representing individual providers and columns detailing various attributes such as provider ID, specialty, service types, utilization metrics, and payment amounts. Researchers can leverage this dataset to address critical research questions related to healthcare delivery, provider performance, and payment trends. Key variables within the dataset measure aspects such as the volume of services provided, the total payments received, and the types of services rendered, enabling a comprehensive analysis of provider behavior and patient care patterns. However, users should be aware of potential limitations in data quality, such as missing values or inaccuracies in provider reporting. Common preprocessing steps may include data cleaning, normalization, and aggregation to prepare the dataset for analysis. The dataset supports a variety of analytical approaches, including regression analysis, machine learning models, and descriptive statistics, making it a versatile tool for healthcare researchers. Typical research applications include evaluating the impact of policy changes on provider behavior, analyzing disparities in service utilization, and assessing the effectiveness of different payment models. Overall, the Medicare Provider Utilization dataset serves as a foundational resource for understanding the dynamics of Medicare services and provider interactions within the healthcare system.",
    "tfidf_keywords": [
      "Medicare",
      "provider utilization",
      "payment data",
      "healthcare analytics",
      "service delivery",
      "CMS",
      "provider performance",
      "utilization metrics",
      "policy evaluation",
      "healthcare research"
    ],
    "semantic_cluster": "healthcare-data-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "healthcare policy",
      "provider performance evaluation",
      "service utilization analysis",
      "payment models",
      "healthcare disparities"
    ],
    "canonical_topics": [
      "healthcare",
      "policy-evaluation",
      "econometrics"
    ]
  },
  {
    "name": "Brazilian eCommerce",
    "description": "100,000 orders (2016-2018) structured in 9 relational tables from Olist",
    "category": "E-Commerce",
    "url": "https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "orders",
      "Brazil",
      "relational data",
      "Kaggle"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Brazilian eCommerce dataset consists of 100,000 orders structured in 9 relational tables from Olist, covering the years 2016 to 2018. This dataset allows researchers to analyze consumer purchasing behavior, pricing strategies, and market trends within the Brazilian e-commerce landscape.",
    "use_cases": [
      "Analyzing consumer purchasing patterns over time",
      "Evaluating the impact of pricing strategies on sales",
      "Investigating the relationship between order characteristics and customer demographics"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What insights can be gained from the Brazilian eCommerce dataset?",
      "How does consumer behavior vary in Brazilian e-commerce?",
      "What are the pricing trends in Olist's eCommerce data?",
      "How can relational data be used to analyze e-commerce orders?",
      "What factors influence order volume in Brazilian e-commerce?",
      "How can this dataset be used for regression analysis in e-commerce?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2016-2018",
    "geographic_scope": "Brazil",
    "size_category": "medium",
    "model_score": 0.0052,
    "image_url": "/images/datasets/brazilian-ecommerce.png",
    "embedding_text": "The Brazilian eCommerce dataset is a comprehensive collection of 100,000 orders from Olist, structured across 9 relational tables. This dataset spans the years 2016 to 2018, providing a rich source of information for researchers interested in the dynamics of e-commerce in Brazil. The data structure includes various tables that capture different aspects of the orders, such as order details, customer information, product specifications, and shipping logistics. Each table contains multiple rows corresponding to individual transactions, with columns that represent key variables such as order ID, product category, customer location, payment method, and delivery time. The collection methodology involved aggregating data from Olist's platform, which serves as a marketplace for various sellers, thus ensuring a diverse range of products and consumer interactions. The dataset's geographic coverage is specifically focused on Brazil, allowing for targeted analysis of local market trends and consumer behavior. Key variables within the dataset measure aspects such as order volume, customer demographics, product pricing, and delivery performance, which can be instrumental in addressing research questions related to consumer preferences and market efficiency. However, researchers should be aware of potential data quality issues, such as missing values or inconsistencies in product categorization, which may require preprocessing steps like data cleaning and normalization. Common analyses that can be conducted using this dataset include regression modeling to identify factors influencing sales, machine learning techniques for predictive analytics, and descriptive statistics to summarize trends over time. Researchers typically leverage this dataset to explore questions around consumer behavior, pricing strategies, and the overall performance of e-commerce platforms in Brazil, making it a valuable resource for both academic and industry-focused studies.",
    "tfidf_keywords": [
      "e-commerce",
      "consumer behavior",
      "pricing strategies",
      "order characteristics",
      "Brazilian market",
      "relational data",
      "data analysis",
      "market trends",
      "transaction data",
      "Olist"
    ],
    "semantic_cluster": "e-commerce-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "marketplaces",
      "data-analysis",
      "pricing",
      "retail"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "marketplaces",
      "pricing"
    ]
  },
  {
    "name": "Open CDP",
    "description": "Omnichannel interaction tracking with AI-driven identity resolution",
    "category": "E-Commerce",
    "url": "https://rees46.com/en/datasets",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "omnichannel",
      "customer data",
      "identity resolution"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Open CDP dataset provides insights into omnichannel interactions by utilizing AI-driven identity resolution techniques. Researchers can leverage this data to analyze customer behavior across various touchpoints, enhancing their understanding of consumer preferences and improving marketing strategies.",
    "use_cases": [
      "Analyzing customer journey across multiple channels",
      "Improving targeted marketing strategies based on customer interactions",
      "Evaluating the effectiveness of omnichannel campaigns",
      "Understanding consumer behavior patterns in e-commerce"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Open CDP dataset?",
      "How does AI-driven identity resolution work in omnichannel tracking?",
      "What insights can be gained from customer data in e-commerce?",
      "How can omnichannel interactions improve customer experience?",
      "What are the applications of identity resolution in marketing?",
      "How to analyze customer behavior using Open CDP data?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0052,
    "image_url": "/images/datasets/open-cdp.jpg",
    "embedding_text": "The Open CDP dataset is designed to facilitate the tracking of omnichannel interactions, leveraging advanced AI-driven identity resolution techniques. This dataset is structured in a tabular format, comprising rows that represent individual customer interactions and columns that detail various attributes such as customer ID, interaction type, timestamp, and channel used. The collection methodology involves aggregating data from multiple sources, including web interactions, mobile app usage, and in-store transactions, providing a comprehensive view of customer behavior across different platforms. While the dataset does not specify temporal or geographic coverage, it is intended for use in e-commerce settings, making it particularly relevant for retail analysis. Key variables within the dataset measure aspects of customer engagement, such as frequency of interactions, types of channels utilized, and conversion rates. Researchers should be aware of potential data quality issues, including incomplete records or discrepancies in customer identification due to privacy concerns. Common preprocessing steps may include data cleaning, normalization, and the integration of disparate data sources to ensure a cohesive dataset for analysis. This dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics, enabling researchers to address critical questions regarding customer behavior, marketing effectiveness, and overall business performance. Typically, researchers utilize the Open CDP dataset to conduct studies that aim to enhance customer experience, optimize marketing strategies, and ultimately drive sales growth in the competitive e-commerce landscape.",
    "tfidf_keywords": [
      "omnichannel",
      "identity resolution",
      "customer engagement",
      "AI-driven",
      "e-commerce",
      "interaction tracking",
      "consumer behavior",
      "data integration",
      "marketing optimization",
      "customer journey",
      "data quality",
      "preprocessing",
      "conversion rates",
      "retail analytics"
    ],
    "semantic_cluster": "customer-engagement-analytics",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "marketing-strategy",
      "data-integration",
      "customer-journey",
      "analytics"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "machine-learning",
      "data-engineering",
      "product-analytics"
    ]
  },
  {
    "name": "Alibaba Ads (IJCAI-18)",
    "description": "6 billion display ad/click logs over 8 days from 100M users",
    "category": "E-Commerce",
    "url": "https://tianchi.aliyun.com/dataset/147588",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "advertising",
      "clicks",
      "large-scale",
      "Alibaba"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "advertising"
    ],
    "summary": "The Alibaba Ads dataset consists of 6 billion display ad and click logs collected over 8 days from 100 million users. This dataset can be utilized to analyze user behavior in response to advertisements, evaluate the effectiveness of advertising strategies, and develop predictive models for ad clicks.",
    "use_cases": [
      "Analyzing user engagement with ads",
      "Evaluating the effectiveness of advertising campaigns",
      "Developing predictive models for ad clicks"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the patterns in user clicks on Alibaba ads?",
      "How do different advertising strategies impact click rates?",
      "What demographic factors influence ad engagement?",
      "Can we predict future ad clicks based on historical data?",
      "What is the distribution of clicks across different ad types?",
      "How does user behavior vary over the 8-day period?",
      "What insights can be drawn from the large-scale ad logs?"
    ],
    "domain_tags": [
      "e-commerce"
    ],
    "data_modality": "tabular",
    "size_category": "massive",
    "model_score": 0.0052,
    "embedding_text": "The Alibaba Ads dataset is a comprehensive collection of 6 billion display ad and click logs, meticulously gathered over a span of 8 days from a diverse user base of 100 million individuals. This dataset is structured in a tabular format, where each row represents an individual ad impression or click event, and the columns include key variables such as user ID, ad ID, timestamp, and click status. The sheer volume of data allows for extensive analysis of user interactions with advertisements, making it a valuable resource for researchers and practitioners in the fields of e-commerce and advertising. The collection methodology involved tracking user behavior on the Alibaba platform, capturing detailed logs of ad impressions and clicks, which provides insights into consumer engagement and preferences. Key variables within the dataset measure aspects such as the frequency of ad exposure, click-through rates, and user demographics, although specific demographic details are not explicitly provided. Researchers may encounter challenges related to data quality, including potential biases in user engagement and the need for preprocessing steps such as data cleaning and normalization to prepare the dataset for analysis. Common research questions that can be addressed using this dataset include examining the effectiveness of different advertising strategies, understanding user behavior patterns, and predicting future ad clicks based on historical trends. The dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics, enabling researchers to derive meaningful insights from the data. Typically, researchers leverage this dataset to study the dynamics of online advertising, assess the impact of ad placements, and develop strategies for optimizing ad performance in the competitive e-commerce landscape.",
    "tfidf_keywords": [
      "click-through-rate",
      "ad impressions",
      "user engagement",
      "advertising strategies",
      "predictive modeling",
      "consumer preferences",
      "data normalization",
      "e-commerce analytics",
      "large-scale data",
      "behavioral analysis"
    ],
    "semantic_cluster": "advertising-analytics",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "consumer-behavior",
      "advertising-effectiveness",
      "predictive-analytics",
      "data-mining",
      "user-experience"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "advertising",
      "machine-learning",
      "data-engineering"
    ]
  },
  {
    "name": "Coveo Shopping (SIGIR-21)",
    "description": "30M+ browsing events with query and image vectors for e-commerce search",
    "category": "E-Commerce",
    "url": "https://github.com/coveooss/SIGIR-ecom-data-challenge",
    "docs_url": null,
    "github_url": "https://github.com/coveooss/SIGIR-ecom-data-challenge",
    "tags": [
      "browsing",
      "search",
      "embeddings",
      "SIGIR"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Coveo Shopping dataset comprises over 30 million browsing events, enriched with query and image vectors specifically designed for e-commerce search applications. Researchers can leverage this dataset to analyze consumer behavior, improve search algorithms, and enhance recommendation systems.",
    "use_cases": [
      "Analyzing consumer behavior trends",
      "Improving search algorithms for e-commerce",
      "Enhancing recommendation systems",
      "Evaluating the effectiveness of search queries"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What are the browsing events in the Coveo Shopping dataset?",
      "How can I analyze consumer behavior using Coveo Shopping data?",
      "What types of queries are included in the Coveo Shopping dataset?",
      "What image vectors are available in the Coveo Shopping dataset?",
      "How can I improve e-commerce search algorithms with this dataset?",
      "What insights can be gained from the Coveo Shopping dataset?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "mixed",
    "size_category": "massive",
    "model_score": 0.0051,
    "image_url": "/images/datasets/coveo-shopping-sigir-21.png",
    "embedding_text": "The Coveo Shopping dataset offers a rich collection of over 30 million browsing events, providing a unique opportunity for researchers and practitioners in the field of e-commerce. This dataset includes a variety of data structures, primarily consisting of rows representing individual browsing events and columns that capture key variables such as user queries, product images, and associated embeddings. The data is structured to facilitate analysis of consumer interactions with e-commerce platforms, allowing for insights into user behavior and preferences. The collection methodology involves tracking user interactions on e-commerce sites, capturing data points that reflect search queries and the corresponding product images viewed by users. This data is invaluable for understanding how users engage with search functionalities and how visual elements influence their browsing experience.\n\nWhile the dataset does not explicitly mention temporal or geographic coverage, it is designed to support a wide range of analyses, including regression, machine learning, and descriptive statistics. Key variables in the dataset measure aspects such as search query frequency, image engagement rates, and user navigation paths. Researchers can utilize this dataset to address various research questions, such as identifying patterns in consumer search behavior, evaluating the effectiveness of different search algorithms, and exploring the impact of visual content on user decisions.\n\nData quality is a critical consideration, and known limitations may include potential biases in user behavior and the representativeness of the browsing events captured. Common preprocessing steps may involve cleaning the data, normalizing query formats, and transforming image vectors into usable formats for analysis. The dataset supports a variety of analytical approaches, enabling researchers to conduct exploratory data analysis, build predictive models, and derive actionable insights for e-commerce strategies. Overall, the Coveo Shopping dataset serves as a powerful resource for advancing research in e-commerce search optimization and consumer behavior analysis.",
    "tfidf_keywords": [
      "browsing-events",
      "query-vectors",
      "image-vectors",
      "e-commerce-search",
      "consumer-behavior",
      "search-algorithms",
      "recommendation-systems",
      "data-collection-methodology",
      "user-engagement",
      "data-preprocessing"
    ],
    "semantic_cluster": "e-commerce-search-analytics",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "consumer-behavior",
      "search-engine-optimization",
      "recommendation-systems",
      "data-mining",
      "user-experience"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "recommendation-systems",
      "machine-learning",
      "data-engineering"
    ]
  },
  {
    "name": "Google Merchandise",
    "description": "3 months obfuscated GA4 e-commerce data (Nov 2020-Jan 2021)",
    "category": "E-Commerce",
    "url": "https://www.kaggle.com/datasets/bigquery/google-analytics-sample",
    "docs_url": "https://support.google.com/analytics/answer/7586738",
    "github_url": null,
    "tags": [
      "Google Analytics",
      "GA4",
      "web analytics"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "web analytics"
    ],
    "summary": "The Google Merchandise dataset contains three months of obfuscated Google Analytics 4 e-commerce data, specifically from November 2020 to January 2021. This dataset can be utilized to analyze consumer behavior, track sales performance, and improve marketing strategies through web analytics insights.",
    "use_cases": [
      "Analyzing sales trends over the holiday season",
      "Evaluating the effectiveness of marketing campaigns",
      "Understanding customer journey through the website",
      "Identifying high-performing product categories"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What insights can be drawn from Google Analytics 4 e-commerce data?",
      "How does consumer behavior change during holiday seasons?",
      "What are the key metrics to evaluate e-commerce performance?",
      "How can web analytics inform marketing decisions?",
      "What trends can be identified in online shopping behavior?",
      "How does traffic source impact sales in e-commerce?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "Nov 2020 - Jan 2021",
    "size_category": "medium",
    "model_score": 0.0051,
    "image_url": "/images/datasets/google-merchandise.jpeg",
    "embedding_text": "The Google Merchandise dataset provides a rich source of e-commerce data captured through Google Analytics 4 over a three-month period from November 2020 to January 2021. This dataset is structured in a tabular format, containing various rows and columns that represent different metrics and dimensions related to online shopping behavior. Key variables may include user sessions, page views, conversion rates, and sales figures, which are essential for understanding consumer interactions with the website. The data collection methodology involves tracking user activity on the Google Merchandise Store, allowing for the analysis of user behavior and sales performance during a critical shopping season that includes Black Friday and the holiday period. Researchers can leverage this dataset to explore various research questions, such as the impact of marketing strategies on sales, seasonal trends in consumer behavior, and the effectiveness of different traffic sources. Common preprocessing steps may include handling missing values, normalizing data, and aggregating metrics for analysis. The dataset supports a range of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile tool for data scientists and researchers interested in e-commerce and consumer behavior. However, it is important to note that the data is obfuscated, which may limit the granularity of insights that can be derived. Overall, this dataset serves as a valuable resource for those looking to enhance their understanding of e-commerce dynamics and improve business strategies based on data-driven insights.",
    "tfidf_keywords": [
      "Google Analytics",
      "GA4",
      "e-commerce",
      "consumer behavior",
      "web analytics",
      "conversion rate",
      "user sessions",
      "sales performance",
      "marketing strategies",
      "data preprocessing"
    ],
    "semantic_cluster": "e-commerce-analytics",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "web-traffic-analysis",
      "marketing-analytics",
      "sales-optimization",
      "data-visualization"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "product-analytics",
      "data-engineering",
      "statistics"
    ]
  },
  {
    "name": "Shopee",
    "description": "Dataset from Shopee's 2020 Code League competition",
    "category": "E-Commerce",
    "url": "https://www.kaggle.com/c/shopee-code-league-2021",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Shopee",
      "competition",
      "Southeast Asia"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Shopee dataset from the 2020 Code League competition provides insights into e-commerce dynamics in Southeast Asia. Researchers can utilize this dataset to analyze consumer behavior, pricing strategies, and competition within the e-commerce sector.",
    "use_cases": [
      "Analyzing consumer purchasing patterns",
      "Evaluating pricing strategies in e-commerce",
      "Comparing competition among e-commerce platforms",
      "Identifying trends in Southeast Asian online shopping"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What insights can be gained from the Shopee 2020 Code League dataset?",
      "How does consumer behavior vary in Southeast Asia's e-commerce?",
      "What pricing strategies are evident in the Shopee dataset?",
      "How can the Shopee dataset be used to analyze competition in e-commerce?",
      "What are the key variables in the Shopee dataset?",
      "How can data from Shopee's competition be utilized for machine learning?",
      "What trends can be identified in e-commerce from the Shopee dataset?",
      "How does the Shopee dataset contribute to understanding market dynamics?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2020",
    "geographic_scope": "Southeast Asia",
    "size_category": "medium",
    "model_score": 0.0051,
    "image_url": "/images/datasets/shopee.jpg",
    "embedding_text": "The Shopee dataset from the 2020 Code League competition is a rich resource for understanding the e-commerce landscape in Southeast Asia. It comprises various data points structured in a tabular format, which includes rows representing individual transactions or user interactions and columns detailing variables such as product categories, pricing, user demographics, and timestamps. This dataset was collected during the competition, where participants were tasked with developing algorithms to optimize various aspects of the Shopee platform. The data is particularly valuable for researchers interested in consumer behavior, as it allows for the exploration of purchasing patterns and preferences among Southeast Asian consumers. Key variables in the dataset may include product IDs, user IDs, transaction amounts, and timestamps, which can be leveraged to conduct analyses ranging from descriptive statistics to more complex machine learning models. However, researchers should be aware of potential limitations in data quality, such as missing values or biases in user representation. Common preprocessing steps may involve cleaning the data, handling missing values, and normalizing transaction amounts for comparative analysis. The dataset supports a variety of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile tool for both academic research and practical applications in the e-commerce sector. Researchers typically use this dataset to address questions related to market trends, pricing strategies, and consumer behavior, contributing to a deeper understanding of the dynamics within the e-commerce industry in Southeast Asia.",
    "tfidf_keywords": [
      "e-commerce",
      "consumer behavior",
      "pricing strategies",
      "Southeast Asia",
      "transaction analysis",
      "market dynamics",
      "user demographics",
      "data preprocessing",
      "machine learning",
      "competition analysis"
    ],
    "semantic_cluster": "e-commerce-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "consumer-behavior",
      "pricing",
      "marketplaces",
      "data-preprocessing",
      "machine-learning"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "pricing",
      "marketplaces",
      "machine-learning",
      "econometrics"
    ]
  },
  {
    "name": "Flipkart",
    "description": "Sales dataset from Indian e-commerce platform Flipkart",
    "category": "E-Commerce",
    "url": "https://www.kaggle.com/datasets/iyumrahul/flipkartsalesdataset",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "India",
      "sales",
      "Kaggle"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "sales"
    ],
    "summary": "The Flipkart dataset provides insights into sales transactions from the Indian e-commerce platform Flipkart. Researchers can analyze consumer purchasing patterns, sales trends, and the impact of various factors on sales performance.",
    "use_cases": [
      "Analyzing sales trends over time",
      "Understanding consumer purchasing behavior",
      "Evaluating the impact of promotions on sales",
      "Segmenting customers based on buying patterns"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the sales trends on Flipkart in India?",
      "How do consumer behaviors vary by product category?",
      "What factors influence sales on Flipkart?",
      "Can we predict future sales based on historical data?",
      "How does seasonality affect Flipkart sales?",
      "What are the most popular products on Flipkart?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "India",
    "size_category": "medium",
    "model_score": 0.0037,
    "image_url": "/images/datasets/flipkart.png",
    "embedding_text": "The Flipkart dataset is a comprehensive collection of sales data from one of India's leading e-commerce platforms, Flipkart. This dataset typically consists of rows representing individual sales transactions, with columns detailing various attributes such as product ID, category, price, quantity sold, and timestamps of purchases. The data structure allows for a rich analysis of consumer behavior and sales dynamics within the Indian e-commerce landscape. The collection methodology likely involves aggregating transactional data from Flipkart's operational databases, ensuring a robust representation of sales activities. Coverage is primarily focused on the Indian market, reflecting the demographic and economic characteristics of Indian consumers. Key variables in the dataset include product categories, sales volume, and pricing, which are essential for measuring performance and understanding market trends. However, researchers should be aware of potential limitations in data quality, such as missing values or inconsistencies in product categorization. Common preprocessing steps may involve cleaning the data, handling missing values, and transforming categorical variables into numerical formats for analysis. This dataset supports a variety of research questions, including the analysis of sales trends over time, the impact of marketing strategies on consumer purchases, and the identification of seasonal patterns in buying behavior. Researchers can employ various analytical techniques such as regression analysis, machine learning models, and descriptive statistics to extract insights from the data. The Flipkart dataset serves as a valuable resource for studies in e-commerce, consumer behavior, and retail analytics, enabling researchers to draw meaningful conclusions about market dynamics and consumer preferences.",
    "tfidf_keywords": [
      "e-commerce",
      "sales-analysis",
      "consumer-behavior",
      "transaction-data",
      "market-trends",
      "product-categorization",
      "price-sensitivity",
      "seasonality",
      "promotional-impact",
      "data-preprocessing"
    ],
    "semantic_cluster": "e-commerce-analytics",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "sales-forecasting",
      "market-analysis",
      "data-visualization",
      "retail-strategy"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "marketplaces",
      "data-engineering",
      "econometrics"
    ]
  },
  {
    "name": "Pakistan e-commerce",
    "description": "500k+ transactions (Mar 2016 - Aug 2018) from Pakistan's largest e-commerce",
    "category": "E-Commerce",
    "url": "https://www.kaggle.com/datasets/zusmani/pakistans-largest-ecommerce-dataset",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Pakistan",
      "transactions",
      "Kaggle"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "transactions"
    ],
    "summary": "The Pakistan e-commerce dataset comprises over 500,000 transactions from the largest e-commerce platform in Pakistan, spanning from March 2016 to August 2018. This dataset can be utilized to analyze consumer behavior, pricing strategies, and market trends within the e-commerce sector in Pakistan.",
    "use_cases": [
      "Analyzing consumer purchasing patterns over time",
      "Evaluating the impact of promotional campaigns on sales",
      "Studying the relationship between product pricing and sales volume",
      "Identifying seasonal trends in e-commerce transactions"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the trends in e-commerce transactions in Pakistan from 2016 to 2018?",
      "How do consumer behaviors vary across different product categories in Pakistan's e-commerce?",
      "What pricing strategies are most effective in the Pakistani e-commerce market?",
      "How has the volume of e-commerce transactions changed over time in Pakistan?",
      "What demographic factors influence online purchasing decisions in Pakistan?",
      "How do seasonal trends affect e-commerce sales in Pakistan?",
      "What are the common payment methods used in Pakistani e-commerce transactions?",
      "How does the size of transactions vary across different regions in Pakistan?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2016-03 to 2018-08",
    "geographic_scope": "Pakistan",
    "size_category": "medium",
    "model_score": 0.0033,
    "image_url": "/images/datasets/pakistan-e-commerce.jpg",
    "embedding_text": "The Pakistan e-commerce dataset provides a comprehensive collection of over 500,000 transactions recorded from March 2016 to August 2018, sourced from the largest e-commerce platform in Pakistan. This dataset is structured in a tabular format, with rows representing individual transactions and columns capturing various attributes such as transaction ID, product category, price, payment method, and timestamps. The data collection methodology involved aggregating transaction records from the platform's database, ensuring a rich representation of consumer behavior in the online retail environment. The temporal coverage of this dataset spans over two years, allowing for longitudinal analysis of trends and patterns in e-commerce activity. Geographically, the dataset is focused on Pakistan, providing insights into the local market dynamics and consumer preferences. Key variables include transaction amounts, product categories, and payment methods, which can be used to measure consumer spending habits and preferences. However, researchers should be aware of potential limitations, such as data quality issues arising from incomplete records or inconsistencies in transaction reporting. Common preprocessing steps may include data cleaning, normalization of transaction amounts, and categorization of product types. This dataset supports various types of analyses, including regression modeling to identify factors influencing sales, machine learning for predictive analytics, and descriptive statistics to summarize transaction trends. Researchers typically use this dataset to address questions related to consumer behavior, pricing strategies, and the overall growth of the e-commerce sector in Pakistan, making it a valuable resource for both academic and industry studies.",
    "tfidf_keywords": [
      "e-commerce",
      "consumer behavior",
      "transaction analysis",
      "pricing strategy",
      "market trends",
      "data quality",
      "longitudinal study",
      "payment methods",
      "product categories",
      "sales volume"
    ],
    "semantic_cluster": "e-commerce-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "pricing",
      "marketplaces",
      "data-analysis",
      "transactional-data"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "pricing",
      "marketplaces",
      "statistics"
    ]
  },
  {
    "name": "Arena Human Preference (55K)",
    "description": "55K+ real-world conversations with human preference labels from Chatbot Arena",
    "category": "AI & LLM",
    "url": "https://huggingface.co/datasets/lmarena-ai/arena-human-preference-55k",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "human preference",
      "LLM evaluation",
      "chatbot arena"
    ],
    "best_for": "Learning LLM evaluation, chatbot quality assessment, and dialogue systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Arena Human Preference dataset consists of over 55,000 real-world conversations that have been labeled with human preference indicators. This dataset can be utilized for evaluating large language models (LLMs) and improving chatbot interactions by analyzing human preferences in conversational settings.",
    "use_cases": [
      "Evaluating chatbot performance",
      "Improving LLM training",
      "Analyzing user preferences in conversations",
      "Conducting sentiment analysis on chatbot interactions"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Arena Human Preference dataset?",
      "How can I use the Arena dataset for LLM evaluation?",
      "What type of conversations are included in the Arena dataset?",
      "What are human preference labels in chatbot interactions?",
      "How many conversations are in the Arena dataset?",
      "What insights can be gained from analyzing the Arena Human Preference dataset?",
      "How does the Arena dataset contribute to chatbot development?",
      "What are the applications of the Arena Human Preference dataset in AI research?"
    ],
    "domain_tags": [
      "AI",
      "chatbots"
    ],
    "data_modality": "text",
    "size_category": "medium",
    "model_score": 0.0025,
    "image_url": "/images/datasets/arena-human-preference-55k.png",
    "embedding_text": "The Arena Human Preference dataset is a comprehensive collection of over 55,000 real-world conversations that have been meticulously labeled with human preference indicators. This dataset is particularly valuable for researchers and practitioners in the fields of artificial intelligence and natural language processing, as it provides a rich resource for evaluating and enhancing large language models (LLMs) and chatbot systems. The dataset's structure consists of rows representing individual conversations, with columns detailing the conversation content and corresponding human preference labels. The collection methodology involved gathering real interactions from users engaging with chatbots in various contexts, ensuring that the dataset reflects authentic conversational dynamics. While the dataset does not specify temporal or geographic coverage, it is designed to capture a diverse range of user interactions, making it applicable to various research questions related to human preferences in conversational AI. Key variables in the dataset include the conversation text and the associated preference labels, which measure user satisfaction and engagement levels. Data quality is generally high, but researchers should be aware of potential biases inherent in user-generated content. Common preprocessing steps may include text normalization, tokenization, and the removal of irrelevant content to prepare the data for analysis. This dataset supports a variety of analytical approaches, including regression analysis, machine learning model training, and descriptive statistics, allowing researchers to explore questions such as how different conversational styles impact user preferences or how LLMs can be fine-tuned to better align with human expectations. Overall, the Arena Human Preference dataset serves as a vital tool for advancing the understanding and development of conversational agents and their ability to meet human needs effectively.",
    "tfidf_keywords": [
      "human preference",
      "LLM evaluation",
      "chatbot interactions",
      "conversational dynamics",
      "user satisfaction",
      "engagement levels",
      "text normalization",
      "tokenization",
      "machine learning",
      "natural language processing"
    ],
    "semantic_cluster": "nlp-for-economics",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "natural-language-processing",
      "machine-learning",
      "user-experience",
      "sentiment-analysis",
      "conversational-AI"
    ],
    "canonical_topics": [
      "machine-learning",
      "natural-language-processing",
      "consumer-behavior"
    ]
  },
  {
    "name": "Used Car Auction (PakWheels)",
    "description": "Listings from PakWheels Pakistani automobile marketplace",
    "category": "Auctions & Marketplaces",
    "url": "https://www.kaggle.com/datasets/asimzahid/pakistans-largest-pakwheels-automobiles-listings",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "cars",
      "Pakistan",
      "listings"
    ],
    "best_for": "Learning auctions & marketplaces analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Used Car Auction dataset from PakWheels contains listings from a prominent Pakistani automobile marketplace, providing insights into the used car market. Researchers can analyze pricing trends, consumer preferences, and market dynamics based on this data.",
    "use_cases": [
      "Analyzing pricing trends in the used car market",
      "Studying consumer preferences for different car brands",
      "Evaluating the impact of car age on auction prices"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the current used car listings on PakWheels?",
      "How do prices of used cars vary across different regions in Pakistan?",
      "What are the most popular car brands listed on PakWheels?",
      "How does the age of a car affect its auction price?",
      "What trends can be observed in used car auctions over time?",
      "What features are most commonly associated with higher auction prices?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Pakistan",
    "size_category": "medium",
    "model_score": 0.0024,
    "image_url": "/images/datasets/used-car-auction-pakwheels.jpg",
    "embedding_text": "The Used Car Auction dataset from PakWheels is a comprehensive collection of listings from a leading automobile marketplace in Pakistan. This dataset is structured in a tabular format, with rows representing individual car listings and columns containing key variables such as car make, model, year, price, and auction details. The data is collected directly from the PakWheels platform, ensuring that it reflects current market conditions and consumer behavior. The dataset covers a wide range of vehicles, allowing for a detailed analysis of the used car market in Pakistan. Key variables include the make and model of the cars, their year of manufacture, auction prices, and additional features that may influence buyer decisions. Researchers can utilize this dataset to address various research questions, such as identifying pricing trends, understanding consumer preferences, and evaluating the impact of vehicle characteristics on auction outcomes. Common preprocessing steps may include cleaning the data to handle missing values, normalizing prices for inflation, and categorizing vehicles based on their attributes. The dataset supports various types of analyses, including regression analysis to model price determinants, machine learning techniques for predictive modeling, and descriptive statistics to summarize market trends. Researchers typically use this dataset to gain insights into the dynamics of the used car market, helping stakeholders make informed decisions regarding pricing and inventory management.",
    "tfidf_keywords": [
      "used cars",
      "auction prices",
      "consumer preferences",
      "PakWheels",
      "automobile marketplace",
      "pricing trends",
      "vehicle characteristics",
      "market dynamics",
      "data analysis",
      "listing data"
    ],
    "semantic_cluster": "marketplace-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "pricing",
      "marketplaces",
      "data-analysis",
      "retail"
    ],
    "canonical_topics": [
      "pricing",
      "marketplaces",
      "consumer-behavior"
    ]
  },
  {
    "name": "Art Auction (Artists for Lahaina)",
    "description": "Artists for Lahaina benefit art auction data (2023)",
    "category": "Auctions & Marketplaces",
    "url": "https://www.kaggle.com/datasets/flkuhm/art-price-dataset",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "art",
      "charity",
      "auctions"
    ],
    "best_for": "Learning auctions & marketplaces analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Art Auction dataset contains data from a benefit art auction organized by Artists for Lahaina in 2023. This dataset can be used to analyze auction dynamics, pricing strategies, and consumer behavior in the context of charity auctions.",
    "use_cases": [
      "Analyzing bidding patterns in charity auctions",
      "Evaluating the impact of artist reputation on auction prices",
      "Studying consumer behavior in the context of charitable giving"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What data is available from the Artists for Lahaina art auction?",
      "How can I analyze charity auction results?",
      "What trends can be observed in art auction pricing?",
      "What are the key variables in the Art Auction dataset?",
      "How does consumer behavior manifest in charity auctions?",
      "What insights can be drawn from the Artists for Lahaina auction data?"
    ],
    "domain_tags": [
      "art",
      "charity"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2023",
    "size_category": "medium",
    "model_score": 0.0022,
    "image_url": "/images/datasets/art-auction-artists-for-lahaina.png",
    "embedding_text": "The Art Auction dataset provides a comprehensive view of the benefit art auction organized by Artists for Lahaina in 2023. This dataset is structured in a tabular format, consisting of rows representing individual auction items and columns detailing various attributes such as artist name, artwork title, starting bid, final bid, and bidder information. The data collection methodology involved gathering auction results directly from the event, ensuring that the information reflects real-time auction dynamics. Key variables in the dataset include the artist's name, which measures the reputation and recognition of the artist, the artwork title, which provides context for the item being auctioned, and the bid amounts, which reflect the financial engagement of participants. The dataset is expected to have certain limitations, such as potential missing data for some auction items or bidders, which may affect the overall analysis. Common preprocessing steps may include cleaning the data to handle missing values, normalizing bid amounts for comparative analysis, and categorizing artworks by genre or medium for deeper insights. Researchers can use this dataset to address various research questions, such as the influence of artist reputation on auction prices, the demographics of bidders participating in charity auctions, and the overall effectiveness of charity auctions in raising funds. The dataset supports various types of analyses, including regression analysis to predict final bid amounts based on starting bids and artist reputation, as well as descriptive statistics to summarize the auction outcomes. Typically, researchers utilize this dataset to explore the intersection of art, charity, and consumer behavior, providing valuable insights into how auctions function as a marketplace for charitable contributions.",
    "geographic_scope": "Lahaina",
    "tfidf_keywords": [
      "charity-auction",
      "art-market",
      "bidding-patterns",
      "consumer-engagement",
      "auction-pricing",
      "artist-reputation",
      "fundraising",
      "artwork-valuation",
      "donor-behavior",
      "auction-dynamics"
    ],
    "semantic_cluster": "charity-auction-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "marketplaces",
      "pricing",
      "charitable-giving",
      "art-economics"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "marketplaces",
      "pricing"
    ]
  },
  {
    "name": "Yahoo A1 Search Advertising Dataset",
    "description": "Search advertising competition dataset with sponsored search auction features and click outcomes",
    "category": "Advertising",
    "url": "https://webscope.sandbox.yahoo.com/catalog.php?datatype=a",
    "docs_url": "https://webscope.sandbox.yahoo.com/",
    "github_url": null,
    "tags": [
      "search advertising",
      "sponsored search",
      "Yahoo",
      "auctions"
    ],
    "best_for": "Sponsored search click prediction and auction dynamics research",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "search advertising",
      "sponsored search",
      "auctions"
    ],
    "summary": "The Yahoo A1 Search Advertising Dataset is a comprehensive collection of data related to search advertising competitions, focusing on sponsored search auction features and click outcomes. Researchers can utilize this dataset to analyze the effectiveness of advertising strategies, understand consumer behavior in search environments, and develop predictive models for click outcomes.",
    "use_cases": [
      "Analyzing the impact of bidding strategies on click outcomes",
      "Evaluating consumer behavior in response to sponsored search ads",
      "Developing machine learning models to predict click-through rates",
      "Investigating auction dynamics in search advertising"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Yahoo A1 Search Advertising Dataset?",
      "How can I access the Yahoo A1 Search Advertising Dataset?",
      "What features are included in the Yahoo A1 Search Advertising Dataset?",
      "What types of analyses can be performed with the Yahoo A1 Search Advertising Dataset?",
      "What are the click outcomes in the Yahoo A1 Search Advertising Dataset?",
      "How does sponsored search work in the context of the Yahoo A1 Search Advertising Dataset?"
    ],
    "domain_tags": [
      "advertising"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "varies",
    "geographic_scope": "United States",
    "size_category": "medium",
    "model_score": 0.0021,
    "embedding_text": "The Yahoo A1 Search Advertising Dataset is a valuable resource for researchers and practitioners in the field of advertising and marketing analytics. This dataset encompasses a variety of features related to sponsored search auctions, including bidding strategies, ad placements, and click outcomes. The data is structured in a tabular format, allowing for straightforward analysis using common data manipulation tools such as pandas. Each row in the dataset represents a unique auction event, while the columns capture essential variables such as bid amounts, impressions, clicks, and conversion rates. The dataset is designed to facilitate research into the dynamics of search advertising, enabling users to explore how different factors influence ad performance and consumer engagement. Researchers can leverage this dataset to address a range of research questions, such as the effectiveness of various bidding strategies, the relationship between ad position and click-through rates, and the overall impact of sponsored search on consumer behavior. Given the competitive nature of search advertising, the dataset provides a rich ground for applying statistical methods and machine learning techniques to uncover insights and optimize advertising strategies. However, users should be aware of potential limitations in data quality, such as missing values or biases in click outcomes, which may necessitate preprocessing steps like data cleaning and normalization. Overall, the Yahoo A1 Search Advertising Dataset serves as a foundational tool for advancing knowledge in search advertising and its implications for marketers.",
    "image_url": "/images/logos/yahoo.png",
    "tfidf_keywords": [
      "search advertising",
      "sponsored search",
      "click outcomes",
      "bidding strategies",
      "ad placements",
      "click-through rates",
      "auction dynamics",
      "consumer engagement",
      "advertising analytics",
      "predictive modeling"
    ],
    "semantic_cluster": "search-advertising-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "consumer-behavior",
      "advertising strategies",
      "machine-learning",
      "data-analysis",
      "predictive-modeling"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "advertising",
      "machine-learning",
      "statistics"
    ]
  },
  {
    "name": "Real-Time Advertisers Auction",
    "description": "Real-time advertiser auction dataset for RTB research",
    "category": "Advertising",
    "url": "https://www.kaggle.com/datasets/saurav9786/real-time-advertisers-auction",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "RTB",
      "auctions",
      "programmatic"
    ],
    "best_for": "Learning advertising analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Real-Time Advertisers Auction dataset provides insights into the dynamics of real-time bidding (RTB) in advertising. Researchers can analyze bidding strategies, auction outcomes, and consumer behavior patterns to inform programmatic advertising practices.",
    "use_cases": [
      "Analyzing bidding strategies in real-time auctions",
      "Evaluating the impact of auction outcomes on advertising effectiveness",
      "Studying consumer behavior in response to programmatic ads"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the structure of the Real-Time Advertisers Auction dataset?",
      "How can I analyze bidding strategies using this dataset?",
      "What variables are included in the real-time advertiser auction data?",
      "What insights can be gained from studying RTB auction outcomes?",
      "How does the dataset support programmatic advertising research?",
      "What preprocessing steps are necessary for analyzing this dataset?",
      "What are the limitations of the Real-Time Advertisers Auction dataset?",
      "How can this dataset inform consumer behavior studies?"
    ],
    "domain_tags": [
      "advertising"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0021,
    "image_url": "/images/datasets/real-time-advertisers-auction.jpg",
    "embedding_text": "The Real-Time Advertisers Auction dataset is a comprehensive collection of data pertaining to real-time bidding (RTB) auctions in the advertising sector. This dataset typically consists of rows representing individual auction events, with columns detailing various attributes such as bid amounts, advertiser identities, timestamps, and auction outcomes. The data structure is designed to facilitate analysis of bidding behavior and auction dynamics, making it a valuable resource for researchers in the field of programmatic advertising. The collection methodology often involves aggregating data from multiple ad exchanges and platforms, ensuring a rich dataset that captures diverse auction scenarios. However, researchers should be aware of potential limitations in data quality, such as missing values or inconsistencies in bid reporting, which may necessitate preprocessing steps like data cleaning and normalization. Key variables within the dataset may include bid price, winning bid, advertiser ID, and impression ID, each measuring different aspects of the auction process. Researchers can employ this dataset to address various research questions, such as the effectiveness of different bidding strategies, the impact of auction design on outcomes, and consumer response to targeted advertising. Types of analyses supported by this dataset include regression analysis, machine learning models, and descriptive statistics, allowing for a multifaceted exploration of the data. Typically, researchers utilize this dataset to inform studies on advertising effectiveness, consumer behavior, and market dynamics in the realm of digital advertising.",
    "tfidf_keywords": [
      "real-time bidding",
      "advertiser auction",
      "programmatic advertising",
      "bidding strategies",
      "auction outcomes",
      "consumer behavior",
      "data preprocessing",
      "advertising effectiveness",
      "market dynamics",
      "data quality"
    ],
    "semantic_cluster": "advertising-auction-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "programmatic-advertising",
      "consumer-behavior",
      "marketplaces",
      "bidding-strategies",
      "data-analysis"
    ],
    "canonical_topics": [
      "pricing",
      "marketplaces",
      "consumer-behavior",
      "experimentation",
      "machine-learning"
    ]
  },
  {
    "name": "Criteo Display Advertising",
    "description": "342GB total with 13 integer features, 26 hashed categorical features",
    "category": "Advertising",
    "url": "https://ailab.criteo.com/ressources/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "CTR prediction",
      "advertising",
      "large-scale"
    ],
    "best_for": "Learning advertising analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "advertising",
      "CTR prediction"
    ],
    "summary": "The Criteo Display Advertising dataset consists of 342GB of data, featuring 13 integer and 26 hashed categorical variables. It is designed for tasks such as click-through rate (CTR) prediction in large-scale advertising scenarios, allowing researchers to develop and test various machine learning models.",
    "use_cases": [
      "Predicting click-through rates for online ads",
      "Evaluating the performance of advertising algorithms",
      "Analyzing user engagement with display ads"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the Criteo Display Advertising dataset?",
      "How can I use the Criteo dataset for CTR prediction?",
      "What features are included in the Criteo Display Advertising dataset?",
      "What types of analyses can be performed on the Criteo dataset?",
      "Where can I find the Criteo Display Advertising dataset?",
      "What are the limitations of the Criteo dataset?"
    ],
    "domain_tags": [
      "advertising"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0021,
    "image_url": "/images/logos/criteo.png",
    "embedding_text": "The Criteo Display Advertising dataset is a comprehensive resource for researchers and practitioners in the field of digital advertising. With a total size of 342GB, it encompasses a rich set of features designed to facilitate the prediction of click-through rates (CTR) in online advertising campaigns. The dataset includes 13 integer features that capture various numerical aspects of user interactions and ad placements, alongside 26 hashed categorical features that represent diverse categorical variables relevant to advertising contexts. The structure of the dataset is tabular, with rows representing individual ad impressions and columns corresponding to the features mentioned. This dataset is particularly valuable for machine learning practitioners looking to develop predictive models that can enhance the effectiveness of advertising strategies. The collection methodology for the Criteo dataset involves aggregating data from numerous ad impressions across various platforms, ensuring a robust representation of user behavior and ad performance. However, researchers should be aware of potential limitations, such as data quality issues stemming from the hashing of categorical features, which may obscure interpretability. Common preprocessing steps include decoding hashed features, handling missing values, and normalizing numerical variables to prepare the dataset for analysis. The dataset supports a range of analytical approaches, including regression analysis and machine learning techniques, making it suitable for both descriptive and predictive modeling. Researchers typically leverage this dataset to explore questions related to user engagement, ad effectiveness, and the optimization of advertising strategies, contributing to a deeper understanding of consumer behavior in the digital marketplace.",
    "tfidf_keywords": [
      "click-through-rate",
      "CTR prediction",
      "advertising algorithms",
      "user engagement",
      "feature engineering",
      "large-scale data",
      "machine learning",
      "predictive modeling",
      "data preprocessing",
      "digital advertising"
    ],
    "semantic_cluster": "advertising-analytics",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "machine-learning",
      "consumer-behavior",
      "predictive-analytics",
      "feature-engineering",
      "data-preprocessing"
    ],
    "canonical_topics": [
      "machine-learning",
      "consumer-behavior",
      "advertising"
    ],
    "benchmark_usage": [
      "CTR prediction",
      "large-scale advertising analysis"
    ]
  },
  {
    "name": "Yoyi",
    "description": "Computational advertising dataset from Chinese ad platform",
    "category": "Advertising",
    "url": "https://apex.sjtu.edu.cn/datasets/7",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "advertising",
      "China",
      "computational ads"
    ],
    "best_for": "Learning advertising analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Yoyi dataset is a computational advertising dataset sourced from a Chinese ad platform, providing insights into advertising strategies and consumer interactions. Researchers can utilize this dataset to analyze advertising effectiveness, consumer behavior, and market trends in the context of computational ads.",
    "use_cases": [
      "Analyzing the effectiveness of different advertising strategies",
      "Modeling consumer behavior based on ad interactions",
      "Evaluating market trends in computational advertising"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What insights can be gained from the Yoyi computational advertising dataset?",
      "How does advertising performance vary across different consumer segments in China?",
      "What are the key factors influencing ad effectiveness in computational advertising?",
      "How can the Yoyi dataset be used to model consumer behavior?",
      "What trends can be identified in advertising strategies using the Yoyi dataset?",
      "How does the Yoyi dataset support regression analysis in advertising research?"
    ],
    "domain_tags": [
      "advertising"
    ],
    "data_modality": "tabular",
    "geographic_scope": "China",
    "size_category": "medium",
    "model_score": 0.0019,
    "image_url": "/images/logos/sjtu.edu.png",
    "embedding_text": "The Yoyi dataset is a comprehensive computational advertising dataset derived from a prominent Chinese ad platform. It consists of tabular data structured with various rows and columns that capture the nuances of advertising interactions and consumer responses. The dataset includes key variables that measure aspects such as ad impressions, click-through rates, and conversion metrics, providing a rich foundation for analysis. The collection methodology involves aggregating data from live advertising campaigns, ensuring that the dataset reflects real-world advertising scenarios. However, researchers should be aware of potential limitations, such as data quality issues stemming from incomplete records or biases in ad targeting. Common preprocessing steps may include data cleaning, normalization, and feature engineering to prepare the dataset for analysis. This dataset supports a range of analytical approaches, including regression analysis and machine learning models, allowing researchers to explore questions related to ad effectiveness, consumer engagement, and market dynamics. By leveraging the Yoyi dataset, researchers can gain valuable insights into the computational advertising landscape in China, making it a significant resource for studies focused on advertising strategies and consumer behavior.",
    "tfidf_keywords": [
      "ad impressions",
      "click-through rate",
      "conversion metrics",
      "consumer engagement",
      "advertising strategies",
      "data normalization",
      "feature engineering",
      "market dynamics",
      "computational advertising",
      "data quality"
    ],
    "semantic_cluster": "advertising-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "consumer-behavior",
      "advertising-strategies",
      "data-preprocessing",
      "regression-analysis",
      "machine-learning"
    ],
    "canonical_topics": [
      "advertising",
      "consumer-behavior",
      "machine-learning"
    ]
  },
  {
    "name": "Avazu",
    "description": "Dataset for click-through rate prediction on mobile ads",
    "category": "Advertising",
    "url": "https://www.kaggle.com/competitions/avazu-ctr-prediction",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "CTR",
      "mobile ads",
      "Kaggle competition"
    ],
    "best_for": "Learning advertising analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "advertising",
      "click-through rate",
      "mobile marketing"
    ],
    "summary": "The Avazu dataset is designed for predicting click-through rates (CTR) on mobile advertisements. It contains features related to ad impressions and user interactions, enabling researchers to build models that forecast ad performance and optimize advertising strategies.",
    "use_cases": [
      "Predicting click-through rates for mobile ads",
      "Optimizing ad placements based on user behavior",
      "Analyzing the effectiveness of mobile advertising campaigns"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Avazu dataset?",
      "How can I use the Avazu dataset for CTR prediction?",
      "Where can I find the Avazu dataset?",
      "What features are included in the Avazu dataset?",
      "What are the applications of the Avazu dataset?",
      "How does the Avazu dataset relate to mobile advertising?"
    ],
    "domain_tags": [
      "advertising"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0019,
    "image_url": "/images/datasets/avazu.png",
    "embedding_text": "The Avazu dataset is a comprehensive resource for researchers and practitioners interested in click-through rate (CTR) prediction specifically for mobile advertisements. This dataset comprises various features that capture user interactions with ads, including ad impressions, user demographics, and contextual information about the ads themselves. The data is structured in a tabular format, allowing for straightforward manipulation and analysis using common data science tools such as Python's pandas library. Each row in the dataset represents an ad impression, while the columns include variables that measure different aspects of the ad and user interactions. Key variables may include user ID, ad ID, timestamp, and various categorical features that describe the ad and the user. The collection methodology for the Avazu dataset typically involves gathering data from real-world mobile advertising campaigns, ensuring that the dataset reflects actual user behavior and ad performance metrics. However, researchers should be aware of potential limitations in data quality, such as missing values or biases in user representation, which may affect the robustness of predictive models built using this dataset. Common preprocessing steps may include handling missing data, encoding categorical variables, and normalizing numerical features to prepare the dataset for analysis. Researchers can leverage the Avazu dataset to address various research questions related to mobile advertising effectiveness, such as identifying factors that influence click-through rates or comparing the performance of different ad strategies. The dataset supports a range of analytical approaches, including regression analysis, machine learning modeling, and descriptive statistics, making it a versatile tool for both academic research and practical applications in the advertising industry. By utilizing the Avazu dataset, researchers can gain insights into consumer behavior in the mobile advertising space and develop strategies to enhance ad targeting and effectiveness.",
    "tfidf_keywords": [
      "click-through rate",
      "mobile advertising",
      "ad impressions",
      "user interactions",
      "predictive modeling",
      "data preprocessing",
      "feature engineering",
      "advertising strategies",
      "consumer behavior",
      "data quality"
    ],
    "semantic_cluster": "advertising-analytics",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "predictive modeling",
      "feature engineering",
      "user behavior analysis",
      "advertising effectiveness",
      "machine learning"
    ],
    "canonical_topics": [
      "machine-learning",
      "consumer-behavior",
      "advertising"
    ]
  },
  {
    "name": "Alpha Vantage",
    "description": "NASDAQ-licensed stock data for 200,000+ tickers with free tier (25 requests/day)",
    "category": "Dataset Aggregators",
    "url": "https://www.alphavantage.co",
    "docs_url": "https://www.alphavantage.co/documentation/",
    "github_url": null,
    "tags": [
      "stocks",
      "free",
      "API",
      "technical indicators"
    ],
    "best_for": "Individual researchers needing free stock data with 50+ technical indicators",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "stocks",
      "finance",
      "API"
    ],
    "summary": "Alpha Vantage provides NASDAQ-licensed stock data for over 200,000 tickers, allowing users to access a free tier of 25 requests per day. This dataset is ideal for financial analysis, enabling users to retrieve stock prices, technical indicators, and other market data for various analytical purposes.",
    "use_cases": [
      "Analyzing stock price trends over time",
      "Comparing technical indicators across different stocks",
      "Building predictive models for stock price movements",
      "Visualizing stock performance using historical data"
    ],
    "audience": [
      "Curious-browser",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What is Alpha Vantage stock data?",
      "How can I access NASDAQ stock data for free?",
      "What are the technical indicators available in Alpha Vantage?",
      "How many tickers does Alpha Vantage cover?",
      "What is the daily request limit for Alpha Vantage API?",
      "How to use Alpha Vantage for stock analysis?",
      "What types of data can I retrieve from Alpha Vantage?",
      "Is Alpha Vantage suitable for beginners in finance?"
    ],
    "domain_tags": [
      "fintech"
    ],
    "data_modality": "time-series",
    "size_category": "medium",
    "model_score": 0.0016,
    "embedding_text": "Alpha Vantage offers a comprehensive dataset of NASDAQ-licensed stock data that encompasses over 200,000 tickers, making it a valuable resource for financial analysts, data scientists, and researchers interested in stock market dynamics. The dataset is structured in a time-series format, where each row typically represents a specific time point for a given stock ticker, and the columns include various attributes such as open price, close price, high price, low price, volume, and technical indicators. This structure allows for straightforward analysis and visualization of stock performance over time. \n\nThe collection methodology for Alpha Vantage involves aggregating data from various financial markets and exchanges, ensuring that users have access to up-to-date and accurate stock information. The data is accessible via a user-friendly API, which allows for easy integration into data analysis workflows. Users can make up to 25 requests per day for free, which is particularly beneficial for individuals and small businesses looking to conduct financial analysis without incurring significant costs. \n\nKey variables in the dataset include stock prices at different time intervals, trading volume, and various technical indicators such as moving averages and relative strength index (RSI). These variables are essential for conducting analyses that seek to understand market trends, evaluate stock performance, and make informed investment decisions. However, users should be aware of potential limitations in data quality, including the possibility of missing data points or discrepancies in reported prices due to market fluctuations. \n\nCommon preprocessing steps may include cleaning the data to handle missing values, normalizing stock prices for comparative analysis, and transforming data into formats suitable for machine learning algorithms. Researchers and analysts can use this dataset to address a variety of research questions, such as predicting future stock prices, assessing the impact of market events on stock performance, or evaluating the effectiveness of trading strategies. \n\nThe types of analyses supported by this dataset range from descriptive statistics to more complex regression analyses and machine learning applications. Researchers typically utilize Alpha Vantage in studies focused on financial forecasting, investment strategy development, and market behavior analysis. Overall, Alpha Vantage serves as a foundational resource for anyone looking to delve into the world of stock market analysis and financial data science.",
    "benchmark_usage": [
      "Technical analysis",
      "Stock performance evaluation"
    ],
    "tfidf_keywords": [
      "NASDAQ",
      "stock data",
      "API",
      "technical indicators",
      "time-series analysis",
      "financial forecasting",
      "trading volume",
      "moving averages",
      "relative strength index",
      "market trends",
      "investment strategies",
      "data integration",
      "data preprocessing",
      "financial analysis",
      "stock performance"
    ],
    "semantic_cluster": "financial-data-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "financial-analysis",
      "time-series-analysis",
      "predictive-modeling",
      "data-visualization",
      "machine-learning"
    ],
    "canonical_topics": [
      "finance",
      "machine-learning",
      "statistics",
      "consumer-behavior"
    ]
  },
  {
    "name": "Tencent Social Ads",
    "description": "Social ad CTR prediction dataset from Tencent",
    "category": "Advertising",
    "url": "https://algo.qq.com/index.html",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "social ads",
      "CTR",
      "Tencent",
      "China"
    ],
    "best_for": "Learning advertising analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "advertising",
      "consumer-behavior"
    ],
    "summary": "The Tencent Social Ads dataset is designed for predicting click-through rates (CTR) for social advertisements. It provides valuable insights into user engagement and advertising effectiveness, enabling researchers and marketers to optimize ad strategies.",
    "use_cases": [
      "Predicting click-through rates for social ads",
      "Analyzing user engagement with advertisements",
      "Optimizing advertising strategies based on CTR data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Tencent Social Ads dataset?",
      "How can I predict CTR using Tencent data?",
      "What variables are included in the Tencent Social Ads dataset?",
      "What insights can be gained from analyzing social ad performance?",
      "How does Tencent's advertising data compare to other platforms?",
      "What machine learning techniques can be applied to CTR prediction?"
    ],
    "domain_tags": [
      "advertising"
    ],
    "data_modality": "tabular",
    "geographic_scope": "China",
    "size_category": "medium",
    "model_score": 0.0016,
    "image_url": "/images/logos/qq.png",
    "embedding_text": "The Tencent Social Ads dataset is a comprehensive resource for researchers and practitioners interested in the dynamics of social advertising. It consists of a structured tabular format, with rows representing individual ad impressions and columns detailing various attributes such as ad content, user demographics, and engagement metrics. The dataset is particularly valuable for its focus on click-through rates (CTR), which serve as a critical measure of ad effectiveness. Researchers can leverage this dataset to explore a range of questions related to consumer behavior, advertising strategies, and the impact of various factors on CTR. The data collection methodology employed by Tencent ensures a rich and diverse dataset, capturing a wide array of user interactions with social ads across the Chinese market. Key variables include user demographics, ad characteristics, and engagement metrics, each providing insights into the factors influencing CTR. However, users should be aware of potential limitations in data quality, such as biases in user representation or the influence of external factors on ad performance. Common preprocessing steps may include handling missing values, normalizing data, and feature engineering to enhance model performance. The dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it a versatile tool for understanding social advertising dynamics. Researchers typically utilize this dataset to develop predictive models for CTR, assess the effectiveness of different advertising strategies, and derive actionable insights that can inform future campaigns.",
    "tfidf_keywords": [
      "click-through-rate",
      "social advertising",
      "user engagement",
      "advertising effectiveness",
      "CTR prediction",
      "consumer behavior",
      "ad optimization",
      "data collection methodology",
      "feature engineering",
      "machine learning"
    ],
    "semantic_cluster": "advertising-analytics",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "consumer-behavior",
      "advertising-strategy",
      "machine-learning",
      "data-analysis",
      "predictive-modeling"
    ],
    "canonical_topics": [
      "machine-learning",
      "consumer-behavior",
      "advertising",
      "statistics"
    ]
  },
  {
    "name": "WRDS (Wharton Research Data Services)",
    "description": "350+ terabytes from CRSP, Compustat, TAQ - de facto standard for academic finance",
    "category": "Dataset Aggregators",
    "url": "https://wrds-www.wharton.upenn.edu",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "finance",
      "CRSP",
      "Compustat",
      "academic",
      "premium"
    ],
    "best_for": "Publication in top finance journals - requires institutional subscription",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "finance",
      "academic",
      "premium"
    ],
    "summary": "WRDS (Wharton Research Data Services) is a comprehensive dataset aggregating over 350 terabytes of financial data from sources such as CRSP, Compustat, and TAQ. It serves as the de facto standard for academic finance research, enabling users to conduct various analyses on stock prices, financial statements, and trading volumes.",
    "use_cases": [
      "Analyzing stock price trends",
      "Evaluating financial performance of companies",
      "Conducting event studies",
      "Performing risk assessments"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is WRDS?",
      "How to access CRSP data through WRDS?",
      "What types of financial data are available in Compustat?",
      "How can I analyze trading volumes using TAQ data?",
      "What are the common uses of WRDS in academic finance?",
      "How to perform regression analysis with WRDS data?",
      "What datasets are included in WRDS?",
      "What is the significance of WRDS in finance research?"
    ],
    "domain_tags": [
      "finance"
    ],
    "data_modality": "tabular",
    "size_category": "massive",
    "model_score": 0.0015,
    "image_url": "/images/logos/upenn.png",
    "embedding_text": "WRDS (Wharton Research Data Services) is an extensive financial dataset that aggregates over 350 terabytes of data from leading financial databases such as CRSP (Center for Research in Security Prices), Compustat, and TAQ (Trade and Quote). This dataset is widely recognized as the de facto standard for academic finance research, providing researchers, analysts, and students with access to a wealth of financial information necessary for rigorous analysis. The data structure typically consists of tabular formats with rows representing individual securities or transactions and columns containing various financial metrics, including stock prices, trading volumes, and company financial statements. Each dataset within WRDS has its own schema, with specific variables tailored to the type of data it represents. For example, CRSP focuses on stock price data, while Compustat provides detailed financial statements of publicly traded companies. The collection methodology involves systematic data gathering from exchanges and financial reports, ensuring high data quality and reliability. However, users should be aware of potential limitations, such as data gaps or discrepancies in reporting periods. Common preprocessing steps may include data cleaning, normalization, and handling missing values to prepare the data for analysis. Researchers typically use WRDS to address a variety of research questions, such as the impact of market events on stock prices, the relationship between financial metrics and company performance, and the evaluation of trading strategies. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile tool for finance-related research. Overall, WRDS is an invaluable resource for anyone looking to conduct in-depth financial analysis and contribute to academic literature in the field of finance.",
    "benchmark_usage": [
      "Common uses include stock price analysis, financial statement comparisons, and trading volume assessments"
    ],
    "tfidf_keywords": [
      "CRSP",
      "Compustat",
      "TAQ",
      "financial analysis",
      "stock prices",
      "trading volumes",
      "financial statements",
      "event studies",
      "risk assessment",
      "academic finance"
    ],
    "semantic_cluster": "financial-data-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "financial-econometrics",
      "panel-data",
      "time-series-analysis",
      "risk-management",
      "market-efficiency"
    ],
    "canonical_topics": [
      "finance",
      "econometrics",
      "statistics"
    ]
  },
  {
    "name": "Online Auctions Collection",
    "description": "Collection of datasets from eBay and experimental auctions",
    "category": "Auctions & Marketplaces",
    "url": "https://www.modelingonlineauctions.com/datasets",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "auctions",
      "eBay",
      "bidding"
    ],
    "best_for": "Learning auctions & marketplaces analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Online Auctions Collection is a comprehensive dataset comprising various datasets from eBay and experimental auctions. Researchers can utilize this collection to analyze bidding behaviors, auction dynamics, and consumer preferences in online marketplaces.",
    "use_cases": [
      "Analyzing bidding strategies in online auctions",
      "Studying consumer behavior in e-commerce",
      "Evaluating the impact of auction design on outcomes"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What datasets are available from eBay auctions?",
      "How can I analyze bidding behavior in online auctions?",
      "What are the characteristics of experimental auctions?",
      "What insights can be gained from eBay auction data?",
      "How do consumer behaviors differ in online bidding?",
      "What factors influence auction outcomes on eBay?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0014,
    "embedding_text": "The Online Auctions Collection is a rich resource for researchers interested in the dynamics of online auctions, particularly those conducted on platforms like eBay. This dataset includes various datasets that capture the intricacies of bidding behavior, auction formats, and consumer interactions in digital marketplaces. The data structure typically consists of rows representing individual auction events, with columns detailing key variables such as auction ID, item description, starting bid, final bid, bidder IDs, and timestamps. These variables allow for a comprehensive analysis of auction outcomes and participant behavior.\n\nThe collection methodology involves aggregating data from eBay's public auction listings and experimental auctions designed to test specific hypotheses about bidding behavior. Researchers can explore temporal aspects of the data, although specific time frames are not explicitly mentioned. Geographic coverage is also broad, as eBay operates in multiple regions, but specific locations are not detailed in the dataset.\n\nKey variables in the dataset measure various aspects of auction dynamics, including bid increments, bidder activity levels, and final sale prices. These metrics are essential for understanding how different factors influence auction outcomes. However, researchers should be aware of potential limitations in data quality, such as incomplete records or variations in auction formats that may affect comparability.\n\nCommon preprocessing steps may include cleaning the data to handle missing values, normalizing bid amounts for inflation, and categorizing items based on type or category for more granular analysis. The dataset supports a range of research questions, such as identifying trends in bidding behavior, understanding the impact of auction design on bidding strategies, and evaluating consumer preferences in online marketplaces.\n\nAnalyses can include regression modeling to predict auction outcomes, machine learning techniques to classify bidding strategies, and descriptive statistics to summarize key trends. Researchers typically use this dataset to inform studies on consumer behavior, pricing strategies, and the effectiveness of different auction formats, making it a valuable asset for both academic and practical applications in the field of e-commerce.",
    "tfidf_keywords": [
      "bidding-strategies",
      "auction-dynamics",
      "consumer-preferences",
      "eBay-data",
      "auction-design",
      "experimental-auctions",
      "online-marketplaces",
      "bidding-behavior",
      "price-optimization",
      "consumer-behavior"
    ],
    "semantic_cluster": "marketplace-experimentation",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "pricing",
      "marketplaces",
      "experimental-design",
      "bidding-theory"
    ],
    "canonical_topics": [
      "marketplaces",
      "consumer-behavior",
      "pricing",
      "experimentation"
    ]
  },
  {
    "name": "Apple App Store Dataset",
    "description": "7,200 iOS apps with pricing, ratings, genres, in-app purchases. Apple app marketplace analysis",
    "category": "App Stores",
    "url": "https://www.kaggle.com/datasets/ramamet4/app-store-apple-data-set-10k-apps",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Apple",
      "iOS",
      "apps",
      "pricing",
      "App Store"
    ],
    "best_for": "Learning app stores analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Apple App Store Dataset contains information on 7,200 iOS applications, including their pricing, ratings, genres, and in-app purchases. This dataset can be utilized for analyzing trends in the Apple app marketplace, understanding consumer behavior, and evaluating pricing strategies.",
    "use_cases": [
      "Analyzing pricing strategies of successful iOS apps",
      "Studying consumer behavior based on app ratings and purchases",
      "Evaluating the impact of app genre on pricing and ratings",
      "Conducting market analysis for new app development"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the pricing trends of iOS apps in the Apple App Store?",
      "How do ratings correlate with in-app purchases for iOS applications?",
      "What genres of apps are most popular in the Apple App Store?",
      "How does the pricing of iOS apps vary across different categories?",
      "What is the distribution of ratings among iOS apps?",
      "How do in-app purchases impact overall app revenue?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0014,
    "image_url": "/images/datasets/apple-app-store-dataset.jpg",
    "embedding_text": "The Apple App Store Dataset is a comprehensive collection of data pertaining to 7,200 iOS applications available on the Apple App Store. This dataset includes various attributes such as pricing, ratings, genres, and in-app purchases, providing a rich resource for analysis of the app marketplace. The data is structured in a tabular format, with each row representing an individual app and columns that detail its characteristics. Key variables include the app's price, user ratings, the genre it belongs to, and whether it offers in-app purchases. These variables are essential for measuring app performance, user satisfaction, and market trends. The dataset was likely collected through automated scraping of the Apple App Store or via APIs, although specific collection methodologies are not detailed. While the dataset is extensive, potential limitations include the accuracy of ratings, which can be influenced by user bias, and the dynamic nature of app pricing, which may change frequently. Common preprocessing steps may involve cleaning the data to handle missing values, normalizing ratings, and categorizing apps into distinct genres. Researchers can utilize this dataset to address various research questions, such as the relationship between app pricing and user ratings, the impact of in-app purchases on revenue, and trends in app popularity across different genres. The dataset supports a range of analyses, including regression analysis to explore correlations, machine learning for predictive modeling, and descriptive statistics to summarize app characteristics. Typically, researchers leverage this dataset to gain insights into market dynamics, consumer preferences, and the effectiveness of different pricing strategies in the competitive landscape of mobile applications.",
    "tfidf_keywords": [
      "iOS-apps",
      "pricing-strategies",
      "consumer-behavior",
      "app-ratings",
      "in-app-purchases",
      "market-analysis",
      "app-genres",
      "user-satisfaction",
      "app-performance",
      "data-collection-methodology"
    ],
    "semantic_cluster": "app-marketplace-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "pricing",
      "consumer-behavior",
      "marketplaces",
      "data-analysis",
      "app-development"
    ],
    "canonical_topics": [
      "pricing",
      "consumer-behavior",
      "marketplaces"
    ]
  },
  {
    "name": "Diginetica Fashion",
    "description": "Clickstream and purchase data for fashion e-commerce",
    "category": "Fashion & Apparel",
    "url": "https://competitions.codalab.org/competitions/11161",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "fashion",
      "clickstream",
      "competition"
    ],
    "best_for": "Learning fashion & apparel analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "Diginetica Fashion provides clickstream and purchase data specifically for fashion e-commerce, allowing researchers to analyze consumer behavior and purchasing patterns. This dataset can be utilized to explore trends in online shopping, assess competition, and develop pricing strategies.",
    "use_cases": [
      "Analyzing consumer purchasing patterns",
      "Assessing the impact of marketing campaigns",
      "Evaluating competition in the fashion e-commerce sector",
      "Developing pricing strategies based on consumer behavior"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the clickstream patterns in fashion e-commerce?",
      "How do purchase decisions vary by demographic?",
      "What factors influence competition in online fashion retail?",
      "How can clickstream data inform pricing strategies?",
      "What trends can be identified in consumer behavior from the dataset?",
      "How does seasonality affect fashion e-commerce purchases?",
      "What is the relationship between clickstream data and conversion rates?",
      "How can machine learning be applied to analyze this dataset?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0013,
    "embedding_text": "The Diginetica Fashion dataset encompasses clickstream and purchase data specifically tailored for the fashion e-commerce sector. This dataset is structured in a tabular format, containing rows that represent individual user sessions and columns that capture various attributes such as timestamps, user identifiers, product IDs, and purchase amounts. The collection methodology involves tracking user interactions on e-commerce platforms, allowing for a comprehensive understanding of consumer behavior in the online fashion market. Key variables include user session duration, product views, and conversion rates, which measure engagement and purchasing efficiency. Researchers can leverage this dataset to address a multitude of research questions, such as identifying trends in consumer purchasing behavior, examining the effects of promotional campaigns, and assessing competitive dynamics within the fashion industry. Common preprocessing steps may involve cleaning the data to handle missing values, normalizing timestamps for analysis, and aggregating user sessions to derive insights on overall performance metrics. The dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics, providing a robust foundation for understanding the intricacies of online shopping in the fashion domain. Researchers typically utilize this dataset to inform strategic decisions in marketing, pricing, and product development, making it a valuable resource for those interested in the intersection of technology and consumer behavior in retail.",
    "tfidf_keywords": [
      "clickstream",
      "purchase-data",
      "consumer-behavior",
      "e-commerce",
      "fashion-retail",
      "data-analysis",
      "user-engagement",
      "conversion-rate",
      "pricing-strategy",
      "market-competition"
    ],
    "semantic_cluster": "e-commerce-consumer-behavior",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "consumer-behavior",
      "pricing",
      "market-competition",
      "data-analysis",
      "e-commerce-strategies"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "pricing",
      "marketplaces",
      "experimentation",
      "machine-learning"
    ]
  },
  {
    "name": "USS (User Satisfaction Simulation)",
    "description": "6,800 dialogues with 5-level satisfaction scale labels across multiple domains",
    "category": "AI & LLM",
    "url": "https://github.com/sunnweiwei/user-satisfaction-simulation",
    "docs_url": null,
    "github_url": "https://github.com/sunnweiwei/user-satisfaction-simulation",
    "tags": [
      "user satisfaction",
      "dialogue",
      "simulation"
    ],
    "best_for": "Learning LLM evaluation, chatbot quality assessment, and dialogue systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The USS (User Satisfaction Simulation) dataset consists of 6,800 dialogues labeled with a 5-level satisfaction scale across various domains. This dataset can be utilized to analyze user satisfaction trends, develop dialogue systems, and improve customer interactions in AI applications.",
    "use_cases": [
      "Analyzing user satisfaction across different dialogue scenarios",
      "Developing machine learning models for predicting user satisfaction",
      "Improving AI dialogue systems based on user feedback",
      "Conducting sentiment analysis on user interactions"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the USS dataset?",
      "How can I access the User Satisfaction Simulation data?",
      "What are the satisfaction levels in the USS dataset?",
      "In which domains is the USS dataset applicable?",
      "What types of analyses can be performed using the USS dataset?",
      "How many dialogues are included in the USS dataset?",
      "What is the satisfaction scale used in the USS dataset?",
      "What insights can be drawn from the USS dataset?"
    ],
    "data_modality": "text",
    "size_category": "medium",
    "model_score": 0.0013,
    "image_url": "/images/datasets/uss-user-satisfaction-simulation.png",
    "embedding_text": "The USS (User Satisfaction Simulation) dataset is a comprehensive collection of 6,800 dialogues that have been meticulously labeled with a 5-level satisfaction scale. This dataset is structured in a tabular format, where each row represents a unique dialogue instance, and the columns capture various attributes such as dialogue content and corresponding satisfaction labels. The primary focus of this dataset is to facilitate research and development in the field of user satisfaction, particularly in the context of artificial intelligence and natural language processing. Researchers can leverage this dataset to explore user satisfaction trends, develop predictive models, and enhance dialogue systems. The dialogues span multiple domains, providing a rich source of data for analyzing how satisfaction levels vary across different contexts. Key variables in the dataset include the dialogue text and the satisfaction rating, which measures the user's level of satisfaction on a scale from 1 to 5. While the dataset offers a valuable resource for understanding user interactions, it is important to note that the quality of the dialogues may vary, and researchers should be aware of potential biases in user responses. Common preprocessing steps may include text normalization, tokenization, and the removal of irrelevant content to prepare the data for analysis. The USS dataset supports a range of analyses, including regression, machine learning, and descriptive statistics, making it a versatile tool for researchers aiming to address questions related to user satisfaction and dialogue effectiveness. Typical research questions might include: How do satisfaction levels differ across various dialogue scenarios? What factors contribute to higher satisfaction ratings? How can AI systems be trained to improve user satisfaction based on historical dialogue data? Overall, the USS dataset serves as a foundational resource for advancing the understanding of user satisfaction in AI-driven dialogue systems.",
    "domain_tags": [
      "AI",
      "LLM"
    ],
    "tfidf_keywords": [
      "user-satisfaction",
      "dialogue-systems",
      "satisfaction-scale",
      "natural-language-processing",
      "predictive-modeling",
      "sentiment-analysis",
      "customer-interaction",
      "AI-applications",
      "data-collection",
      "dialogue-analysis"
    ],
    "semantic_cluster": "user-satisfaction-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "natural-language-processing",
      "machine-learning",
      "sentiment-analysis",
      "user-experience",
      "dialogue-systems"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "machine-learning",
      "natural-language-processing"
    ]
  },
  {
    "name": "ConvAI Dataset",
    "description": "4,750 human-to-bot dialogues with thumbs up/down feedback plus quality scores",
    "category": "AI & LLM",
    "url": "http://convai.io/2017/data/dataset_description.pdf",
    "docs_url": "http://convai.io/2017/data/dataset_description.pdf",
    "github_url": null,
    "tags": [
      "chatbot",
      "human feedback",
      "dialogue quality"
    ],
    "best_for": "Learning LLM evaluation, chatbot quality assessment, and dialogue systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The ConvAI Dataset consists of 4,750 dialogues between humans and bots, enriched with feedback in the form of thumbs up/down and quality scores. This dataset can be utilized to analyze chatbot performance, improve dialogue systems, and assess user satisfaction with AI interactions.",
    "use_cases": [
      "Evaluating chatbot performance",
      "Improving dialogue generation models",
      "Analyzing user feedback on AI interactions"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the ConvAI Dataset?",
      "How can I analyze chatbot dialogues?",
      "What feedback is included in the ConvAI Dataset?",
      "What are the quality scores in the ConvAI Dataset?",
      "How many dialogues are in the ConvAI Dataset?",
      "What types of analyses can be performed with the ConvAI Dataset?"
    ],
    "domain_tags": [
      "AI"
    ],
    "data_modality": "text",
    "size_category": "medium",
    "model_score": 0.0013,
    "embedding_text": "The ConvAI Dataset is a rich resource for researchers and developers working in the field of artificial intelligence and natural language processing. It comprises 4,750 human-to-bot dialogues, which are structured to include both thumbs up/down feedback and quality scores, providing a comprehensive view of user interactions with chatbot systems. The dialogues are formatted in a tabular structure, where each row represents a unique interaction, and the columns capture various attributes such as the dialogue text, user feedback, and quality metrics. This structured approach allows for easy manipulation and analysis using data processing tools such as pandas. The dataset was collected through interactions with chatbot systems designed to simulate human-like conversations, ensuring a diverse range of topics and dialogue styles. However, it is important to note that the dataset may have limitations regarding the representativeness of the dialogues, as they are influenced by the specific design of the chatbot and the contexts in which they were deployed. Researchers can leverage this dataset to explore various research questions, such as the effectiveness of different dialogue strategies, the impact of user feedback on chatbot performance, and the overall quality of AI-generated dialogues. Common preprocessing steps may include text normalization, tokenization, and the removal of irrelevant or redundant information. The dataset supports a variety of analytical approaches, including regression analysis, machine learning model training, and descriptive statistics, making it a versatile tool for advancing the understanding of human-computer interaction. Researchers typically utilize the ConvAI Dataset in studies aimed at enhancing the capabilities of conversational agents, improving user experience, and contributing to the broader field of natural language processing.",
    "tfidf_keywords": [
      "dialogue-system",
      "user-feedback",
      "quality-scores",
      "chatbot-performance",
      "natural-language-processing",
      "human-computer-interaction",
      "data-collection-methodology",
      "text-normalization",
      "machine-learning",
      "conversational-agents"
    ],
    "semantic_cluster": "nlp-for-economics",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "natural-language-processing",
      "user-experience",
      "machine-learning",
      "human-computer-interaction",
      "dialogue-systems"
    ],
    "canonical_topics": [
      "natural-language-processing",
      "machine-learning",
      "consumer-behavior"
    ]
  },
  {
    "name": "CoAID COVID Misinformation",
    "description": "4,251 news articles and 296K claims about COVID-19 healthcare misinformation. Fact-checked with ground truth labels",
    "category": "Content Moderation",
    "url": "https://github.com/cuilimeng/CoAID",
    "docs_url": null,
    "github_url": "https://github.com/cuilimeng/CoAID",
    "tags": [
      "COVID-19",
      "misinformation",
      "fact-checking",
      "healthcare",
      "fake news"
    ],
    "best_for": "Learning content moderation analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "healthcare",
      "misinformation",
      "fact-checking"
    ],
    "summary": "The CoAID COVID Misinformation dataset contains 4,251 news articles and 296,000 claims related to COVID-19 healthcare misinformation, all fact-checked with ground truth labels. Researchers can utilize this dataset to analyze misinformation trends, evaluate fact-checking effectiveness, and understand public perception of COVID-19 healthcare information.",
    "use_cases": [
      "Analyzing the prevalence of COVID-19 misinformation in media.",
      "Evaluating the effectiveness of fact-checking initiatives.",
      "Studying the impact of misinformation on public health behavior.",
      "Understanding the types of claims made about COVID-19 healthcare."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the CoAID COVID Misinformation dataset?",
      "How can I access the CoAID dataset for COVID-19 misinformation?",
      "What types of claims are included in the CoAID COVID Misinformation dataset?",
      "How is the CoAID dataset fact-checked?",
      "What research can be conducted using the CoAID COVID Misinformation dataset?",
      "What are the key variables in the CoAID COVID Misinformation dataset?",
      "What is the size of the CoAID COVID Misinformation dataset?",
      "What topics does the CoAID COVID Misinformation dataset cover?"
    ],
    "domain_tags": [
      "healthcare"
    ],
    "data_modality": "text",
    "size_category": "medium",
    "model_score": 0.0013,
    "image_url": "/images/datasets/coaid-covid-misinformation.png",
    "embedding_text": "The CoAID COVID Misinformation dataset is a comprehensive collection of 4,251 news articles and 296,000 claims that pertain to healthcare misinformation surrounding COVID-19. This dataset is particularly valuable for researchers and practitioners in the fields of public health, media studies, and misinformation analysis. The data structure consists of rows representing individual claims and articles, with columns detailing the claim text, associated article, fact-checking labels, and other relevant metadata. The collection methodology involved aggregating news articles from various reputable sources and systematically identifying claims that were subsequently fact-checked, ensuring a robust ground truth for analysis. Key variables in this dataset include the claim text, the source of the claim, the fact-checking outcome, and the context in which the claim was made. Researchers can leverage this dataset to address critical research questions such as the dynamics of misinformation spread, the effectiveness of different fact-checking strategies, and the relationship between misinformation and public health outcomes. Common preprocessing steps may include text normalization, entity recognition, and sentiment analysis to prepare the data for various analytical approaches. The dataset supports a range of analyses, including regression modeling, machine learning classification tasks, and descriptive statistics to uncover patterns in misinformation. Researchers typically use this dataset to conduct studies that inform public health communication strategies and enhance the understanding of misinformation's impact on society.",
    "tfidf_keywords": [
      "COVID-19",
      "misinformation",
      "fact-checking",
      "healthcare",
      "news articles",
      "claims",
      "ground truth",
      "public health",
      "media studies",
      "data analysis",
      "public perception",
      "information dissemination",
      "misinformation trends",
      "fact-checking effectiveness"
    ],
    "semantic_cluster": "misinformation-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "public health",
      "media literacy",
      "information dissemination",
      "data analysis",
      "health communication"
    ],
    "canonical_topics": [
      "healthcare",
      "natural-language-processing",
      "consumer-behavior"
    ]
  },
  {
    "name": "MobileRec",
    "description": "19.3M user reviews from 700K users across 10K apps in 48 categories. Google Play app recommendation research",
    "category": "App Stores",
    "url": "https://github.com/mhmaqbool/mobilerec",
    "docs_url": null,
    "github_url": "https://github.com/mhmaqbool/mobilerec",
    "tags": [
      "mobile apps",
      "reviews",
      "Google Play",
      "recommendations",
      "app store"
    ],
    "best_for": "Learning app stores analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "consumer-behavior",
      "recommendation-systems"
    ],
    "summary": "The MobileRec dataset comprises 19.3 million user reviews from 700,000 users across 10,000 apps in 48 categories, specifically focused on Google Play app recommendations. This dataset can be utilized to analyze user sentiment, app performance, and trends in mobile app usage, providing valuable insights for developers and researchers in the app ecosystem.",
    "use_cases": [
      "Sentiment analysis of user reviews to gauge app performance.",
      "Trend analysis of user preferences across different app categories.",
      "Development of recommendation systems based on user feedback.",
      "Comparative analysis of app ratings to identify successful features."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What insights can be gained from analyzing user reviews in MobileRec?",
      "How do user ratings correlate with app categories in MobileRec?",
      "What trends can be identified in mobile app recommendations using MobileRec?",
      "How can sentiment analysis be applied to the reviews in MobileRec?",
      "What are the most common themes in user feedback for mobile apps?",
      "How does user engagement vary across different app categories in MobileRec?",
      "What factors influence user ratings in the MobileRec dataset?",
      "How can MobileRec be used to improve app recommendation algorithms?"
    ],
    "domain_tags": [
      "technology",
      "mobile-apps"
    ],
    "data_modality": "text",
    "size_category": "massive",
    "model_score": 0.0013,
    "image_url": "/images/datasets/mobilerec.png",
    "embedding_text": "The MobileRec dataset is a comprehensive collection of user reviews gathered from Google Play, featuring a total of 19.3 million reviews contributed by approximately 700,000 users. This dataset encompasses a wide array of 10,000 apps spanning 48 distinct categories, making it an invaluable resource for researchers and developers interested in the mobile app ecosystem. The data structure consists of rows representing individual user reviews and columns that include variables such as user ID, app ID, review text, rating, and category. The reviews provide qualitative insights into user experiences, preferences, and sentiments regarding various mobile applications. The collection methodology involves scraping user-generated content from the Google Play platform, ensuring a rich and diverse dataset that reflects real-world user interactions. However, researchers should be aware of potential data quality issues, including biases in user feedback and the presence of spam reviews, which may affect the overall analysis. Common preprocessing steps include text cleaning, sentiment scoring, and categorization of reviews based on themes or topics. The dataset supports a variety of analyses, including regression analysis to examine the relationship between app features and user ratings, as well as machine learning techniques for predictive modeling and classification tasks. Researchers typically leverage MobileRec to explore research questions related to user behavior, app performance, and the effectiveness of recommendation systems, ultimately aiming to enhance the user experience in mobile applications.",
    "tfidf_keywords": [
      "user-reviews",
      "sentiment-analysis",
      "app-categories",
      "mobile-apps",
      "recommendation-systems",
      "user-engagement",
      "feedback-themes",
      "data-scraping",
      "qualitative-insights",
      "text-cleaning",
      "predictive-modeling",
      "user-behavior",
      "app-performance"
    ],
    "semantic_cluster": "mobile-app-recommendations",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "sentiment-analysis",
      "user-experience",
      "machine-learning",
      "data-mining",
      "text-analysis"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "recommendation-systems",
      "machine-learning",
      "data-engineering"
    ]
  },
  {
    "name": "KuaiSAR",
    "description": "5M search actions, 14.6M recommendation events from 25k users",
    "category": "Entertainment & Media",
    "url": "https://kuaisar.github.io/",
    "docs_url": "https://kuaisar.github.io/",
    "github_url": null,
    "tags": [
      "search",
      "recommendations",
      "video",
      "Kuaishou"
    ],
    "best_for": "Learning entertainment & media analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "consumer-behavior",
      "recommendation-systems"
    ],
    "summary": "The KuaiSAR dataset contains 5 million search actions and 14.6 million recommendation events from 25,000 users, providing insights into user interactions within the entertainment and media sector. Researchers can analyze user behavior, optimize recommendation algorithms, and study search patterns.",
    "use_cases": [
      "Analyzing user search behavior to improve recommendation algorithms.",
      "Studying the effectiveness of different types of recommendations in media consumption.",
      "Exploring patterns in user engagement with search and recommendation features.",
      "Evaluating the impact of user demographics on search and recommendation interactions."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the search actions recorded in the KuaiSAR dataset?",
      "How can I analyze recommendation events from KuaiSAR?",
      "What insights can be gained from user interactions in KuaiSAR?",
      "What is the size of the KuaiSAR dataset?",
      "How many users contributed to the KuaiSAR dataset?",
      "What types of recommendations are included in KuaiSAR?",
      "What search patterns can be identified in the KuaiSAR dataset?",
      "How does KuaiSAR help in understanding user behavior?"
    ],
    "domain_tags": [
      "entertainment",
      "media"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0013,
    "image_url": "/images/logos/github.png",
    "embedding_text": "The KuaiSAR dataset is a rich resource for researchers interested in understanding user interactions within the entertainment and media landscape. It comprises 5 million search actions and 14.6 million recommendation events gathered from a diverse pool of 25,000 users. This dataset is structured in a tabular format, where each row represents an individual search action or recommendation event, and columns include variables such as user ID, event type, timestamp, and content ID. The collection methodology involves tracking user interactions on the Kuaishou platform, a popular short video app, providing a comprehensive view of user behavior in a digital environment. The dataset captures a wide range of user interactions, making it suitable for various analyses, including regression modeling, machine learning applications, and descriptive statistics. Key variables in the dataset measure user engagement, search frequency, and recommendation success rates. However, researchers should be aware of potential limitations, such as data sparsity for less popular content and the need for preprocessing steps like normalization and handling missing values. Common research questions that can be addressed using this dataset include understanding the factors influencing user search behavior, evaluating the effectiveness of recommendation algorithms, and exploring the relationship between search actions and subsequent recommendations. The dataset supports a variety of analyses, allowing researchers to derive insights into user preferences, optimize content delivery, and enhance the overall user experience on digital platforms. Typically, researchers utilize this dataset to conduct experiments on recommendation systems, analyze consumer behavior, and develop strategies for improving user engagement in the entertainment sector.",
    "tfidf_keywords": [
      "user-interactions",
      "recommendation-algorithms",
      "search-patterns",
      "engagement-analysis",
      "content-consumption",
      "behavioral-insights",
      "digital-platforms",
      "user-behavior",
      "data-collection-methodology",
      "entertainment-media"
    ],
    "semantic_cluster": "user-behavior-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "user-engagement",
      "recommendation-systems",
      "search-engine-optimization",
      "data-analysis",
      "consumer-behavior"
    ],
    "canonical_topics": [
      "recommendation-systems",
      "consumer-behavior",
      "machine-learning"
    ]
  },
  {
    "name": "Upworthy News Headlines",
    "description": "32,487 headline/image experiments on 538M assignments",
    "category": "Advertising",
    "url": "https://upworthy.natematias.com/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "A/B testing",
      "headlines",
      "media",
      "experimentation"
    ],
    "best_for": "Learning advertising analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "media",
      "experimentation",
      "A/B testing"
    ],
    "summary": "The Upworthy News Headlines dataset consists of 32,487 experiments involving headlines and images across 538 million assignments. This dataset allows researchers to analyze the effectiveness of different headlines in capturing audience attention and engagement, facilitating insights into media strategies and consumer behavior.",
    "use_cases": [
      "Analyzing the impact of headlines on user engagement metrics.",
      "Comparing the effectiveness of different visual elements in media.",
      "Conducting A/B tests to optimize content for target audiences."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the most effective headlines in media?",
      "How does image choice affect headline performance?",
      "What patterns can be identified in A/B testing results?",
      "How can this dataset inform advertising strategies?",
      "What variables are most predictive of engagement in headlines?",
      "How do different demographics respond to various headlines?"
    ],
    "domain_tags": [
      "advertising"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0012,
    "image_url": "/images/datasets/upworthy-news-headlines.png",
    "embedding_text": "The Upworthy News Headlines dataset is a comprehensive collection of 32,487 experiments focused on the effectiveness of various headlines and images in media content. Each entry in the dataset corresponds to a unique experiment where different combinations of headlines and images were tested across a staggering 538 million assignments. The data is structured in a tabular format, with rows representing individual experiments and columns capturing key variables such as headline text, image used, engagement metrics, and demographic information of the audience. This dataset is particularly valuable for researchers and practitioners interested in the fields of advertising, media studies, and consumer behavior, as it provides rich insights into how different elements of content can influence audience engagement. The collection methodology involved systematic A/B testing, where variations of headlines and images were presented to users, and their interactions were tracked to measure performance. Key variables in the dataset include headline text, image attributes, engagement rates, and user demographics, which together allow for a multifaceted analysis of media effectiveness. While the dataset is extensive, researchers should be aware of potential limitations such as the lack of contextual information surrounding each experiment, which may influence the interpretation of results. Common preprocessing steps may include text normalization of headlines, categorization of images, and the handling of missing engagement data. Researchers can utilize this dataset to address a variety of questions, such as identifying which headlines yield the highest engagement rates or how different demographic groups respond to specific content types. The dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it a versatile resource for both academic and commercial research. Overall, the Upworthy News Headlines dataset serves as a powerful tool for understanding the dynamics of media content and its impact on audience behavior.",
    "tfidf_keywords": [
      "A/B testing",
      "headline effectiveness",
      "media engagement",
      "consumer behavior",
      "advertising strategies",
      "content optimization",
      "visual elements",
      "user demographics",
      "engagement metrics",
      "experiment design"
    ],
    "semantic_cluster": "media-experimentation",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "A/B testing",
      "user engagement",
      "media strategy",
      "content analysis",
      "advertising effectiveness"
    ],
    "canonical_topics": [
      "experimentation",
      "consumer-behavior",
      "advertising"
    ]
  },
  {
    "name": "Crypto Art (SuperRare)",
    "description": "Bids and transactions from SuperRare NFT platform",
    "category": "Auctions & Marketplaces",
    "url": "https://www.kaggle.com/datasets/franceschet/superrare",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "NFT",
      "crypto",
      "art",
      "auctions"
    ],
    "best_for": "Learning auctions & marketplaces analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Crypto Art (SuperRare) dataset contains bids and transactions from the SuperRare NFT platform, providing insights into the evolving market for digital art. Researchers can analyze bidding patterns, transaction volumes, and market trends to understand consumer behavior and pricing dynamics in the NFT space.",
    "use_cases": [
      "Analyzing bidding patterns to identify trends in consumer behavior.",
      "Examining the relationship between bid amounts and final sale prices.",
      "Investigating the impact of market events on transaction volumes.",
      "Studying the demographics of buyers and sellers in the NFT space."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the transaction trends on the SuperRare NFT platform?",
      "How do bids fluctuate over time for digital art?",
      "What factors influence the pricing of NFTs on SuperRare?",
      "Can we analyze the bidding behavior of users in the crypto art market?",
      "What is the volume of transactions on the SuperRare platform?",
      "How does the NFT market compare to traditional art auctions?"
    ],
    "domain_tags": [
      "art",
      "technology"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0011,
    "image_url": "/images/datasets/crypto-art-superrare.png",
    "embedding_text": "The Crypto Art (SuperRare) dataset is a comprehensive collection of bids and transactions from the SuperRare NFT platform, which specializes in the sale of unique digital art pieces as non-fungible tokens (NFTs). This dataset is structured in a tabular format, containing rows that represent individual transactions and bids, with columns capturing key variables such as transaction ID, bid amount, timestamp, artist information, and the specific NFT being auctioned. The collection methodology involves aggregating data directly from the SuperRare platform, ensuring that the dataset reflects real-time market dynamics and user interactions within the NFT space. While the dataset does not explicitly mention temporal or geographic coverage, it encompasses a range of transactions that can be analyzed to uncover trends over time and across different types of artwork. Key variables in this dataset include bid amounts, which measure the monetary value users are willing to pay for NFTs, and timestamps, which allow for the analysis of bidding patterns over specific periods. However, researchers should be aware of potential limitations in data quality, such as missing values or inconsistencies in user-reported information. Common preprocessing steps may include cleaning the data to handle missing entries, normalizing bid amounts for inflation, and categorizing NFTs based on artistic style or medium. This dataset supports a variety of research questions, including the examination of factors influencing bid amounts, the identification of trends in NFT sales, and the exploration of user behavior in online auctions. Analysts can employ various methods, including regression analysis, machine learning techniques, and descriptive statistics, to derive insights from the data. Researchers typically use this dataset to study market dynamics, consumer behavior in digital art transactions, and the broader implications of NFTs on the art industry.",
    "tfidf_keywords": [
      "non-fungible tokens",
      "digital art",
      "bidding patterns",
      "transaction volume",
      "market trends",
      "crypto art",
      "SuperRare",
      "NFT pricing",
      "consumer behavior",
      "auction dynamics"
    ],
    "semantic_cluster": "nft-market-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "marketplaces",
      "consumer-behavior",
      "pricing",
      "digital-economy",
      "art-market"
    ],
    "canonical_topics": [
      "marketplaces",
      "consumer-behavior",
      "pricing",
      "econometrics",
      "finance"
    ]
  },
  {
    "name": "FCC Spectrum Auctions",
    "description": "87+ auctions (1994-present) with round-by-round bidding data. Complete bid histories, reserve prices, winners. Auction theory empirics",
    "category": "Auctions & Marketplaces",
    "url": "https://www.fcc.gov/auctions-summary",
    "docs_url": "https://www.fcc.gov/auction/",
    "github_url": null,
    "tags": [
      "spectrum auctions",
      "FCC",
      "bidding",
      "auction theory",
      "telecommunications"
    ],
    "best_for": "Learning auctions & marketplaces analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The FCC Spectrum Auctions dataset includes comprehensive round-by-round bidding data from over 87 auctions conducted by the FCC since 1994. Researchers can analyze bidding behaviors, reserve prices, and auction outcomes to study auction theory and market dynamics in telecommunications.",
    "use_cases": [
      "Analyzing the impact of reserve prices on bidding behavior.",
      "Studying the evolution of auction strategies over time.",
      "Evaluating the effectiveness of auction formats in telecommunications."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the bidding patterns in FCC spectrum auctions?",
      "How do reserve prices affect auction outcomes?",
      "What insights can be drawn from the complete bid histories of FCC auctions?",
      "How has auction theory been applied in the context of telecommunications?",
      "What are the trends in FCC spectrum auctions from 1994 to present?",
      "How do winners of FCC auctions compare in terms of bidding strategies?"
    ],
    "domain_tags": [
      "telecommunications"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1994-present",
    "size_category": "medium",
    "model_score": 0.0011,
    "image_url": "/images/datasets/fcc-spectrum-auctions.jpg",
    "embedding_text": "The FCC Spectrum Auctions dataset provides a detailed account of over 87 auctions conducted by the Federal Communications Commission (FCC) from 1994 to the present, featuring round-by-round bidding data. This dataset is structured in a tabular format, where each row represents a unique bidding event across various auctions, and columns include key variables such as auction ID, bidder ID, bid amount, round number, and reserve prices. The collection methodology involves aggregating data from official FCC records and auction reports, ensuring a comprehensive view of the bidding landscape in the telecommunications sector. The dataset covers a significant temporal span, allowing researchers to analyze trends and changes in auction dynamics over nearly three decades. Key variables such as bid amounts and reserve prices measure the competitive landscape and economic implications of spectrum allocation. However, researchers should be aware of potential limitations, including the variability in auction formats and the influence of regulatory changes over time, which may affect comparability across different auctions. Common preprocessing steps include cleaning bid data, normalizing bid amounts for inflation, and categorizing bidders based on their characteristics. This dataset supports a variety of research questions, such as examining the relationship between reserve prices and bidding behavior, understanding the strategic interactions among bidders, and evaluating the overall efficiency of auction outcomes. Analysts can employ various methodologies, including regression analysis, machine learning techniques, and descriptive statistics, to extract insights from the data. Researchers typically utilize this dataset to inform studies on auction theory, market design, and the economic implications of spectrum allocation in telecommunications, contributing to a deeper understanding of competitive bidding processes and their impact on market efficiency.",
    "tfidf_keywords": [
      "spectrum auctions",
      "FCC",
      "bidding strategies",
      "reserve prices",
      "auction theory",
      "telecommunications",
      "bid histories",
      "market dynamics",
      "bidding behavior",
      "auction formats",
      "economic implications",
      "competitive landscape",
      "data aggregation",
      "regulatory changes"
    ],
    "semantic_cluster": "auction-theory-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "auction theory",
      "market design",
      "bidding strategies",
      "economic analysis",
      "telecommunications policy"
    ],
    "canonical_topics": [
      "econometrics",
      "marketplaces",
      "pricing",
      "consumer-behavior",
      "finance"
    ]
  },
  {
    "name": "EU TED Procurement",
    "description": "800K+ procurement notices annually. All EU public contracts above thresholds. Structured XML since 2006. Cross-country procurement research",
    "category": "Auctions & Marketplaces",
    "url": "https://ted.europa.eu/TED/browse/browseByMap.do",
    "docs_url": "https://simap.ted.europa.eu/web/simap/standard-forms-for-public-procurement",
    "github_url": null,
    "tags": [
      "EU",
      "procurement",
      "tenders",
      "government",
      "cross-country"
    ],
    "best_for": "Learning auctions & marketplaces analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "procurement",
      "government",
      "tenders"
    ],
    "summary": "The EU TED Procurement dataset contains over 800,000 procurement notices annually, encompassing all EU public contracts above specified thresholds. It provides a structured XML format since 2006, facilitating cross-country procurement research and analysis.",
    "use_cases": [
      "Analyzing trends in public procurement across EU member states.",
      "Comparative studies of procurement practices in different countries.",
      "Evaluating the impact of government contracts on local economies.",
      "Researching compliance and transparency in public procurement."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the latest EU procurement notices?",
      "How can I analyze EU public contracts?",
      "What trends can be identified in EU tenders?",
      "What are the thresholds for EU public contracts?",
      "How does procurement vary across EU countries?",
      "What are the common types of government contracts in the EU?",
      "How can I access structured XML data for EU procurement?",
      "What research has been conducted on EU procurement practices?"
    ],
    "domain_tags": [
      "government",
      "public-sector"
    ],
    "data_modality": "mixed",
    "temporal_coverage": "2006-present",
    "geographic_scope": "European Union",
    "size_category": "massive",
    "model_score": 0.0011,
    "image_url": "/images/logos/europa.png",
    "embedding_text": "The EU TED Procurement dataset is a comprehensive collection of procurement notices that exceeds 800,000 entries annually, representing all public contracts within the European Union that surpass specific monetary thresholds. This dataset has been structured in XML format since 2006, making it an invaluable resource for researchers and practitioners interested in public procurement dynamics across Europe. The data structure includes various key variables such as contract values, issuing authorities, tender descriptions, and award details, which facilitate in-depth analysis of procurement trends and practices. Researchers typically utilize this dataset to explore questions related to the efficiency and transparency of public procurement processes, assess compliance with EU regulations, and conduct cross-country comparisons of procurement strategies. The dataset's collection methodology relies on official public announcements and submissions from EU member states, ensuring a high level of data integrity and reliability. However, users should be aware of potential limitations, such as variations in reporting practices across countries and the challenges of data completeness. Common preprocessing steps may include data cleaning, normalization of contract values, and the extraction of relevant features for analysis. The dataset supports a variety of analytical approaches, including regression analysis, machine learning models, and descriptive statistics, making it suitable for both exploratory and confirmatory research.",
    "tfidf_keywords": [
      "procurement-notices",
      "EU-public-contracts",
      "structured-XML",
      "cross-country-research",
      "government-tenders",
      "contract-thresholds",
      "data-integrity",
      "transparency",
      "public-sector-analysis",
      "compliance-evaluation"
    ],
    "semantic_cluster": "public-procurement-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "public-administration",
      "contract-management",
      "policy-evaluation",
      "economic-impact-analysis",
      "data-visualization"
    ],
    "canonical_topics": [
      "policy-evaluation",
      "econometrics",
      "marketplaces"
    ]
  },
  {
    "name": "Twitch Gamers Social Network",
    "description": "168K nodes with mutual follower relationships. 6 ML tasks including churn, affiliate status, view count prediction",
    "category": "Entertainment & Media",
    "url": "https://snap.stanford.edu/data/twitch-social-networks.html",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Twitch",
      "social network",
      "gaming",
      "followers",
      "SNAP"
    ],
    "best_for": "Learning entertainment & media analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "social-network-analysis",
      "machine-learning",
      "gaming"
    ],
    "summary": "The Twitch Gamers Social Network dataset consists of 168,000 nodes representing users and their mutual follower relationships. This dataset can be utilized for various machine learning tasks such as predicting user churn, affiliate status, and view counts, providing insights into user interactions within the gaming community.",
    "use_cases": [
      "Predicting user churn in gaming platforms",
      "Analyzing follower dynamics in social networks",
      "Understanding affiliate marketing impacts in gaming",
      "Forecasting view counts based on social interactions"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the mutual follower relationships in the Twitch Gamers Social Network?",
      "How can I predict user churn using the Twitch dataset?",
      "What machine learning tasks can be performed with Twitch social network data?",
      "How do followers interact within the Twitch gaming community?",
      "What insights can be derived from analyzing Twitch gamers' social networks?",
      "Can I use this dataset to study affiliate marketing in gaming?",
      "What are the key variables in the Twitch Gamers Social Network dataset?",
      "How large is the dataset of Twitch gamers?"
    ],
    "domain_tags": [
      "entertainment",
      "media"
    ],
    "data_modality": "graph",
    "size_category": "medium",
    "model_score": 0.001,
    "image_url": "/images/logos/stanford.png",
    "embedding_text": "The Twitch Gamers Social Network dataset is a rich resource for understanding the dynamics of social interactions among gamers on the Twitch platform. Comprising 168,000 nodes, each representing a user, the dataset captures mutual follower relationships, providing a comprehensive view of how users connect and interact within this vibrant community. The data structure is primarily graph-based, where nodes represent individual gamers and edges denote follower relationships. This allows for various analyses, including social network analysis and machine learning applications. Key variables in the dataset include user IDs, follower counts, and interaction metrics, which can be leveraged to explore patterns of engagement and influence among users. Researchers can utilize this dataset to address critical questions related to user behavior, such as predicting churn rates, assessing affiliate marketing effectiveness, and forecasting view counts based on social interactions. Common preprocessing steps may involve cleaning the data to remove inactive users, normalizing follower counts, and transforming the data into a suitable format for machine learning algorithms. The dataset supports a range of analytical techniques, including regression analysis, machine learning classification tasks, and descriptive statistics. Researchers typically employ this dataset to gain insights into user engagement strategies, optimize content delivery, and enhance user retention on gaming platforms. However, it is essential to consider potential limitations, such as data quality issues arising from inactive accounts or incomplete follower relationships, which may affect the robustness of analyses conducted using this dataset.",
    "tfidf_keywords": [
      "mutual-follower-relationships",
      "user-churn-prediction",
      "affiliate-marketing",
      "social-network-analysis",
      "view-count-forecasting",
      "gaming-community-dynamics",
      "interaction-metrics",
      "graph-data-structure",
      "machine-learning-tasks",
      "user-engagement-strategies"
    ],
    "semantic_cluster": "social-network-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "social-network-theory",
      "machine-learning",
      "user-behavior-analysis",
      "predictive-modeling",
      "data-mining"
    ],
    "canonical_topics": [
      "machine-learning",
      "consumer-behavior",
      "social-network-analysis"
    ],
    "benchmark_usage": [
      "Predicting churn, affiliate status, view count"
    ]
  },
  {
    "name": "Soso (KDD Cup 2012)",
    "description": "KDD Cup 2012 Track 2 for sponsored search CTR prediction",
    "category": "Advertising",
    "url": "https://www.kaggle.com/competitions/kddcup2012-track2",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "sponsored search",
      "KDD",
      "CTR"
    ],
    "best_for": "Learning advertising analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Soso dataset from the KDD Cup 2012 is designed for predicting click-through rates (CTR) in sponsored search advertising. It provides a rich set of features that can be utilized to build predictive models for understanding user behavior and optimizing ad placements.",
    "use_cases": [
      "Predicting click-through rates for ads",
      "Analyzing user engagement with sponsored search",
      "Optimizing ad bidding strategies"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the KDD Cup 2012 Soso dataset?",
      "How can I access the Soso dataset for CTR prediction?",
      "What features are included in the Soso dataset?",
      "What research has been conducted using the KDD Cup 2012 data?",
      "How is the Soso dataset structured?",
      "What methodologies are suitable for analyzing the Soso dataset?",
      "What are the limitations of the Soso dataset?",
      "How can I apply machine learning to the Soso dataset?"
    ],
    "domain_tags": [
      "advertising"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.001,
    "image_url": "/images/datasets/soso-kdd-cup-2012.png",
    "embedding_text": "The Soso dataset from the KDD Cup 2012 is a comprehensive resource aimed at advancing research in sponsored search advertising, particularly focusing on click-through rate (CTR) prediction. This dataset consists of a structured tabular format with numerous rows and columns, each representing various features relevant to ad impressions and user interactions. The data schema includes variables such as user demographics, ad characteristics, and contextual information about the search queries. The collection methodology employed for this dataset involved aggregating data from real-world sponsored search campaigns, ensuring a diverse representation of user behavior across different contexts. While the dataset does not explicitly mention temporal or geographic coverage, it is assumed to reflect a broad range of user interactions over the period of the competition. Key variables in the dataset measure aspects such as user engagement, ad effectiveness, and contextual relevance, providing valuable insights into user preferences and behavior patterns. However, researchers should be aware of potential data quality issues, including missing values and biases inherent in the data collection process. Common preprocessing steps may involve handling missing data, normalizing features, and encoding categorical variables to prepare the dataset for analysis. The Soso dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it a versatile tool for researchers and practitioners in the field of advertising and digital marketing. Researchers typically use this dataset to explore questions related to user engagement, ad performance, and the effectiveness of different advertising strategies, contributing to the broader understanding of consumer behavior in digital environments.",
    "tfidf_keywords": [
      "click-through-rate",
      "sponsored-search",
      "ad-impressions",
      "user-engagement",
      "predictive-modeling",
      "feature-engineering",
      "data-preprocessing",
      "regression-analysis",
      "machine-learning",
      "advertising-strategies"
    ],
    "semantic_cluster": "ctr-prediction-methods",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "predictive-modeling",
      "user-engagement",
      "ad-performance",
      "feature-engineering",
      "machine-learning"
    ],
    "canonical_topics": [
      "machine-learning",
      "consumer-behavior",
      "advertising"
    ]
  },
  {
    "name": "Alibaba Ads Dataset",
    "description": "Advertising dataset from Alibaba for ad targeting and prediction",
    "category": "Advertising",
    "url": "https://tianchi.aliyun.com/dataset/148347",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "advertising",
      "targeting",
      "Alibaba"
    ],
    "best_for": "Learning advertising analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "advertising"
    ],
    "summary": "The Alibaba Ads Dataset is a comprehensive collection of advertising data specifically designed for ad targeting and prediction. Researchers and practitioners can utilize this dataset to analyze advertising effectiveness, optimize targeting strategies, and enhance prediction models for consumer behavior.",
    "use_cases": [
      "Analyzing the effectiveness of advertising campaigns",
      "Optimizing ad targeting strategies",
      "Predicting consumer responses to advertisements"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Alibaba Ads Dataset?",
      "How can I use the Alibaba Ads Dataset for ad targeting?",
      "What insights can be gained from the Alibaba Ads Dataset?",
      "What variables are included in the Alibaba Ads Dataset?",
      "How does advertising data from Alibaba influence consumer behavior?",
      "What prediction models can be applied to the Alibaba Ads Dataset?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.001,
    "embedding_text": "The Alibaba Ads Dataset is a rich advertising dataset that provides insights into ad targeting and prediction strategies within the e-commerce sector. It consists of a structured tabular format, containing various rows and columns that represent different advertising campaigns, their respective performance metrics, and consumer interactions. The dataset is collected from Alibaba's advertising platform, capturing a wide range of variables that measure ad impressions, clicks, conversions, and user demographics. This dataset is particularly valuable for researchers and data scientists interested in understanding the dynamics of online advertising and consumer behavior. The key variables typically include ad ID, campaign ID, impressions, clicks, conversion rates, and user demographic information, which together allow for a comprehensive analysis of advertising effectiveness. However, users should be aware of potential limitations such as data sparsity in certain campaigns and the need for preprocessing steps like normalization and handling missing values. Common research questions that can be addressed using this dataset include evaluating the impact of different advertising strategies on consumer engagement and predicting the likelihood of conversion based on user characteristics. The dataset supports various types of analyses, including regression analysis, machine learning models, and descriptive statistics, making it a versatile tool for both academic research and practical applications in marketing. Researchers typically use this dataset to build predictive models, conduct A/B testing, and derive insights that inform advertising strategies, ultimately enhancing the effectiveness of online marketing efforts.",
    "tfidf_keywords": [
      "ad targeting",
      "consumer behavior",
      "advertising effectiveness",
      "conversion rates",
      "online marketing",
      "data preprocessing",
      "predictive modeling",
      "e-commerce analytics",
      "user demographics",
      "advertising campaigns"
    ],
    "semantic_cluster": "advertising-prediction",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "machine-learning",
      "consumer-behavior",
      "advertising-strategies",
      "data-preprocessing",
      "predictive-analytics"
    ],
    "canonical_topics": [
      "advertising",
      "consumer-behavior",
      "machine-learning"
    ]
  },
  {
    "name": "Netflix Prize",
    "description": "100M+ anonymous movie ratings from 480k users",
    "category": "Entertainment & Media",
    "url": "https://www.kaggle.com/datasets/netflix-inc/netflix-prize-data",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "movies",
      "ratings",
      "recommendations",
      "classic"
    ],
    "best_for": "Learning entertainment & media analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "recommendation-systems",
      "consumer-behavior"
    ],
    "summary": "The Netflix Prize dataset consists of over 100 million anonymous movie ratings provided by approximately 480,000 users. This rich dataset can be utilized to develop and test recommendation algorithms, analyze user preferences, and explore trends in movie ratings.",
    "use_cases": [
      "Building recommendation systems",
      "Analyzing user rating patterns",
      "Exploring trends in movie preferences"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the Netflix Prize dataset?",
      "How can I analyze movie ratings from the Netflix Prize?",
      "What algorithms can be tested on the Netflix Prize dataset?",
      "What insights can be gained from user ratings in the Netflix Prize?",
      "How do user preferences vary in the Netflix Prize dataset?",
      "What are the challenges in analyzing the Netflix Prize dataset?"
    ],
    "domain_tags": [
      "entertainment",
      "media"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.001,
    "image_url": "/images/datasets/netflix-prize.jpg",
    "embedding_text": "The Netflix Prize dataset is a comprehensive collection of over 100 million anonymous movie ratings from approximately 480,000 users, making it a significant resource for researchers and practitioners in the fields of data science and machine learning. The dataset is structured in a tabular format, where each row represents a unique rating given by a user to a specific movie. Key variables in the dataset include user IDs, movie IDs, and the corresponding ratings, which typically range from 1 to 5 stars. The collection methodology involved gathering ratings from users who opted to participate in the Netflix Prize competition, which aimed to improve the company's recommendation algorithms. While the dataset provides a wealth of information, it is important to note that it contains anonymous ratings, which limits demographic insights about the users. Researchers often preprocess the data by handling missing values, normalizing ratings, and splitting the dataset into training and testing subsets for model evaluation. The Netflix Prize dataset supports various types of analyses, including regression modeling, machine learning techniques, and descriptive statistics. Common research questions addressed using this dataset include understanding user preferences, identifying trends in movie ratings over time, and evaluating the effectiveness of different recommendation strategies. Researchers typically use this dataset to benchmark their algorithms against established methods, making it a critical resource for advancing the field of recommendation systems.",
    "tfidf_keywords": [
      "collaborative-filtering",
      "matrix-factorization",
      "user-item-interaction",
      "implicit-feedback",
      "rating-prediction",
      "recommendation-engine",
      "data-sparsity",
      "evaluation-metrics",
      "cold-start-problem",
      "user-segmentation"
    ],
    "semantic_cluster": "recommendation-systems",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "machine-learning",
      "user-experience",
      "data-mining",
      "behavioral-economics",
      "predictive-analytics"
    ],
    "canonical_topics": [
      "recommendation-systems",
      "consumer-behavior",
      "machine-learning"
    ],
    "benchmark_usage": [
      "Testing recommendation algorithms",
      "Evaluating user behavior"
    ]
  },
  {
    "name": "MicroLens",
    "description": "1 billion interactions from 34 million users on 1 million micro-videos",
    "category": "Entertainment & Media",
    "url": "https://github.com/westlake-repl/MicroLens",
    "docs_url": null,
    "github_url": "https://github.com/westlake-repl/MicroLens",
    "tags": [
      "video",
      "micro-video",
      "large-scale",
      "recommendations"
    ],
    "best_for": "Learning entertainment & media analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "consumer-behavior",
      "recommendation-systems"
    ],
    "summary": "MicroLens is a dataset comprising 1 billion interactions from 34 million users on 1 million micro-videos. This extensive dataset allows researchers to analyze user engagement, develop recommendation algorithms, and study trends in micro-video consumption.",
    "use_cases": [
      "Analyzing user engagement metrics for micro-videos",
      "Developing machine learning models for video recommendations",
      "Studying consumer behavior in the context of video consumption",
      "Identifying trends in micro-video popularity over time"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the user engagement patterns in micro-videos?",
      "How can we improve recommendations for micro-video content?",
      "What demographic factors influence micro-video interactions?",
      "What trends can be identified in user interactions with micro-videos?",
      "How does user behavior vary across different micro-video genres?",
      "What is the impact of video length on user engagement?"
    ],
    "domain_tags": [
      "entertainment",
      "media"
    ],
    "data_modality": "tabular",
    "size_category": "massive",
    "model_score": 0.001,
    "image_url": "/images/datasets/microlens.png",
    "embedding_text": "The MicroLens dataset is a comprehensive collection of 1 billion interactions from 34 million users across 1 million micro-videos. This dataset is structured in a tabular format, where each row represents an interaction and includes key variables such as user ID, video ID, interaction type (like, comment, share), timestamp, and possibly demographic information about the users. The collection methodology is based on real-time tracking of user interactions on a micro-video platform, ensuring a rich dataset that captures diverse user behaviors and preferences. Due to the large scale of the dataset, it provides a unique opportunity to analyze trends and patterns in user engagement with micro-videos, making it valuable for researchers and practitioners in the fields of entertainment and media. Key variables in the dataset allow for the measurement of user engagement levels, the effectiveness of recommendations, and the popularity of specific video content. However, researchers should be aware of potential limitations, such as biases in user demographics or the influence of external factors on video interactions. Common preprocessing steps may include cleaning the data to remove duplicates, handling missing values, and normalizing interaction types for analysis. The dataset supports various types of analyses, including regression analysis to identify factors influencing user engagement, machine learning for developing recommendation systems, and descriptive statistics to summarize user behavior. Researchers typically utilize this dataset to address questions related to consumer behavior, the effectiveness of different video formats, and the dynamics of user engagement in the rapidly evolving landscape of micro-video content.",
    "tfidf_keywords": [
      "user-engagement",
      "micro-video",
      "recommendation-algorithms",
      "interaction-patterns",
      "consumer-behavior",
      "video-consumption",
      "data-analytics",
      "machine-learning",
      "trends-analysis",
      "demographic-factors",
      "video-popularity",
      "content-recommendation",
      "user-interactions",
      "engagement-metrics",
      "video-length"
    ],
    "semantic_cluster": "video-engagement-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "user-experience",
      "data-mining",
      "machine-learning",
      "behavioral-analysis",
      "content-strategy"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "recommendation-systems",
      "machine-learning",
      "data-engineering",
      "statistics"
    ]
  },
  {
    "name": "Tesco Grocery 1.0",
    "description": "Grocery purchases from Tesco stores via loyalty cards",
    "category": "Grocery & Supermarkets",
    "url": "https://figshare.com/collections/Tesco_Grocery_1_0/4769354",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "grocery",
      "loyalty cards",
      "UK",
      "Tesco"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Tesco Grocery 1.0 dataset contains detailed records of grocery purchases made at Tesco stores through loyalty cards. This dataset can be utilized to analyze consumer purchasing behavior, evaluate pricing strategies, and understand trends in grocery shopping within the UK market.",
    "use_cases": [
      "Analyzing consumer behavior based on loyalty card data",
      "Evaluating the effectiveness of promotional campaigns",
      "Identifying purchasing patterns over time",
      "Assessing the impact of pricing changes on sales"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the grocery purchasing trends at Tesco?",
      "How do loyalty cards influence consumer behavior in the UK?",
      "What products are most frequently purchased by Tesco loyalty card users?",
      "How can Tesco's sales data inform pricing strategies?",
      "What demographic insights can be drawn from Tesco loyalty card purchases?",
      "How does seasonality affect grocery purchases at Tesco?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "UK",
    "size_category": "medium",
    "model_score": 0.001,
    "image_url": "/images/logos/figshare.png",
    "embedding_text": "The Tesco Grocery 1.0 dataset is a rich source of information derived from grocery purchases made at Tesco stores through loyalty cards. This dataset typically consists of rows representing individual transactions, with columns capturing various attributes such as transaction date, product identifiers, quantities purchased, prices, and customer identifiers. The collection methodology involves aggregating data from loyalty card transactions, which provides insights into consumer purchasing behavior and preferences. Although the dataset is focused on the UK market, it offers a comprehensive view of grocery shopping patterns among Tesco customers. Key variables in the dataset include transaction date, product categories, and purchase amounts, which allow researchers to measure trends in consumer spending and product popularity. However, data quality may be affected by factors such as incomplete customer profiles or variations in loyalty card usage across different demographics. Common preprocessing steps may include data cleaning to handle missing values, normalization of product identifiers, and aggregation of transactions by time periods for analysis. Researchers can leverage this dataset to address questions related to consumer behavior, pricing strategies, and the effectiveness of marketing campaigns. The dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics. Typically, researchers use this data to conduct studies on consumer purchasing trends, assess the impact of loyalty programs, and evaluate the effectiveness of promotional strategies in the retail sector.",
    "tfidf_keywords": [
      "loyalty-cards",
      "consumer-purchasing-behavior",
      "grocery-shopping",
      "pricing-strategies",
      "transaction-data",
      "retail-analytics",
      "promotional-campaigns",
      "seasonality",
      "product-popularity",
      "data-preprocessing"
    ],
    "semantic_cluster": "consumer-behavior-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "retail-analytics",
      "pricing-strategies",
      "promotional-analysis",
      "data-preprocessing"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "pricing",
      "experimentation"
    ]
  },
  {
    "name": "Ipinyou RTB",
    "description": "Real-time bidding (RTB) dataset for CTR prediction",
    "category": "Advertising",
    "url": "https://github.com/wnzhang/make-ipinyou-data",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "RTB",
      "advertising",
      "CTR",
      "programmatic"
    ],
    "best_for": "Learning advertising analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Ipinyou RTB dataset is designed for real-time bidding applications, specifically focusing on click-through rate (CTR) prediction. Researchers and practitioners can utilize this dataset to develop and test algorithms that optimize advertising strategies in programmatic advertising environments.",
    "use_cases": [
      "Developing CTR prediction models",
      "Testing advertising algorithms",
      "Analyzing bidding strategies"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the Ipinyou RTB dataset?",
      "How can I use the Ipinyou RTB dataset for CTR prediction?",
      "What variables are included in the Ipinyou RTB dataset?",
      "What are the common preprocessing steps for the Ipinyou RTB dataset?",
      "What research questions can be addressed using the Ipinyou RTB dataset?",
      "What types of analyses can be performed with the Ipinyou RTB dataset?"
    ],
    "domain_tags": [
      "advertising"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.001,
    "image_url": "/images/datasets/ipinyou-rtb.png",
    "embedding_text": "The Ipinyou RTB dataset is a comprehensive resource for researchers and practitioners in the field of advertising technology, particularly focusing on real-time bidding (RTB) for click-through rate (CTR) prediction. This dataset typically consists of a structured format with rows representing individual ad impressions and columns capturing various attributes such as ad ID, user demographics, time of impression, and contextual information about the ad placement. The data is collected through a combination of user interactions and ad server logs, providing a rich source of information for modeling user behavior in response to online advertisements. Key variables within the dataset include CTR, which measures the effectiveness of ads, and various features that describe the ad and user context, such as device type, time of day, and geographic location. Researchers often employ this dataset to explore questions related to user engagement, ad effectiveness, and the optimization of bidding strategies in programmatic advertising. Common preprocessing steps may involve handling missing values, normalizing features, and encoding categorical variables to prepare the data for analysis. The dataset supports a range of analytical techniques, including regression analysis, machine learning algorithms, and descriptive statistics, making it a versatile tool for both academic research and practical applications in the advertising industry. However, users should be aware of potential limitations in data quality, such as biases in user behavior or incomplete data entries, which could affect the robustness of their findings. Overall, the Ipinyou RTB dataset serves as a valuable asset for advancing knowledge in advertising analytics and enhancing the effectiveness of digital marketing strategies.",
    "tfidf_keywords": [
      "real-time bidding",
      "click-through rate",
      "CTR prediction",
      "advertising algorithms",
      "programmatic advertising",
      "user engagement",
      "bidding strategies",
      "data preprocessing",
      "machine learning",
      "regression analysis"
    ],
    "semantic_cluster": "advertising-analytics",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "machine-learning",
      "consumer-behavior",
      "advertising-optimization",
      "data-preprocessing",
      "predictive-modeling"
    ],
    "canonical_topics": [
      "machine-learning",
      "advertising",
      "consumer-behavior"
    ]
  },
  {
    "name": "FRED (Federal Reserve Economic Data)",
    "description": "816,000+ US macroeconomic time series from 100+ sources with free API",
    "category": "Dataset Aggregators",
    "url": "https://fred.stlouisfed.org",
    "docs_url": "https://fred.stlouisfed.org/docs/api/fred/",
    "github_url": null,
    "tags": [
      "macro",
      "economics",
      "time series",
      "Fed",
      "API"
    ],
    "best_for": "US macroeconomic research - Paul Krugman called it 'the most amazing economics website'",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "macroeconomics",
      "economic indicators"
    ],
    "summary": "FRED (Federal Reserve Economic Data) is a comprehensive repository of over 816,000 US macroeconomic time series sourced from more than 100 different organizations. This dataset provides a free API that allows users to access and analyze a wide range of economic indicators, making it an invaluable resource for researchers, policymakers, and data analysts interested in understanding economic trends and patterns.",
    "use_cases": [
      "Analyzing trends in unemployment rates over time",
      "Comparing GDP growth across different states",
      "Studying the impact of interest rates on inflation",
      "Evaluating the effects of fiscal policy on economic growth"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is FRED and how can I access its data?",
      "How many economic time series are available in FRED?",
      "What types of economic indicators can I find in FRED?",
      "How do I use the FRED API for economic analysis?",
      "What sources contribute to the FRED dataset?",
      "How can I visualize data from FRED?",
      "What are the key features of the FRED dataset?",
      "How can FRED data be used in economic research?"
    ],
    "domain_tags": [
      "finance",
      "economics"
    ],
    "data_modality": "time-series",
    "size_category": "massive",
    "geographic_scope": "United States",
    "model_score": 0.001,
    "embedding_text": "The FRED (Federal Reserve Economic Data) dataset is a rich and extensive collection of over 816,000 time series data points that represent various macroeconomic indicators in the United States. This dataset is compiled from more than 100 different sources, including government agencies, international organizations, and academic institutions, ensuring a comprehensive coverage of economic data. The data structure typically consists of rows representing individual time series and columns that include variables such as date, value, and source. Each time series can measure different economic phenomena, such as inflation rates, employment statistics, and GDP growth, providing a multifaceted view of the economy. Researchers and analysts often utilize FRED to address a variety of research questions, such as examining the relationship between interest rates and inflation or assessing the impact of economic policies on growth. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics. However, users should be aware of potential limitations, such as data quality issues and the need for preprocessing steps like normalization and handling missing values. Overall, FRED serves as a vital resource for those looking to conduct economic research, offering an accessible API for data retrieval and analysis.",
    "tfidf_keywords": [
      "macroeconomic indicators",
      "economic time series",
      "FRED API",
      "data visualization",
      "economic analysis",
      "GDP growth",
      "unemployment rates",
      "inflation",
      "fiscal policy",
      "data sources"
    ],
    "semantic_cluster": "economic-data-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "macroeconomics",
      "economic modeling",
      "data visualization",
      "time series analysis",
      "policy evaluation"
    ],
    "canonical_topics": [
      "econometrics",
      "forecasting",
      "finance",
      "consumer-behavior"
    ]
  },
  {
    "name": "ASOS Experiments",
    "description": "99 real e-commerce experiments with daily checkpoints from ASOS",
    "category": "Fashion & Apparel",
    "url": "https://www.kaggle.com/datasets/marinazmieva/asos-digital-experiments-dataset",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "A/B testing",
      "e-commerce",
      "fashion",
      "Kaggle"
    ],
    "best_for": "Learning fashion & apparel analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The ASOS Experiments dataset comprises 99 real e-commerce experiments conducted by ASOS, featuring daily checkpoints. This dataset allows researchers and practitioners to analyze the impact of various A/B testing strategies on consumer behavior and sales performance in the fashion industry.",
    "use_cases": [
      "Analyzing the effectiveness of different marketing strategies in e-commerce.",
      "Studying consumer behavior changes in response to A/B tests.",
      "Evaluating pricing strategies based on experimental data.",
      "Investigating the impact of website design changes on sales."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the ASOS Experiments?",
      "How can I analyze e-commerce A/B testing data?",
      "What insights can be gained from ASOS's e-commerce experiments?",
      "What variables are included in the ASOS Experiments dataset?",
      "How does A/B testing affect consumer behavior in fashion?",
      "What are the daily checkpoints in the ASOS Experiments?",
      "How can I use the ASOS Experiments for my research?",
      "What methodologies are used in the ASOS Experiments?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0009,
    "image_url": "/images/datasets/asos-experiments.png",
    "embedding_text": "The ASOS Experiments dataset is a rich resource for researchers and practitioners in the field of e-commerce, particularly within the fashion industry. It consists of 99 real-world experiments conducted by ASOS, a leading online fashion retailer, with daily checkpoints that provide detailed insights into consumer behavior and sales performance. The dataset is structured in a tabular format, containing various rows and columns that represent different experiments and their corresponding metrics. Key variables may include experiment identifiers, treatment groups, control groups, conversion rates, and other performance indicators that measure the impact of different strategies on sales outcomes. The collection methodology likely involved systematic A/B testing, where ASOS implemented changes to their website or marketing strategies and tracked the results over time. While the dataset does not explicitly mention temporal or geographic coverage, it is assumed to reflect the online shopping behavior of consumers engaging with ASOS's platform. Researchers can utilize this dataset to address a variety of research questions, such as the effectiveness of specific marketing tactics, the influence of pricing changes on consumer purchasing decisions, and the overall impact of website design modifications on user engagement. Common preprocessing steps may include cleaning the data, handling missing values, and transforming variables for analysis. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile tool for understanding the dynamics of e-commerce. Researchers typically use the ASOS Experiments dataset to derive actionable insights that inform business strategies, optimize marketing efforts, and enhance customer experiences in the competitive landscape of online retail.",
    "tfidf_keywords": [
      "A/B testing",
      "e-commerce",
      "consumer behavior",
      "fashion retail",
      "conversion rates",
      "marketing strategies",
      "website design",
      "pricing strategies",
      "experimental data",
      "sales performance",
      "user engagement",
      "data analysis",
      "performance indicators",
      "treatment groups",
      "control groups"
    ],
    "semantic_cluster": "ecommerce-experimentation",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "causal-inference",
      "treatment-effects",
      "experimental-design",
      "consumer-behavior",
      "data-analysis"
    ],
    "canonical_topics": [
      "experimentation",
      "consumer-behavior",
      "pricing",
      "data-engineering",
      "statistics"
    ]
  },
  {
    "name": "Romania Tenders",
    "description": "Public tender data (2007-2016) from Romania",
    "category": "Auctions & Marketplaces",
    "url": "https://www.kaggle.com/datasets/gpreda/public-tenders-romania-20072016",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "tenders",
      "government",
      "Romania"
    ],
    "best_for": "Learning auctions & marketplaces analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "auctions",
      "government",
      "public procurement"
    ],
    "summary": "The Romania Tenders dataset provides comprehensive public tender data from Romania covering the years 2007 to 2016. Researchers and analysts can utilize this dataset to explore trends in government procurement, analyze bidding behaviors, and assess market dynamics within the Romanian context.",
    "use_cases": [
      "Analyzing the distribution of tender amounts over the years.",
      "Examining the frequency of tenders by sector to identify market opportunities.",
      "Studying the impact of government procurement on local economies.",
      "Investigating bidding strategies and their outcomes in public tenders."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the trends in public tenders in Romania from 2007 to 2016?",
      "How do government procurement practices vary across different sectors in Romania?",
      "What insights can be drawn from analyzing bid amounts in Romanian tenders?",
      "How has the number of tenders changed over the years in Romania?",
      "What types of goods and services are most commonly procured by the Romanian government?",
      "What are the common characteristics of successful bids in Romanian tenders?"
    ],
    "domain_tags": [
      "government",
      "public sector"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2007-2016",
    "geographic_scope": "Romania",
    "size_category": "medium",
    "model_score": 0.0009,
    "image_url": "/images/datasets/romania-tenders.jpg",
    "embedding_text": "The Romania Tenders dataset is a rich source of public procurement data that spans nearly a decade, from 2007 to 2016. This dataset is structured in a tabular format, with rows representing individual tender entries and columns capturing key variables such as tender ID, title, description, bid amounts, and awarding entities. The data is collected from official government sources, ensuring a high level of reliability and accuracy. However, users should be aware of potential limitations, such as missing values or inconsistencies in the reporting of certain tenders. Common preprocessing steps may include data cleaning to handle missing entries and normalization of bid amounts for comparative analysis. Researchers can leverage this dataset to address a variety of research questions, such as identifying trends in government spending, analyzing the competitive landscape of bidders, and evaluating the effectiveness of procurement policies. The dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics. By utilizing this dataset, researchers can gain valuable insights into the dynamics of public procurement in Romania, contributing to a better understanding of government market interactions and economic implications.",
    "tfidf_keywords": [
      "public procurement",
      "tender analysis",
      "government contracts",
      "bid evaluation",
      "market dynamics",
      "economic impact",
      "competitive bidding",
      "procurement policies",
      "data cleaning",
      "statistical analysis"
    ],
    "semantic_cluster": "public-procurement-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "government contracting",
      "market analysis",
      "bidding strategies",
      "economic evaluation",
      "data preprocessing"
    ],
    "canonical_topics": [
      "econometrics",
      "marketplaces",
      "policy-evaluation"
    ]
  },
  {
    "name": "Outbrain Click Prediction",
    "description": "Click prediction based on browsing history from Outbrain",
    "category": "Advertising",
    "url": "https://www.kaggle.com/competitions/outbrain-click-prediction",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "click prediction",
      "content",
      "Kaggle"
    ],
    "best_for": "Learning advertising analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Outbrain Click Prediction dataset provides insights into user click behavior based on browsing history. It can be utilized to develop predictive models that enhance content recommendation systems and optimize advertising strategies.",
    "use_cases": [
      "Predicting user clicks on content",
      "Optimizing ad placements based on user behavior",
      "Analyzing browsing patterns to improve recommendations"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Outbrain Click Prediction dataset?",
      "How can I use browsing history for click prediction?",
      "What machine learning techniques are applicable to click prediction?",
      "Where can I find datasets for advertising click prediction?",
      "What are the key variables in the Outbrain dataset?",
      "How does user behavior influence click rates?",
      "What preprocessing steps are needed for click prediction data?",
      "What insights can be derived from click prediction models?"
    ],
    "domain_tags": [
      "advertising"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0009,
    "image_url": "/images/datasets/outbrain-click-prediction.png",
    "embedding_text": "The Outbrain Click Prediction dataset is structured in a tabular format, consisting of rows that represent individual user sessions and columns that capture various features related to user behavior and content interaction. Key variables may include user ID, session ID, content ID, click status, and features derived from browsing history. This dataset is invaluable for researchers and practitioners in the field of advertising and recommendation systems, as it allows for the exploration of user engagement and click-through rates based on historical data. The collection methodology involves aggregating user browsing data from Outbrain's platform, which serves as a content recommendation engine. While the dataset offers rich insights, it is essential to consider data quality and limitations, such as potential biases in user behavior and the representativeness of the sample. Common preprocessing steps include handling missing values, encoding categorical variables, and normalizing numerical features to prepare the data for analysis. Researchers can leverage this dataset to address various research questions, such as identifying factors that influence click behavior, developing machine learning models for prediction, and conducting regression analyses to understand the relationship between user features and click outcomes. The dataset supports a range of analyses, including descriptive statistics, regression modeling, and machine learning techniques, making it a versatile resource for those studying consumer behavior and advertising effectiveness.",
    "tfidf_keywords": [
      "click prediction",
      "browsing history",
      "user engagement",
      "recommendation systems",
      "advertising optimization",
      "machine learning",
      "predictive modeling",
      "user behavior",
      "data preprocessing",
      "feature engineering"
    ],
    "semantic_cluster": "click-prediction-modeling",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "machine-learning",
      "consumer-behavior",
      "recommendation-systems",
      "advertising",
      "data-preprocessing"
    ],
    "canonical_topics": [
      "machine-learning",
      "consumer-behavior",
      "recommendation-systems",
      "advertising"
    ]
  },
  {
    "name": "Criteo Terabyte",
    "description": "342GB, 45M samples with 13 integer features and 26 hashed categorical features for CTR prediction",
    "category": "Advertising",
    "url": "https://huggingface.co/datasets/criteo/CriteoClickLogs",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "CTR",
      "advertising",
      "large-scale",
      "benchmark"
    ],
    "best_for": "Learning advertising analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Criteo Terabyte dataset is a large-scale collection designed for click-through rate (CTR) prediction, containing 342GB of data with 45 million samples. It features 13 integer attributes and 26 hashed categorical features, making it suitable for various machine learning and statistical analyses.",
    "use_cases": [
      "Predicting click-through rates for online ads",
      "Benchmarking machine learning models for CTR prediction",
      "Analyzing user behavior in digital advertising"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the Criteo Terabyte dataset?",
      "How can I use the Criteo dataset for CTR prediction?",
      "What features are included in the Criteo Terabyte dataset?",
      "Where can I find large-scale advertising datasets?",
      "What are the applications of CTR prediction?",
      "How to preprocess the Criteo dataset for analysis?"
    ],
    "domain_tags": [
      "advertising"
    ],
    "data_modality": "tabular",
    "size_category": "large",
    "benchmark_usage": [
      "CTR prediction benchmark"
    ],
    "model_score": 0.0009,
    "image_url": "/images/datasets/criteo-terabyte.png",
    "embedding_text": "The Criteo Terabyte dataset is a substantial resource for researchers and practitioners in the field of advertising and machine learning. With a size of 342GB and containing 45 million samples, this dataset is structured in a tabular format, featuring 13 integer variables and 26 hashed categorical features. The primary focus of this dataset is to facilitate click-through rate (CTR) prediction, a crucial metric in online advertising that helps in evaluating the effectiveness of ad placements. The dataset's structure allows for a variety of analyses, including regression and machine learning techniques, making it a versatile tool for understanding user interactions with advertisements. Researchers typically utilize this dataset to develop and benchmark predictive models, assess algorithm performance, and explore user behavior patterns in digital advertising contexts. The key variables within the dataset include both integer features, which may represent numerical attributes such as user demographics or ad characteristics, and hashed categorical features that encode various categorical data points, ensuring privacy while enabling analysis. Given the dataset's size and complexity, common preprocessing steps may include handling missing values, normalizing numerical features, and decoding hashed categorical variables for interpretability. While the dataset offers rich insights, it is important to acknowledge potential limitations, such as biases in user behavior or ad exposure that may affect the generalizability of findings. Overall, the Criteo Terabyte dataset serves as a benchmark for CTR prediction tasks, providing a foundation for both academic research and practical applications in the advertising industry.",
    "tfidf_keywords": [
      "click-through rate",
      "CTR prediction",
      "large-scale dataset",
      "advertising analytics",
      "machine learning",
      "data preprocessing",
      "user behavior",
      "benchmarking",
      "integer features",
      "hashed categorical features"
    ],
    "semantic_cluster": "advertising-analytics",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "machine-learning",
      "consumer-behavior",
      "data-preprocessing",
      "predictive-modeling",
      "advertising-strategies"
    ],
    "canonical_topics": [
      "machine-learning",
      "advertising",
      "consumer-behavior"
    ]
  },
  {
    "name": "Fliggy Travel",
    "description": "Travel-related data from Alibaba's online travel platform",
    "category": "Travel & Hospitality",
    "url": "https://tianchi.aliyun.com/dataset/113649",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "travel",
      "Alibaba",
      "bookings"
    ],
    "best_for": "Learning travel & hospitality analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "Fliggy Travel dataset provides travel-related data from Alibaba's online travel platform, offering insights into consumer behavior and booking trends. Researchers can utilize this dataset to analyze travel patterns, pricing strategies, and the impact of various factors on travel bookings.",
    "use_cases": [
      "Analyzing consumer booking trends over time.",
      "Evaluating the impact of pricing strategies on travel bookings.",
      "Studying the effects of seasonal variations on travel patterns."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What travel data is available from Alibaba's Fliggy platform?",
      "How can I analyze travel booking trends using Fliggy data?",
      "What insights can be gained from Fliggy Travel dataset?",
      "Are there any datasets related to travel and hospitality from Alibaba?",
      "How does Fliggy data reflect consumer behavior in travel?",
      "What are the key variables in the Fliggy Travel dataset?"
    ],
    "domain_tags": [
      "travel",
      "e-commerce"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0008,
    "embedding_text": "The Fliggy Travel dataset is a comprehensive collection of travel-related data sourced from Alibaba's online travel platform, Fliggy. This dataset is structured in a tabular format, consisting of various rows and columns that capture essential variables related to travel bookings. Key variables may include booking dates, travel destinations, customer demographics, and pricing information, which collectively provide a rich landscape for analysis. The data is collected through user interactions on the Fliggy platform, reflecting real-time consumer behavior and preferences in the travel industry. Researchers can leverage this dataset to explore numerous research questions, such as the influence of pricing on booking decisions, seasonal trends in travel, and consumer behavior patterns. Common preprocessing steps may involve cleaning the data to handle missing values, normalizing pricing information, and aggregating data by time periods or geographic locations. The dataset supports various types of analyses, including regression analysis to identify factors influencing bookings, machine learning techniques for predictive modeling, and descriptive statistics to summarize travel trends. However, researchers should be aware of potential limitations, such as data quality issues stemming from user-generated content and the need for careful interpretation of the results. Overall, the Fliggy Travel dataset serves as a valuable resource for understanding the dynamics of the travel market and can be instrumental in studies focusing on e-commerce, consumer behavior, and pricing strategies.",
    "tfidf_keywords": [
      "travel-bookings",
      "consumer-preferences",
      "pricing-strategies",
      "seasonal-trends",
      "data-collection-methodology",
      "demographic-analysis",
      "travel-patterns",
      "e-commerce-insights",
      "data-preprocessing",
      "predictive-modeling"
    ],
    "semantic_cluster": "travel-data-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "e-commerce",
      "data-analysis",
      "travel-industry",
      "pricing-strategy"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "pricing",
      "marketplaces"
    ]
  },
  {
    "name": "AEA Data and Code Repository",
    "description": "Replication packages for all AEA publications since 2019 with DOI-assigned packages",
    "category": "Dataset Aggregators",
    "url": "https://www.openicpsr.org/openicpsr/search/aea/studies",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "replication",
      "AEA",
      "economics",
      "reproducibility"
    ],
    "best_for": "Verifying published empirical work with data, code, and documentation",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "replication",
      "economics",
      "reproducibility"
    ],
    "summary": "The AEA Data and Code Repository provides replication packages for all American Economic Association publications since 2019, allowing researchers to access and reproduce the results of published studies. This dataset is essential for verifying findings and ensuring the integrity of economic research.",
    "use_cases": [
      "Verifying findings of economic studies",
      "Conducting meta-analyses of AEA publications",
      "Teaching reproducibility in economics courses"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the replication packages available for AEA publications?",
      "How can I access AEA's replication data?",
      "What is the significance of DOI-assigned packages in economics?",
      "How does the AEA Data Repository support reproducibility?",
      "What types of analyses can be performed using AEA replication data?",
      "Where can I find AEA publications since 2019 with replication packages?"
    ],
    "domain_tags": [
      "economics"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0008,
    "embedding_text": "The AEA Data and Code Repository serves as a comprehensive resource for researchers in the field of economics, providing access to replication packages for all publications by the American Economic Association (AEA) since 2019. This dataset is structured to include various variables that correspond to the methodologies and findings presented in AEA publications, allowing users to engage in rigorous verification of research outcomes. The collection methodology involves the systematic gathering of data and code used in published studies, ensuring that each package is associated with a Digital Object Identifier (DOI) for easy referencing and accessibility. The repository aims to enhance the reproducibility of economic research, a critical aspect of scientific inquiry. Researchers can utilize this dataset to address a range of research questions, such as the validity of specific economic theories, the effectiveness of policy interventions, or the robustness of empirical findings. Common preprocessing steps may include data cleaning, standardization of formats, and alignment of variable definitions across different studies. The types of analyses supported by this dataset include regression analyses, machine learning applications, and descriptive statistics, making it a versatile tool for both novice and experienced researchers. By providing a platform for replication, the AEA Data and Code Repository not only fosters academic integrity but also encourages collaboration and innovation within the economics community.",
    "benchmark_usage": [
      "Verifying research results",
      "Supporting academic integrity"
    ],
    "tfidf_keywords": [
      "replication-packages",
      "DOI",
      "economic-research",
      "data-reproducibility",
      "verification",
      "meta-analysis",
      "empirical-findings",
      "academic-integrity",
      "research-methodologies",
      "data-access"
    ],
    "semantic_cluster": "replication-in-economics",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "reproducibility",
      "data-verification",
      "empirical-economics",
      "research-integrity",
      "economic-methodology"
    ],
    "canonical_topics": [
      "econometrics",
      "causal-inference",
      "experimentation"
    ]
  },
  {
    "name": "LaDe (Cainiao)",
    "description": "10.6M+ packages with 619K trajectories and GPS data from Alibaba logistics",
    "category": "Logistics & Supply Chain",
    "url": "https://huggingface.co/datasets/Cainiao-AI/LaDe",
    "docs_url": null,
    "github_url": "https://huggingface.co/datasets/Cainiao-AI/LaDe",
    "tags": [
      "packages",
      "trajectories",
      "Alibaba",
      "delivery"
    ],
    "best_for": "Learning logistics & supply chain analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "logistics",
      "delivery"
    ],
    "summary": "The LaDe (Cainiao) dataset comprises over 10.6 million packages with 619,000 trajectories and GPS data sourced from Alibaba logistics. This dataset enables researchers and analysts to explore various aspects of logistics and supply chain management, including delivery efficiency and route optimization.",
    "use_cases": [
      "Analyzing delivery routes for efficiency improvements",
      "Studying the impact of logistics on consumer behavior",
      "Optimizing package handling and transportation strategies"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the trajectories of packages in the LaDe dataset?",
      "How can GPS data from Alibaba logistics improve delivery efficiency?",
      "What insights can be gained from analyzing 10.6M+ packages?",
      "What patterns can be identified in the logistics data from Cainiao?",
      "How does package delivery vary across different regions?",
      "What are the implications of trajectory data for supply chain optimization?"
    ],
    "domain_tags": [
      "logistics",
      "e-commerce"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0007,
    "image_url": "/images/datasets/lade-cainiao.png",
    "embedding_text": "The LaDe (Cainiao) dataset is a comprehensive collection of logistics data, featuring over 10.6 million packages along with 619,000 trajectories and GPS data sourced from Alibaba's logistics operations. This dataset is structured in a tabular format, with rows representing individual packages and columns capturing various attributes such as package ID, trajectory points, timestamps, and GPS coordinates. The collection methodology involves tracking packages through the logistics network, ensuring that the data reflects real-world delivery scenarios. While the dataset does not specify temporal or geographic coverage, it provides a rich source of information for analyzing logistics patterns and delivery efficiencies. Key variables include package trajectories, which measure the path taken by each package, and GPS data, which provides precise location information at various points in the delivery process. Researchers utilizing this dataset can address a range of research questions, such as identifying inefficiencies in delivery routes, understanding consumer behavior in relation to delivery times, and optimizing logistics operations. Common preprocessing steps may include cleaning the data for missing values, normalizing GPS coordinates, and segmenting trajectories for analysis. The dataset supports various types of analyses, including regression models to predict delivery times, machine learning techniques for route optimization, and descriptive statistics to summarize package delivery patterns. Researchers typically use this dataset to inform studies on logistics efficiency, supply chain management, and the impact of delivery systems on consumer satisfaction.",
    "tfidf_keywords": [
      "logistics",
      "GPS data",
      "package trajectories",
      "delivery efficiency",
      "supply chain",
      "route optimization",
      "Alibaba logistics",
      "data analysis",
      "consumer behavior",
      "package handling",
      "transportation strategies",
      "logistics network",
      "real-world scenarios",
      "data preprocessing",
      "machine learning"
    ],
    "semantic_cluster": "logistics-data-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "supply-chain-management",
      "data-analysis",
      "machine-learning",
      "consumer-behavior",
      "route-optimization"
    ],
    "canonical_topics": [
      "machine-learning",
      "consumer-behavior",
      "logistics",
      "data-engineering",
      "optimization"
    ]
  },
  {
    "name": "ICPSR",
    "description": "World's largest social science archive - 250,000+ files across 16,000 studies since 1962",
    "category": "Dataset Aggregators",
    "url": "https://www.icpsr.umich.edu",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "social science",
      "surveys",
      "GSS",
      "ANES",
      "academic"
    ],
    "best_for": "GSS, ANES, World Values Survey - essential for empirical economics research",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The ICPSR dataset is the world's largest social science archive, containing over 250,000 files across 16,000 studies since 1962. Researchers can utilize this extensive repository for various analyses in social science, including surveys and academic research.",
    "use_cases": [
      "Analyzing trends in social attitudes over time using survey data.",
      "Conducting comparative studies between different demographic groups.",
      "Evaluating the impact of social policies using historical datasets.",
      "Exploring public opinion through the General Social Survey (GSS) data."
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What studies are available in the ICPSR dataset?",
      "How can I access social science data from ICPSR?",
      "What types of surveys are included in the ICPSR archive?",
      "What is the historical coverage of the ICPSR dataset?",
      "How can ICPSR data be used for academic research?",
      "What are the key variables measured in ICPSR studies?",
      "How do I cite data from ICPSR in my research?",
      "What are the common data formats available in ICPSR?"
    ],
    "domain_tags": [
      "academic"
    ],
    "data_modality": "tabular",
    "size_category": "massive",
    "model_score": 0.0007,
    "image_url": "/images/logos/umich.png",
    "embedding_text": "The ICPSR (Inter-university Consortium for Political and Social Research) dataset serves as a vital resource for researchers in the social sciences, offering a comprehensive archive that includes over 250,000 files derived from more than 16,000 studies since its inception in 1962. This dataset is structured in a tabular format, with rows representing individual data points or responses and columns corresponding to various variables collected during the studies. The data encompasses a wide range of social science topics, primarily focusing on surveys, public opinion, and demographic information. Researchers can expect to find key variables that measure social attitudes, behaviors, and demographic characteristics, which are essential for conducting analyses in fields such as sociology, political science, and economics. The collection methodology involves rigorous standards for data collection and preservation, ensuring that the data is reliable and valid for academic research. However, users should be aware of potential limitations, such as missing data or variations in survey methodologies across different studies. Common preprocessing steps may include data cleaning, normalization, and handling of missing values to prepare the dataset for analysis. Researchers typically utilize the ICPSR dataset to address various research questions, such as examining the relationship between socioeconomic factors and political participation or analyzing shifts in public opinion over time. The dataset supports a range of analytical methods, including regression analysis, machine learning techniques, and descriptive statistics, making it a versatile tool for social science research. Overall, the ICPSR dataset is an invaluable resource for scholars seeking to explore complex social phenomena through empirical data.",
    "tfidf_keywords": [
      "social science",
      "surveys",
      "public opinion",
      "demographics",
      "data collection",
      "data preservation",
      "data cleaning",
      "regression analysis",
      "machine learning",
      "descriptive statistics",
      "sociology",
      "political science",
      "economics",
      "academic research",
      "data validity"
    ],
    "semantic_cluster": "social-science-archive",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "sociology",
      "political science",
      "econometrics",
      "public opinion research",
      "demographic analysis"
    ],
    "canonical_topics": [
      "social-science",
      "statistics",
      "consumer-behavior"
    ]
  },
  {
    "name": "UK Gift Shop (Online Retail)",
    "description": "Online retail transactions (2010-2011) from UK gift retailer",
    "category": "Grocery & Supermarkets",
    "url": "http://archive.ics.uci.edu/dataset/352/online+retail",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "retail",
      "UK",
      "UCI",
      "transactions"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The UK Gift Shop dataset contains online retail transactions from a UK gift retailer during the years 2010-2011. This dataset can be utilized to analyze consumer purchasing patterns, evaluate pricing strategies, and explore trends in online retail behavior.",
    "use_cases": [
      "Analyzing consumer purchasing trends over time",
      "Evaluating the impact of pricing changes on sales",
      "Identifying popular product categories",
      "Exploring seasonal variations in online retail transactions"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the online retail transaction trends in the UK for 2010-2011?",
      "How do pricing strategies affect consumer behavior in online gift shopping?",
      "What types of products were most frequently purchased in UK online retail during 2010-2011?",
      "How can we analyze seasonal trends in online gift purchases?",
      "What demographic factors influence online gift shopping behavior?",
      "How do transaction volumes vary by product category in the UK gift shop?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2010-2011",
    "geographic_scope": "UK",
    "size_category": "medium",
    "model_score": 0.0007,
    "image_url": "/images/logos/uci.png",
    "embedding_text": "The UK Gift Shop dataset comprises a collection of online retail transactions recorded by a UK-based gift retailer during the years 2010 and 2011. This dataset is structured in a tabular format, featuring rows that represent individual transactions and columns that capture various attributes of each transaction, such as transaction ID, product details, purchase date, customer information, and transaction amount. The data collection methodology involved gathering transaction records from the retailer's online platform, ensuring that it reflects a comprehensive view of consumer behavior during this period. The dataset covers a specific temporal range from 2010 to 2011, focusing on the UK market, which allows researchers to analyze trends and patterns in online retail specific to this geographic region. Key variables within the dataset include transaction ID, product category, purchase date, and total amount spent, which can be used to measure sales performance, customer preferences, and seasonal buying habits. Data quality is generally high, although researchers should be aware of potential limitations such as missing values or inconsistencies in customer data. Common preprocessing steps may include cleaning the data to handle missing values, normalizing transaction amounts, and categorizing products for analysis. Researchers can leverage this dataset to address various research questions, such as understanding consumer purchasing trends, evaluating the effectiveness of marketing strategies, and exploring the impact of pricing on sales. The dataset supports a range of analytical techniques, including regression analysis, machine learning models, and descriptive statistics, making it a versatile resource for both academic and commercial research in the field of e-commerce. Typically, researchers use this dataset to conduct studies on consumer behavior, pricing strategies, and market trends, providing valuable insights into the dynamics of online retail.",
    "tfidf_keywords": [
      "online retail",
      "consumer purchasing patterns",
      "pricing strategies",
      "transaction analysis",
      "seasonal trends",
      "product categories",
      "UK market",
      "e-commerce behavior",
      "sales performance",
      "customer preferences"
    ],
    "semantic_cluster": "e-commerce-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "pricing",
      "market-analysis",
      "transaction-data",
      "e-commerce"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "pricing",
      "marketplaces"
    ]
  },
  {
    "name": "Criteo AI Lab Datasets",
    "description": "World's largest public ML dataset - 1TB Click Logs with 4 billion advertising events",
    "category": "Dataset Aggregators",
    "url": "https://ailab.criteo.com/ressources/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "advertising",
      "CTR",
      "recommendations",
      "benchmark"
    ],
    "best_for": "Click-through rate prediction and recommendation systems benchmarks",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "advertising"
    ],
    "summary": "The Criteo AI Lab Datasets represent the world's largest public machine learning dataset, comprising 1TB of click logs with 4 billion advertising events. This dataset can be utilized for various analyses including click-through rate (CTR) prediction, recommendation system development, and benchmarking machine learning models in advertising contexts.",
    "use_cases": [
      "Predicting click-through rates for online ads",
      "Developing recommendation systems based on user interactions",
      "Benchmarking machine learning algorithms for advertising",
      "Analyzing consumer behavior in response to advertising"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the Criteo AI Lab Datasets?",
      "How can I access the Criteo advertising dataset?",
      "What analyses can be performed on the Criteo click logs?",
      "What are the key variables in the Criteo dataset?",
      "How does the Criteo dataset support machine learning research?",
      "What are the limitations of the Criteo AI Lab Datasets?",
      "How to use Criteo datasets for benchmarking?",
      "What types of models can be trained on Criteo data?"
    ],
    "domain_tags": [
      "advertising",
      "e-commerce"
    ],
    "data_modality": "tabular",
    "size_category": "massive",
    "benchmark_usage": [
      "CTR prediction",
      "recommendation system benchmarking"
    ],
    "model_score": 0.0007,
    "embedding_text": "The Criteo AI Lab Datasets are a significant resource for researchers and practitioners in the field of machine learning, particularly in advertising and e-commerce. This dataset is structured as a tabular format, consisting of 1TB of click logs that encapsulate 4 billion advertising events. Each entry in the dataset includes various features that capture user interactions with advertisements, such as timestamps, user IDs, ad IDs, and contextual information about the ads displayed. The collection methodology for this dataset involves aggregating real-world user interactions from Criteo's advertising platform, ensuring a rich and diverse set of data points for analysis. However, users should be aware of potential data quality issues, such as missing values or biases inherent in user behavior. Common preprocessing steps may include data cleaning, normalization, and feature engineering to prepare the data for analysis. Researchers can leverage this dataset to address a multitude of research questions, including the effectiveness of different advertising strategies, user engagement patterns, and the development of predictive models for click-through rates. The dataset supports various types of analyses, including regression analysis, machine learning model training, and descriptive statistics. Overall, the Criteo AI Lab Datasets serve as a foundational tool for advancing knowledge in advertising effectiveness and consumer behavior within the digital marketplace.",
    "tfidf_keywords": [
      "click-through-rate",
      "advertising-events",
      "machine-learning",
      "recommendation-systems",
      "benchmarking",
      "user-interaction",
      "data-preprocessing",
      "feature-engineering",
      "consumer-behavior",
      "predictive-modeling"
    ],
    "semantic_cluster": "advertising-analytics",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "machine-learning",
      "advertising-analytics",
      "consumer-behavior",
      "data-preprocessing",
      "predictive-modeling"
    ],
    "canonical_topics": [
      "machine-learning",
      "advertising",
      "consumer-behavior",
      "recommendation-systems"
    ]
  },
  {
    "name": "Federal Procurement Data System (FPDS)",
    "description": "Comprehensive U.S. government contract database with 50+ million unclassified actions and 200+ data elements per transaction",
    "category": "Defense Economics",
    "url": "https://www.fpds.gov/",
    "docs_url": "https://www.fpds.gov/wiki/index.php/V1.4_Atom_Feed_FAQ",
    "github_url": null,
    "tags": [
      "procurement",
      "contracts",
      "government",
      "acquisition"
    ],
    "best_for": "Analyzing U.S. defense contracting patterns and vendor relationships",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Federal Procurement Data System (FPDS) is a comprehensive database that contains over 50 million unclassified U.S. government contract actions, providing detailed insights into government procurement activities. Researchers and analysts can utilize this dataset to explore trends in government spending, analyze contract awards, and assess the impact of procurement policies.",
    "use_cases": [
      "Analyzing government spending patterns over time.",
      "Evaluating the effectiveness of procurement policies.",
      "Identifying trends in contract types and award amounts.",
      "Assessing the impact of government contracts on local economies."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the key trends in U.S. government procurement?",
      "How much has the government spent on contracts in the last decade?",
      "What types of contracts are most commonly awarded?",
      "Which agencies are the largest recipients of government contracts?",
      "How do procurement practices vary across different sectors?",
      "What are the common characteristics of awarded contracts?"
    ],
    "domain_tags": [
      "government",
      "defense"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2004-present",
    "geographic_scope": "United States",
    "size_category": "massive",
    "model_score": 0.0007,
    "image_url": "/images/logos/fpds.png",
    "embedding_text": "The Federal Procurement Data System (FPDS) serves as a vital resource for understanding the intricacies of U.S. government contracting. With over 50 million unclassified actions recorded, the FPDS provides a rich dataset that encompasses more than 200 data elements per transaction. Each entry in the database represents a unique government procurement action, detailing various aspects such as the contracting agency, the type of contract awarded, the dollar value, and the vendor involved. The data is structured in a tabular format, where each row corresponds to a distinct procurement action, and the columns represent the diverse variables that describe these actions. Key variables include contract award amounts, dates, agency identifiers, and vendor information, which collectively allow for comprehensive analysis of government contracting behavior.\n\nThe collection methodology for the FPDS involves systematic data gathering from federal agencies, ensuring that the information is consistently updated and reflects the most current procurement activities. While the dataset is extensive, it is important to note that it primarily includes unclassified actions, which may limit the scope of certain analyses. Researchers often engage with the FPDS to address a variety of research questions, such as examining the trends in government spending, evaluating the effectiveness of procurement strategies, and understanding the dynamics of government contracts across different sectors.\n\nCommon preprocessing steps for utilizing the FPDS include cleaning the data to handle missing values, standardizing agency names, and aggregating contract actions to analyze trends over time. The dataset supports various types of analyses, including regression analysis to assess the impact of contract awards on economic outcomes, machine learning techniques for predictive modeling, and descriptive statistics to summarize procurement trends. Researchers typically leverage the FPDS in studies related to public policy, economic impact assessments, and evaluations of government efficiency in procurement processes. Overall, the FPDS stands as a critical tool for those seeking to understand the complexities of government contracting and its implications for economic and policy analysis.",
    "tfidf_keywords": [
      "government-contracts",
      "procurement-data",
      "contract-awards",
      "federal-agencies",
      "spending-patterns",
      "vendor-analysis",
      "contract-types",
      "policy-evaluation",
      "economic-impact",
      "data-collection-methodology",
      "contracting-agency",
      "government-spending",
      "unclassified-actions",
      "data-analytics",
      "public-policy"
    ],
    "semantic_cluster": "government-procurement-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "public-policy",
      "economic-analysis",
      "contract-management",
      "data-analytics",
      "government-spending"
    ],
    "canonical_topics": [
      "policy-evaluation",
      "econometrics",
      "consumer-behavior"
    ]
  },
  {
    "name": "TalkingData AdTracking Fraud Detection",
    "description": "200 million clicks with 0.25% positive fraud class for mobile ad fraud detection benchmarking",
    "category": "Fraud Detection",
    "url": "https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/data",
    "docs_url": "https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection",
    "github_url": null,
    "tags": [
      "fraud detection",
      "mobile",
      "imbalanced",
      "TalkingData"
    ],
    "best_for": "Developing and benchmarking ad fraud detection models",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "fraud detection",
      "mobile",
      "ad tracking"
    ],
    "summary": "The TalkingData AdTracking Fraud Detection dataset contains 200 million clicks, with a mere 0.25% representing positive fraud cases. This dataset is designed for benchmarking mobile ad fraud detection methods, allowing researchers to evaluate and compare various algorithms and models in identifying fraudulent activities in mobile advertising.",
    "use_cases": [
      "Evaluating machine learning models for fraud detection in mobile advertising.",
      "Benchmarking algorithms for identifying fraudulent clicks.",
      "Analyzing the characteristics of fraudulent versus non-fraudulent clicks.",
      "Developing strategies to mitigate ad fraud in mobile applications."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the TalkingData AdTracking Fraud Detection dataset?",
      "How can I use the TalkingData dataset for mobile ad fraud detection?",
      "What are the characteristics of the clicks in the TalkingData dataset?",
      "How does the fraud rate in the TalkingData dataset compare to other datasets?",
      "What methods can be applied to analyze the TalkingData AdTracking dataset?",
      "Are there any known limitations of the TalkingData AdTracking dataset?",
      "What preprocessing steps are needed for the TalkingData dataset?",
      "What insights can be gained from analyzing mobile ad fraud using TalkingData?"
    ],
    "domain_tags": [
      "advertising",
      "technology"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "4 days",
    "geographic_scope": "China",
    "size_category": "medium",
    "model_score": 0.0007,
    "image_url": "/images/datasets/talkingdata-adtracking-fraud-detection.jpg",
    "embedding_text": "The TalkingData AdTracking Fraud Detection dataset is a substantial collection of 200 million clicks, specifically curated for the purpose of benchmarking mobile ad fraud detection techniques. This dataset is characterized by a low positive fraud class rate of 0.25%, making it an ideal resource for researchers and practitioners aiming to develop and evaluate algorithms that can effectively identify fraudulent activities in mobile advertising. The dataset is structured in a tabular format, consisting of various rows and columns that capture essential variables related to each click, including user identifiers, timestamps, ad impressions, and fraud labels. The collection methodology for this dataset involves aggregating click data from mobile ad networks, ensuring a diverse representation of user interactions with ads across different applications and platforms. However, it is important to note that the dataset may have limitations in terms of data quality, including potential biases in user behavior and the representativeness of the sample. Common preprocessing steps may include data cleaning to handle missing values, normalization of features, and encoding categorical variables for analysis. Researchers can leverage this dataset to address a variety of research questions, such as identifying patterns in fraudulent clicks, comparing the performance of different fraud detection algorithms, and exploring the impact of various features on the likelihood of fraud. The types of analyses supported by this dataset range from regression analyses to machine learning applications, enabling a comprehensive examination of mobile ad fraud dynamics. Overall, the TalkingData AdTracking Fraud Detection dataset serves as a valuable resource for advancing the understanding of fraud detection in the mobile advertising sector.",
    "benchmark_usage": [
      "Benchmarking mobile ad fraud detection methods"
    ],
    "tfidf_keywords": [
      "ad fraud detection",
      "clickstream data",
      "fraudulent clicks",
      "benchmarking algorithms",
      "mobile advertising",
      "data preprocessing",
      "machine learning models",
      "data quality",
      "user behavior analysis",
      "click data analysis"
    ],
    "semantic_cluster": "mobile-ad-fraud-detection",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "machine-learning",
      "data-preprocessing",
      "fraud-detection",
      "clickstream-analysis",
      "algorithm-evaluation"
    ],
    "canonical_topics": [
      "machine-learning",
      "consumer-behavior",
      "statistics",
      "data-engineering",
      "econometrics"
    ]
  },
  {
    "name": "TouringPlans Disney World Data",
    "description": "Posted vs actual wait times for Disney World attractions from 2012-present. Premium dataset for validating queue estimates.",
    "category": "Entertainment & Media",
    "url": "https://touringplans.com/walt-disney-world/crowd-calendar#DataSets",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "queueing",
      "theme-park",
      "wait-times",
      "Disney",
      "posted-vs-actual"
    ],
    "best_for": "Queueing theory - compare posted vs actual wait times, optimal touring plan optimization",
    "image_url": "/images/logos/touringplans.png",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "entertainment",
      "data-analysis"
    ],
    "summary": "The TouringPlans Disney World Data provides insights into the discrepancies between posted and actual wait times for attractions at Disney World from 2012 to the present. This dataset can be utilized to validate queue estimates and improve visitor experience by analyzing wait time patterns.",
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the actual wait times for Disney World attractions?",
      "How do posted wait times compare to actual wait times at Disney World?",
      "What trends can be observed in Disney World wait times from 2012 to present?",
      "How can I validate queue estimates using Disney World wait time data?",
      "What factors influence wait times at Disney World attractions?",
      "How has the wait time data changed over the years at Disney World?",
      "What are the implications of wait time discrepancies for Disney World visitors?",
      "How can data on wait times improve the planning of visits to Disney World?"
    ],
    "use_cases": [
      "Analyzing trends in wait times over different seasons at Disney World.",
      "Validating the accuracy of queue estimates provided by Disney World.",
      "Comparing wait times across different attractions to identify patterns.",
      "Assessing the impact of special events on wait times at Disney World."
    ],
    "embedding_text": "The TouringPlans Disney World Data is a comprehensive dataset that captures the posted versus actual wait times for various attractions at Disney World from 2012 to the present. This dataset is structured in a tabular format, consisting of rows representing individual observations of wait times and columns detailing key variables such as attraction name, posted wait time, actual wait time, date, and time of day. The data collection methodology involves gathering real-time wait time data from Disney World attractions, allowing for a rich analysis of visitor experiences over time. Researchers can leverage this dataset to explore a variety of research questions, such as the accuracy of posted wait times, the factors influencing wait times, and the overall visitor experience at the park. The dataset supports various types of analyses, including regression analysis to identify trends and machine learning techniques to predict wait times based on historical data. However, users should be aware of potential limitations, such as data quality issues arising from incomplete records or variations in how wait times are recorded. Common preprocessing steps may include cleaning the data for missing values, normalizing wait times, and categorizing attractions based on their characteristics. Overall, this dataset serves as a valuable resource for researchers and analysts interested in understanding visitor behavior and optimizing the experience at Disney World.",
    "domain_tags": [
      "entertainment"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2012-present",
    "geographic_scope": "Disney World",
    "size_category": "medium",
    "tfidf_keywords": [
      "wait-times",
      "queue-estimates",
      "Disney-World",
      "attraction-analysis",
      "temporal-trends",
      "data-validation",
      "visitor-experience",
      "posted-vs-actual",
      "real-time-data",
      "data-collection-methodology"
    ],
    "semantic_cluster": "queueing-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "data-visualization",
      "time-series-analysis",
      "visitor-experience",
      "predictive-modeling",
      "statistical-analysis"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "statistics",
      "data-engineering"
    ],
    "model_score": 0.0007
  },
  {
    "name": "Criteo Uplift Prediction Dataset",
    "description": "~25 million rows with treatment indicators for benchmarking Individual Treatment Effect (ITE) estimation in advertising",
    "category": "Causal Inference",
    "url": "https://ailab.criteo.com/criteo-uplift-prediction-dataset/",
    "docs_url": "https://ailab.criteo.com/criteo-uplift-prediction-dataset/",
    "github_url": null,
    "tags": [
      "uplift modeling",
      "causal inference",
      "ITE",
      "Criteo"
    ],
    "best_for": "Benchmarking uplift and treatment effect models for advertising",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Criteo Uplift Prediction Dataset contains approximately 25 million rows of data, featuring treatment indicators that are essential for benchmarking Individual Treatment Effect (ITE) estimation in advertising. Researchers can utilize this dataset to analyze the impact of different advertising strategies on consumer behavior and optimize marketing campaigns.",
    "use_cases": [
      "Benchmarking advertising strategies",
      "Estimating treatment effects in marketing",
      "Analyzing consumer response to different ads"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the Criteo Uplift Prediction Dataset?",
      "How can I use the Criteo dataset for uplift modeling?",
      "What are the treatment indicators in the Criteo dataset?",
      "What is Individual Treatment Effect estimation?",
      "How does uplift modeling apply to advertising?",
      "What insights can be derived from the Criteo Uplift Prediction Dataset?",
      "What statistical methods are suitable for analyzing this dataset?",
      "How can I benchmark ITE using the Criteo dataset?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "30 days",
    "geographic_scope": "Global",
    "size_category": "medium",
    "model_score": 0.0006,
    "image_url": "/images/logos/criteo.png",
    "embedding_text": "The Criteo Uplift Prediction Dataset is a comprehensive resource for researchers and practitioners interested in causal inference and uplift modeling within the advertising domain. With approximately 25 million rows, this dataset provides a rich tapestry of treatment indicators that are crucial for estimating Individual Treatment Effects (ITE). The dataset's structure is primarily tabular, consisting of various columns that capture treatment assignments, control groups, and response variables related to consumer behavior in response to advertising interventions. Researchers can leverage this dataset to explore the nuances of how different advertising strategies impact consumer decisions, enabling them to optimize their marketing efforts effectively. The collection methodology for this dataset is rooted in real-world advertising scenarios, where treatment indicators are derived from actual campaigns run by Criteo, a leading advertising platform. This ensures that the data reflects genuine consumer interactions and responses, enhancing its applicability for empirical research. However, users should be aware of potential data quality issues, such as missing values or biases inherent in the treatment assignments. Common preprocessing steps may include handling missing data, normalizing response variables, and encoding categorical features to prepare the dataset for analysis. Researchers typically employ various analytical techniques, including regression analysis and machine learning models, to derive insights from the dataset. The dataset supports a range of research questions, from understanding the effectiveness of specific advertising strategies to evaluating the overall impact of marketing interventions on consumer behavior. By utilizing the Criteo Uplift Prediction Dataset, researchers can contribute to the growing body of knowledge in causal inference and uplift modeling, ultimately driving more effective advertising strategies.",
    "benchmark_usage": [
      "Benchmarking Individual Treatment Effect (ITE) estimation"
    ],
    "tfidf_keywords": [
      "uplift modeling",
      "causal inference",
      "Individual Treatment Effect",
      "advertising strategies",
      "consumer behavior",
      "treatment indicators",
      "benchmarking",
      "marketing optimization",
      "response variables",
      "empirical research"
    ],
    "semantic_cluster": "uplift-modeling-advertising",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "causal-inference",
      "treatment-effects",
      "marketing-analytics",
      "advertising-optimization",
      "consumer-response"
    ],
    "canonical_topics": [
      "causal-inference",
      "experimentation",
      "machine-learning",
      "consumer-behavior"
    ]
  },
  {
    "name": "AXA Driver Telematics (Kaggle)",
    "description": "Driving behavior dataset with 50K driver trips characterized by second-by-second GPS coordinates for usage-based insurance",
    "category": "Insurance & Actuarial",
    "url": "https://www.kaggle.com/c/axa-driver-telematics-analysis",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "telematics",
      "UBI",
      "driving-behavior",
      "auto-insurance",
      "GPS-data"
    ],
    "best_for": "Usage-based insurance pricing, driver risk scoring, and behavior analysis",
    "difficulty": "beginner",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "insurance",
      "driving-behavior",
      "usage-based-insurance"
    ],
    "summary": "The AXA Driver Telematics dataset provides detailed insights into driving behavior through 50,000 recorded trips, characterized by second-by-second GPS coordinates. This dataset can be utilized for analyzing driving patterns, assessing risk for usage-based insurance, and developing predictive models for auto insurance pricing.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the AXA Driver Telematics dataset?",
      "How can I analyze driving behavior using this dataset?",
      "What insights can be gained from GPS data in insurance?",
      "How does telematics data influence usage-based insurance?",
      "What are common preprocessing steps for driving behavior data?",
      "What variables are included in the AXA Driver Telematics dataset?",
      "How can regression analysis be applied to this dataset?",
      "What are the limitations of the AXA Driver Telematics dataset?"
    ],
    "use_cases": [
      "Analyzing risk factors for auto insurance",
      "Developing predictive models for driving behavior",
      "Assessing the impact of driving patterns on insurance pricing"
    ],
    "domain_tags": [
      "insurance"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0006,
    "image_url": "/images/datasets/axa-driver-telematics-kaggle.png",
    "embedding_text": "The AXA Driver Telematics dataset is a comprehensive collection of driving behavior data, consisting of 50,000 trips recorded with second-by-second GPS coordinates. This dataset is particularly valuable for researchers and practitioners in the insurance industry, especially those focused on usage-based insurance (UBI) models. The data structure includes various variables such as trip duration, distance traveled, speed, and GPS coordinates, which can be used to analyze driving patterns and behaviors. The collection methodology involves capturing real-time GPS data from drivers, providing a rich source of information for understanding how different driving behaviors correlate with risk factors in auto insurance. Researchers can explore key variables that measure aspects of driving behavior, such as acceleration patterns, braking frequency, and adherence to speed limits. However, users should be aware of potential limitations in data quality, such as inaccuracies in GPS tracking or variations in driver behavior that may not be captured. Common preprocessing steps may include cleaning the data to handle missing values, normalizing GPS coordinates, and aggregating trip data for analysis. The dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics. Researchers typically use this dataset to address questions related to risk assessment in insurance, the effectiveness of UBI programs, and the development of predictive algorithms to improve pricing strategies. By leveraging the insights gained from this dataset, stakeholders can enhance their understanding of driving behaviors and make informed decisions regarding insurance policies and risk management.",
    "tfidf_keywords": [
      "telematics",
      "usage-based insurance",
      "driving behavior",
      "GPS coordinates",
      "risk assessment",
      "predictive modeling",
      "auto insurance",
      "data preprocessing",
      "regression analysis",
      "insurance pricing"
    ],
    "semantic_cluster": "driving-behavior-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "predictive-modeling",
      "risk-assessment",
      "machine-learning",
      "data-preprocessing",
      "insurance-pricing"
    ],
    "canonical_topics": [
      "insurance",
      "consumer-behavior",
      "machine-learning",
      "econometrics"
    ]
  },
  {
    "name": "Outbrain Click Prediction Dataset",
    "description": "Content recommendation dataset with 2 billion page views and user engagement data from Outbrain",
    "category": "Advertising",
    "url": "https://www.kaggle.com/c/outbrain-click-prediction/data",
    "docs_url": "https://www.kaggle.com/c/outbrain-click-prediction",
    "github_url": null,
    "tags": [
      "content recommendation",
      "native advertising",
      "Outbrain",
      "engagement"
    ],
    "best_for": "Native advertising and content recommendation CTR prediction",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "content recommendation",
      "native advertising",
      "user engagement"
    ],
    "summary": "The Outbrain Click Prediction Dataset is a comprehensive content recommendation dataset that includes 2 billion page views and user engagement data sourced from Outbrain. This dataset can be utilized to analyze user behavior, improve recommendation algorithms, and study the effectiveness of native advertising strategies.",
    "use_cases": [
      "Analyzing user engagement trends over time",
      "Improving machine learning models for content recommendations",
      "Evaluating the effectiveness of advertising campaigns",
      "Studying the impact of content features on user clicks"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Outbrain Click Prediction Dataset?",
      "How can I access user engagement data from Outbrain?",
      "What insights can be derived from the Outbrain Click Prediction Dataset?",
      "How does user engagement vary across different content types?",
      "What are the key variables in the Outbrain Click Prediction Dataset?",
      "How can this dataset improve content recommendation systems?",
      "What analysis techniques are suitable for this dataset?",
      "What challenges are associated with analyzing large datasets like Outbrain's?"
    ],
    "domain_tags": [
      "advertising",
      "digital marketing"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "14 days",
    "geographic_scope": "Global",
    "size_category": "massive",
    "model_score": 0.0006,
    "image_url": "/images/datasets/outbrain-click-prediction-dataset.png",
    "embedding_text": "The Outbrain Click Prediction Dataset is a large-scale content recommendation dataset that encompasses a staggering 2 billion page views and user engagement data, specifically collected from the Outbrain platform. This dataset is structured in a tabular format, containing rows that represent individual page views and various columns that capture key variables related to user engagement, such as the time of the view, user ID, content ID, and click-through rates. The data collection methodology involves tracking user interactions with content recommendations on the Outbrain platform, providing a rich source of information for researchers and data scientists interested in understanding user behavior in digital advertising contexts.\n\nThe dataset covers a wide range of user interactions, allowing for the analysis of temporal patterns in user engagement and the effectiveness of different content types. Key variables in the dataset include user demographics, content characteristics, and engagement metrics, which can be used to measure the success of content recommendations and advertising strategies. However, researchers should be aware of potential data quality issues, such as missing values or biases in user behavior, which may affect the outcomes of analyses.\n\nCommon preprocessing steps for this dataset may include cleaning the data to handle missing values, normalizing engagement metrics, and encoding categorical variables for machine learning applications. Researchers can leverage this dataset to address various research questions, such as how content features influence user clicks, the effectiveness of different recommendation algorithms, and the overall impact of advertising on user engagement.\n\nThe dataset supports a variety of analytical techniques, including regression analysis, machine learning modeling, and descriptive statistics, making it a versatile resource for both academic and industry research. Researchers typically use this dataset to develop and refine recommendation systems, evaluate advertising strategies, and gain insights into consumer behavior in the digital landscape.",
    "tfidf_keywords": [
      "click-through-rate",
      "user engagement",
      "content recommendation",
      "native advertising",
      "machine learning",
      "data preprocessing",
      "user behavior analysis",
      "digital marketing",
      "advertising effectiveness",
      "recommendation algorithms"
    ],
    "semantic_cluster": "content-recommendation-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "user behavior",
      "advertising strategies",
      "machine learning",
      "data analysis",
      "digital marketing"
    ],
    "canonical_topics": [
      "recommendation-systems",
      "machine-learning",
      "consumer-behavior",
      "advertising",
      "data-engineering"
    ]
  },
  {
    "name": "CAISO OASIS",
    "description": "California ISO market data including prices, generation, demand, and transmission",
    "category": "Energy",
    "url": "http://oasis.caiso.com/",
    "docs_url": "http://www.caiso.com/market/Pages/MarketProcesses.aspx",
    "github_url": null,
    "tags": [
      "California",
      "prices",
      "wholesale",
      "real-time",
      "day-ahead"
    ],
    "best_for": "Studying wholesale electricity market dynamics and renewable integration",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "energy",
      "market-analysis",
      "pricing"
    ],
    "summary": "The CAISO OASIS dataset provides comprehensive market data from the California Independent System Operator, including essential metrics such as prices, generation, demand, and transmission. Researchers and analysts can leverage this dataset to conduct market analyses, assess energy trends, and optimize resource allocation in the energy sector.",
    "use_cases": [
      "Analyzing price fluctuations in the California energy market",
      "Assessing the impact of generation capacity on market prices",
      "Evaluating demand response strategies in real-time",
      "Conducting forecasting studies for energy supply and demand"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the current wholesale prices in the California ISO market?",
      "How does generation data correlate with demand in California's energy market?",
      "What trends can be observed in day-ahead pricing for California ISO?",
      "How does transmission capacity impact energy prices in California?",
      "What are the historical trends in California's energy generation?",
      "How can I analyze real-time demand data from CAISO OASIS?"
    ],
    "domain_tags": [
      "energy"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2009-present",
    "geographic_scope": "California",
    "size_category": "medium",
    "model_score": 0.0006,
    "embedding_text": "The CAISO OASIS dataset is a vital resource for understanding the dynamics of the California energy market. It encompasses a range of market data, including prices, generation, demand, and transmission metrics, structured in a tabular format that facilitates analysis. The dataset typically includes rows representing time intervals and columns for various market indicators. Key variables may include real-time prices, day-ahead prices, generation capacity, and demand levels, each measuring different aspects of the energy market. Researchers can utilize this dataset to explore a variety of research questions, such as the relationship between generation and demand, the impact of transmission constraints on pricing, and the effectiveness of market interventions. Common preprocessing steps might involve cleaning the data for missing values, normalizing price data, and aggregating generation and demand figures for comparative analysis. The dataset supports various types of analyses, including regression models, machine learning applications, and descriptive statistics, making it a versatile tool for both academic research and practical applications in energy economics. However, users should be aware of potential limitations, such as data granularity and the influence of external factors like policy changes and weather conditions on market behavior. Overall, CAISO OASIS serves as a foundational dataset for those looking to delve into energy market analysis and contribute to the understanding of California's unique energy landscape.",
    "tfidf_keywords": [
      "California ISO",
      "market data",
      "wholesale prices",
      "generation",
      "demand",
      "transmission",
      "real-time analysis",
      "day-ahead pricing",
      "energy trends",
      "resource allocation",
      "market dynamics",
      "price fluctuations",
      "demand response",
      "forecasting",
      "energy supply"
    ],
    "semantic_cluster": "energy-market-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "energy economics",
      "market analysis",
      "price forecasting",
      "demand forecasting",
      "resource optimization"
    ],
    "canonical_topics": [
      "econometrics",
      "forecasting",
      "pricing",
      "consumer-behavior",
      "energy"
    ]
  },
  {
    "name": "Indian Grocery (Flipkart Supermart)",
    "description": "Flipkart Supermart transaction and product details",
    "category": "Grocery & Supermarkets",
    "url": "https://www.kaggle.com/datasets/aryansingh95/flipkart-grocery-transaction-and-product-details",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "grocery",
      "India",
      "Flipkart"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Indian Grocery dataset from Flipkart Supermart contains transaction and product details related to grocery purchases in India. Researchers can utilize this dataset to analyze consumer behavior, pricing strategies, and market trends in the grocery sector.",
    "use_cases": [
      "Analyzing consumer purchasing patterns over time.",
      "Evaluating the effectiveness of promotional strategies.",
      "Studying the impact of product placement on sales.",
      "Investigating price elasticity of demand for grocery items."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the transaction trends in the Indian grocery market?",
      "How do pricing strategies affect consumer purchases on Flipkart?",
      "What products are most frequently bought in Indian grocery stores?",
      "How does seasonality impact grocery sales in India?",
      "What demographic factors influence grocery shopping behavior?",
      "How can we analyze customer loyalty through transaction data?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "India",
    "size_category": "medium",
    "model_score": 0.0006,
    "image_url": "/images/datasets/indian-grocery-flipkart-supermart.png",
    "embedding_text": "The Indian Grocery dataset from Flipkart Supermart provides a comprehensive view of transaction and product details within the grocery sector in India. This dataset is structured in a tabular format, consisting of rows representing individual transactions and columns detailing various attributes such as product ID, transaction date, quantity purchased, price, and customer demographics. The data is collected from Flipkart's online grocery platform, capturing a wide range of consumer interactions with products over time. Researchers can explore key variables such as product categories, pricing, and customer profiles, which can help in understanding market dynamics and consumer preferences. However, the dataset may have limitations regarding data quality, including potential inaccuracies in transaction reporting or missing demographic information. Common preprocessing steps may include handling missing values, normalizing price data, and aggregating transactions for analysis. This dataset supports various types of analyses, including regression analysis to examine the relationship between price and quantity sold, machine learning models for predicting consumer behavior, and descriptive statistics to summarize purchasing trends. Researchers typically use this dataset to address questions related to consumer behavior, pricing strategies, and market segmentation, making it a valuable resource for studies in e-commerce and retail economics.",
    "tfidf_keywords": [
      "transaction-analysis",
      "consumer-behavior",
      "pricing-strategy",
      "grocery-market",
      "e-commerce",
      "data-preprocessing",
      "market-segmentation",
      "sales-prediction",
      "customer-loyalty",
      "product-placement"
    ],
    "semantic_cluster": "e-commerce-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "pricing",
      "marketplaces",
      "data-preprocessing",
      "sales-analytics"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "pricing",
      "marketplaces",
      "data-engineering"
    ]
  },
  {
    "name": "FERC Form 714",
    "description": "Hourly electricity load data from major U.S. utilities and regional transmission organizations",
    "category": "Energy",
    "url": "https://www.ferc.gov/industries-data/electric/general-information/electric-industry-forms/form-714-annual-electric",
    "docs_url": "https://www.ferc.gov/industries-data/electric/general-information/electric-industry-forms/form-714-annual-electric",
    "github_url": null,
    "tags": [
      "load",
      "demand",
      "hourly",
      "utilities"
    ],
    "best_for": "Hourly demand analysis, load forecasting, and peak demand studies",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The FERC Form 714 dataset contains hourly electricity load data collected from major U.S. utilities and regional transmission organizations. This dataset can be utilized for analyzing electricity demand patterns, forecasting future load requirements, and understanding the dynamics of energy consumption across different regions.",
    "use_cases": [
      "Forecasting electricity demand for future planning",
      "Analyzing peak load periods to optimize energy distribution",
      "Studying the impact of weather on electricity consumption",
      "Evaluating the effectiveness of energy efficiency programs"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the FERC Form 714 dataset?",
      "How can I access hourly electricity load data from U.S. utilities?",
      "What insights can be gained from analyzing FERC Form 714 data?",
      "Where can I find data on electricity demand patterns?",
      "What are the key variables in the FERC Form 714 dataset?",
      "How do regional transmission organizations report electricity load?",
      "What methodologies are used to analyze hourly load data?",
      "What trends can be observed in U.S. electricity consumption?"
    ],
    "domain_tags": [
      "energy"
    ],
    "data_modality": "time-series",
    "temporal_coverage": "2006-present",
    "geographic_scope": "United States",
    "size_category": "medium",
    "model_score": 0.0006,
    "image_url": "/images/logos/ferc.png",
    "embedding_text": "The FERC Form 714 dataset is a comprehensive collection of hourly electricity load data sourced from major utilities and regional transmission organizations across the United States. This dataset is structured in a tabular format, with each row representing a specific hour of electricity load recorded by the reporting entities. Key variables typically include the timestamp, utility name, load values, and possibly other contextual information such as weather conditions or regional identifiers. The collection methodology involves standardized reporting by utilities to the Federal Energy Regulatory Commission (FERC), ensuring consistency and reliability in the data. However, users should be aware of potential limitations, such as reporting discrepancies or variations in data collection methods among different utilities. Common preprocessing steps may include handling missing values, normalizing load data, and aggregating data for specific time frames or regions. Researchers can leverage this dataset to address various research questions, such as identifying trends in electricity consumption, assessing the impact of policy changes on load patterns, or developing predictive models for future demand. The dataset supports a range of analytical techniques, including regression analysis, machine learning, and descriptive statistics, making it a valuable resource for energy economists, data scientists, and policy analysts. By analyzing the FERC Form 714 dataset, researchers can gain insights into the dynamics of electricity demand, which can inform decision-making in energy management and policy formulation.",
    "tfidf_keywords": [
      "hourly load data",
      "electricity demand",
      "regional transmission organizations",
      "utilities",
      "forecasting",
      "energy consumption",
      "load patterns",
      "data collection methodology",
      "energy efficiency",
      "peak load analysis"
    ],
    "semantic_cluster": "energy-demand-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "time-series analysis",
      "energy forecasting",
      "load forecasting",
      "data preprocessing",
      "policy evaluation"
    ],
    "canonical_topics": [
      "forecasting",
      "econometrics",
      "consumer-behavior",
      "policy-evaluation"
    ]
  },
  {
    "name": "Alibaba Fashion Combo",
    "description": "Fashion item combinations from Alibaba for outfit recommendation",
    "category": "Fashion & Apparel",
    "url": "https://tianchi.aliyun.com/dataset/131519",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "fashion",
      "outfit",
      "recommendation"
    ],
    "best_for": "Learning fashion & apparel analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Alibaba Fashion Combo dataset consists of various fashion item combinations sourced from Alibaba, designed to assist in outfit recommendations. Researchers and developers can utilize this dataset to analyze consumer preferences, improve recommendation algorithms, and enhance user experience in fashion e-commerce.",
    "use_cases": [
      "Analyzing consumer preferences in fashion",
      "Developing recommendation algorithms for e-commerce",
      "Studying trends in fashion combinations",
      "Evaluating the effectiveness of outfit recommendations"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the best outfit combinations from Alibaba?",
      "How can I use Alibaba Fashion Combo for outfit recommendations?",
      "What fashion items are popular on Alibaba?",
      "How do consumers select outfits based on Alibaba data?",
      "What patterns can be identified in Alibaba fashion combinations?",
      "How can this dataset improve fashion recommendation systems?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0006,
    "embedding_text": "The Alibaba Fashion Combo dataset is a curated collection of fashion item combinations specifically designed for the purpose of outfit recommendations. This dataset is structured in a tabular format, comprising rows that represent individual combinations of fashion items, and columns that detail various attributes of these items, such as item type, color, style, and other relevant features. The data is sourced from Alibaba, a leading e-commerce platform, which provides a rich variety of fashion products. The collection methodology involves aggregating data from user interactions and product listings on the platform, ensuring that the dataset reflects current trends and consumer preferences in the fashion industry. The dataset does not specify temporal or geographic coverage, but it is assumed to represent a diverse range of fashion items available on Alibaba at the time of collection. Key variables in the dataset include item categories, which measure the types of clothing or accessories, and attributes that describe the characteristics of each item, such as color and style. While the dataset is robust, potential limitations include the variability in item availability and the subjective nature of fashion preferences, which may affect the generalizability of findings derived from it. Common preprocessing steps may involve cleaning the data to handle missing values, normalizing item attributes for analysis, and encoding categorical variables for use in machine learning models. Researchers can leverage this dataset to address various research questions, such as identifying popular fashion trends, understanding consumer behavior in outfit selection, and evaluating the effectiveness of different recommendation strategies. The dataset supports a range of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile resource for both academic and commercial research in the fashion e-commerce domain. Typically, researchers utilize this dataset to develop and test recommendation algorithms, analyze consumer preferences, and explore the dynamics of fashion trends, thereby contributing valuable insights to the field of e-commerce and consumer behavior.",
    "tfidf_keywords": [
      "outfit-recommendation",
      "fashion-combinations",
      "consumer-preferences",
      "e-commerce",
      "Alibaba",
      "fashion-trends",
      "recommendation-algorithms",
      "data-aggregation",
      "item-attributes",
      "fashion-industry"
    ],
    "semantic_cluster": "fashion-recommendation-systems",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "recommendation-systems",
      "consumer-behavior",
      "data-analysis",
      "e-commerce",
      "fashion-trends"
    ],
    "canonical_topics": [
      "recommendation-systems",
      "consumer-behavior",
      "machine-learning"
    ]
  },
  {
    "name": "Criteo Counterfactual Learning",
    "description": "25M logged interactions with counterfactual propensity scores. Gold standard for offline policy evaluation and causal inference in ads",
    "category": "Advertising",
    "url": "https://ailab.criteo.com/criteo-uplift-prediction-dataset/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "causal inference",
      "counterfactual",
      "advertising",
      "uplift modeling",
      "offline evaluation"
    ],
    "best_for": "Learning advertising analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [],
    "summary": "The Criteo Counterfactual Learning dataset consists of 25 million logged interactions with counterfactual propensity scores, serving as a gold standard for offline policy evaluation and causal inference in advertising. Researchers can utilize this dataset to evaluate the effectiveness of different advertising strategies and to understand causal relationships in ad performance.",
    "use_cases": [
      "Evaluating the effectiveness of different ad placements",
      "Understanding consumer behavior in response to ads",
      "Analyzing the impact of ad features on performance",
      "Conducting uplift modeling to optimize ad strategies"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the Criteo Counterfactual Learning dataset?",
      "How can I use counterfactual propensity scores in advertising?",
      "What are the applications of causal inference in ads?",
      "How does offline policy evaluation work in advertising?",
      "What insights can be gained from analyzing logged interactions in ads?",
      "What is uplift modeling and how is it applied in this dataset?",
      "What are the key variables in the Criteo dataset?",
      "How can I evaluate advertising strategies using this dataset?"
    ],
    "domain_tags": [
      "advertising"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0006,
    "embedding_text": "The Criteo Counterfactual Learning dataset is a comprehensive collection of 25 million logged interactions, specifically designed to facilitate counterfactual analysis in the realm of advertising. This dataset is particularly valuable for researchers and practitioners interested in causal inference and policy evaluation, providing a gold standard for offline experiments. The data structure consists of various rows and columns, each representing different logged interactions and their associated counterfactual propensity scores. Key variables within the dataset include user interactions, ad impressions, and outcomes, which collectively measure the effectiveness of advertising strategies. The collection methodology involves aggregating user interaction data from Criteo's advertising platform, ensuring a robust dataset that reflects real-world advertising scenarios. However, researchers should be aware of potential limitations, such as the representativeness of the data and the challenges in generalizing findings across different contexts. Common preprocessing steps may include data cleaning, normalization, and feature engineering to prepare the dataset for analysis. Researchers can address a variety of research questions using this dataset, such as evaluating the impact of specific ad features on user engagement or determining the causal effects of different advertising strategies. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile tool for understanding advertising dynamics. Overall, the Criteo Counterfactual Learning dataset serves as a critical resource for advancing knowledge in causal inference and advertising effectiveness.",
    "benchmark_usage": [
      "Offline policy evaluation",
      "Causal inference in advertising"
    ],
    "tfidf_keywords": [
      "counterfactual",
      "propensity scores",
      "offline evaluation",
      "uplift modeling",
      "advertising strategies",
      "causal inference",
      "logged interactions",
      "policy evaluation",
      "consumer behavior",
      "ad performance"
    ],
    "semantic_cluster": "causal-inference-advertising",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "causal-inference",
      "uplift-modeling",
      "policy-evaluation",
      "advertising-strategies",
      "consumer-behavior"
    ],
    "canonical_topics": [
      "causal-inference",
      "experimentation",
      "advertising",
      "machine-learning",
      "policy-evaluation"
    ]
  },
  {
    "name": "Avazu Click-Through Rate Dataset",
    "description": "Mobile advertising dataset with 40+ million ad click records from Avazu mobile advertising platform",
    "category": "Advertising",
    "url": "https://www.kaggle.com/c/avazu-ctr-prediction/data",
    "docs_url": "https://www.kaggle.com/c/avazu-ctr-prediction",
    "github_url": null,
    "tags": [
      "mobile ads",
      "CTR",
      "Kaggle",
      "Avazu"
    ],
    "best_for": "Mobile-specific CTR prediction and feature engineering",
    "difficulty": "beginner",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "mobile advertising",
      "consumer behavior",
      "data analysis"
    ],
    "summary": "The Avazu Click-Through Rate Dataset is a comprehensive collection of over 40 million ad click records from the Avazu mobile advertising platform. This dataset allows researchers and data scientists to analyze mobile ad performance, understand consumer behavior, and develop predictive models for click-through rates.",
    "use_cases": [
      "Analyzing the effectiveness of mobile ad campaigns",
      "Predicting click-through rates using machine learning models",
      "Understanding consumer behavior in mobile advertising",
      "Evaluating the impact of ad features on click rates"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Avazu Click-Through Rate Dataset?",
      "How can I analyze mobile ad performance using the Avazu dataset?",
      "What insights can be gained from the Avazu mobile advertising dataset?",
      "Where can I find the Avazu Click-Through Rate Dataset?",
      "What are the key variables in the Avazu dataset?",
      "How does the Avazu dataset support regression analysis?",
      "What preprocessing steps are needed for the Avazu dataset?",
      "What research questions can be addressed with the Avazu dataset?"
    ],
    "domain_tags": [
      "advertising"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "10 days",
    "geographic_scope": "Global",
    "size_category": "medium",
    "model_score": 0.0006,
    "image_url": "/images/datasets/avazu-click-through-rate-dataset.png",
    "embedding_text": "The Avazu Click-Through Rate Dataset is a rich resource for those interested in mobile advertising analytics. Comprising over 40 million records, this dataset provides a detailed view of ad click behaviors and performance metrics. The data is structured in a tabular format, with rows representing individual ad impressions and clicks, and columns containing variables such as ad ID, user ID, timestamp, and click status. The collection methodology involves gathering data from the Avazu mobile advertising platform, which serves ads to a diverse user base, thus ensuring a wide range of ad interactions. While the dataset does not specify temporal or geographic coverage, it is assumed to encompass a broad demographic due to its extensive user engagement. Key variables include the click status, which indicates whether an ad was clicked, and various ad attributes that can be analyzed to determine their influence on click-through rates. Researchers often preprocess this data by handling missing values, encoding categorical variables, and normalizing numerical features to prepare for analysis. Common research questions include examining the factors that drive click-through rates, the effectiveness of different ad formats, and the impact of user demographics on ad engagement. The dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics. Researchers typically use this dataset to develop predictive models, conduct A/B testing, and derive insights into consumer behavior in the mobile advertising space.",
    "tfidf_keywords": [
      "click-through rate",
      "mobile advertising",
      "ad performance",
      "consumer behavior",
      "predictive modeling",
      "data preprocessing",
      "regression analysis",
      "machine learning",
      "ad features",
      "user engagement",
      "data collection",
      "advertising metrics",
      "data analysis",
      "ad impressions",
      "click status"
    ],
    "semantic_cluster": "mobile-advertising-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "advertising metrics",
      "consumer behavior",
      "predictive analytics",
      "data preprocessing",
      "machine learning"
    ],
    "canonical_topics": [
      "machine-learning",
      "consumer-behavior",
      "advertising",
      "data-engineering",
      "statistics"
    ]
  },
  {
    "name": "iPinYou RTB Dataset",
    "description": "Real-time bidding dataset from Chinese DSP with ~35GB of bid requests, impressions, clicks, and conversions with bidding prices",
    "category": "Advertising",
    "url": "https://contest.ipinyou.com/",
    "docs_url": "https://github.com/wnzhang/make-ipinyou-data",
    "github_url": "https://github.com/wnzhang/make-ipinyou-data",
    "tags": [
      "RTB",
      "bidding",
      "conversions",
      "iPinYou"
    ],
    "best_for": "Bid optimization and RTB algorithm research with actual market prices",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "advertising",
      "consumer-behavior",
      "bidding"
    ],
    "summary": "The iPinYou RTB Dataset is a comprehensive collection of real-time bidding data from a Chinese Demand-Side Platform (DSP), encompassing approximately 35GB of bid requests, impressions, clicks, and conversions along with their associated bidding prices. This dataset allows researchers and analysts to explore various aspects of online advertising, including bidding strategies, conversion rates, and consumer interactions in the digital marketplace.",
    "use_cases": [
      "Analyzing bidding strategies and their effectiveness",
      "Examining the relationship between bid prices and conversion rates",
      "Investigating consumer behavior in response to online advertisements",
      "Evaluating the performance of different advertising campaigns"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the iPinYou RTB Dataset?",
      "How can I analyze bid requests in the iPinYou dataset?",
      "What insights can be gained from the iPinYou RTB Dataset?",
      "What variables are included in the iPinYou RTB Dataset?",
      "How does real-time bidding work in the context of the iPinYou dataset?",
      "What are the implications of bidding prices on conversions in the iPinYou dataset?"
    ],
    "domain_tags": [
      "advertising"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "9 days (2013)",
    "geographic_scope": "China",
    "size_category": "large",
    "model_score": 0.0006,
    "image_url": "/images/datasets/ipinyou-rtb-dataset.png",
    "embedding_text": "The iPinYou RTB Dataset is a significant resource for researchers and practitioners in the field of online advertising and real-time bidding (RTB). This dataset comprises a vast collection of bid requests, impressions, clicks, and conversions, totaling approximately 35GB of data sourced from a leading Chinese demand-side platform (DSP). The data structure includes various columns that represent key variables such as bid prices, timestamps, user interactions, and conversion outcomes, which are critical for analyzing the dynamics of online advertising. The collection methodology involves capturing real-time bidding events, providing a granular view of how bids are placed and how they correlate with user engagement and conversion metrics. While the dataset offers a rich source of information, it is essential to be aware of potential data quality issues, such as missing values or biases in user behavior that may affect the analysis. Common preprocessing steps may include data cleaning, normalization, and feature engineering to prepare the data for various analytical techniques. Researchers can leverage this dataset to address a range of research questions, such as understanding the impact of bid prices on conversion rates, analyzing consumer behavior patterns in response to advertisements, and evaluating the effectiveness of different bidding strategies. The dataset supports various types of analyses, including regression analysis, machine learning models, and descriptive statistics, making it a versatile tool for both academic research and practical applications in the advertising industry. Overall, the iPinYou RTB Dataset serves as a valuable asset for those looking to explore the intricacies of real-time bidding and its implications for online marketing strategies.",
    "tfidf_keywords": [
      "real-time bidding",
      "DSP",
      "bid requests",
      "impressions",
      "clicks",
      "conversions",
      "bidding prices",
      "consumer behavior",
      "advertising strategies",
      "data analysis"
    ],
    "semantic_cluster": "real-time-bidding-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "advertising-analytics",
      "digital-marketing",
      "consumer-engagement",
      "bidding-strategies",
      "conversion-optimization"
    ],
    "canonical_topics": [
      "machine-learning",
      "consumer-behavior",
      "advertising"
    ]
  },
  {
    "name": "Harvard Dataverse Auctions",
    "description": "Auction-related replication datasets from Harvard Dataverse",
    "category": "Advertising",
    "url": "https://dataverse.harvard.edu/dataverse/harvard",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "auctions",
      "replication",
      "academic"
    ],
    "best_for": "Learning advertising analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Harvard Dataverse Auctions dataset comprises auction-related replication datasets that facilitate academic research in the field of advertising. Researchers can utilize this dataset to analyze bidding behaviors, pricing strategies, and consumer decision-making processes in auction settings.",
    "use_cases": [
      "Analyzing consumer bidding strategies",
      "Evaluating the impact of auction design on outcomes",
      "Studying price elasticity in auction settings"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the key variables in the Harvard Dataverse Auctions dataset?",
      "How can I analyze bidding behavior using this dataset?",
      "What insights can be gained from auction-related data?",
      "What replication studies are available in the Harvard Dataverse?",
      "How does auction design influence consumer behavior?",
      "What are common preprocessing steps for auction datasets?"
    ],
    "domain_tags": [
      "advertising"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0006,
    "image_url": "/images/logos/harvard.png",
    "embedding_text": "The Harvard Dataverse Auctions dataset is a collection of auction-related replication datasets designed to support academic research in advertising and consumer behavior. This dataset typically consists of structured tabular data, where each row represents a unique auction instance, and columns capture various attributes such as bidder identities, bid amounts, auction types, and timestamps. The collection methodology involves aggregating data from multiple auction platforms and academic studies, ensuring a diverse representation of auction scenarios. Researchers can explore key variables such as bid increments, final sale prices, and bidder demographics, which provide insights into bidding behaviors and market dynamics. However, researchers should be aware of potential limitations regarding data quality, including missing values and biases inherent in self-reported auction data. Common preprocessing steps may include data cleaning, normalization, and transformation to prepare the dataset for analysis. This dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics. Researchers typically use this dataset to address questions related to consumer decision-making, auction efficiency, and the effects of different auction formats on bidding behavior. By leveraging the insights gained from this dataset, scholars can contribute to the broader understanding of auction mechanisms and their implications for advertising strategies.",
    "tfidf_keywords": [
      "bidding-strategy",
      "auction-design",
      "consumer-decision-making",
      "price-elasticity",
      "replication-study",
      "advertising-economics",
      "market-dynamics",
      "data-preprocessing",
      "behavioral-insights",
      "bidder-demographics"
    ],
    "semantic_cluster": "auction-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "auction-theory",
      "marketplace-dynamics",
      "behavioral-economics",
      "econometrics"
    ],
    "canonical_topics": [
      "experimentation",
      "consumer-behavior",
      "econometrics",
      "pricing",
      "marketplaces"
    ]
  },
  {
    "name": "NBER Public Use Data Archive",
    "description": "Eclectic mix of economic, demographic, and enterprise data from NBER-affiliated research projects",
    "category": "Data Portals",
    "url": "https://www.nber.org/research/data",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "research",
      "research",
      "NBER",
      "demographics",
      "enterprise"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The NBER Public Use Data Archive provides a diverse collection of economic, demographic, and enterprise data sourced from various research projects affiliated with the National Bureau of Economic Research (NBER). Researchers can utilize this dataset to conduct analyses on economic trends, demographic shifts, and enterprise behaviors, facilitating a deeper understanding of economic phenomena.",
    "use_cases": [
      "Analyzing economic trends over time using demographic data.",
      "Examining the impact of enterprise behavior on economic outcomes.",
      "Conducting regression analyses to explore relationships between variables.",
      "Utilizing demographic data to inform policy decisions."
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What types of economic data are available in the NBER Public Use Data Archive?",
      "How can I access demographic data from NBER-affiliated research projects?",
      "What enterprise data does the NBER Public Use Data Archive include?",
      "Where can I find research datasets related to economics and demographics?",
      "What is the NBER Public Use Data Archive and what data does it offer?",
      "How can I use NBER data for economic research?",
      "What are the key features of the NBER Public Use Data Archive?",
      "Is the NBER Public Use Data Archive suitable for beginners in data analysis?"
    ],
    "domain_tags": [
      "economics",
      "demographics",
      "enterprise"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0006,
    "image_url": "/images/datasets/nber-public-use-data-archive.jpg",
    "embedding_text": "The NBER Public Use Data Archive is a comprehensive repository that houses an eclectic mix of economic, demographic, and enterprise data derived from various research projects affiliated with the National Bureau of Economic Research (NBER). This dataset is structured in a tabular format, consisting of numerous rows and columns that represent different variables pertinent to economic research. The data includes key variables that measure economic indicators, demographic characteristics, and enterprise metrics, providing a rich resource for researchers and analysts alike. The collection methodology involves gathering data from NBER-affiliated studies, ensuring a high level of reliability and relevance for economic analysis. However, users should be aware of potential limitations in data quality, such as missing values or inconsistencies that may arise from the diverse sources. Common preprocessing steps may include data cleaning, normalization, and transformation to prepare the dataset for analysis. Researchers typically leverage this archive to address a variety of research questions, such as examining the effects of economic policies, understanding demographic shifts, and analyzing enterprise performance. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile tool for both novice and experienced data scientists. By utilizing the NBER Public Use Data Archive, researchers can gain valuable insights into economic phenomena, contributing to the broader understanding of economic dynamics and informing policy decisions.",
    "tfidf_keywords": [
      "economic-indicators",
      "demographic-characteristics",
      "enterprise-metrics",
      "data-reliability",
      "data-cleaning",
      "normalization",
      "regression-analysis",
      "machine-learning",
      "descriptive-statistics",
      "policy-analysis",
      "NBER-affiliated-research"
    ],
    "semantic_cluster": "economic-data-archive",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "economic-research",
      "data-analysis",
      "policy-evaluation",
      "demographic-studies",
      "enterprise-analysis"
    ],
    "canonical_topics": [
      "econometrics",
      "consumer-behavior",
      "policy-evaluation"
    ]
  },
  {
    "name": "Census Business Dynamics Statistics",
    "description": "8M+ establishments with firm age data. Job creation/destruction, startups, exits. Longitudinal firm dynamics since 1977",
    "category": "Data Portals",
    "url": "https://www.census.gov/programs-surveys/bds.html",
    "docs_url": "https://www.census.gov/programs-surveys/bds/documentation.html",
    "github_url": null,
    "tags": [
      "Census",
      "firm dynamics",
      "startups",
      "employment",
      "longitudinal"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [],
    "summary": "The Census Business Dynamics Statistics dataset provides a comprehensive view of over 8 million establishments, detailing firm age data and metrics related to job creation, destruction, startups, and exits. This longitudinal dataset, which spans from 1977 to the present, allows researchers to analyze firm dynamics and understand trends in employment and business lifecycle.",
    "use_cases": [
      "Analyzing the impact of economic policies on job creation",
      "Studying the lifecycle of startups and their survival rates",
      "Investigating the relationship between firm age and employment levels"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the trends in job creation and destruction since 1977?",
      "How do firm ages correlate with startup rates?",
      "What insights can be drawn from longitudinal firm dynamics?",
      "What is the impact of business exits on local economies?",
      "How has the landscape of employment changed over the decades?",
      "What factors influence firm longevity?"
    ],
    "domain_tags": [
      "employment",
      "business-dynamics"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1977-present",
    "size_category": "massive",
    "model_score": 0.0006,
    "image_url": "/images/datasets/census-business-dynamics-statistics.jpg",
    "embedding_text": "The Census Business Dynamics Statistics dataset is a rich resource for understanding the dynamics of U.S. businesses over time. With over 8 million establishments represented, the dataset includes key variables such as firm age, job creation and destruction rates, and metrics related to startups and exits. The data is structured in a tabular format, with rows representing individual establishments and columns capturing various attributes, including firm age, employment figures, and operational status. This dataset has been collected through rigorous methodologies, leveraging administrative records and surveys to ensure comprehensive coverage of the business landscape. Researchers can utilize this dataset to address a variety of research questions, such as examining the effects of economic policies on job creation or analyzing the survival rates of startups. The longitudinal nature of the data allows for in-depth analyses, including regression models and machine learning techniques, to uncover trends and patterns in firm dynamics. However, researchers should be aware of potential limitations, such as data quality issues or changes in reporting standards over time. Common preprocessing steps may include data cleaning, handling missing values, and transforming variables for analysis. Overall, this dataset serves as a foundational tool for economists, policymakers, and data scientists interested in labor economics and industrial organization.",
    "tfidf_keywords": [
      "firm age",
      "job creation",
      "job destruction",
      "longitudinal data",
      "business exits",
      "startup rates",
      "economic policy",
      "employment trends",
      "business lifecycle",
      "establishment dynamics"
    ],
    "semantic_cluster": "business-dynamics-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "labor-economics",
      "industrial-organization",
      "causal-inference",
      "panel-data",
      "business-analytics"
    ],
    "canonical_topics": [
      "labor-economics",
      "industrial-organization",
      "econometrics"
    ],
    "benchmark_usage": [
      "Job creation/destruction analysis",
      "Startup and exit trends",
      "Longitudinal studies of firm dynamics"
    ]
  },
  {
    "name": "DrivenData Water Supply Forecasting (2024)",
    "description": "Western US water supply data from Bureau of Reclamation, $500K prize pool for seasonal forecasting",
    "category": "Data Portals",
    "url": "https://www.drivendata.org/competitions/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "climate",
      "forecasting",
      "2024",
      "government data",
      "real-world",
      "time series"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "climate",
      "forecasting",
      "government data",
      "real-world",
      "time series"
    ],
    "summary": "The DrivenData Water Supply Forecasting dataset provides comprehensive data on water supply in the Western United States, sourced from the Bureau of Reclamation. This dataset can be utilized for seasonal forecasting, enabling researchers and practitioners to develop predictive models and contribute to water resource management efforts.",
    "use_cases": [
      "Developing predictive models for seasonal water supply forecasting",
      "Analyzing the impact of climate change on water resources",
      "Evaluating government policies related to water management",
      "Creating visualizations to communicate water supply trends"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the DrivenData Water Supply Forecasting dataset?",
      "How can I access the Bureau of Reclamation water supply data?",
      "What are the seasonal forecasting challenges in the Western US?",
      "What variables are included in the DrivenData water supply dataset?",
      "How can this dataset be used for climate forecasting?",
      "What is the prize pool for the DrivenData Water Supply Forecasting competition?",
      "What time series analysis techniques can be applied to this dataset?",
      "What are the implications of water supply forecasting for government policy?"
    ],
    "domain_tags": [
      "government",
      "environment"
    ],
    "data_modality": "time-series",
    "temporal_coverage": "2024",
    "geographic_scope": "Western US",
    "size_category": "medium",
    "model_score": 0.0006,
    "embedding_text": "The DrivenData Water Supply Forecasting dataset is a valuable resource for researchers and practitioners interested in understanding and predicting water supply dynamics in the Western United States. This dataset is derived from the Bureau of Reclamation, which is responsible for managing water resources in this region. The dataset includes various variables related to water supply, such as historical water levels, precipitation data, and other climatic factors that influence water availability. The structure of the dataset typically consists of rows representing time points and columns representing different variables, allowing for time series analysis. Researchers can leverage this dataset to address critical research questions related to seasonal forecasting, water resource management, and the effects of climate change on water supply. Common preprocessing steps may include handling missing data, normalizing variables, and transforming time series data into a suitable format for analysis. The dataset supports various types of analyses, including regression modeling, machine learning approaches, and descriptive statistics. Researchers often use this dataset to develop predictive models that can inform policy decisions and improve water management strategies. However, it is essential to consider data quality and limitations, such as potential gaps in historical data or variability in measurement techniques. Overall, the DrivenData Water Supply Forecasting dataset serves as a crucial tool for advancing knowledge in the field of water resource management and climate forecasting.",
    "tfidf_keywords": [
      "water supply",
      "seasonal forecasting",
      "Bureau of Reclamation",
      "climate change",
      "predictive modeling",
      "time series analysis",
      "water resource management",
      "data preprocessing",
      "regression modeling",
      "machine learning"
    ],
    "semantic_cluster": "water-resource-forecasting",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "climate-change",
      "predictive-modeling",
      "time-series-analysis",
      "water-management",
      "environmental-policy"
    ],
    "canonical_topics": [
      "forecasting",
      "machine-learning",
      "statistics",
      "policy-evaluation"
    ]
  },
  {
    "name": "Google Play Store Dataset",
    "description": "2.3M apps with ratings, reviews, categories, sizes, installs. Android app marketplace data",
    "category": "App Stores",
    "url": "https://www.kaggle.com/datasets/gauthamp10/google-playstore-apps",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Google Play",
      "Android",
      "apps",
      "ratings",
      "Kaggle"
    ],
    "best_for": "Learning app stores analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Google Play Store Dataset comprises 2.3 million apps, providing comprehensive information on ratings, reviews, categories, sizes, and installs from the Android app marketplace. This dataset allows for various analyses, including app performance evaluation, market trend analysis, and consumer behavior insights.",
    "use_cases": [
      "Analyzing the relationship between app ratings and install counts.",
      "Identifying trends in app category popularity over time.",
      "Evaluating the impact of app size on user ratings.",
      "Exploring user sentiment through review analysis."
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the most popular app categories on Google Play?",
      "How do app ratings correlate with the number of installs?",
      "What trends can be observed in app sizes over time?",
      "How do user reviews impact app ratings?",
      "What are the common features of highly-rated apps?",
      "How does the app install count vary across different categories?",
      "What is the distribution of app ratings in the Google Play Store?",
      "How do app sizes relate to user ratings and reviews?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0006,
    "image_url": "/images/datasets/google-play-store-dataset.jpg",
    "embedding_text": "The Google Play Store Dataset is a rich collection of data encompassing over 2.3 million Android applications, offering insights into various aspects of the app marketplace. The dataset includes key variables such as app ratings, reviews, categories, sizes, and install counts, which are essential for understanding app performance and consumer preferences. The data is structured in a tabular format, with rows representing individual apps and columns detailing their attributes. This structure facilitates straightforward analysis and visualization. \n\nThe collection methodology for this dataset typically involves web scraping techniques to gather data from the Google Play Store, ensuring a comprehensive capture of the app landscape. However, users should be aware of potential data quality issues, such as missing values or inconsistencies in user-generated content, particularly in reviews. Common preprocessing steps may include cleaning the data to handle missing values, normalizing ratings, and categorizing apps based on their features. \n\nResearchers and analysts can utilize this dataset to address various research questions, such as examining the correlation between app ratings and the number of installs, or investigating how user reviews influence overall app performance. The dataset supports multiple types of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile tool for both academic research and industry applications. \n\nIn studies, researchers typically leverage this dataset to explore market trends, assess consumer behavior, and evaluate the effectiveness of marketing strategies in the app ecosystem. By analyzing the data, insights can be gained into user preferences, competitive dynamics, and the overall health of the app marketplace, contributing to informed decision-making for developers and marketers alike.",
    "tfidf_keywords": [
      "app ratings",
      "user reviews",
      "install counts",
      "market trends",
      "consumer behavior",
      "web scraping",
      "data quality",
      "preprocessing",
      "app categories",
      "Android applications",
      "data analysis",
      "sentiment analysis",
      "data visualization",
      "performance evaluation"
    ],
    "semantic_cluster": "app-marketplace-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "data-analysis",
      "market-trends",
      "sentiment-analysis",
      "app-performance"
    ],
    "canonical_topics": [
      "marketplaces",
      "consumer-behavior",
      "data-engineering",
      "statistics"
    ]
  },
  {
    "name": "FakeNewsNet",
    "description": "23K news articles labeled fake/real with social context. Includes PolitiFact and GossipCop sources",
    "category": "Content Moderation",
    "url": "https://github.com/KaiDMML/FakeNewsNet",
    "docs_url": null,
    "github_url": "https://github.com/KaiDMML/FakeNewsNet",
    "tags": [
      "fake news",
      "misinformation",
      "social media",
      "fact-checking"
    ],
    "best_for": "Learning content moderation analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "content moderation",
      "fake news detection",
      "social media analysis"
    ],
    "summary": "FakeNewsNet is a dataset comprising 23,000 news articles labeled as either fake or real, providing valuable insights into the landscape of misinformation. It includes sources from PolitiFact and GossipCop, making it a useful resource for researchers and practitioners interested in fact-checking and understanding the dynamics of news dissemination on social media.",
    "use_cases": [
      "Analyzing the spread of misinformation on social media platforms.",
      "Developing machine learning models for fake news detection.",
      "Evaluating the effectiveness of fact-checking organizations.",
      "Studying the social context surrounding news articles."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the FakeNewsNet dataset?",
      "How can I access the FakeNewsNet dataset?",
      "What sources are included in the FakeNewsNet dataset?",
      "What types of analyses can be performed with the FakeNewsNet dataset?",
      "How is the FakeNewsNet dataset structured?",
      "What are the key variables in the FakeNewsNet dataset?",
      "What research questions can be addressed using the FakeNewsNet dataset?",
      "What preprocessing steps are needed for the FakeNewsNet dataset?"
    ],
    "domain_tags": [
      "media",
      "technology"
    ],
    "data_modality": "text",
    "size_category": "medium",
    "model_score": 0.0006,
    "image_url": "/images/datasets/fakenewsnet.png",
    "embedding_text": "The FakeNewsNet dataset is a comprehensive collection of 23,000 news articles that have been meticulously labeled as either fake or real. This dataset is particularly valuable for researchers and practitioners in the fields of content moderation, misinformation studies, and social media analysis. It draws from reputable sources such as PolitiFact and GossipCop, which are well-known for their fact-checking capabilities. The structure of the dataset includes various columns that capture essential variables such as article text, publication date, source, and labels indicating the veracity of the news. The collection methodology involved scraping articles from these sources, ensuring a diverse representation of news topics and styles. While the dataset is robust, researchers should be aware of potential limitations, including biases in source selection and the evolving nature of misinformation. Common preprocessing steps may include text normalization, tokenization, and the removal of stop words to prepare the data for analysis. Researchers can leverage this dataset to address critical questions regarding the factors that contribute to the spread of fake news, the role of social media in amplifying misinformation, and the effectiveness of various fact-checking strategies. The dataset supports a range of analyses, including regression modeling, machine learning classification tasks, and descriptive statistics. By utilizing the FakeNewsNet dataset, researchers can gain insights into the dynamics of news dissemination and contribute to the broader discourse on media literacy and misinformation.",
    "tfidf_keywords": [
      "fake news",
      "misinformation",
      "fact-checking",
      "PolitiFact",
      "GossipCop",
      "social media",
      "news articles",
      "content moderation",
      "data labeling",
      "machine learning",
      "text analysis",
      "information dissemination",
      "media bias",
      "news verification",
      "article classification"
    ],
    "semantic_cluster": "fake-news-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "natural-language-processing",
      "machine-learning",
      "text-mining",
      "information-retrieval",
      "social-network-analysis"
    ],
    "canonical_topics": [
      "natural-language-processing",
      "machine-learning",
      "consumer-behavior",
      "policy-evaluation"
    ],
    "benchmark_usage": [
      "Common uses include fake news detection and misinformation analysis."
    ]
  },
  {
    "name": "SIPRI Arms Industry Database",
    "description": "Financial data on the 100 largest defense companies worldwide including revenue, profits, and employment",
    "category": "Defense Economics",
    "url": "https://www.sipri.org/databases/armsindustry",
    "docs_url": "https://www.sipri.org/databases/armsindustry/sources-and-methods",
    "github_url": null,
    "tags": [
      "defense industry",
      "contractors",
      "companies",
      "SIPRI"
    ],
    "best_for": "Analyzing defense industry concentration and contractor performance",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "defense industry",
      "economics",
      "financial analysis"
    ],
    "summary": "The SIPRI Arms Industry Database provides comprehensive financial data on the 100 largest defense companies globally, including key metrics such as revenue, profits, and employment figures. This dataset allows researchers and analysts to explore trends in the defense sector, assess company performance, and understand the economic implications of defense spending.",
    "use_cases": [
      "Analyzing the financial health of major defense contractors.",
      "Investigating employment trends within the defense sector.",
      "Comparing revenue and profit margins across different defense companies.",
      "Studying the impact of government defense spending on company performance."
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the largest defense companies by revenue?",
      "How do profits in the defense industry compare across companies?",
      "What employment trends can be observed in the defense sector?",
      "How has the financial performance of defense contractors changed over time?",
      "What is the relationship between defense spending and company profits?",
      "Which companies are the top employers in the defense industry?"
    ],
    "domain_tags": [
      "defense",
      "economics"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2002-present",
    "geographic_scope": "Global",
    "size_category": "medium",
    "model_score": 0.0006,
    "embedding_text": "The SIPRI Arms Industry Database is a vital resource for understanding the financial landscape of the global defense sector. This dataset encompasses detailed financial information on the 100 largest defense companies worldwide, including critical variables such as revenue, profits, and employment figures. The data is structured in a tabular format, with rows representing individual companies and columns detailing various financial metrics. Researchers can leverage this dataset to conduct a variety of analyses, including regression analyses to identify trends and correlations within the defense industry. The collection methodology involves rigorous data gathering from reputable sources, ensuring high data quality and reliability. However, users should be aware of potential limitations, such as variations in reporting standards across different countries and companies. Common preprocessing steps may include normalization of financial figures and handling missing data. This dataset supports a range of research questions, from assessing the economic impact of defense spending to evaluating the competitive landscape of the defense industry. Analysts typically use this data to perform descriptive statistics, financial modeling, and comparative analyses, making it an essential tool for anyone studying defense economics.",
    "tfidf_keywords": [
      "defense contractors",
      "financial metrics",
      "revenue analysis",
      "profit margins",
      "employment statistics",
      "industry trends",
      "economic impact",
      "data normalization",
      "comparative analysis",
      "financial modeling"
    ],
    "semantic_cluster": "defense-economics-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "financial analysis",
      "economic indicators",
      "defense spending",
      "industry performance",
      "market analysis"
    ],
    "canonical_topics": [
      "econometrics",
      "finance",
      "industrial-organization"
    ]
  },
  {
    "name": "Spotify Million Playlist",
    "description": "1M playlists with 2M unique tracks from 300K artists. RecSys 2018 Challenge for playlist continuation research",
    "category": "Entertainment & Media",
    "url": "https://www.aicrowd.com/challenges/spotify-million-playlist-dataset-challenge",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Spotify",
      "playlists",
      "music",
      "recommendations",
      "RecSys"
    ],
    "best_for": "Learning entertainment & media analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "recommendation-systems",
      "music",
      "data-analysis"
    ],
    "summary": "The Spotify Million Playlist dataset consists of 1 million playlists featuring 2 million unique tracks from 300,000 artists, designed for research in playlist continuation. Researchers can utilize this dataset to explore music recommendations, analyze user preferences, and develop algorithms for enhancing playlist generation.",
    "use_cases": [
      "Analyzing user preferences in music playlists",
      "Developing algorithms for playlist continuation",
      "Exploring trends in music consumption",
      "Evaluating the effectiveness of recommendation systems"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Spotify Million Playlist dataset?",
      "How can I access the Spotify Million Playlist data?",
      "What types of analyses can be performed on the Spotify Million Playlist?",
      "What are the unique tracks in the Spotify Million Playlist dataset?",
      "How many artists are represented in the Spotify Million Playlist?",
      "What research can be conducted using the Spotify Million Playlist dataset?",
      "What are the key features of the Spotify Million Playlist?",
      "How does the Spotify Million Playlist contribute to recommendation systems research?"
    ],
    "domain_tags": [
      "entertainment",
      "media"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0006,
    "image_url": "/images/datasets/spotify-million-playlist.png",
    "embedding_text": "The Spotify Million Playlist dataset is a rich resource for researchers interested in the intersection of music and technology, particularly in the realm of recommendation systems. This dataset comprises 1 million playlists, each containing a diverse selection of tracks, totaling 2 million unique songs contributed by approximately 300,000 artists. The structure of the dataset is primarily tabular, with rows representing individual playlists and columns capturing various attributes such as playlist ID, track IDs, artist IDs, and potentially additional metadata related to the tracks or playlists. The collection methodology for this dataset is based on data extracted from Spotify's platform, focusing on user-generated playlists that reflect real-world listening habits and preferences.\n\nThe dataset's coverage is extensive in terms of the variety of music it encompasses, but it does not specify temporal or geographic limitations, making it applicable to a wide range of analyses. Key variables include playlist identifiers, track identifiers, and artist identifiers, which allow researchers to measure relationships between playlists and their constituent tracks, analyze artist popularity, and explore the dynamics of playlist creation.\n\nData quality is generally high, given that it originates from a well-established platform like Spotify; however, limitations may include biases inherent in user-generated content and potential gaps in metadata for certain tracks or artists. Common preprocessing steps might involve cleaning the data to remove duplicates, handling missing values, and transforming categorical variables into a suitable format for analysis.\n\nResearchers can leverage this dataset to address various research questions, such as understanding the factors that influence playlist popularity, the evolution of music trends over time, and the effectiveness of different recommendation strategies. The types of analyses supported by this dataset include regression analysis, machine learning applications for predictive modeling, and descriptive statistics to summarize playlist characteristics.\n\nIn studies, researchers typically use the Spotify Million Playlist dataset to develop and test algorithms aimed at enhancing music recommendation systems, providing insights into user behavior, and contributing to the broader understanding of music consumption patterns in the digital age.",
    "tfidf_keywords": [
      "playlist-continuation",
      "music-recommendations",
      "user-preferences",
      "algorithm-development",
      "data-extraction",
      "track-attributes",
      "artist-popularity",
      "music-consumption",
      "user-generated-content",
      "data-preprocessing",
      "predictive-modeling",
      "descriptive-statistics",
      "recommendation-strategies"
    ],
    "semantic_cluster": "music-recommendation-systems",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "recommendation-systems",
      "user-behavior",
      "data-mining",
      "music-analytics",
      "machine-learning"
    ],
    "canonical_topics": [
      "recommendation-systems",
      "machine-learning",
      "consumer-behavior"
    ]
  },
  {
    "name": "Queue-Times API",
    "description": "Real-time wait time data API for 80+ theme parks worldwide. Live queueing data for attractions.",
    "category": "Entertainment & Media",
    "url": "https://queue-times.com/en-US/pages/api",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "queueing",
      "theme-park",
      "wait-times",
      "API",
      "real-time",
      "global"
    ],
    "best_for": "Queueing theory - real-time queue monitoring, cross-park comparison, demand forecasting",
    "image_url": "/images/logos/queue-times.png",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "entertainment",
      "data-collection"
    ],
    "summary": "The Queue-Times API provides real-time wait time data for over 80 theme parks around the world, allowing users to access live queueing information for various attractions. This dataset can be utilized for analyzing visitor patterns, optimizing park operations, and enhancing visitor experience through informed decision-making.",
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Queue-Times API?",
      "How can I access real-time wait times for theme parks?",
      "What data does the Queue-Times API provide?",
      "How does the Queue-Times API improve visitor experience?",
      "What are the benefits of using the Queue-Times API for theme parks?",
      "Can I get global wait time data for attractions?",
      "How many theme parks are covered by the Queue-Times API?",
      "What types of attractions are included in the Queue-Times API?"
    ],
    "use_cases": [
      "Analyzing visitor wait times to improve park operations",
      "Optimizing attraction scheduling based on real-time data",
      "Enhancing visitor experience through informed planning",
      "Comparing wait times across different parks globally"
    ],
    "embedding_text": "The Queue-Times API is a robust resource that provides real-time wait time data for over 80 theme parks worldwide, offering a unique opportunity for researchers and analysts interested in the entertainment sector. The data structure typically includes various attributes such as park name, attraction name, current wait time, and timestamp, allowing for comprehensive analyses of visitor behavior and operational efficiency. Data is collected through a combination of user-generated inputs and automated tracking systems, ensuring a high level of accuracy and timeliness. While the dataset covers a wide geographic area, specific demographic details may not be included, which could limit certain types of analysis. Key variables such as wait times and attraction popularity can be instrumental in addressing research questions related to visitor flow, operational bottlenecks, and overall park efficiency. However, users should be aware of potential limitations in data quality, including variability in user reporting and system reliability. Common preprocessing steps may involve cleaning the data for inconsistencies, normalizing wait times across different parks, and aggregating data for trend analysis. Researchers can leverage this dataset for various types of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it a versatile tool for those studying consumer behavior in the entertainment industry. The Queue-Times API serves as a valuable asset for enhancing visitor experiences, optimizing park operations, and informing strategic decisions based on real-time insights.",
    "domain_tags": [
      "entertainment"
    ],
    "data_modality": "mixed",
    "geographic_scope": "global",
    "size_category": "medium",
    "tfidf_keywords": [
      "real-time",
      "queueing",
      "theme-parks",
      "wait-times",
      "API",
      "attractions",
      "visitor-patterns",
      "data-collection",
      "operational-efficiency",
      "consumer-behavior",
      "live-data",
      "global-coverage",
      "park-optimization",
      "visitor-experience",
      "data-analytics"
    ],
    "semantic_cluster": "real-time-queueing-data",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "visitor-flow",
      "operational-bottlenecks",
      "consumer-behavior",
      "data-analytics",
      "entertainment-industry"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "data-engineering",
      "statistics"
    ],
    "model_score": 0.0006
  },
  {
    "name": "Nasdaq Data Link",
    "description": "250+ datasets from 400+ publishers with API access - formerly Quandl",
    "category": "Dataset Aggregators",
    "url": "https://data.nasdaq.com",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "finance",
      "Quandl",
      "alternative data",
      "API"
    ],
    "best_for": "Financial econometrics and alternative data research with Python/R/Excel",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Nasdaq Data Link provides access to over 250 datasets from more than 400 publishers, previously known as Quandl. Users can leverage this extensive collection for various financial analyses, utilizing API access to integrate data into their applications seamlessly.",
    "use_cases": [
      "Financial trend analysis",
      "Market research",
      "Investment strategy development",
      "Data visualization for financial metrics"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What datasets are available on Nasdaq Data Link?",
      "How can I access financial data via API?",
      "What is the history of Quandl and its transformation into Nasdaq Data Link?",
      "Which publishers contribute to Nasdaq Data Link?",
      "What types of financial analyses can I perform with this dataset?",
      "How do I integrate Nasdaq Data Link with Python?",
      "What are the benefits of using API access for financial datasets?",
      "What alternative data can I find on Nasdaq Data Link?"
    ],
    "domain_tags": [
      "fintech"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0005,
    "image_url": "/images/datasets/nasdaq-data-link.jpg",
    "embedding_text": "The Nasdaq Data Link is a comprehensive platform that aggregates over 250 datasets from more than 400 publishers, offering a wealth of financial data for researchers, analysts, and developers. This dataset is particularly valuable for those interested in finance and alternative data, as it provides API access to a diverse range of information. The data structure typically consists of rows and columns, where each row represents a unique observation and each column corresponds to a specific variable, such as stock prices, trading volumes, or economic indicators. The collection methodology involves partnerships with various publishers, ensuring a broad spectrum of data sources that cover different aspects of the financial market. While the dataset does not specify temporal or geographic coverage, it is known to include a variety of financial metrics that can be analyzed over time. Key variables may include daily stock prices, historical trading volumes, and other financial indicators that measure market performance. Users should be aware of potential limitations in data quality, such as inconsistencies across different publishers or gaps in historical data. Common preprocessing steps may involve cleaning the data, handling missing values, and transforming variables for analysis. Researchers can address a range of questions using this dataset, such as identifying trends in stock performance, evaluating the impact of economic events on market behavior, or developing predictive models for investment strategies. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics. Researchers typically use the Nasdaq Data Link in studies to enhance their understanding of financial markets, inform investment decisions, and explore the implications of alternative data sources on traditional financial metrics.",
    "tfidf_keywords": [
      "financial-data",
      "API-access",
      "alternative-data",
      "market-research",
      "investment-strategy",
      "data-aggregation",
      "Quandl",
      "financial-metrics",
      "data-visualization",
      "trading-volumes",
      "economic-indicators",
      "data-quality",
      "preprocessing",
      "predictive-modeling",
      "financial-analytics"
    ],
    "semantic_cluster": "financial-data-aggregation",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "financial-analysis",
      "data-integration",
      "market-trends",
      "investment-research",
      "alternative-data"
    ],
    "canonical_topics": [
      "finance",
      "data-engineering",
      "consumer-behavior"
    ]
  },
  {
    "name": "Criteo Kaggle CTR Dataset",
    "description": "Standard CTR prediction benchmark with ~45 million records across 7 days, widely used for model comparison",
    "category": "Advertising",
    "url": "https://www.kaggle.com/c/criteo-display-ad-challenge/data",
    "docs_url": "https://www.kaggle.com/c/criteo-display-ad-challenge",
    "github_url": null,
    "tags": [
      "CTR prediction",
      "benchmark",
      "Kaggle",
      "Criteo"
    ],
    "best_for": "Benchmarking CTR prediction models against standard baseline",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "advertising"
    ],
    "summary": "The Criteo Kaggle CTR Dataset is a standard benchmark for click-through rate (CTR) prediction, containing approximately 45 million records collected over a week. It is widely utilized for model comparison in advertising analytics and machine learning applications.",
    "use_cases": [
      "Model comparison for CTR prediction",
      "Feature engineering for advertising analytics",
      "Benchmarking machine learning algorithms",
      "Evaluating click prediction models"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the Criteo Kaggle CTR Dataset?",
      "How can I use the Criteo dataset for CTR prediction?",
      "What are the key variables in the Criteo CTR dataset?",
      "Where can I find the Criteo Kaggle CTR Dataset?",
      "What are common preprocessing steps for CTR datasets?",
      "How is the Criteo dataset structured?",
      "What analyses can be performed with the Criteo CTR dataset?",
      "What are the limitations of the Criteo Kaggle CTR Dataset?"
    ],
    "domain_tags": [
      "advertising"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "7 days",
    "geographic_scope": "Global",
    "size_category": "medium",
    "benchmark_usage": [
      "Model comparison",
      "Benchmarking algorithms"
    ],
    "model_score": 0.0005,
    "image_url": "/images/datasets/criteo-kaggle-ctr-dataset.png",
    "embedding_text": "The Criteo Kaggle CTR Dataset serves as a pivotal resource for researchers and practitioners in the field of advertising analytics, particularly in the area of click-through rate (CTR) prediction. This dataset comprises approximately 45 million records collected over a span of seven days, making it a substantial benchmark for evaluating various machine learning models. The dataset is structured in a tabular format, featuring numerous columns that represent different features relevant to online advertising. Key variables typically include user demographics, ad characteristics, and contextual information surrounding the ad display. Researchers often leverage this dataset to address critical questions regarding user behavior, ad effectiveness, and the overall performance of predictive models. Common preprocessing steps involve handling missing values, encoding categorical variables, and normalizing numerical features to enhance model performance. The dataset's extensive size allows for robust statistical analyses, including regression and machine learning techniques, enabling users to derive insights into consumer behavior and optimize advertising strategies. However, it is essential to be aware of potential limitations, such as biases in the data collection process and the need for careful interpretation of results. Overall, the Criteo Kaggle CTR Dataset is a valuable asset for those looking to delve into the intricacies of CTR prediction and its applications in the advertising domain.",
    "tfidf_keywords": [
      "click-through rate",
      "CTR prediction",
      "advertising analytics",
      "machine learning",
      "feature engineering",
      "model comparison",
      "benchmarking",
      "user behavior",
      "data preprocessing",
      "predictive modeling"
    ],
    "semantic_cluster": "advertising-analytics",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "machine-learning",
      "consumer-behavior",
      "advertising-strategies",
      "data-preprocessing",
      "predictive-modeling"
    ],
    "canonical_topics": [
      "machine-learning",
      "advertising",
      "consumer-behavior"
    ]
  },
  {
    "name": "Stanford Amazon/Beer",
    "description": "Amazon product data and BeerAdvocate reviews from Stanford SNAP",
    "category": "Entertainment & Media",
    "url": "https://snap.stanford.edu/data/#amazon",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "reviews",
      "Stanford",
      "beer",
      "Amazon"
    ],
    "best_for": "Learning entertainment & media analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "text-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "reviews"
    ],
    "summary": "The Stanford Amazon/Beer dataset combines product data from Amazon with user reviews from BeerAdvocate, providing a rich resource for analyzing consumer preferences and product evaluations in the beer market. Researchers can leverage this dataset to explore correlations between product attributes and review sentiments, as well as to conduct comparative analyses of consumer behavior across different platforms.",
    "use_cases": [
      "Analyzing the relationship between beer characteristics and consumer ratings.",
      "Comparing review sentiments across different beer brands.",
      "Exploring trends in consumer preferences over time.",
      "Conducting regression analysis to predict beer ratings based on product features."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the key features of the Stanford Amazon/Beer dataset?",
      "How can I analyze consumer reviews from BeerAdvocate?",
      "What insights can be gained from Amazon product data?",
      "How do beer ratings correlate with product features?",
      "What trends can be identified in consumer behavior regarding beer?",
      "How can I use this dataset for sentiment analysis?"
    ],
    "domain_tags": [
      "retail",
      "entertainment"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0005,
    "image_url": "/images/logos/stanford.png",
    "embedding_text": "The Stanford Amazon/Beer dataset is a unique collection that integrates product data from Amazon with user-generated reviews from BeerAdvocate, specifically curated by Stanford SNAP. This dataset is structured in a tabular format, containing rows that represent individual products and their associated reviews, with columns detailing various attributes such as product ID, beer style, alcohol content, user ratings, and textual reviews. The collection methodology involves scraping data from Amazon and BeerAdvocate, ensuring a comprehensive representation of consumer opinions and product features in the beer market. Key variables in this dataset include product identifiers, review scores, and textual content of reviews, which measure consumer satisfaction and sentiment towards different beer products. However, researchers should be aware of potential data quality issues, such as missing values in reviews or inconsistencies in product descriptions, which may require preprocessing steps like text normalization and handling of null entries. This dataset supports a variety of analyses, including regression modeling to understand the impact of product features on consumer ratings, machine learning techniques for sentiment classification, and descriptive statistics to summarize consumer preferences. Researchers typically utilize this dataset to address questions related to consumer behavior, product evaluation, and market trends in the beer industry.",
    "tfidf_keywords": [
      "sentiment-analysis",
      "consumer-preferences",
      "product-evaluation",
      "text-mining",
      "regression-modeling",
      "data-scraping",
      "beer-reviews",
      "e-commerce-data",
      "user-generated-content",
      "market-analysis"
    ],
    "semantic_cluster": "consumer-behavior-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "sentiment-analysis",
      "market-research",
      "data-scraping",
      "text-mining",
      "consumer-behavior"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "recommendation-systems",
      "machine-learning",
      "natural-language-processing",
      "econometrics"
    ]
  },
  {
    "name": "Goodreads",
    "description": "Book information and user reviews from Goodreads platform",
    "category": "Entertainment & Media",
    "url": "https://mengtingwan.github.io/data/goodreads.html#datasets",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "books",
      "reviews",
      "recommendations"
    ],
    "best_for": "Learning entertainment & media analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "consumer-behavior",
      "recommendation-systems"
    ],
    "summary": "The Goodreads dataset contains comprehensive book information and user-generated reviews from the Goodreads platform. This dataset allows researchers and analysts to explore trends in reading preferences, analyze user sentiment, and develop recommendation systems based on user reviews.",
    "use_cases": [
      "Analyzing trends in book ratings over time",
      "Developing a recommendation system based on user reviews",
      "Sentiment analysis of user reviews to gauge reader satisfaction",
      "Exploring the relationship between book genres and user engagement"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the most popular books on Goodreads?",
      "How do user reviews correlate with book ratings?",
      "What genres are most frequently reviewed on Goodreads?",
      "How has the average rating of books changed over time?",
      "What are common themes in user reviews for top-rated books?",
      "How do recommendations differ across user demographics?"
    ],
    "domain_tags": [
      "entertainment",
      "media"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0005,
    "image_url": "/images/logos/github.png",
    "embedding_text": "The Goodreads dataset is a rich collection of book information and user reviews sourced from the Goodreads platform, which is a popular social cataloging website that allows users to freely search its database of books, reviews, and recommendations. The dataset is structured in a tabular format, typically consisting of rows representing individual books and columns that include various attributes such as book titles, authors, genres, publication dates, user ratings, and textual reviews. Each review provides insights into user sentiments and preferences, making it a valuable resource for understanding consumer behavior in the literary market. The collection methodology involves aggregating user-generated content from the Goodreads platform, which ensures a diverse range of opinions and experiences related to different books. However, it is important to note that the dataset may have limitations in terms of data quality, as user reviews can vary in depth and may include biases based on personal preferences. Common preprocessing steps may include text normalization, sentiment scoring, and filtering out reviews that do not meet certain quality criteria. Researchers can leverage this dataset to address various research questions, such as identifying trends in book popularity, analyzing the impact of user reviews on book sales, or exploring the relationship between book characteristics and user ratings. The dataset supports a variety of analyses, including regression analysis, machine learning applications for recommendation systems, and descriptive statistics to summarize user engagement. Typically, researchers utilize this dataset to conduct studies on consumer behavior, develop predictive models for book recommendations, and perform sentiment analysis to better understand reader satisfaction and preferences.",
    "tfidf_keywords": [
      "user-reviews",
      "book-recommendations",
      "sentiment-analysis",
      "consumer-preferences",
      "reading-trends",
      "genre-analysis",
      "rating-prediction",
      "text-mining",
      "data-aggregation",
      "user-engagement"
    ],
    "semantic_cluster": "recommendation-systems",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "sentiment-analysis",
      "text-mining",
      "recommendation-systems",
      "data-aggregation"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "recommendation-systems",
      "natural-language-processing"
    ]
  },
  {
    "name": "Ukraine Procurement (ProZorro)",
    "description": "Public procurement data from ProZorro system",
    "category": "Auctions & Marketplaces",
    "url": "https://www.kaggle.com/datasets/oleksastepaniuk/prozorro-public-procurement-dataset",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "procurement",
      "government",
      "Ukraine"
    ],
    "best_for": "Learning auctions & marketplaces analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "procurement",
      "government",
      "Ukraine"
    ],
    "summary": "The Ukraine Procurement dataset contains public procurement data sourced from the ProZorro system, which facilitates transparency in government spending. Researchers and analysts can utilize this dataset to explore procurement trends, analyze bidding behaviors, and assess the efficiency of public spending in Ukraine.",
    "use_cases": [
      "Analyzing bidding patterns in public procurement",
      "Assessing the impact of procurement policies on market competition",
      "Evaluating the efficiency of public spending in Ukraine",
      "Investigating the relationship between procurement practices and economic outcomes"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the ProZorro procurement system in Ukraine?",
      "How can I access public procurement data from Ukraine?",
      "What trends can be identified in Ukraine's public procurement?",
      "How does government procurement work in Ukraine?",
      "What are the key variables in the ProZorro dataset?",
      "How can procurement data be analyzed for efficiency?",
      "What are the implications of procurement data for government transparency?",
      "What tools can be used to analyze ProZorro procurement data?"
    ],
    "domain_tags": [
      "government",
      "public-sector"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Ukraine",
    "size_category": "medium",
    "model_score": 0.0005,
    "image_url": "/images/datasets/ukraine-procurement-prozorro.png",
    "embedding_text": "The Ukraine Procurement dataset, derived from the ProZorro system, provides a comprehensive view of public procurement activities in Ukraine. It is structured in a tabular format, consisting of rows representing individual procurement transactions and columns detailing various attributes such as procurement ID, supplier information, contract values, and dates of transactions. The data is collected through the ProZorro platform, which was established to enhance transparency and efficiency in government procurement processes. The dataset covers a range of procurement types and is instrumental for researchers interested in the dynamics of public sector spending. Key variables within the dataset include contract amounts, supplier names, and procurement categories, which allow for in-depth analysis of spending patterns and supplier performance. However, researchers should be aware of potential data quality issues, such as incomplete records or discrepancies in supplier reporting. Common preprocessing steps may include data cleaning, normalization, and the handling of missing values to prepare the dataset for analysis. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, enabling users to address research questions related to procurement efficiency, market competition, and the effectiveness of public spending. Researchers typically leverage this dataset to conduct empirical studies that inform policy decisions and enhance the understanding of public procurement mechanisms.",
    "tfidf_keywords": [
      "public-procurement",
      "ProZorro",
      "government-transparency",
      "bidding-patterns",
      "contract-values",
      "supplier-performance",
      "procurement-efficiency",
      "market-competition",
      "data-quality",
      "empirical-studies",
      "public-spending",
      "procurement-policies",
      "economic-outcomes",
      "data-cleaning"
    ],
    "semantic_cluster": "public-procurement-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "government-transparency",
      "public-sector-economics",
      "data-analysis",
      "market-competition",
      "policy-evaluation"
    ],
    "canonical_topics": [
      "policy-evaluation",
      "econometrics",
      "consumer-behavior",
      "marketplaces"
    ]
  },
  {
    "name": "Indian Sales",
    "description": "Sales forecasting dataset for small basket items in India",
    "category": "Grocery & Supermarkets",
    "url": "https://www.kaggle.com/datasets/girishvutukuri/sales-forecasting-for-small-basket",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "India",
      "forecasting",
      "retail"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Indian Sales dataset is designed for sales forecasting of small basket items in India. It provides valuable insights into retail sales patterns, enabling users to predict future sales and optimize inventory management.",
    "use_cases": [
      "Forecasting future sales trends for small basket items",
      "Analyzing consumer purchasing behavior in grocery stores",
      "Optimizing inventory management based on sales predictions"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Indian Sales dataset?",
      "How can I use the Indian Sales dataset for forecasting?",
      "What insights can be derived from the Indian Sales dataset?",
      "Where can I find sales forecasting datasets for India?",
      "What variables are included in the Indian Sales dataset?",
      "How to analyze retail sales data in India?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "India",
    "size_category": "medium",
    "model_score": 0.0005,
    "image_url": "/images/datasets/indian-sales.png",
    "embedding_text": "The Indian Sales dataset serves as a comprehensive resource for researchers and practitioners interested in sales forecasting within the grocery and supermarket sector in India. This dataset is structured in a tabular format, containing rows that represent individual sales transactions and columns that capture various attributes such as item identifiers, sales amounts, timestamps, and possibly demographic information about the customers. The collection methodology may involve aggregating sales data from point-of-sale systems across various retail locations, ensuring a diverse representation of consumer behavior. While the exact temporal and demographic coverage is not specified, the dataset is expected to reflect recent sales trends in the Indian market, making it relevant for current analyses. Key variables likely include sales volume, product categories, and time of purchase, which can be instrumental in understanding purchasing patterns and seasonal trends. However, users should be aware of potential limitations such as missing data or biases in the collection process. Common preprocessing steps may include handling missing values, normalizing sales figures, and encoding categorical variables for analysis. Researchers can leverage this dataset to address a variety of research questions, such as how different factors influence sales performance or the impact of promotional campaigns on consumer behavior. The dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it a versatile tool for both academic research and practical applications in retail management. By utilizing the Indian Sales dataset, analysts can gain insights that inform strategic decisions, enhance customer engagement, and improve overall sales performance in the competitive grocery market.",
    "tfidf_keywords": [
      "sales-forecasting",
      "retail-analytics",
      "consumer-purchasing",
      "inventory-optimization",
      "grocery-sales",
      "time-series-analysis",
      "data-preprocessing",
      "transaction-data",
      "predictive-modeling",
      "market-trends"
    ],
    "semantic_cluster": "sales-forecasting-methods",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "predictive-analytics",
      "consumer-behavior",
      "time-series-forecasting",
      "inventory-management",
      "data-visualization"
    ],
    "canonical_topics": [
      "forecasting",
      "consumer-behavior",
      "machine-learning",
      "statistics"
    ]
  },
  {
    "name": "EU Open Data Portal",
    "description": "2 million datasets from 205 catalogues across 36 European countries",
    "category": "Dataset Aggregators",
    "url": "https://data.europa.eu",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "EU",
      "Europe",
      "Eurostat",
      "open data"
    ],
    "best_for": "European economic data including comprehensive Eurostat statistics",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The EU Open Data Portal provides access to 2 million datasets from 205 catalogues across 36 European countries. This extensive resource allows users to explore a wide range of data, facilitating research and analysis in various fields such as economics, social sciences, and public policy.",
    "use_cases": [
      "Analyzing economic trends across European countries",
      "Researching social indicators and public policy impacts"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What datasets are available on the EU Open Data Portal?",
      "How can I access datasets from European countries?",
      "What types of data does Eurostat provide?",
      "Where can I find open data related to EU policies?",
      "What is the scope of datasets in the EU Open Data Portal?",
      "How many catalogues are included in the EU Open Data Portal?"
    ],
    "domain_tags": [
      "public policy",
      "economics"
    ],
    "data_modality": "tabular",
    "size_category": "massive",
    "geographic_scope": "Europe",
    "model_score": 0.0005,
    "image_url": "/images/logos/europa.png",
    "embedding_text": "The EU Open Data Portal serves as a comprehensive repository for datasets sourced from various catalogues across Europe. It encompasses approximately 2 million datasets that span a diverse array of topics, including economic indicators, demographic statistics, and environmental data. The data is structured in a tabular format, allowing for easy access and manipulation by researchers and analysts. Each dataset typically consists of rows and columns, where rows represent individual records and columns correspond to specific variables or attributes. The portal aggregates data from 205 catalogues, ensuring a wide-ranging coverage of information pertinent to the European context. The datasets are collected from various governmental and institutional sources, ensuring a level of reliability and accuracy. However, users should be aware of potential limitations regarding data quality, such as inconsistencies in data reporting across different countries or time periods. Common preprocessing steps may include data cleaning, normalization, and transformation to prepare the datasets for analysis. Researchers can utilize this portal to address a multitude of research questions, such as examining the effects of economic policies on social outcomes or analyzing trends in public health across different regions. The datasets support various types of analyses, including regression analysis, machine learning applications, and descriptive statistics. Typically, researchers leverage this data to inform policy decisions, conduct comparative studies, and explore socio-economic phenomena across Europe.",
    "tfidf_keywords": [
      "open data",
      "EU datasets",
      "Eurostat",
      "data aggregation",
      "public policy",
      "economic indicators",
      "demographic statistics",
      "environmental data",
      "data quality",
      "data preprocessing",
      "data analysis",
      "social indicators",
      "research methodology"
    ],
    "semantic_cluster": "open-data-portal",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "data-collection",
      "public-administration",
      "social-research",
      "economic-analysis",
      "data-visualization"
    ],
    "canonical_topics": [
      "data-engineering",
      "econometrics",
      "consumer-behavior",
      "policy-evaluation"
    ]
  },
  {
    "name": "IPUMS",
    "description": "Harmonized microdata from US Census (1850-present), ACS, CPS, and 103+ countries' censuses",
    "category": "Dataset Aggregators",
    "url": "https://www.ipums.org",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "census",
      "microdata",
      "harmonized",
      "demographics"
    ],
    "best_for": "Harmonized census microdata across time and countries - free for academic research",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "IPUMS provides harmonized microdata from the US Census and various international censuses, allowing researchers to analyze demographic trends over time. This dataset can be used for various analyses, including understanding population changes, socioeconomic factors, and demographic characteristics across different regions and time periods.",
    "use_cases": [
      "Analyzing demographic shifts in the US over the last century.",
      "Comparing socioeconomic factors across different countries using harmonized data.",
      "Studying the impact of policy changes on population demographics.",
      "Conducting longitudinal studies on population health and economic status."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the IPUMS dataset?",
      "How can I access US Census microdata?",
      "What types of demographic data are available in IPUMS?",
      "How does IPUMS harmonize data from different countries?",
      "What research questions can be answered using IPUMS?",
      "What are the key variables in the IPUMS dataset?",
      "How can I analyze census data with IPUMS?",
      "What is the temporal coverage of the IPUMS dataset?"
    ],
    "domain_tags": [
      "demographics",
      "sociology",
      "economics"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0005,
    "image_url": "/images/logos/ipums.png",
    "embedding_text": "The IPUMS dataset offers a comprehensive collection of harmonized microdata from the US Census, American Community Survey (ACS), Current Population Survey (CPS), and over 103 countries' censuses, spanning from 1850 to the present. This dataset is structured in a tabular format, consisting of rows representing individual respondents and columns representing various demographic, social, and economic variables. Key variables include age, sex, race, income, education level, and employment status, which are essential for analyzing population characteristics and trends. The data collection methodology involves aggregating and harmonizing data from diverse sources, ensuring consistency and comparability across different datasets. Researchers often utilize IPUMS for a range of analyses, including regression modeling, machine learning applications, and descriptive statistics. Common preprocessing steps may include handling missing values, normalizing data, and transforming categorical variables into numerical formats for analysis. The dataset supports a variety of research questions, such as examining the effects of immigration on labor markets, understanding income inequality over time, and exploring the relationship between education and employment outcomes. Due to its extensive coverage and rich variables, IPUMS is a valuable resource for social scientists, economists, and policymakers aiming to derive insights from historical and contemporary demographic data.",
    "tfidf_keywords": [
      "harmonized microdata",
      "demographic analysis",
      "US Census",
      "international censuses",
      "socioeconomic factors",
      "longitudinal studies",
      "data harmonization",
      "population health",
      "income inequality",
      "education outcomes"
    ],
    "semantic_cluster": "demographic-data-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "sociology",
      "econometrics",
      "population studies",
      "data analysis",
      "social research"
    ],
    "canonical_topics": [
      "demographics",
      "econometrics",
      "consumer-behavior"
    ]
  },
  {
    "name": "Patreon Creator Data",
    "description": "279K+ active creators with membership tiers and patron counts. Creator economy platform metrics from Graphtreon",
    "category": "Creator Economy",
    "url": "https://graphtreon.com/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Patreon",
      "creators",
      "memberships",
      "subscriptions",
      "creator economy"
    ],
    "best_for": "Learning creator economy analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "creator-economy",
      "subscriptions",
      "membership-models"
    ],
    "summary": "The Patreon Creator Data dataset includes information on over 279,000 active creators, detailing their membership tiers and patron counts. This dataset provides insights into the dynamics of the creator economy, allowing researchers and analysts to explore trends in subscriptions and creator engagement.",
    "use_cases": [
      "Analyzing the impact of membership tiers on creator revenue.",
      "Exploring trends in creator engagement over time.",
      "Comparing patron counts across different content categories.",
      "Investigating the relationship between creator activity and subscription growth."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the average number of patrons per creator on Patreon?",
      "How do membership tiers affect creator earnings?",
      "What trends can be observed in the creator economy from Patreon data?",
      "How does the number of active creators on Patreon change over time?",
      "What are the most common membership tiers among successful creators?",
      "How does patron count vary across different creator categories?"
    ],
    "domain_tags": [
      "creator-economy"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0005,
    "image_url": "/images/logos/graphtreon.png",
    "embedding_text": "The Patreon Creator Data dataset offers a comprehensive view of the creator economy, featuring over 279,000 active creators along with detailed metrics on their membership tiers and patron counts. This dataset is structured in a tabular format, where each row represents an individual creator, and columns include variables such as creator ID, membership tier details, patron counts, and possibly additional metrics related to creator performance. The data is sourced from Graphtreon, a platform that aggregates metrics from various creator economy platforms, ensuring a rich and diverse dataset for analysis. Researchers can leverage this dataset to explore a multitude of research questions, such as the effectiveness of different membership tiers in attracting patrons, the overall growth trends within the creator economy, and the comparative analysis of patron counts across various creator categories. Common preprocessing steps may include data cleaning to handle any inconsistencies, normalization of patron counts, and categorization of creators based on their content type. The dataset supports various types of analyses, including regression analysis to understand the factors influencing creator success, machine learning models for predictive analytics, and descriptive statistics to summarize creator engagement metrics. Researchers typically use this dataset to gain insights into the evolving landscape of the creator economy, examining how creators monetize their content through subscriptions and memberships, and identifying patterns that can inform future strategies for both creators and platforms. However, it is important to note that the dataset may have limitations, such as potential biases in creator representation or variations in data collection methods across different platforms. Overall, the Patreon Creator Data serves as a valuable resource for understanding the dynamics of the creator economy and the factors that contribute to creator success.",
    "tfidf_keywords": [
      "creator-economy",
      "membership-tiers",
      "patron-counts",
      "subscription-models",
      "creator-engagement",
      "Graphtreon",
      "data-analysis",
      "creator-performance",
      "revenue-models",
      "content-categories",
      "active-creators",
      "patron-trends",
      "creator-metrics"
    ],
    "semantic_cluster": "creator-economy-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "subscription-economy",
      "digital-content-creation",
      "monetization-strategies",
      "audience-engagement",
      "marketplace-dynamics"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "marketplaces",
      "econometrics"
    ]
  },
  {
    "name": "Brazilian Drugs (ANVISA)",
    "description": "Sales data for controlled substances reported by ANVISA",
    "category": "Grocery & Supermarkets",
    "url": "https://www.kaggle.com/datasets/tiagoacardoso/venda-medicamentos-controlados-anvisa",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "pharmaceuticals",
      "Brazil",
      "regulated"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Brazilian Drugs (ANVISA) dataset contains sales data for controlled substances reported by ANVISA, providing insights into the pharmaceutical market in Brazil. Researchers can analyze trends in drug sales, assess the impact of regulations, and explore consumer behavior related to pharmaceuticals.",
    "use_cases": [
      "Analyzing sales trends over time for specific drugs",
      "Evaluating the impact of policy changes on drug sales",
      "Investigating consumer purchasing patterns in the pharmaceutical sector",
      "Conducting market segmentation analysis based on sales data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the sales trend of controlled substances in Brazil?",
      "How do regulations affect pharmaceutical sales in Brazil?",
      "What are the most commonly sold controlled substances?",
      "How does consumer behavior vary across different regions in Brazil?",
      "What demographic factors influence the sales of pharmaceuticals?",
      "How can we predict future sales of controlled substances in Brazil?"
    ],
    "domain_tags": [
      "healthcare",
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Brazil",
    "size_category": "medium",
    "model_score": 0.0005,
    "image_url": "/images/datasets/brazilian-drugs-anvisa.jpeg",
    "embedding_text": "The Brazilian Drugs (ANVISA) dataset is a comprehensive collection of sales data for controlled substances reported by ANVISA, the Brazilian Health Regulatory Agency. This dataset is structured in a tabular format, with rows representing individual sales transactions and columns capturing various attributes such as drug name, sales volume, price, and date of sale. The collection methodology involves systematic reporting by pharmacies and healthcare providers to ANVISA, ensuring a reliable source of information on the pharmaceutical market. The dataset covers the Brazilian market, providing insights into sales patterns across different regions and demographics. Key variables include drug identification, sales figures, and regulatory compliance indicators, which measure the volume and frequency of sales for each controlled substance. Data quality is generally high, but limitations may include reporting inconsistencies and potential underreporting in certain areas. Common preprocessing steps involve cleaning the data to handle missing values, normalizing sales figures, and aggregating data for analysis. Researchers can use this dataset to address various research questions, such as the effects of regulatory changes on drug sales, consumer purchasing behavior, and market dynamics within the pharmaceutical industry. The dataset supports a range of analyses, including regression modeling, machine learning applications, and descriptive statistics. Typically, researchers utilize this dataset in studies focused on healthcare policy evaluation, market analysis, and consumer behavior research, making it a valuable resource for those interested in the intersection of economics and healthcare.",
    "tfidf_keywords": [
      "controlled-substances",
      "pharmaceutical-sales",
      "ANVISA",
      "market-analysis",
      "consumer-behavior",
      "regulatory-impact",
      "sales-trends",
      "data-preprocessing",
      "healthcare-policy",
      "market-segmentation",
      "predictive-modeling"
    ],
    "semantic_cluster": "pharmaceutical-market-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "causal-inference",
      "consumer-behavior",
      "healthcare-policy",
      "market-analysis",
      "regression-modeling"
    ],
    "canonical_topics": [
      "econometrics",
      "healthcare",
      "consumer-behavior",
      "policy-evaluation"
    ]
  },
  {
    "name": "JD.com Search",
    "description": "170,000 users' real search queries (2021-2022) from JD.com",
    "category": "E-Commerce",
    "url": "https://github.com/rucliujn/JDsearch",
    "docs_url": null,
    "github_url": "https://github.com/rucliujn/JDsearch",
    "tags": [
      "search queries",
      "e-commerce search",
      "China"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The JD.com Search dataset contains real search queries from 170,000 users on JD.com between 2021 and 2022. This dataset can be utilized to analyze consumer behavior, search trends, and e-commerce dynamics in the Chinese market.",
    "use_cases": [
      "Analyzing consumer behavior based on search queries",
      "Identifying trends in e-commerce search patterns",
      "Optimizing product listings based on search data",
      "Conducting market research in the Chinese e-commerce sector"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the common search queries on JD.com?",
      "How do search queries reflect consumer preferences in e-commerce?",
      "What trends can be identified from JD.com search data?",
      "How can search queries inform product recommendations?",
      "What are the seasonal variations in search queries on JD.com?",
      "How do search queries differ across product categories?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "text",
    "temporal_coverage": "2021-2022",
    "geographic_scope": "China",
    "size_category": "medium",
    "model_score": 0.0005,
    "image_url": "/images/datasets/jdcom-search.png",
    "embedding_text": "The JD.com Search dataset is a rich collection of 170,000 real search queries generated by users on JD.com, one of the largest e-commerce platforms in China, during the years 2021 and 2022. This dataset is structured in a tabular format, where each row represents an individual search query, and the columns may include variables such as user ID, search term, timestamp, and possibly additional metadata related to the search context. The collection methodology likely involved tracking user interactions on the JD.com platform, capturing search queries in real-time to ensure a comprehensive representation of user behavior. The temporal coverage of this dataset spans two years, providing insights into how search behaviors may have evolved over time, particularly in response to market trends, seasonal events, and promotional activities. Geographically, the dataset is focused on China, reflecting the unique consumer landscape and preferences within this market. Key variables in this dataset include the search terms themselves, which measure user interest and intent, and the timestamps that can help analyze trends over time. Researchers should be aware of potential data quality issues, such as incomplete queries or variations in user behavior that may not be fully captured. Common preprocessing steps may include cleaning the search terms to remove noise, normalizing text for analysis, and possibly categorizing queries into broader themes for more structured analysis. This dataset supports a variety of research questions, such as understanding consumer preferences, identifying emerging trends in product searches, and evaluating the effectiveness of marketing strategies. Analysts can employ various methods, including regression analysis, machine learning techniques, and descriptive statistics, to derive insights from the data. Researchers typically use this dataset to inform product development, enhance user experience on e-commerce platforms, and contribute to broader studies on consumer behavior in digital marketplaces.",
    "tfidf_keywords": [
      "search queries",
      "consumer behavior",
      "e-commerce trends",
      "user intent",
      "data preprocessing",
      "market research",
      "product optimization",
      "seasonal variations",
      "search patterns",
      "Chinese market"
    ],
    "semantic_cluster": "e-commerce-search-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "market-research",
      "data-analysis",
      "search-engine-optimization",
      "user-experience"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "marketplaces",
      "data-engineering"
    ]
  },
  {
    "name": "Alibaba Cloud Theme",
    "description": "Themed dataset related to Alibaba Cloud services",
    "category": "E-Commerce",
    "url": "https://tianchi.aliyun.com/dataset/9716",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "cloud",
      "Alibaba",
      "themed"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "cloud-computing"
    ],
    "summary": "The Alibaba Cloud Theme dataset provides a collection of data related to Alibaba Cloud services, focusing on various aspects of e-commerce. Researchers can utilize this dataset to analyze cloud service usage patterns, consumer behavior, and the impact of cloud solutions on e-commerce performance.",
    "use_cases": [
      "Analyzing consumer behavior in cloud service adoption",
      "Evaluating the impact of cloud services on e-commerce sales",
      "Identifying trends in cloud service usage across different sectors"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the key features of Alibaba Cloud services?",
      "How does Alibaba Cloud impact e-commerce?",
      "What trends can be observed in cloud service usage?",
      "What are the consumer preferences for cloud services?",
      "How do different industries utilize Alibaba Cloud?",
      "What are the challenges faced by e-commerce businesses using cloud solutions?"
    ],
    "domain_tags": [
      "e-commerce"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0005,
    "embedding_text": "The Alibaba Cloud Theme dataset is a comprehensive collection focused on Alibaba Cloud services, particularly in the context of e-commerce. This dataset includes various data points that reflect the usage patterns, service offerings, and consumer interactions with Alibaba's cloud solutions. The data structure typically consists of rows representing individual transactions or user interactions, with columns detailing variables such as service type, user demographics, transaction amounts, and timestamps. The collection methodology may involve aggregating data from Alibaba's internal analytics, user surveys, and third-party market research, ensuring a rich dataset that captures the dynamics of cloud service utilization in the e-commerce sector. While the dataset does not specify temporal or geographic coverage, it is designed to provide insights into consumer behavior and service performance metrics. Key variables in the dataset include service type, user engagement metrics, and transaction values, which measure the effectiveness and popularity of various Alibaba Cloud offerings. Researchers may encounter data quality issues such as missing values or inconsistencies, necessitating common preprocessing steps like data cleaning, normalization, and transformation to prepare the dataset for analysis. This dataset supports a range of analyses, including regression analysis to identify factors influencing service adoption, machine learning techniques for predictive modeling, and descriptive statistics to summarize user behavior trends. Researchers typically leverage this dataset to explore questions related to the effectiveness of cloud services in enhancing e-commerce operations, the relationship between cloud service features and consumer preferences, and the overall impact of cloud technology on market dynamics.",
    "tfidf_keywords": [
      "cloud-computing",
      "e-commerce",
      "consumer-behavior",
      "service-adoption",
      "data-analysis",
      "transaction-metrics",
      "user-engagement",
      "market-research",
      "cloud-services",
      "Alibaba"
    ],
    "semantic_cluster": "cloud-ecommerce-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "cloud-computing",
      "e-commerce",
      "consumer-behavior",
      "data-analysis",
      "market-research"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "machine-learning",
      "data-engineering"
    ]
  },
  {
    "name": "Rakuten SIGIR",
    "description": "E-commerce dataset for SIGIR workshop from Rakuten",
    "category": "E-Commerce",
    "url": "https://sigir-ecom.github.io/ecom2018/data-task.html",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "SIGIR",
      "e-commerce",
      "search"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "search"
    ],
    "summary": "The Rakuten SIGIR dataset is an e-commerce dataset specifically designed for the SIGIR workshop. It provides valuable insights into consumer behavior and search patterns within the e-commerce domain.",
    "use_cases": [
      "Analyzing consumer search behavior",
      "Evaluating e-commerce search algorithms",
      "Improving product recommendation systems"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Rakuten SIGIR dataset?",
      "How can I access the Rakuten SIGIR dataset?",
      "What insights can be derived from the Rakuten SIGIR dataset?",
      "What are the key features of the Rakuten SIGIR dataset?",
      "How is the Rakuten SIGIR dataset structured?",
      "What research can be conducted using the Rakuten SIGIR dataset?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0005,
    "embedding_text": "The Rakuten SIGIR dataset is a comprehensive e-commerce dataset curated for the SIGIR workshop, focusing on search behavior and consumer interactions within online retail environments. It consists of a structured tabular format, encompassing various rows and columns that detail user interactions, product searches, and purchase behaviors. The dataset is designed to facilitate research in understanding how consumers navigate e-commerce platforms and the effectiveness of search algorithms in delivering relevant product results. The collection methodology involves aggregating data from Rakuten's e-commerce platform, ensuring a rich source of real-world user interactions. Key variables within the dataset may include user IDs, product IDs, search queries, timestamps, and interaction types, which collectively measure aspects of consumer behavior and search efficiency. While the dataset offers valuable insights, researchers should be aware of potential limitations, such as data sparsity or biases in user interactions. Common preprocessing steps may involve cleaning the data, handling missing values, and normalizing search queries for analysis. Researchers can utilize this dataset to address various research questions, such as the impact of search algorithms on consumer choices, the effectiveness of product placements, and trends in consumer search behavior over time. The dataset supports a range of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it a versatile tool for scholars and practitioners in the fields of e-commerce and consumer behavior. Typically, researchers leverage this dataset to enhance understanding of user engagement with e-commerce platforms, optimize search functionalities, and develop more effective recommendation systems that cater to consumer preferences.",
    "tfidf_keywords": [
      "consumer-behavior",
      "search-patterns",
      "e-commerce",
      "user-interactions",
      "product-search",
      "search-algorithms",
      "recommendation-systems",
      "data-collection-methodology",
      "data-preprocessing",
      "behavioral-analysis"
    ],
    "semantic_cluster": "e-commerce-search-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "recommendation-systems",
      "search-engine-optimization",
      "data-mining",
      "user-experience"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "recommendation-systems",
      "marketplaces",
      "data-engineering",
      "experimentation"
    ]
  },
  {
    "name": "Amazon Reviews (2023)",
    "description": "571M reviews (1996-2023), 33 categories, 48M items - comprehensive Amazon review dataset",
    "category": "E-Commerce",
    "url": "https://cseweb.ucsd.edu/~jmcauley/datasets.html#amazon_reviews",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "reviews",
      "Amazon",
      "large-scale",
      "sentiment"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "sentiment-analysis"
    ],
    "summary": "The Amazon Reviews dataset is a comprehensive collection of 571 million reviews spanning from 1996 to 2023 across 33 categories and 48 million items. This dataset enables researchers and data scientists to perform sentiment analysis, consumer behavior studies, and large-scale data mining in the e-commerce sector.",
    "use_cases": [
      "Sentiment analysis of consumer reviews to gauge product satisfaction.",
      "Trend analysis of reviews over time to identify shifts in consumer preferences.",
      "Comparative analysis of review patterns across different product categories.",
      "Exploring the relationship between review ratings and sales performance."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the trends in Amazon reviews over the years?",
      "How can sentiment analysis be performed on Amazon reviews?",
      "What categories of products receive the most reviews on Amazon?",
      "How do review counts correlate with product sales?",
      "What insights can be drawn from consumer behavior in Amazon reviews?",
      "How can we analyze the sentiment of reviews across different categories?",
      "What are the most common themes in Amazon reviews?",
      "How does review length affect the perceived helpfulness of a review?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "text",
    "temporal_coverage": "1996-2023",
    "size_category": "massive",
    "model_score": 0.0005,
    "image_url": "/images/datasets/amazon-reviews-2023.png",
    "embedding_text": "The Amazon Reviews dataset is a substantial resource for researchers and data scientists interested in e-commerce and consumer behavior. It comprises 571 million reviews collected over a span of nearly three decades, covering 33 distinct product categories and 48 million unique items. The dataset is structured in a tabular format, with each row representing an individual review and columns containing variables such as review text, rating, product ID, category, and timestamps. This rich dataset allows for a variety of analyses, including sentiment analysis, trend detection, and consumer behavior studies. Researchers can utilize this dataset to explore key variables such as review ratings, review length, and the frequency of reviews, which can provide insights into product performance and customer satisfaction. The collection methodology involves scraping data from Amazon's platform, ensuring a comprehensive representation of consumer opinions across a wide range of products. However, users should be aware of potential data quality issues, such as the presence of spam reviews or biased ratings, which may affect the reliability of analyses. Common preprocessing steps include cleaning review text, removing duplicates, and standardizing rating scales. The dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it a versatile tool for academic research and commercial applications. Researchers typically leverage this dataset to answer questions related to consumer preferences, product performance, and market trends, enhancing our understanding of the dynamics within the e-commerce landscape.",
    "tfidf_keywords": [
      "sentiment-analysis",
      "consumer-behavior",
      "review-patterns",
      "e-commerce",
      "large-scale-data",
      "data-mining",
      "trend-analysis",
      "review-length",
      "product-categories",
      "data-quality",
      "preprocessing",
      "time-series-analysis",
      "text-mining",
      "market-trends",
      "data-collection-methodology"
    ],
    "semantic_cluster": "e-commerce-consumer-insights",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "sentiment-analysis",
      "data-mining",
      "text-analysis",
      "consumer-behavior",
      "market-research"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "machine-learning",
      "natural-language-processing",
      "data-engineering",
      "statistics"
    ]
  },
  {
    "name": "Walmart Sales",
    "description": "General sales data including CPI and unemployment rate",
    "category": "Grocery & Supermarkets",
    "url": "https://www.kaggle.com/datasets/yasserh/walmart-dataset",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Walmart",
      "macro",
      "sales"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Walmart Sales dataset contains general sales data, including key economic indicators such as the Consumer Price Index (CPI) and unemployment rates. This dataset can be utilized to analyze sales trends, understand consumer behavior, and evaluate the impact of macroeconomic factors on retail performance.",
    "use_cases": [
      "Analyzing the impact of economic indicators on sales performance.",
      "Evaluating seasonal trends in grocery sales.",
      "Investigating consumer behavior patterns in relation to price changes.",
      "Conducting regression analysis to forecast future sales."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the sales trends at Walmart over the past years?",
      "How do CPI and unemployment rates affect Walmart's sales?",
      "What insights can be drawn from Walmart's sales data?",
      "How can Walmart's sales data be used to predict future sales?",
      "What is the relationship between macroeconomic indicators and grocery sales?",
      "How does consumer behavior change in response to economic conditions?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0005,
    "image_url": "/images/datasets/walmart-sales.jpg",
    "embedding_text": "The Walmart Sales dataset is a comprehensive collection of sales data that includes critical economic indicators such as the Consumer Price Index (CPI) and unemployment rates. This dataset is structured in a tabular format, consisting of rows representing individual sales transactions and columns detailing various attributes such as sales amount, date, product categories, and the aforementioned economic indicators. The data is typically collected from Walmart's sales records, providing a rich source for understanding retail dynamics. Researchers and analysts can leverage this dataset to explore how macroeconomic factors influence consumer purchasing behavior and sales performance in the grocery sector. Key variables in this dataset include total sales, product categories, CPI, and unemployment rates, each measuring different aspects of economic activity and consumer behavior. While the dataset offers valuable insights, it may have limitations regarding data completeness and accuracy, which researchers should consider during analysis. Common preprocessing steps include handling missing values, normalizing sales figures, and transforming economic indicators for comparative analysis. This dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics, enabling researchers to address questions related to sales forecasting, economic impact assessments, and consumer behavior analysis. Overall, the Walmart Sales dataset serves as a vital resource for studies focused on retail economics and the interplay between macroeconomic conditions and consumer spending.",
    "tfidf_keywords": [
      "Consumer Price Index",
      "unemployment rate",
      "sales trends",
      "grocery sales",
      "economic indicators",
      "consumer behavior",
      "regression analysis",
      "forecasting",
      "retail performance",
      "seasonal trends",
      "macro-economic factors",
      "price changes",
      "sales forecasting"
    ],
    "semantic_cluster": "retail-economics-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "econometrics",
      "pricing",
      "sales-forecasting",
      "macroeconomics"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "econometrics",
      "pricing",
      "forecasting",
      "retail"
    ]
  },
  {
    "name": "Office Supplies (DMDA 2023)",
    "description": "Office supply sales for DMDA 2023 workshop challenge",
    "category": "Grocery & Supermarkets",
    "url": "https://sites.google.com/view/dmdaworkshop2023/data-challenge",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "office supplies",
      "forecasting",
      "workshop"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Office Supplies dataset for DMDA 2023 workshop challenge contains sales data for various office supplies. It can be utilized for forecasting sales trends, analyzing consumer behavior, and optimizing pricing strategies.",
    "use_cases": [
      "Forecasting future sales of office supplies",
      "Analyzing consumer purchasing patterns",
      "Optimizing pricing strategies for office supplies"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What are the sales trends for office supplies in 2023?",
      "How can forecasting be applied to office supply sales?",
      "What consumer behaviors can be analyzed from office supply sales data?",
      "What pricing strategies are effective for office supplies?",
      "How do seasonal trends affect office supply sales?",
      "What is the impact of workshop challenges on office supply sales?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0005,
    "image_url": "/images/datasets/office-supplies-dmda-2023.png",
    "embedding_text": "The Office Supplies dataset for the DMDA 2023 workshop challenge provides a comprehensive view of sales data related to office supplies. This dataset is structured in a tabular format, consisting of rows representing individual sales transactions and columns detailing various attributes such as product type, quantity sold, sale price, and date of transaction. The data collection methodology likely involves aggregating sales records from retail transactions, possibly sourced from point-of-sale systems or e-commerce platforms. While the dataset does not specify temporal or geographic coverage, it is assumed to reflect sales data from the year 2023, focusing on office supplies relevant to the workshop context. Key variables in this dataset include product categories, sales volume, and pricing, which can be used to measure trends in consumer demand and pricing effectiveness. Researchers may encounter limitations regarding data quality, such as missing values or inconsistencies in product categorization, which may necessitate preprocessing steps like data cleaning and normalization. The dataset supports various types of analyses, including regression modeling to predict future sales, machine learning techniques for consumer behavior analysis, and descriptive statistics to summarize sales performance. Researchers typically leverage this dataset to explore questions related to market trends, pricing strategies, and the impact of promotional events on sales outcomes.",
    "tfidf_keywords": [
      "sales forecasting",
      "consumer purchasing patterns",
      "pricing strategies",
      "office supplies",
      "retail analytics",
      "workshop challenge",
      "data analysis",
      "e-commerce sales",
      "market trends",
      "data preprocessing"
    ],
    "semantic_cluster": "sales-forecasting",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "sales-analysis",
      "market-research",
      "consumer-behavior",
      "data-visualization",
      "predictive-modeling"
    ],
    "canonical_topics": [
      "forecasting",
      "consumer-behavior",
      "pricing"
    ]
  },
  {
    "name": "Metacritic Video Games",
    "description": "Video game reviews and metadata from Metacritic",
    "category": "Entertainment & Media",
    "url": "https://tianchi.aliyun.com/dataset/144719",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "video games",
      "reviews",
      "ratings"
    ],
    "best_for": "Learning entertainment & media analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "video games",
      "reviews",
      "ratings"
    ],
    "summary": "The Metacritic Video Games dataset contains comprehensive reviews and metadata related to video games, sourced from Metacritic. Researchers and analysts can utilize this dataset to explore trends in video game ratings, analyze consumer preferences, and assess the impact of reviews on sales and popularity.",
    "use_cases": [
      "Analyzing the relationship between critic scores and user scores.",
      "Exploring the impact of release dates on video game ratings.",
      "Comparing ratings across different gaming platforms.",
      "Investigating trends in video game reviews over time."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the top-rated video games according to Metacritic?",
      "How do video game reviews correlate with sales figures?",
      "What trends can be observed in video game ratings over the years?",
      "How do different genres of video games compare in terms of ratings?",
      "What factors influence the ratings of video games on Metacritic?",
      "How do user reviews differ from critic reviews in video games?"
    ],
    "domain_tags": [
      "entertainment",
      "media"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0005,
    "embedding_text": "The Metacritic Video Games dataset is a rich repository of video game reviews and metadata, providing a structured format that includes various attributes such as game titles, release dates, critic scores, user scores, and genre classifications. This dataset is particularly valuable for researchers and data scientists interested in the entertainment and media sectors, as it allows for in-depth analysis of consumer behavior and market trends in the video gaming industry. The data is typically organized in a tabular format, with each row representing a unique video game and columns containing relevant metadata. Key variables include critic ratings, user ratings, and genre, which can be used to measure the performance and reception of video games. The collection methodology involves aggregating reviews from Metacritic, a well-known platform that compiles critic and user reviews, ensuring a comprehensive overview of each game's reception. However, users should be aware of potential limitations in data quality, such as the influence of review bias and the variability in the number of reviews per game. Common preprocessing steps may include handling missing values, normalizing scores, and categorizing games by genre. Researchers can address various research questions using this dataset, such as examining the correlation between review scores and sales, analyzing trends in game ratings over time, and exploring the differences between critic and user reviews. The dataset supports a variety of analytical approaches, including regression analysis, machine learning techniques, and descriptive statistics, making it a versatile tool for exploring the dynamics of the video game market. Overall, the Metacritic Video Games dataset serves as a foundational resource for studies in consumer behavior, market analysis, and the impact of reviews on product success in the gaming industry.",
    "tfidf_keywords": [
      "Metacritic",
      "video game reviews",
      "critic scores",
      "user scores",
      "genre classification",
      "consumer behavior",
      "market trends",
      "data analysis",
      "video gaming industry",
      "review bias"
    ],
    "semantic_cluster": "video-game-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "consumer-behavior",
      "market-analysis",
      "data-visualization",
      "sentiment-analysis",
      "product-reviews"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "marketplaces",
      "recommendation-systems",
      "statistics",
      "data-engineering"
    ]
  },
  {
    "name": "Polish Grocery",
    "description": "Yearly sales data (2018) from Polish grocery shop",
    "category": "Grocery & Supermarkets",
    "url": "https://www.kaggle.com/datasets/agatii/total-sale-2018-yearly-data-of-grocery-shop",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "grocery",
      "Poland",
      "yearly data"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Polish Grocery dataset provides yearly sales data from a Polish grocery shop for the year 2018. This dataset can be utilized to analyze consumer purchasing patterns, evaluate pricing strategies, and understand market trends in the grocery sector.",
    "use_cases": [
      "Analyzing consumer behavior in grocery shopping",
      "Evaluating the impact of pricing changes on sales",
      "Identifying seasonal trends in grocery purchases",
      "Comparing sales performance across different product categories"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the yearly sales figures for Polish grocery shops?",
      "How do consumer purchasing patterns vary in Poland?",
      "What pricing strategies are effective in the Polish grocery market?",
      "What trends can be identified from grocery sales data in Poland?",
      "How does the grocery market in Poland compare to other countries?",
      "What factors influence yearly sales in Polish grocery shops?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2018",
    "geographic_scope": "Poland",
    "size_category": "medium",
    "model_score": 0.0005,
    "image_url": "/images/datasets/polish-grocery.jpg",
    "embedding_text": "The Polish Grocery dataset is a comprehensive collection of yearly sales data from a grocery shop in Poland for the year 2018. This dataset is structured in a tabular format, comprising various rows and columns that represent different sales transactions, product categories, and customer demographics. Each row corresponds to a unique sales transaction, while the columns include variables such as product ID, product category, quantity sold, sale price, and total revenue generated. The data collection methodology involved gathering sales records from the grocery shop's point-of-sale system, ensuring that the dataset reflects actual sales activity within the specified timeframe. The primary variables of interest in this dataset include total sales revenue, quantity sold, and product categories, which provide insights into consumer preferences and purchasing behavior. Researchers can utilize this dataset to address a variety of research questions, such as identifying trends in consumer spending, evaluating the effectiveness of promotional campaigns, and analyzing the impact of pricing strategies on sales performance. Common preprocessing steps may include cleaning the data to handle missing values, normalizing sales figures, and categorizing products for more detailed analysis. The dataset supports various types of analyses, including descriptive statistics, regression analysis, and machine learning models to predict future sales trends. Researchers typically use this dataset to conduct studies on consumer behavior, pricing optimization, and market analysis within the retail sector, making it a valuable resource for those interested in the intersection of economics and consumer behavior.",
    "tfidf_keywords": [
      "yearly sales",
      "consumer purchasing patterns",
      "pricing strategies",
      "grocery market",
      "Poland",
      "sales analysis",
      "market trends",
      "product categories",
      "sales transactions",
      "retail analytics"
    ],
    "semantic_cluster": "grocery-market-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "pricing",
      "market-analysis",
      "sales-forecasting",
      "retail-strategy"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "pricing",
      "econometrics"
    ]
  },
  {
    "name": "gridstatus",
    "description": "Unified Python API for accessing real-time and historical data from all major U.S. ISOs",
    "category": "Energy",
    "url": "https://www.gridstatus.io/",
    "docs_url": "https://docs.gridstatus.io/",
    "github_url": "https://github.com/gridstatus/gridstatus",
    "tags": [
      "ISO",
      "API",
      "real-time",
      "Python",
      "unified"
    ],
    "best_for": "Accessing standardized data across multiple U.S. electricity markets",
    "difficulty": "beginner",
    "prerequisites": [
      "Python",
      "API usage",
      "data manipulation"
    ],
    "topic_tags": [
      "energy",
      "real-time data",
      "data access"
    ],
    "summary": "The gridstatus dataset provides a unified Python API for accessing both real-time and historical data from all major U.S. Independent System Operators (ISOs). This dataset is useful for researchers and developers looking to analyze energy market dynamics, grid performance, and historical trends in electricity consumption and generation.",
    "use_cases": [
      "Analyzing real-time electricity demand and supply.",
      "Comparing historical energy consumption patterns across different ISOs.",
      "Developing applications that require real-time energy data.",
      "Conducting research on grid reliability and performance."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the gridstatus dataset?",
      "How can I access real-time energy data from U.S. ISOs?",
      "What historical data is available through the gridstatus API?",
      "How does the gridstatus API work?",
      "What programming language is used for gridstatus?",
      "What types of data can I retrieve from the gridstatus API?",
      "How can I analyze energy market trends using gridstatus?",
      "What are the major U.S. ISOs included in the gridstatus dataset?"
    ],
    "domain_tags": [
      "energy"
    ],
    "data_modality": "time-series",
    "temporal_coverage": "varies by ISO",
    "geographic_scope": "United States",
    "size_category": "medium",
    "model_score": 0.0005,
    "image_url": "/images/datasets/gridstatus.png",
    "embedding_text": "The gridstatus dataset is a comprehensive resource designed for accessing real-time and historical data from all major U.S. Independent System Operators (ISOs) through a unified Python API. This dataset is structured to facilitate the retrieval of energy market data, which includes variables related to electricity generation, consumption, and grid performance. Researchers and developers can leverage this dataset to gain insights into the dynamics of energy markets, evaluate grid reliability, and analyze trends in electricity usage over time. The data structure typically includes rows representing time intervals and columns for various metrics such as generation capacity, demand levels, and ISO-specific data points. The collection methodology involves aggregating data from multiple ISOs, ensuring a broad coverage of the U.S. energy landscape. Key variables within the dataset measure aspects such as real-time electricity prices, generation sources (e.g., renewable vs. non-renewable), and grid stability indicators. While the dataset aims to provide accurate and timely information, users should be aware of potential limitations, such as data latency and discrepancies between different ISOs. Common preprocessing steps may include data cleaning, normalization, and time-series formatting to prepare the data for analysis. Researchers can utilize this dataset to address a variety of questions, such as the impact of renewable energy integration on grid performance, the correlation between demand spikes and pricing, and historical trends in energy consumption. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile tool for both academic research and practical applications in the energy sector.",
    "tfidf_keywords": [
      "real-time data",
      "historical data",
      "Independent System Operators",
      "energy market analysis",
      "Python API",
      "electricity generation",
      "grid performance",
      "demand response",
      "renewable energy",
      "data aggregation"
    ],
    "semantic_cluster": "energy-data-access",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "energy economics",
      "data analysis",
      "time-series forecasting",
      "grid reliability",
      "market dynamics"
    ],
    "canonical_topics": [
      "forecasting",
      "machine-learning",
      "statistics",
      "data-engineering"
    ]
  },
  {
    "name": "Papers With Code Datasets",
    "description": "Datasets linked to research papers, code implementations, and SOTA leaderboards",
    "category": "Dataset Aggregators",
    "url": "https://paperswithcode.com/datasets",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "reproducibility",
      "SOTA",
      "benchmarks",
      "papers"
    ],
    "best_for": "Tracking which datasets power cutting-edge ML research with code links",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The 'Papers With Code Datasets' is a collection of datasets associated with research papers, providing access to code implementations and state-of-the-art (SOTA) leaderboards. This resource allows researchers and practitioners to explore various datasets, benchmark models, and replicate findings in the field of machine learning and data science.",
    "use_cases": [
      "Benchmarking machine learning models",
      "Replicating research findings",
      "Comparing performance across different datasets"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What datasets are linked to research papers?",
      "How can I find code implementations for specific datasets?",
      "What are the current SOTA leaderboards for various datasets?",
      "Which datasets are used for benchmarking in machine learning?",
      "How can I replicate findings from research papers using these datasets?",
      "What is the significance of reproducibility in machine learning datasets?"
    ],
    "domain_tags": [
      "technology",
      "education"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "benchmark_usage": [
      "Benchmarking machine learning models against SOTA"
    ],
    "model_score": 0.0005,
    "image_url": "/images/datasets/papers-with-code-datasets.png",
    "embedding_text": "The 'Papers With Code Datasets' serves as a vital resource for researchers and practitioners in the fields of machine learning and data science. This dataset aggregates various datasets that are linked to academic research papers, providing a comprehensive platform for accessing not only the datasets themselves but also the corresponding code implementations and state-of-the-art (SOTA) leaderboards. The structure of the data typically includes rows representing individual datasets and columns detailing various attributes such as dataset names, associated research papers, performance metrics, and links to code repositories. The collection methodology involves curating datasets from a wide array of research publications, ensuring that they are relevant and widely used within the community. The coverage of this dataset spans multiple domains, allowing users to explore datasets from various fields of study, although specific temporal or geographic coverage is not explicitly mentioned. Key variables within the dataset may include dataset names, descriptions, performance metrics, and links to associated papers and code, which measure the effectiveness and applicability of different models across diverse tasks. While the dataset is rich in content, users should be aware of potential limitations such as the variability in data quality across different datasets and the need for careful preprocessing to ensure compatibility with specific machine learning frameworks. Common preprocessing steps may involve data cleaning, normalization, and transformation to prepare the datasets for analysis. Researchers can leverage this dataset to address a variety of research questions, such as evaluating the performance of different algorithms, understanding trends in model development, and exploring the reproducibility of research findings. The types of analyses supported by this dataset include regression analysis, machine learning model evaluation, and descriptive statistics, making it a versatile tool for both academic and practical applications. Typically, researchers utilize this dataset to benchmark their models against established SOTA results, facilitating the advancement of knowledge and innovation in the field.",
    "tfidf_keywords": [
      "benchmarking",
      "reproducibility",
      "machine-learning",
      "datasets",
      "SOTA",
      "code-implementations",
      "performance-metrics",
      "data-science",
      "research-papers",
      "model-evaluation"
    ],
    "semantic_cluster": "dataset-aggregation",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "machine-learning",
      "benchmarking",
      "reproducibility",
      "data-science",
      "model-evaluation"
    ],
    "canonical_topics": [
      "machine-learning",
      "experimentation",
      "data-engineering",
      "consumer-behavior"
    ]
  },
  {
    "name": "CSIS Significant Cyber Incidents",
    "description": "Curated list of major cyber attacks with losses exceeding $1 million, maintained by leading security think tank",
    "category": "Cybersecurity",
    "url": "https://www.csis.org/programs/strategic-technologies-program/significant-cyber-incidents",
    "docs_url": "https://www.csis.org/programs/strategic-technologies-program/significant-cyber-incidents",
    "github_url": null,
    "tags": [
      "cyber incidents",
      "major attacks",
      "economic impact"
    ],
    "best_for": "Analyzing high-impact cyber events for economic research",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The CSIS Significant Cyber Incidents dataset is a curated list of major cyber attacks that have resulted in financial losses exceeding $1 million. This dataset can be used to analyze trends in cyber incidents, assess their economic impact, and inform cybersecurity strategies.",
    "use_cases": [
      "Analyzing the frequency and types of cyber incidents over time.",
      "Assessing the economic impact of cyber attacks on various sectors.",
      "Identifying patterns in the targeting of organizations by cyber attackers."
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the significant cyber incidents recorded by CSIS?",
      "How many cyber attacks have resulted in losses over $1 million?",
      "What trends can be observed in major cyber attacks?",
      "What is the economic impact of cyber incidents?",
      "How does the CSIS dataset categorize cyber attacks?",
      "What types of organizations are frequently targeted in cyber incidents?"
    ],
    "domain_tags": [
      "cybersecurity"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2006-present",
    "geographic_scope": "Global",
    "size_category": "medium",
    "model_score": 0.0004,
    "image_url": "/images/datasets/csis-significant-cyber-incidents.jpeg",
    "embedding_text": "The CSIS Significant Cyber Incidents dataset is a comprehensive collection of documented cyber attacks that have led to significant financial losses, specifically those exceeding $1 million. This dataset is maintained by the Center for Strategic and International Studies (CSIS), a leading think tank in security studies. The data structure typically consists of rows representing individual cyber incidents, with columns detailing key attributes such as the date of the incident, the type of attack, the affected organization, the estimated financial loss, and the geographical location of the incident. The collection methodology involves rigorous research, drawing from various sources including news articles, government reports, and cybersecurity analyses to ensure accuracy and comprehensiveness. While the dataset provides valuable insights into the landscape of cyber threats, it is important to note that data quality may vary depending on the availability of information and the reporting practices of affected organizations. Common preprocessing steps may include data cleaning to handle missing values, normalization of financial loss figures, and categorization of incidents based on attack types. Researchers can utilize this dataset to address a variety of research questions, such as identifying trends in cyber attacks over time, analyzing the economic implications of these incidents, and evaluating the effectiveness of cybersecurity measures. The dataset supports various types of analyses, including descriptive statistics to summarize incident characteristics, regression analyses to explore relationships between attack types and financial losses, and machine learning techniques to predict future incidents based on historical data. Overall, the CSIS Significant Cyber Incidents dataset serves as a crucial resource for researchers, policymakers, and cybersecurity professionals seeking to understand and mitigate the risks associated with cyber threats.",
    "tfidf_keywords": [
      "cyber incidents",
      "financial losses",
      "security think tank",
      "major attacks",
      "economic impact",
      "data collection",
      "cybersecurity strategies",
      "incident analysis",
      "attack types",
      "data quality",
      "preprocessing",
      "trends in cyber attacks",
      "targeted organizations",
      "cyber threat landscape"
    ],
    "semantic_cluster": "cybersecurity-incident-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "cyber risk assessment",
      "incident response",
      "economic analysis",
      "data analysis",
      "cyber threat intelligence"
    ],
    "canonical_topics": [
      "policy-evaluation",
      "statistics",
      "finance",
      "machine-learning",
      "consumer-behavior"
    ]
  },
  {
    "name": "Netflix Viewing Behavior",
    "description": "1.7M episodes/movies watched by 1,060 users over 1 year. Watch patterns, session length, preferences, predictability metrics",
    "category": "Entertainment & Media",
    "url": "https://ieeexplore.ieee.org/document/9500874",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Netflix",
      "streaming",
      "viewing behavior",
      "sessions",
      "video"
    ],
    "best_for": "Learning entertainment & media analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "consumer-behavior",
      "streaming",
      "video"
    ],
    "summary": "The Netflix Viewing Behavior dataset contains detailed records of 1.7 million episodes and movies watched by 1,060 users over the course of one year. This rich dataset allows researchers to analyze viewing patterns, session lengths, user preferences, and predictability metrics, providing insights into consumer behavior in the streaming industry.",
    "use_cases": [
      "Analyzing user engagement and session lengths on streaming platforms.",
      "Predicting user preferences for content recommendations.",
      "Studying the impact of viewing behavior on subscription retention.",
      "Exploring patterns in binge-watching and its implications for content production."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the viewing patterns of Netflix users?",
      "How does session length vary among different users?",
      "What preferences can be identified in Netflix viewing behavior?",
      "How predictable are Netflix viewing habits?",
      "What metrics can be derived from Netflix viewing data?",
      "How can user preferences influence content recommendations on Netflix?",
      "What insights can be gained from analyzing 1.7M episodes watched?",
      "How do user demographics affect viewing behavior on Netflix?"
    ],
    "domain_tags": [
      "entertainment",
      "media"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2022",
    "size_category": "medium",
    "model_score": 0.0004,
    "image_url": "/images/datasets/netflix-viewing-behavior.png",
    "embedding_text": "The Netflix Viewing Behavior dataset is a comprehensive collection of data detailing the viewing habits of users on the Netflix platform. It encompasses 1.7 million episodes and movies watched by 1,060 users over a one-year period, providing a rich source of information for researchers interested in understanding consumer behavior in the streaming media landscape. The dataset is structured in a tabular format, with rows representing individual viewing sessions and columns capturing various attributes such as user ID, episode/movie title, watch duration, session length, and timestamp. This structure allows for easy manipulation and analysis using data processing libraries like pandas. The collection methodology involved tracking user interactions on the Netflix platform, ensuring that the data reflects real-world viewing behaviors. However, it is important to note that the dataset may have limitations regarding data quality, such as potential biases in user selection or incomplete session records. Common preprocessing steps might include handling missing values, normalizing session lengths, and aggregating data for specific analyses. Researchers can leverage this dataset to address a variety of research questions, such as identifying trends in viewing preferences, analyzing the impact of session length on user satisfaction, and predicting future viewing behaviors based on historical data. The dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics. By utilizing this dataset, researchers can gain valuable insights into how users interact with streaming content, which can inform content creation, marketing strategies, and user experience enhancements.",
    "tfidf_keywords": [
      "viewing behavior",
      "session length",
      "predictability metrics",
      "user preferences",
      "binge-watching",
      "streaming patterns",
      "content recommendations",
      "user engagement",
      "data analysis",
      "consumer behavior",
      "watch patterns",
      "media consumption",
      "video streaming",
      "entertainment analytics",
      "user demographics"
    ],
    "semantic_cluster": "streaming-consumer-behavior",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "consumer-behavior",
      "data-analysis",
      "user-experience",
      "predictive-modeling",
      "media-consumption",
      "engagement-metrics",
      "recommendation-systems",
      "binge-watching",
      "streaming-analytics"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "recommendation-systems",
      "data-engineering",
      "machine-learning",
      "statistics"
    ]
  },
  {
    "name": "YouTube User Watch History",
    "description": "1.8M videos watched by 243 users over 1.5 years. Recommendation engine performance, caching research, viewing patterns",
    "category": "Entertainment & Media",
    "url": "https://netsg.cs.sfu.ca/youtubedata/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "YouTube",
      "watch history",
      "recommendations",
      "video",
      "user behavior"
    ],
    "best_for": "Learning entertainment & media analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "user behavior",
      "recommendations"
    ],
    "summary": "The YouTube User Watch History dataset contains information on 1.8 million videos watched by 243 users over a period of 1.5 years. This dataset can be utilized to analyze viewing patterns, evaluate recommendation engine performance, and conduct research on caching strategies.",
    "use_cases": [
      "Analyzing user viewing patterns to improve recommendation algorithms.",
      "Evaluating the performance of caching strategies based on user watch history.",
      "Studying the relationship between user behavior and video engagement.",
      "Conducting research on the effectiveness of different recommendation systems."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the viewing patterns in YouTube user watch history?",
      "How can recommendation engines be improved using watch history data?",
      "What caching strategies can be derived from user watch history?",
      "What insights can be gained from analyzing 1.8M YouTube videos?",
      "How do user behaviors vary in video consumption on YouTube?",
      "What is the impact of watch history on video recommendations?"
    ],
    "domain_tags": [
      "entertainment",
      "media"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1.5 years",
    "size_category": "medium",
    "model_score": 0.0004,
    "image_url": "/images/logos/sfu.png",
    "embedding_text": "The YouTube User Watch History dataset is a comprehensive collection of user interactions with videos on the YouTube platform, encompassing a total of 1.8 million videos watched by 243 users over a span of 1.5 years. This dataset is structured in a tabular format, where each row represents a unique video view by a user, and the columns include variables such as user ID, video ID, watch duration, timestamp, and possibly other engagement metrics. The data collection methodology involves tracking user interactions through the YouTube platform, capturing detailed information about viewing habits and preferences. Researchers can leverage this dataset to explore various research questions, such as the impact of watch history on video recommendations, the effectiveness of different caching strategies, and the analysis of user behavior patterns over time. The dataset supports a range of analyses, including regression analysis, machine learning applications, and descriptive statistics, allowing researchers to gain insights into user engagement and the performance of recommendation systems. Common preprocessing steps may include data cleaning to handle missing values, normalization of watch durations, and transformation of timestamps into usable formats for time-series analysis. However, researchers should be aware of potential limitations, such as biases in user selection, variations in viewing habits, and the influence of external factors on user engagement. Overall, this dataset serves as a valuable resource for understanding user behavior on YouTube and improving recommendation algorithms.",
    "tfidf_keywords": [
      "user behavior",
      "recommendation systems",
      "video engagement",
      "caching strategies",
      "viewing patterns",
      "watch history",
      "machine learning",
      "data analysis",
      "user interactions",
      "video consumption"
    ],
    "semantic_cluster": "user-behavior-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "recommendation-systems",
      "user-engagement",
      "machine-learning",
      "data-analysis",
      "behavioral-economics"
    ],
    "canonical_topics": [
      "recommendation-systems",
      "consumer-behavior",
      "machine-learning",
      "data-engineering"
    ]
  },
  {
    "name": "Harvard Dataverse",
    "description": "Global network of 120+ Dataverse installations hosting 75,000+ datasets with free 1TB storage",
    "category": "Dataset Aggregators",
    "url": "https://dataverse.harvard.edu",
    "docs_url": "https://guides.dataverse.org",
    "github_url": "https://github.com/IQSS/dataverse",
    "tags": [
      "social science",
      "replication",
      "DOI",
      "academic"
    ],
    "best_for": "Social science data with support for Stata, SPSS, and R formats",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Harvard Dataverse is a global network that hosts over 75,000 datasets across more than 120 Dataverse installations, providing researchers with free access to 1TB of storage. This platform allows users to discover, share, and cite datasets, facilitating replication and academic research in various fields, particularly social sciences.",
    "use_cases": [],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What datasets are available in the Harvard Dataverse?",
      "How can I access datasets on social science research?",
      "What is the storage capacity offered by Harvard Dataverse?",
      "Where can I find replication datasets for academic studies?",
      "How many Dataverse installations are there globally?",
      "What types of datasets can be found in the Harvard Dataverse?"
    ],
    "domain_tags": [
      "academic"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0004,
    "image_url": "/images/datasets/harvard-dataverse.png",
    "embedding_text": "The Harvard Dataverse serves as a comprehensive platform for researchers, providing a robust infrastructure for hosting and sharing datasets. With over 75,000 datasets available, it supports a wide range of academic disciplines, particularly in the social sciences. The data structure typically includes various rows and columns that represent different datasets, with key variables measuring diverse aspects of social phenomena. The collection methodology involves contributions from researchers worldwide, ensuring a rich diversity of data sources. While the platform offers significant storage capacity, users should be aware of potential limitations in data quality, as datasets are contributed by various authors and may vary in rigor and completeness. Common preprocessing steps may include data cleaning and normalization to prepare datasets for analysis. Researchers utilize the Harvard Dataverse to address a multitude of research questions, from exploring social trends to conducting replication studies. The platform supports various types of analyses, including regression, machine learning, and descriptive statistics, making it a versatile tool for academic inquiry. Overall, the Harvard Dataverse exemplifies a vital resource for advancing research and fostering collaboration in the academic community.",
    "benchmark_usage": [
      "Data sharing",
      "Replication studies"
    ],
    "tfidf_keywords": [
      "Dataverse",
      "datasets",
      "social science",
      "replication",
      "academic",
      "storage",
      "research",
      "data sharing",
      "DOI",
      "network"
    ],
    "semantic_cluster": "dataset-aggregation",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "data-sharing",
      "academic-research",
      "open-data",
      "social-science",
      "replication-studies"
    ],
    "canonical_topics": [
      "data-engineering",
      "consumer-behavior"
    ]
  },
  {
    "name": "Brazilian Store Chain",
    "description": "Sales data from Brazilian retail chain",
    "category": "Grocery & Supermarkets",
    "url": "https://www.kaggle.com/datasets/marcio486/sales-data-for-a-chain-of-brazilian-stores",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "retail",
      "Brazil",
      "chain stores"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Brazilian Store Chain dataset contains sales data from a retail chain in Brazil, providing insights into consumer purchasing behavior and sales trends. Researchers can utilize this dataset to perform analyses related to pricing strategies, inventory management, and consumer preferences in the grocery sector.",
    "use_cases": [
      "Analyzing the impact of promotions on sales performance.",
      "Identifying consumer purchasing patterns over time.",
      "Evaluating the effectiveness of pricing strategies.",
      "Forecasting future sales based on historical data."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the sales trends in Brazilian grocery stores?",
      "How do pricing strategies affect consumer behavior in Brazil?",
      "What factors influence sales in Brazilian retail chains?",
      "How can sales data be used to optimize inventory in grocery stores?",
      "What are the seasonal patterns in Brazilian supermarket sales?",
      "How does consumer behavior vary across different regions in Brazil?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Brazil",
    "size_category": "medium",
    "model_score": 0.0004,
    "image_url": "/images/datasets/brazilian-store-chain.png",
    "embedding_text": "The Brazilian Store Chain dataset is a comprehensive collection of sales data from a prominent retail chain operating in Brazil, specifically focusing on grocery and supermarket sectors. This dataset is structured in a tabular format, consisting of multiple rows and columns that represent various sales transactions over a defined period. Each row corresponds to a unique sales transaction, while the columns capture key variables such as transaction ID, product details, quantity sold, sale price, date of transaction, and store location. The data collection methodology typically involves point-of-sale systems that record each transaction in real-time, ensuring high accuracy and reliability. However, researchers should be aware of potential limitations, such as missing values or inconsistencies in product categorization, which may require preprocessing steps like data cleaning and normalization before analysis. The dataset's key variables enable researchers to measure sales performance, analyze consumer purchasing behavior, and identify trends in product popularity. Common research questions that can be addressed include understanding the impact of pricing changes on sales volume, examining seasonal variations in consumer purchases, and exploring the relationship between promotional activities and sales spikes. The dataset supports various types of analyses, including regression analysis to predict sales based on historical data, machine learning techniques for clustering consumer behavior, and descriptive statistics to summarize sales trends. Researchers typically utilize this dataset to inform strategic decisions in marketing, inventory management, and pricing optimization, making it a valuable resource for both academic studies and practical applications in the retail sector.",
    "tfidf_keywords": [
      "sales-trends",
      "consumer-purchasing-behavior",
      "pricing-strategies",
      "inventory-management",
      "promotions",
      "seasonal-patterns",
      "retail-analysis",
      "grocery-sector",
      "transaction-data",
      "Brazilian-retail",
      "data-cleaning",
      "machine-learning",
      "regression-analysis",
      "descriptive-statistics"
    ],
    "semantic_cluster": "retail-sales-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "pricing",
      "sales-forecasting",
      "inventory-optimization",
      "market-analysis"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "pricing",
      "statistics",
      "data-engineering"
    ]
  },
  {
    "name": "Netflix Engagement Reports",
    "description": "Hours viewed for every Netflix title (original and licensed) watched >50K hours. First public streaming metrics since 2021",
    "category": "Entertainment & Media",
    "url": "https://about.netflix.com/en/news/what-we-watched-a-netflix-engagement-report",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Netflix",
      "streaming",
      "engagement",
      "viewership",
      "hours watched"
    ],
    "best_for": "Learning entertainment & media analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "entertainment",
      "media",
      "viewership"
    ],
    "summary": "The Netflix Engagement Reports dataset provides insights into the hours viewed for every Netflix title that has been watched for over 50,000 hours. This dataset serves as the first public streaming metrics released since 2021, allowing researchers and analysts to explore trends in streaming engagement and viewership patterns.",
    "use_cases": [
      "Analyzing viewership trends over time for Netflix titles.",
      "Comparing engagement metrics between original and licensed content.",
      "Identifying popular genres based on hours viewed.",
      "Examining the impact of marketing campaigns on viewership."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the hours viewed for Netflix titles?",
      "How can I analyze Netflix viewership trends?",
      "What streaming metrics are available for Netflix?",
      "What titles have over 50K hours viewed on Netflix?",
      "How does Netflix engagement vary by title?",
      "What insights can be drawn from Netflix streaming data?"
    ],
    "domain_tags": [
      "entertainment",
      "media"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0004,
    "image_url": "/images/datasets/netflix-engagement-reports.png",
    "embedding_text": "The Netflix Engagement Reports dataset is a comprehensive collection of streaming metrics that captures the hours viewed for every Netflix title, both original and licensed, that has surpassed 50,000 hours of viewing. This dataset is significant as it marks the first public release of streaming metrics by Netflix since 2021, providing valuable insights into viewer engagement and content performance. The dataset is structured in a tabular format, with rows representing individual titles and columns detailing various metrics related to viewership. Key variables may include title names, hours viewed, release dates, and possibly genre classifications, although specific schema details are not provided. The collection methodology for this dataset involves aggregating viewership data from Netflix's streaming platform, ensuring that only titles with substantial engagement are included. This focus on titles with over 50,000 hours viewed allows for a concentrated analysis of popular content, making it a useful resource for researchers and analysts interested in understanding consumer behavior in the streaming landscape. However, potential limitations include the lack of demographic data, which could provide deeper insights into viewer preferences and behaviors. Researchers can utilize this dataset to address various research questions, such as identifying trends in viewership over time, comparing the performance of original versus licensed content, and analyzing the impact of external factors on engagement metrics. Common preprocessing steps may involve cleaning the data, handling missing values, and possibly normalizing viewership metrics for comparative analysis. The dataset supports a range of analyses, including descriptive statistics to summarize viewership patterns, regression analyses to explore relationships between variables, and machine learning techniques to predict future engagement based on historical data. Overall, the Netflix Engagement Reports dataset serves as a vital tool for understanding the dynamics of streaming viewership, enabling researchers to draw meaningful conclusions about content performance and audience engagement in the rapidly evolving media landscape.",
    "tfidf_keywords": [
      "streaming-metrics",
      "viewership-trends",
      "Netflix-titles",
      "hours-viewed",
      "content-performance",
      "consumer-behavior",
      "engagement-analysis",
      "original-content",
      "licensed-content",
      "data-aggregation"
    ],
    "semantic_cluster": "streaming-engagement-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "data-analysis",
      "viewership-patterns",
      "content-strategy",
      "media-analytics"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "data-engineering",
      "product-analytics"
    ]
  },
  {
    "name": "Store Item Demand",
    "description": "50 items across 10 different stores over 5 years",
    "category": "Grocery & Supermarkets",
    "url": "https://www.kaggle.com/competitions/demand-forecasting-kernels-only",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "demand forecasting",
      "Kaggle competition",
      "time series"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Store Item Demand dataset contains information on the demand for 50 different items across 10 stores over a period of 5 years. This dataset can be utilized for demand forecasting, time series analysis, and understanding consumer purchasing patterns.",
    "use_cases": [
      "Forecasting future demand for grocery items",
      "Analyzing seasonal trends in item sales",
      "Comparing demand across different stores",
      "Identifying factors affecting consumer purchasing behavior"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the demand for items in grocery stores?",
      "How can I forecast demand using historical sales data?",
      "What patterns can be observed in store item demand over time?",
      "How does demand vary across different stores?",
      "What factors influence item demand in supermarkets?",
      "How can time series analysis be applied to grocery sales data?",
      "What are the challenges in forecasting demand for grocery items?",
      "How can machine learning improve demand forecasting?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "time-series",
    "temporal_coverage": "5 years",
    "size_category": "medium",
    "model_score": 0.0004,
    "image_url": "/images/datasets/store-item-demand.jpg",
    "embedding_text": "The Store Item Demand dataset is a comprehensive collection that encapsulates the demand for 50 distinct items sold across 10 different grocery stores over a span of 5 years. This dataset is structured in a tabular format with rows representing individual sales transactions and columns detailing various attributes such as item identifiers, store identifiers, sales quantities, and timestamps. The data collection methodology likely involved aggregating sales data from point-of-sale systems across the participating stores, ensuring a rich temporal dimension that allows for the exploration of trends and patterns over time. Key variables in this dataset include item IDs, store IDs, demand quantities, and time indicators, which collectively enable researchers to measure and analyze consumer behavior and sales trends. However, potential limitations include data quality issues such as missing values or outliers, which are common in retail datasets. Preprocessing steps may involve cleaning the data, handling missing values, and transforming the data into a suitable format for analysis. Researchers can leverage this dataset to address a variety of research questions, such as understanding the impact of promotions on sales, analyzing seasonal demand fluctuations, and developing predictive models for future sales. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile resource for both academic research and practical applications in the retail sector. Typically, researchers utilize this dataset to build demand forecasting models, conduct exploratory data analysis, and derive insights that can inform inventory management and marketing strategies.",
    "benchmark_usage": [
      "Demand forecasting",
      "Time series analysis"
    ],
    "tfidf_keywords": [
      "demand-forecasting",
      "time-series-analysis",
      "consumer-purchasing-patterns",
      "sales-quantities",
      "seasonal-trends",
      "retail-analytics",
      "point-of-sale-data",
      "predictive-modeling",
      "data-cleaning",
      "exploratory-data-analysis"
    ],
    "semantic_cluster": "demand-forecasting-methods",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "time-series",
      "regression-analysis",
      "consumer-behavior",
      "sales-analytics",
      "inventory-management"
    ],
    "canonical_topics": [
      "forecasting",
      "consumer-behavior",
      "statistics",
      "machine-learning",
      "data-engineering"
    ]
  },
  {
    "name": "Spotify Music Streaming Sessions (MSSD)",
    "description": "150M+ listening sessions with skips, track features, and playlist context. The largest public music streaming behavior dataset",
    "category": "Entertainment & Media",
    "url": "https://paperswithcode.com/dataset/mssd",
    "docs_url": "https://arxiv.org/abs/1901.09851",
    "github_url": null,
    "tags": [
      "Spotify",
      "streaming",
      "sessions",
      "skips",
      "large-scale"
    ],
    "best_for": "Learning entertainment & media analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "consumer-behavior",
      "music-industry",
      "data-analysis"
    ],
    "summary": "The Spotify Music Streaming Sessions (MSSD) dataset contains over 150 million listening sessions, providing insights into user behavior, track features, and playlist context. Researchers can utilize this dataset to analyze music consumption patterns, study user engagement, and explore the impact of various factors on listening behavior.",
    "use_cases": [
      "Analyzing user engagement and skip rates in music streaming.",
      "Studying the impact of playlist context on listening behavior.",
      "Exploring trends in music consumption over time.",
      "Investigating the relationship between track features and user preferences."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the features of the Spotify Music Streaming Sessions dataset?",
      "How can I analyze user skips in music streaming data?",
      "What insights can be gained from large-scale music streaming behavior?",
      "How does playlist context affect listening sessions on Spotify?",
      "What variables are included in the Spotify Music Streaming Sessions dataset?",
      "How can I use this dataset for consumer behavior analysis?",
      "What are the potential research questions for analyzing music streaming data?",
      "What is the significance of track features in streaming sessions?"
    ],
    "domain_tags": [
      "entertainment",
      "media"
    ],
    "data_modality": "tabular",
    "size_category": "massive",
    "model_score": 0.0004,
    "image_url": "/images/datasets/spotify-music-streaming-sessions-mssd.png",
    "embedding_text": "The Spotify Music Streaming Sessions (MSSD) dataset is a comprehensive collection of over 150 million listening sessions, making it one of the largest public datasets available for analyzing music streaming behavior. The dataset encompasses a variety of variables that capture user interactions with music tracks, including skips, track features, and playlist context. Each session is structured in a tabular format, with rows representing individual listening sessions and columns detailing various attributes such as track ID, user ID, session duration, skip status, and contextual information about playlists. This rich dataset allows researchers to delve into the intricacies of music consumption patterns, providing a robust foundation for various analytical approaches. Researchers typically utilize this dataset to address key research questions related to consumer behavior in the music industry, such as understanding the factors that influence user engagement and the dynamics of playlist curation. The dataset's extensive coverage of user interactions enables analyses that range from descriptive statistics to more complex modeling techniques, including regression analysis and machine learning applications. However, researchers should be mindful of potential data quality issues, such as missing values or biases in user behavior, which may necessitate preprocessing steps like data cleaning and normalization. Overall, the MSSD serves as a valuable resource for those interested in exploring the evolving landscape of music streaming and its implications for artists, labels, and consumers alike.",
    "tfidf_keywords": [
      "listening sessions",
      "user engagement",
      "track features",
      "playlist context",
      "music consumption",
      "skips",
      "large-scale dataset",
      "behavior analysis",
      "streaming patterns",
      "consumer behavior"
    ],
    "semantic_cluster": "music-consumption-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "consumer-behavior",
      "music-industry",
      "data-analysis",
      "user-engagement",
      "playlist-curation"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "machine-learning",
      "data-engineering"
    ]
  },
  {
    "name": "Innerwear Data",
    "description": "Data scraped from Victoria's Secret and other innerwear retailers",
    "category": "Fashion & Apparel",
    "url": "https://www.kaggle.com/datasets/PromptCloudHQ/innerwear-data-from-victorias-secret-and-others",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "fashion",
      "retail",
      "scraped data"
    ],
    "best_for": "Learning fashion & apparel analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Innerwear Data consists of information scraped from Victoria's Secret and other innerwear retailers, providing insights into the fashion and apparel market. This dataset can be used to analyze consumer preferences, pricing strategies, and trends in the innerwear segment.",
    "use_cases": [
      "Analyzing pricing strategies of innerwear brands",
      "Understanding consumer preferences in the innerwear market",
      "Identifying trends in fashion and apparel sales",
      "Comparing product offerings across different retailers"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the pricing trends in innerwear?",
      "How do consumer preferences vary across different innerwear brands?",
      "What insights can be drawn from scraped data on innerwear sales?",
      "How does Victoria's Secret compare to other retailers in terms of product offerings?",
      "What factors influence consumer behavior in the innerwear market?",
      "How can this dataset be used to analyze retail performance in the fashion industry?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0004,
    "image_url": "/images/datasets/innerwear-data.jpg",
    "embedding_text": "The Innerwear Data is a comprehensive dataset that provides valuable insights into the fashion and apparel industry, specifically focusing on innerwear products. This dataset is structured in a tabular format, consisting of rows representing individual products and columns detailing various attributes such as brand, price, product type, and other relevant features. The data is collected through web scraping techniques from prominent retailers, including Victoria's Secret, capturing a wide array of innerwear items available in the market. The collection methodology involves automated scripts that extract product information from retailer websites, ensuring a rich dataset that reflects current market offerings. While the dataset does not specify temporal or geographic coverage, it encompasses a diverse range of products, allowing for a broad analysis of consumer behavior and market trends. Key variables within the dataset include product names, prices, and categories, which can be utilized to measure pricing strategies and consumer preferences. However, users should be aware of potential data quality issues, such as missing values or inconsistencies due to the scraping process. Common preprocessing steps may include data cleaning, normalization, and feature extraction to prepare the dataset for analysis. Researchers can leverage this dataset to address various research questions, such as identifying pricing trends, analyzing consumer preferences, and comparing product offerings across different retailers. The dataset supports a variety of analytical approaches, including regression analysis, machine learning, and descriptive statistics, making it a versatile resource for both academic and commercial research in the fashion industry. Typically, researchers utilize this dataset to conduct market analyses, develop pricing models, and explore consumer behavior patterns, contributing to a deeper understanding of the dynamics within the innerwear segment of the fashion market.",
    "tfidf_keywords": [
      "web-scraping",
      "consumer-preferences",
      "pricing-strategies",
      "fashion-industry",
      "innerwear-market",
      "data-quality",
      "product-categories",
      "market-analysis",
      "descriptive-statistics",
      "regression-analysis",
      "machine-learning"
    ],
    "semantic_cluster": "fashion-retail-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "pricing",
      "market-analysis",
      "data-scraping",
      "fashion-trends"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "pricing",
      "marketplaces"
    ]
  },
  {
    "name": "Amazon AWS Open Data",
    "description": "Registry of Open Data with analysis-ready datasets",
    "category": "Data Portals",
    "url": "https://registry.opendata.aws/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "AWS",
      "open data",
      "cloud",
      "various"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Amazon AWS Open Data is a registry that hosts a variety of analysis-ready datasets available for public use. Researchers and developers can leverage these datasets for various applications, including data analysis, machine learning, and cloud computing projects.",
    "use_cases": [],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What datasets are available in the Amazon AWS Open Data registry?",
      "How can I access analysis-ready datasets on AWS?",
      "What types of open data does Amazon AWS provide?",
      "Can I use AWS Open Data for machine learning projects?",
      "What are the benefits of using Amazon AWS for open data?",
      "How to find datasets related to cloud computing on AWS?"
    ],
    "domain_tags": [
      "cloud",
      "data portals"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0004,
    "embedding_text": "The Amazon AWS Open Data registry serves as a comprehensive platform for accessing a diverse array of datasets that are ready for analysis. This registry is designed to facilitate research and development by providing datasets that span various domains and topics, making them suitable for a wide range of applications. The data structure typically includes multiple rows and columns, with each dataset containing variables that are relevant to its specific context. Researchers can expect to find datasets that cover topics such as e-commerce, consumer behavior, and pricing strategies, among others. The collection methodology for these datasets varies, as they are sourced from different contributors and organizations, ensuring a rich variety of data types and formats. While the registry does not explicitly mention temporal or geographic coverage, it is implied that the datasets may encompass a broad range of time periods and locations, depending on the individual datasets. Key variables within the datasets often measure aspects such as sales figures, customer demographics, and product attributes, providing valuable insights for analysis. However, users should be aware of potential limitations in data quality, which may arise from inconsistencies in data collection methods or the nature of the data sources. Common preprocessing steps may include data cleaning, normalization, and transformation to prepare the datasets for analysis. Researchers typically utilize these datasets to address a variety of research questions, such as understanding market trends, predicting consumer behavior, or evaluating the impact of pricing strategies. The types of analyses supported by these datasets include regression analysis, machine learning applications, and descriptive statistics, making them versatile tools for both academic and commercial research. Overall, the Amazon AWS Open Data registry is an invaluable resource for those looking to leverage open data in their projects, providing a gateway to a wealth of information that can drive innovation and insights across multiple sectors.",
    "tfidf_keywords": [
      "analysis-ready",
      "cloud computing",
      "data diversity",
      "data sources",
      "data quality",
      "data cleaning",
      "machine learning",
      "consumer behavior",
      "e-commerce",
      "pricing strategies"
    ],
    "semantic_cluster": "open-data-resources",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "data-analysis",
      "machine-learning",
      "cloud-services",
      "open-data",
      "data-collection"
    ],
    "canonical_topics": [
      "data-engineering",
      "consumer-behavior",
      "machine-learning"
    ]
  },
  {
    "name": "NeurIPS Competition Data",
    "description": "Top-tier conference with competitions and benchmarks",
    "category": "Data Portals",
    "url": "https://nips.cc/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "NeurIPS",
      "ML",
      "benchmarks"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "NeurIPS",
      "ML",
      "benchmarks"
    ],
    "summary": "The NeurIPS Competition Data provides a comprehensive dataset for various machine learning competitions held at the NeurIPS conference. Researchers and practitioners can utilize this data to benchmark algorithms, test hypotheses, and develop innovative solutions in the field of machine learning.",
    "use_cases": [
      "Benchmarking machine learning algorithms",
      "Developing new ML models",
      "Testing hypotheses in ML research"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the NeurIPS Competition Data?",
      "How can I use NeurIPS data for benchmarking?",
      "What competitions are included in the NeurIPS dataset?",
      "Where can I find machine learning benchmarks?",
      "What types of machine learning tasks are covered by NeurIPS?",
      "How do I access the NeurIPS Competition Data?"
    ],
    "domain_tags": [
      "technology",
      "education"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0004,
    "image_url": "/images/logos/nips.png",
    "embedding_text": "The NeurIPS Competition Data serves as a vital resource for researchers and practitioners in the field of machine learning, providing a structured dataset that encompasses a variety of competitions held at the prestigious NeurIPS conference. The data is typically organized in a tabular format, consisting of rows representing individual competition entries and columns detailing various attributes such as performance metrics, algorithm descriptions, and participant information. This structured approach allows for efficient analysis and comparison of different machine learning models and techniques. The collection methodology for this dataset involves gathering submissions from participants in various competitions, ensuring a diverse range of approaches and solutions are represented. While the dataset does not specify temporal or geographic coverage, it is widely recognized for its contribution to advancing machine learning research. Key variables within the dataset often include performance scores, algorithm types, and other relevant metrics that measure the effectiveness of different approaches. However, users should be aware of potential limitations in data quality, such as variations in reporting standards among participants. Common preprocessing steps may include normalization of performance metrics, handling missing values, and encoding categorical variables to facilitate analysis. Researchers typically utilize this dataset to address a range of questions, including the effectiveness of specific algorithms, comparisons between different methodologies, and the exploration of new techniques in machine learning. The types of analyses supported by this dataset include regression analyses, machine learning model evaluations, and descriptive statistics, making it a versatile tool for both academic and practical applications in the field.",
    "benchmark_usage": [
      "Common uses include benchmarking algorithms and testing new methodologies."
    ],
    "tfidf_keywords": [
      "NeurIPS",
      "machine-learning",
      "benchmarking",
      "competitions",
      "performance-metrics",
      "algorithm-comparison",
      "data-collection",
      "model-evaluation",
      "hypothesis-testing",
      "ML-research"
    ],
    "semantic_cluster": "ml-benchmarking-datasets",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "machine-learning",
      "algorithm-evaluation",
      "data-benchmarking",
      "performance-analysis",
      "competition-data"
    ],
    "canonical_topics": [
      "machine-learning",
      "experimentation",
      "statistics"
    ]
  },
  {
    "name": "Data Mining Cup",
    "description": "Industry-sponsored data mining competitions",
    "category": "Data Portals",
    "url": "https://www.data-mining-cup.com/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "data mining",
      "industry",
      "competitions"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "data mining",
      "competitions",
      "industry"
    ],
    "summary": "The Data Mining Cup is a series of industry-sponsored competitions aimed at fostering innovation in data mining techniques. Participants can engage with real-world datasets to develop and test their analytical skills, contributing to advancements in data science and machine learning.",
    "use_cases": [
      "Developing predictive models",
      "Testing data mining techniques",
      "Collaborating with industry partners"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Data Mining Cup?",
      "How can I participate in data mining competitions?",
      "What datasets are available for data mining challenges?",
      "What skills are needed for industry-sponsored data mining competitions?",
      "Where can I find information on past Data Mining Cup competitions?",
      "What are the benefits of participating in data mining competitions?"
    ],
    "domain_tags": [
      "industry"
    ],
    "size_category": "medium",
    "model_score": 0.0004,
    "embedding_text": "The Data Mining Cup offers a unique platform for participants to engage in industry-sponsored data mining competitions. These competitions are designed to challenge participants with real-world datasets, encouraging the development and application of innovative data mining techniques. The data structure typically involves a mix of numerical and categorical variables, allowing for a wide range of analytical approaches. Participants can expect to encounter various data types, including structured data suitable for regression analysis and unstructured data that may require advanced machine learning techniques. The collection methodology for the datasets often involves collaboration with industry partners, ensuring the relevance and applicability of the data to current market challenges. While specific temporal or geographic coverage may not be explicitly stated, the datasets are generally reflective of contemporary industry issues. Key variables within the datasets are likely to measure performance metrics, user behaviors, or market trends, providing a rich foundation for analysis. However, participants should be aware of potential data quality issues, such as missing values or biases inherent in the data collection process. Common preprocessing steps may include data cleaning, normalization, and feature engineering to prepare the datasets for analysis. The Data Mining Cup can address a variety of research questions, such as predicting consumer behavior, optimizing pricing strategies, or evaluating the effectiveness of marketing campaigns. The types of analyses supported include regression, machine learning, and descriptive statistics, allowing for comprehensive insights into the data. Researchers and practitioners typically use this dataset to refine their analytical skills, explore innovative solutions to industry problems, and collaborate with peers in the field of data science.",
    "data_modality": "mixed",
    "tfidf_keywords": [
      "data mining",
      "competitions",
      "predictive modeling",
      "machine learning",
      "data analysis",
      "industry collaboration",
      "data quality",
      "feature engineering",
      "consumer behavior",
      "market trends"
    ],
    "semantic_cluster": "data-mining-competitions",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "machine-learning",
      "data-analysis",
      "predictive-modeling",
      "consumer-behavior",
      "industry-collaboration"
    ],
    "canonical_topics": [
      "machine-learning",
      "experimentation",
      "consumer-behavior"
    ]
  },
  {
    "name": "DrivenData",
    "description": "Data science competitions for social impact",
    "category": "Data Portals",
    "url": "https://www.drivendata.org/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "social impact",
      "competitions",
      "non-profit"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "DrivenData hosts data science competitions aimed at addressing social issues through innovative solutions. Participants can engage with real-world datasets to develop models and analyses that contribute to social impact initiatives.",
    "use_cases": [
      "Developing predictive models for social issues",
      "Analyzing competition results to improve data science skills",
      "Collaborating with non-profits to create impactful solutions"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the latest data science competitions for social impact?",
      "How can I participate in DrivenData competitions?",
      "What datasets are available on DrivenData?",
      "What skills are needed for data science competitions?",
      "How do data science competitions contribute to social impact?",
      "What types of challenges are presented in DrivenData competitions?"
    ],
    "domain_tags": [
      "non-profit",
      "social impact"
    ],
    "size_category": "medium",
    "model_score": 0.0004,
    "embedding_text": "DrivenData is a platform that organizes data science competitions with a focus on social impact. The competitions typically involve a variety of datasets that participants can use to develop predictive models and analyses aimed at solving pressing social issues. The data structure often includes tabular formats with various features relevant to the specific challenges posed by the competitions. Participants are encouraged to explore the datasets, which may include demographic information, social metrics, and other relevant variables that can inform their analyses. DrivenData's collection methodology is centered around sourcing real-world data from non-profit organizations and social enterprises, ensuring that the challenges presented are grounded in actual social problems. The coverage of the data can vary depending on the specific competition, but it generally spans a range of social issues, including health, education, and economic development. Key variables in the datasets often measure outcomes related to these social issues, such as health outcomes, educational attainment, or economic indicators. Data quality can vary, and participants should be aware of potential limitations such as missing data or biases in the datasets. Common preprocessing steps may include data cleaning, normalization, and feature engineering to prepare the data for analysis. Researchers and data scientists typically use this data to address research questions related to social impact, such as evaluating the effectiveness of interventions or understanding the factors that contribute to social challenges. The types of analyses supported by the data include regression analysis, machine learning models, and descriptive statistics, allowing participants to apply a variety of techniques to derive insights and solutions. DrivenData serves as a valuable resource for those looking to enhance their data science skills while contributing to meaningful social change.",
    "data_modality": "mixed",
    "tfidf_keywords": [
      "data science competitions",
      "social impact",
      "predictive modeling",
      "non-profit datasets",
      "data preprocessing",
      "machine learning",
      "data analysis",
      "social issues",
      "competition challenges",
      "impactful solutions"
    ],
    "semantic_cluster": "social-impact-competitions",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "machine-learning",
      "social-analytics",
      "data-visualization",
      "impact-evaluation",
      "community-engagement"
    ],
    "canonical_topics": [
      "machine-learning",
      "consumer-behavior",
      "policy-evaluation"
    ]
  },
  {
    "name": "Google Cloud Public Datasets",
    "description": "20+ petabytes across 200+ datasets with 1TB free BigQuery queries monthly",
    "category": "Dataset Aggregators",
    "url": "https://cloud.google.com/datasets",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "cloud",
      "BigQuery",
      "climate",
      "blockchain",
      "COVID"
    ],
    "best_for": "SQL-based analysis of climate, blockchain, and Google Trends data",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Google Cloud Public Datasets provide access to over 20 petabytes of data across more than 200 datasets, enabling users to perform extensive analyses using BigQuery. This dataset is particularly useful for researchers and data scientists interested in cloud computing, climate studies, blockchain technology, and the impact of COVID-19.",
    "use_cases": [
      "Analyzing climate change trends using historical climate data.",
      "Conducting blockchain transaction analysis for research.",
      "Studying the economic impact of COVID-19 through various datasets.",
      "Performing machine learning tasks on large datasets available in BigQuery."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the available datasets in Google Cloud Public Datasets?",
      "How can I access the Google Cloud Public Datasets?",
      "What types of analyses can I perform with Google Cloud Public Datasets?",
      "Are there any costs associated with using Google Cloud Public Datasets?",
      "What is the size of the datasets available in Google Cloud?",
      "How frequently are the Google Cloud Public Datasets updated?",
      "What are some use cases for Google Cloud Public Datasets?",
      "Can I use Google Cloud Public Datasets for machine learning?"
    ],
    "domain_tags": [
      "cloud",
      "climate",
      "blockchain",
      "healthcare"
    ],
    "data_modality": "tabular",
    "size_category": "massive",
    "model_score": 0.0004,
    "image_url": "/images/datasets/google-cloud-public-datasets.png",
    "embedding_text": "The Google Cloud Public Datasets initiative offers a vast repository of over 20 petabytes of data, encompassing more than 200 datasets that cater to a wide array of research and analytical needs. This dataset is structured primarily in a tabular format, allowing users to leverage Google BigQuery for efficient querying and analysis. Each dataset may contain various rows and columns, with key variables that measure diverse aspects such as climate metrics, blockchain transactions, and public health data related to COVID-19. The collection methodology involves aggregating data from multiple sources, ensuring a comprehensive coverage of topics relevant to contemporary research. However, users should be aware of potential limitations in data quality, which may arise from inconsistencies in data collection or updates. Common preprocessing steps may include data cleaning, normalization, and transformation to prepare the datasets for analysis. Researchers typically utilize these datasets to address a range of research questions, from understanding climate change impacts to analyzing economic trends during the pandemic. The datasets support various types of analyses, including regression, machine learning, and descriptive statistics, making them invaluable for data scientists and researchers looking to derive insights from large-scale data. Overall, the Google Cloud Public Datasets serve as a critical resource for those engaged in data-driven research across multiple domains.",
    "tfidf_keywords": [
      "BigQuery",
      "cloud-computing",
      "climate-data",
      "blockchain-analysis",
      "COVID-19-impact",
      "data-aggregation",
      "public-health-data",
      "machine-learning",
      "data-analytics",
      "dataset-repository",
      "data-quality",
      "data-preprocessing",
      "temporal-analysis",
      "geospatial-data",
      "data-visualization"
    ],
    "semantic_cluster": "cloud-data-analytics",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "data-engineering",
      "machine-learning",
      "cloud-computing",
      "big-data",
      "data-visualization"
    ],
    "canonical_topics": [
      "machine-learning",
      "data-engineering",
      "statistics",
      "consumer-behavior",
      "forecasting"
    ]
  },
  {
    "name": "Ukraine eCommerce (Fozzy)",
    "description": "E-commerce sales data from Fozzy Group retail chain in Ukraine",
    "category": "Grocery & Supermarkets",
    "url": "https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "retail",
      "Ukraine",
      "sales"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Ukraine eCommerce dataset from Fozzy Group provides detailed sales data from the retail chain in Ukraine. This dataset can be utilized to analyze consumer purchasing patterns, evaluate pricing strategies, and assess market trends in the grocery and supermarket sector.",
    "use_cases": [
      "Analyzing consumer purchasing patterns",
      "Evaluating pricing strategies",
      "Assessing market trends in the grocery sector"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the sales trends in the Ukrainian e-commerce market?",
      "How does pricing affect consumer behavior in grocery shopping?",
      "What are the peak sales periods for Fozzy Group?",
      "How do sales vary across different product categories?",
      "What demographic factors influence online grocery shopping in Ukraine?",
      "What are the implications of e-commerce growth for traditional retail in Ukraine?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Ukraine",
    "size_category": "medium",
    "model_score": 0.0004,
    "image_url": "/images/datasets/ukraine-ecommerce-fozzy.jpg",
    "embedding_text": "The Ukraine eCommerce dataset from Fozzy Group is a comprehensive collection of sales data that captures various aspects of consumer behavior within the grocery and supermarket sector in Ukraine. The dataset is structured in a tabular format, consisting of rows that represent individual sales transactions and columns that detail key variables such as product categories, sales amounts, transaction dates, and customer demographics. This rich dataset allows researchers and analysts to explore a wide range of research questions related to consumer purchasing patterns, pricing strategies, and market trends. The data collection methodology employed by Fozzy Group ensures a high level of accuracy and reliability, although users should be aware of potential limitations such as data completeness and the influence of external factors on sales figures. Common preprocessing steps may include data cleaning, normalization, and exploratory data analysis to prepare the dataset for deeper statistical analyses. Researchers can leverage this dataset for various types of analyses, including regression modeling, machine learning applications, and descriptive statistics. By examining the sales data, analysts can uncover insights into consumer behavior, identify peak sales periods, and assess the impact of pricing changes on sales performance. Overall, the Ukraine eCommerce dataset serves as a valuable resource for those interested in understanding the dynamics of the retail market in Ukraine and the factors driving e-commerce growth in the region.",
    "tfidf_keywords": [
      "e-commerce",
      "consumer behavior",
      "grocery sales",
      "pricing strategy",
      "retail analytics",
      "sales trends",
      "market analysis",
      "transaction data",
      "online shopping",
      "Ukrainian market"
    ],
    "semantic_cluster": "ecommerce-analytics",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "pricing",
      "market-analysis",
      "sales-forecasting",
      "retail-strategy"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "pricing",
      "marketplaces"
    ]
  },
  {
    "name": "EIA-860 Generator Inventory",
    "description": "Annual survey of all U.S. electric generators including capacity, technology, ownership, and location",
    "category": "Energy",
    "url": "https://www.eia.gov/electricity/data/eia860/",
    "docs_url": "https://www.eia.gov/electricity/data/eia860/",
    "github_url": null,
    "tags": [
      "generators",
      "capacity",
      "technology",
      "annual"
    ],
    "best_for": "Understanding the U.S. electricity generation fleet composition and trends",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "energy",
      "electricity",
      "infrastructure"
    ],
    "summary": "The EIA-860 Generator Inventory dataset provides a comprehensive annual survey of all electric generators in the U.S., detailing their capacity, technology, ownership, and location. Researchers can leverage this dataset to analyze trends in energy generation, assess the impact of different technologies on capacity, and explore ownership structures within the electric power sector.",
    "use_cases": [
      "Analyzing the distribution of electric generator technologies across the U.S.",
      "Assessing the impact of ownership structures on energy production capacity.",
      "Investigating trends in electric generation capacity over multiple years.",
      "Comparing the performance of different technologies in energy generation."
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the capacity of electric generators in the U.S.?",
      "How does technology affect generator ownership?",
      "What trends can be observed in U.S. electric generation?",
      "How do different states compare in terms of generator capacity?",
      "What types of technologies are most common in U.S. electric generators?",
      "How has the ownership of electric generators changed over time?"
    ],
    "domain_tags": [
      "energy"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2001-present",
    "geographic_scope": "United States",
    "size_category": "medium",
    "model_score": 0.0004,
    "image_url": "/images/logos/eia.png",
    "embedding_text": "The EIA-860 Generator Inventory dataset is an essential resource for researchers and analysts interested in the electric power sector in the United States. This dataset is structured in a tabular format, consisting of rows that represent individual electric generators and columns that capture various attributes such as capacity, technology type, ownership details, and geographic location. The data is collected annually through a comprehensive survey conducted by the Energy Information Administration (EIA), which is the primary source of energy statistics in the U.S. The dataset covers a wide range of electric generators, including those powered by fossil fuels, nuclear, and renewable sources, providing a holistic view of the U.S. energy landscape. Key variables in the dataset include generator capacity measured in megawatts (MW), technology classification (e.g., solar, wind, natural gas), ownership type (e.g., public, private), and location details, which can be used to analyze regional energy production capabilities. While the dataset is robust, researchers should be aware of potential limitations, such as reporting inconsistencies or changes in survey methodology over time. Common preprocessing steps may include cleaning the data for missing values, standardizing technology classifications, and aggregating data by geographic regions or ownership types for more focused analyses. This dataset supports a variety of research questions, such as examining the relationship between technology type and capacity, exploring the dynamics of ownership in the energy sector, and assessing the implications of energy policies on generator deployment. Analysts can employ various methods, including regression analysis, machine learning techniques, and descriptive statistics, to extract insights from the data. Researchers typically use the EIA-860 dataset to inform studies on energy policy, market dynamics, and technological advancements in the electric generation sector.",
    "tfidf_keywords": [
      "electric generators",
      "capacity",
      "technology types",
      "ownership structures",
      "energy production",
      "geographic distribution",
      "renewable energy",
      "fossil fuels",
      "nuclear power",
      "energy statistics",
      "EIA",
      "electric power sector",
      "data collection methodology",
      "regional analysis",
      "energy policies"
    ],
    "semantic_cluster": "energy-generation-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "energy policy",
      "electricity markets",
      "renewable energy technologies",
      "capacity planning",
      "infrastructure analysis"
    ],
    "canonical_topics": [
      "energy",
      "econometrics",
      "policy-evaluation"
    ]
  },
  {
    "name": "PJM Data Miner",
    "description": "Comprehensive market data from PJM, the largest U.S. regional transmission organization",
    "category": "Energy",
    "url": "https://dataminer2.pjm.com/",
    "docs_url": "https://www.pjm.com/markets-and-operations/data-dictionary",
    "github_url": null,
    "tags": [
      "PJM",
      "Eastern US",
      "prices",
      "wholesale",
      "capacity"
    ],
    "best_for": "Analyzing the largest U.S. electricity market covering 13 states",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The PJM Data Miner provides comprehensive market data from PJM, the largest U.S. regional transmission organization. This dataset allows users to analyze electricity prices, capacity, and other market dynamics in the Eastern U.S. region, facilitating insights into wholesale energy markets.",
    "use_cases": [
      "Analyzing trends in electricity pricing over time.",
      "Evaluating the impact of capacity changes on market prices.",
      "Conducting regression analysis on wholesale energy prices.",
      "Comparing PJM market data with other regional transmission organizations."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the PJM Data Miner?",
      "How can I access comprehensive market data from PJM?",
      "What types of electricity prices are included in the PJM dataset?",
      "What insights can be gained from PJM's wholesale market data?",
      "How does PJM capacity data impact energy pricing?",
      "What are the key features of the PJM Data Miner?"
    ],
    "domain_tags": [
      "energy"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1999-present",
    "geographic_scope": "Eastern US",
    "size_category": "medium",
    "model_score": 0.0004,
    "image_url": "/images/logos/pjm.png",
    "embedding_text": "The PJM Data Miner is a rich dataset that provides comprehensive market data from the Pennsylvania-New Jersey-Maryland (PJM) regional transmission organization, which is recognized as the largest in the United States. This dataset encompasses a variety of market variables, including electricity prices, capacity metrics, and other relevant indicators that characterize the wholesale energy market in the Eastern U.S. The data is structured in a tabular format, consisting of rows and columns that represent various market attributes over time. Each row typically corresponds to a specific time period, while the columns represent key variables such as price, demand, supply, and capacity levels. The collection methodology for this dataset involves aggregating data from multiple sources within the PJM organization, ensuring a comprehensive view of market dynamics. Coverage is primarily focused on the Eastern U.S., specifically within the PJM service area, which includes several states and regions. Key variables in this dataset measure critical aspects of the energy market, such as real-time and day-ahead prices, capacity forecasts, and historical demand levels. These variables are essential for understanding market behavior and for conducting various forms of analysis. However, users should be aware of potential limitations in data quality, such as missing values or discrepancies in reporting, which may require preprocessing steps like data cleaning and normalization. Common preprocessing steps include handling missing data, transforming variables for analysis, and aggregating data to the desired temporal granularity. Researchers can leverage this dataset to address a range of research questions, such as the impact of capacity changes on pricing, the correlation between demand and supply, and the effects of regulatory changes on market dynamics. The PJM Data Miner supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile tool for energy market research. Typically, researchers utilize this dataset to explore market trends, evaluate policy impacts, and develop predictive models for future pricing and capacity scenarios.",
    "tfidf_keywords": [
      "electricity-pricing",
      "wholesale-market",
      "capacity-analysis",
      "market-dynamics",
      "PJM-organization",
      "energy-demand",
      "supply-forecasting",
      "real-time-prices",
      "day-ahead-prices",
      "data-cleaning",
      "regression-analysis",
      "predictive-modeling",
      "market-trends",
      "policy-impact",
      "data-aggregation"
    ],
    "semantic_cluster": "energy-market-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "market-dynamics",
      "electricity-pricing",
      "capacity-planning",
      "energy-economics",
      "data-analysis"
    ],
    "canonical_topics": [
      "econometrics",
      "forecasting",
      "pricing",
      "consumer-behavior",
      "energy"
    ]
  },
  {
    "name": "Italian Grocers",
    "description": "Receipt-level sales data from Italian grocery stores",
    "category": "Grocery & Supermarkets",
    "url": "https://data.mendeley.com/datasets/s8dgbs3rng/1",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "grocery",
      "Italy",
      "receipts"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Italian Grocers dataset contains receipt-level sales data from Italian grocery stores, providing insights into consumer purchasing patterns and pricing strategies. Researchers can analyze this data to understand trends in grocery shopping and the factors influencing consumer behavior in Italy.",
    "use_cases": [
      "Analyzing consumer purchasing trends over time",
      "Evaluating the impact of pricing strategies on sales",
      "Investigating the relationship between product categories and sales volume",
      "Studying the effects of promotions on consumer behavior"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the sales trends in Italian grocery stores?",
      "How do prices vary across different grocery items in Italy?",
      "What consumer behaviors can be inferred from receipt-level data?",
      "How can I analyze grocery sales data for pricing strategies?",
      "What insights can be gained from Italian grocery store receipts?",
      "How does seasonality affect grocery sales in Italy?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Italy",
    "size_category": "medium",
    "model_score": 0.0004,
    "image_url": "/images/logos/mendeley.png",
    "embedding_text": "The Italian Grocers dataset is a rich source of receipt-level sales data from grocery stores across Italy, designed to facilitate a comprehensive understanding of consumer purchasing behaviors and pricing dynamics within the retail sector. This dataset typically consists of rows representing individual sales transactions, with columns detailing various attributes such as product identifiers, quantities sold, prices, timestamps, and potentially customer demographics. The collection methodology for this dataset likely involves aggregating sales data from multiple grocery stores, ensuring a diverse representation of consumer behaviors across different regions and store types in Italy. While the exact temporal coverage is not specified, the dataset is expected to provide insights into sales patterns over a defined period, making it suitable for time-series analyses. The geographic scope is explicitly mentioned as Italy, allowing researchers to focus on local market dynamics and consumer preferences. Key variables in the dataset may include product categories, sales amounts, and transaction dates, which can be leveraged to measure trends in consumer spending, seasonal fluctuations, and the impact of pricing strategies on sales performance. Data quality is crucial for meaningful analysis; thus, researchers should be aware of potential limitations such as missing values, inconsistencies in product categorization, or variations in store reporting practices. Common preprocessing steps may include data cleaning, normalization of product identifiers, and the aggregation of sales data to facilitate analysis. Researchers can utilize this dataset to address various research questions, such as identifying the most popular products, analyzing price elasticity, or exploring the effects of promotions on sales volume. The dataset supports a range of analytical techniques, including regression analysis, machine learning models, and descriptive statistics, making it a versatile tool for both academic research and practical applications in the retail industry. By understanding the intricacies of consumer behavior through this dataset, researchers can contribute valuable insights to the fields of marketing, economics, and retail management.",
    "tfidf_keywords": [
      "receipt-level data",
      "consumer purchasing behavior",
      "pricing strategies",
      "sales trends",
      "grocery retail",
      "product categories",
      "seasonality",
      "promotions",
      "data preprocessing",
      "transaction analysis"
    ],
    "semantic_cluster": "consumer-behavior-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "pricing",
      "data-analysis",
      "retail-marketing",
      "sales-analytics"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "pricing",
      "statistics"
    ]
  },
  {
    "name": "Walmart (M5)",
    "description": "Hierarchical sales data for 3,049 products across 10 stores",
    "category": "Grocery & Supermarkets",
    "url": "https://www.kaggle.com/competitions/m5-forecasting-accuracy",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "forecasting",
      "hierarchical",
      "Walmart",
      "M5 competition"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Walmart (M5) dataset contains hierarchical sales data for 3,049 products across 10 stores, making it a valuable resource for forecasting and understanding consumer purchasing patterns. Researchers can utilize this dataset to develop predictive models and analyze sales trends in the grocery and supermarket sector.",
    "use_cases": [
      "Developing sales forecasting models",
      "Analyzing consumer purchasing behavior",
      "Evaluating the impact of promotions on sales",
      "Comparing sales performance across different stores"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Walmart M5 dataset?",
      "How can I access hierarchical sales data for Walmart?",
      "What types of forecasting can be done with Walmart sales data?",
      "Where can I find datasets for grocery and supermarket sales?",
      "What are the key variables in the Walmart M5 dataset?",
      "How is the Walmart M5 dataset structured?",
      "What analysis can be performed on Walmart sales data?",
      "What are the applications of the Walmart M5 dataset in research?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0004,
    "image_url": "/images/datasets/walmart-m5.jpg",
    "embedding_text": "The Walmart (M5) dataset is a comprehensive collection of hierarchical sales data encompassing 3,049 products sold across 10 different Walmart stores. This dataset is structured in a tabular format, where each row represents a unique sales transaction, and the columns include variables such as product identifiers, store identifiers, sales quantities, and timestamps. The hierarchical nature of the data allows for multi-level analysis, enabling researchers to explore sales trends at both the product and store levels. The collection methodology for this dataset involves aggregating sales records from Walmart's point-of-sale systems, ensuring a rich source of transactional data. While the dataset does not specify temporal or geographic coverage, it is implied that the data pertains to a specific time frame relevant to the M5 competition, which focuses on sales forecasting. Key variables in the dataset measure sales performance, including units sold and revenue generated, providing insights into consumer behavior and market dynamics. However, researchers should be aware of potential limitations in data quality, such as missing values or inconsistencies in product categorization. Common preprocessing steps may include handling missing data, normalizing sales figures, and transforming categorical variables for analysis. This dataset supports a variety of analytical approaches, including regression analysis, machine learning models, and descriptive statistics, making it suitable for addressing research questions related to sales forecasting, consumer behavior analysis, and retail strategy optimization. Researchers typically leverage this dataset to build predictive models that can inform inventory management, pricing strategies, and promotional effectiveness, ultimately enhancing decision-making processes in the retail sector.",
    "tfidf_keywords": [
      "hierarchical-sales-data",
      "forecasting-models",
      "consumer-purchasing-patterns",
      "sales-trends",
      "retail-analysis",
      "Walmart-sales",
      "predictive-analytics",
      "promotional-impact",
      "multi-level-analysis",
      "transactional-data",
      "data-preprocessing",
      "sales-quantities",
      "revenue-analysis",
      "inventory-management"
    ],
    "semantic_cluster": "retail-sales-forecasting",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "sales-forecasting",
      "consumer-behavior",
      "data-analysis",
      "predictive-modeling",
      "market-research"
    ],
    "canonical_topics": [
      "forecasting",
      "consumer-behavior",
      "machine-learning",
      "statistics"
    ]
  },
  {
    "name": "Mexican Grocery",
    "description": "Data from a Mexican grocery store",
    "category": "Grocery & Supermarkets",
    "url": "https://www.kaggle.com/datasets/martinezjosegpe/grocery-store",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "grocery",
      "Mexico",
      "retail"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Mexican Grocery dataset contains data from a Mexican grocery store, providing insights into consumer purchasing patterns and pricing strategies. Researchers can use this dataset to analyze retail trends, consumer behavior, and market dynamics within the grocery sector in Mexico.",
    "use_cases": [
      "Analyzing consumer purchasing patterns",
      "Studying pricing strategies in retail",
      "Evaluating the impact of promotions on sales"
    ],
    "audience": [
      "Curious-browser",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "What are the purchasing trends in Mexican grocery stores?",
      "How do prices vary across different grocery items in Mexico?",
      "What consumer behaviors are evident in the Mexican grocery market?",
      "How does seasonality affect grocery sales in Mexico?",
      "What are the most popular grocery items among Mexican consumers?",
      "How do promotions impact sales in Mexican grocery stores?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Mexico",
    "size_category": "medium",
    "model_score": 0.0004,
    "image_url": "/images/datasets/mexican-grocery.png",
    "embedding_text": "The Mexican Grocery dataset is a valuable resource for researchers and analysts interested in the grocery retail sector in Mexico. This dataset typically consists of various rows and columns that capture transactional data from a grocery store, including variables such as item names, prices, quantities sold, and timestamps of purchases. The data structure allows for detailed analysis of consumer behavior and pricing strategies. The collection methodology may involve direct data extraction from point-of-sale systems or surveys conducted within the grocery store environment, ensuring that the data reflects real-world purchasing patterns. Coverage is primarily geographic, focusing on the Mexican market, and may include demographic insights depending on the specific variables collected. Key variables in this dataset measure aspects such as sales volume, item popularity, and pricing trends, which can be instrumental in understanding market dynamics. However, researchers should be aware of potential limitations in data quality, such as missing values or biases in consumer reporting. Common preprocessing steps may include data cleaning, normalization, and transformation to prepare the dataset for analysis. This dataset can address a variety of research questions, such as identifying trends in consumer purchasing over time, evaluating the effectiveness of pricing strategies, and understanding the impact of promotions on sales. It supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics. Researchers typically use this dataset to generate insights that inform business strategies, enhance customer understanding, and optimize inventory management within the grocery retail sector.",
    "tfidf_keywords": [
      "consumer-purchasing-patterns",
      "pricing-strategies",
      "retail-trends",
      "sales-volume",
      "item-popularity",
      "promotional-impact",
      "market-dynamics",
      "transactional-data",
      "point-of-sale",
      "grocery-sector"
    ],
    "semantic_cluster": "grocery-retail-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "pricing",
      "market-analysis",
      "retail-strategy",
      "sales-analytics"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "pricing",
      "marketplaces"
    ]
  },
  {
    "name": "Brazil Medical",
    "description": "Medicine sales data in Brazil",
    "category": "Grocery & Supermarkets",
    "url": "https://www.kaggle.com/datasets/tgomesjuliana/brazil-medicine-sales",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "pharmaceuticals",
      "Brazil",
      "healthcare"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "healthcare",
      "pharmaceuticals",
      "consumer-behavior"
    ],
    "summary": "The Brazil Medical dataset contains comprehensive data on medicine sales in Brazil, providing insights into the pharmaceuticals market. Researchers can utilize this dataset to analyze sales trends, consumer purchasing behavior, and the overall healthcare landscape in Brazil.",
    "use_cases": [
      "Analyzing sales trends in the Brazilian pharmaceuticals market",
      "Examining consumer purchasing behavior related to healthcare products",
      "Investigating the impact of pricing on medicine sales",
      "Comparing sales data across different regions in Brazil"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the trends in medicine sales in Brazil?",
      "How do pharmaceutical sales vary across different regions in Brazil?",
      "What factors influence consumer purchasing behavior in the Brazilian healthcare market?",
      "How has the medicine sales data changed over time?",
      "What are the most popular pharmaceuticals sold in Brazil?",
      "How do pricing strategies affect medicine sales in Brazil?"
    ],
    "domain_tags": [
      "healthcare",
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Brazil",
    "size_category": "medium",
    "model_score": 0.0004,
    "image_url": "/images/datasets/brazil-medical.jpg",
    "embedding_text": "The Brazil Medical dataset is a rich source of information regarding medicine sales in Brazil, structured in a tabular format. It includes various rows and columns that represent different sales transactions, capturing key variables such as product type, sales volume, pricing, and time of sale. The data is likely collected from various retail and pharmacy outlets across Brazil, reflecting a comprehensive snapshot of the pharmaceuticals market. Researchers can explore this dataset to address several research questions, such as identifying trends in medicine sales, understanding consumer behavior, and evaluating the impact of pricing strategies on sales performance. Common preprocessing steps may include cleaning the data for missing values, normalizing prices, and aggregating sales data by different dimensions such as time or product category. The dataset supports various types of analyses, including regression analysis to identify factors influencing sales, descriptive statistics to summarize sales trends, and machine learning techniques for predictive modeling. However, researchers should be aware of potential limitations in data quality, such as inconsistencies in reporting or variations in data collection methods across different sources. Overall, the Brazil Medical dataset serves as a valuable resource for those interested in the intersection of healthcare and retail, providing insights that can inform both academic research and practical business strategies.",
    "tfidf_keywords": [
      "pharmaceuticals",
      "consumer-behavior",
      "sales-trends",
      "pricing-strategies",
      "Brazilian-market",
      "healthcare-products",
      "retail-analysis",
      "medicine-sales",
      "data-cleaning",
      "predictive-modeling"
    ],
    "semantic_cluster": "healthcare-retail-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "retail-analysis",
      "pricing-strategies",
      "sales-trends",
      "market-research"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "healthcare",
      "econometrics"
    ]
  },
  {
    "name": "EIA-923 Power Plant Operations",
    "description": "Monthly power plant fuel consumption, generation, and emissions data for all U.S. generators",
    "category": "Energy",
    "url": "https://www.eia.gov/electricity/data/eia923/",
    "docs_url": "https://www.eia.gov/electricity/data/eia923/",
    "github_url": null,
    "tags": [
      "power plants",
      "generation",
      "fuel",
      "emissions",
      "monthly"
    ],
    "best_for": "Analyzing power plant operations, fuel mix trends, and emissions patterns",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The EIA-923 Power Plant Operations dataset provides comprehensive monthly data on fuel consumption, electricity generation, and emissions from all U.S. power plants. Researchers and analysts can utilize this dataset to explore trends in energy production, assess environmental impacts, and inform policy decisions regarding energy use and emissions.",
    "use_cases": [
      "Analyzing trends in fuel consumption across different power plants.",
      "Assessing the environmental impact of power generation in the U.S.",
      "Comparing emissions data to identify the most efficient power plants.",
      "Investigating the relationship between fuel types and electricity generation."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the EIA-923 Power Plant Operations dataset?",
      "How can I access monthly power plant fuel consumption data?",
      "What are the emissions statistics for U.S. power plants?",
      "How does fuel consumption vary among different power plants?",
      "What trends can be observed in U.S. electricity generation?",
      "How does the EIA-923 dataset support energy policy analysis?"
    ],
    "domain_tags": [
      "energy"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2001-present",
    "geographic_scope": "United States",
    "size_category": "medium",
    "model_score": 0.0004,
    "image_url": "/images/logos/eia.png",
    "embedding_text": "The EIA-923 Power Plant Operations dataset is a vital resource for understanding the dynamics of electricity generation in the United States. It includes detailed monthly records of fuel consumption, generation output, and emissions for all operational power plants across the country. The dataset is structured in a tabular format, with rows representing individual power plants and columns capturing various attributes such as fuel type, generation capacity, emissions levels, and operational status. This comprehensive data allows researchers to analyze trends over time, compare performance across different facilities, and assess the environmental impact of energy production. The data is collected by the U.S. Energy Information Administration (EIA) through mandatory reporting from power plant operators, ensuring a high level of accuracy and reliability. However, users should be aware of potential limitations, such as variations in reporting practices and the exclusion of certain smaller plants. Common preprocessing steps may include data cleaning, normalization, and transformation to prepare the dataset for analysis. Researchers can leverage this dataset to address a range of research questions, such as the impact of regulatory changes on emissions, the efficiency of different fuel types, and the correlation between energy production methods and environmental outcomes. The dataset supports various analytical approaches, including regression analysis, machine learning models, and descriptive statistics, making it a versatile tool for energy economists, environmental scientists, and policy analysts. Overall, the EIA-923 dataset serves as a foundational resource for studies aimed at improving energy efficiency, reducing emissions, and informing sustainable energy policies.",
    "tfidf_keywords": [
      "fuel consumption",
      "electricity generation",
      "emissions data",
      "power plants",
      "monthly reporting",
      "energy policy",
      "environmental impact",
      "data analysis",
      "U.S. generators",
      "energy efficiency",
      "EIA-923",
      "generation capacity",
      "operational status",
      "reporting practices"
    ],
    "semantic_cluster": "energy-data-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "energy economics",
      "environmental analysis",
      "data visualization",
      "statistical modeling",
      "policy evaluation"
    ],
    "canonical_topics": [
      "econometrics",
      "forecasting",
      "policy-evaluation",
      "statistics",
      "energy"
    ]
  },
  {
    "name": "Turkish Drugs",
    "description": "Drug sales data from Turkey",
    "category": "Grocery & Supermarkets",
    "url": "https://www.kaggle.com/datasets/emrahaydemr/drug-sales-data",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "pharmaceuticals",
      "Turkey",
      "sales"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Turkish Drugs dataset contains drug sales data from Turkey, providing insights into the pharmaceutical market. Researchers can analyze sales trends, consumer behavior, and pricing strategies within the grocery and supermarket sector.",
    "use_cases": [
      "Analyzing sales trends over time",
      "Comparing drug sales across different regions",
      "Evaluating the impact of pricing on sales",
      "Understanding consumer purchasing behavior"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the trends in drug sales in Turkey?",
      "How do pricing strategies affect pharmaceutical sales?",
      "What consumer behaviors can be observed in the Turkish drug market?",
      "How has the drug sales market in Turkey evolved?",
      "What are the seasonal patterns in drug sales?",
      "How do different regions in Turkey compare in terms of drug sales?"
    ],
    "domain_tags": [
      "retail",
      "healthcare"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Turkey",
    "size_category": "medium",
    "model_score": 0.0004,
    "image_url": "/images/datasets/turkish-drugs.jpg",
    "embedding_text": "The Turkish Drugs dataset is a comprehensive collection of drug sales data from Turkey, structured in a tabular format. The dataset includes various columns representing key variables such as drug names, sales volumes, prices, and possibly demographic information about consumers. The data is collected from retail pharmacies and supermarkets across Turkey, reflecting the dynamic nature of the pharmaceutical market in the region. Researchers can utilize this dataset to explore a range of research questions, including the analysis of sales trends, the impact of pricing strategies on consumer purchasing behavior, and regional comparisons of drug sales. The dataset's coverage is primarily geographic, focusing on Turkey, and it provides a snapshot of the pharmaceutical landscape within the country. However, researchers should be aware of potential limitations regarding data quality, such as inconsistencies in sales reporting and variations in data collection methodologies across different retailers. Common preprocessing steps may include data cleaning, normalization of sales figures, and handling missing values. The dataset supports various types of analyses, including regression analysis to assess the relationship between price and sales volume, as well as descriptive statistics to summarize sales trends. Researchers typically use this dataset to inform studies on consumer behavior, pricing strategies, and market dynamics within the healthcare sector, making it a valuable resource for those interested in the intersection of economics and healthcare.",
    "tfidf_keywords": [
      "pharmaceuticals",
      "drug sales",
      "consumer behavior",
      "pricing strategy",
      "retail pharmacies",
      "sales trends",
      "Turkey",
      "market analysis",
      "healthcare sector",
      "data collection"
    ],
    "semantic_cluster": "pharmaceutical-market-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "pricing",
      "market-analysis",
      "healthcare",
      "retail"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "pricing",
      "healthcare"
    ]
  },
  {
    "name": "CFR Cyber Operations Tracker",
    "description": "Database of publicly known state-sponsored cyber incidents since 2005 with threat actor attribution",
    "category": "Cybersecurity",
    "url": "https://www.cfr.org/cyber-operations/",
    "docs_url": "https://www.cfr.org/cyber-operations/",
    "github_url": null,
    "tags": [
      "cyber attacks",
      "nation-state",
      "attribution",
      "incidents"
    ],
    "best_for": "Tracking state-sponsored cyber operations and attack patterns",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The CFR Cyber Operations Tracker is a comprehensive database that catalogs publicly known state-sponsored cyber incidents since 2005, providing insights into threat actor attribution. Researchers and analysts can utilize this dataset to explore trends in cyber attacks, understand the motivations behind nation-state actions, and assess the implications for cybersecurity policy.",
    "use_cases": [
      "Analyzing the frequency and types of cyber incidents over time.",
      "Identifying common threat actors and their methods of operation.",
      "Assessing the impact of specific cyber incidents on international relations.",
      "Exploring correlations between geopolitical events and cyber attacks."
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the major state-sponsored cyber incidents since 2005?",
      "How can I analyze trends in cyber attacks by nation-state actors?",
      "What threat actors are attributed to specific cyber incidents?",
      "What patterns can be identified in state-sponsored cyber operations?",
      "How has the landscape of cyber incidents evolved over the years?",
      "What are the implications of state-sponsored cyber incidents for national security?"
    ],
    "domain_tags": [
      "cybersecurity"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2005-present",
    "geographic_scope": "Global",
    "size_category": "medium",
    "model_score": 0.0004,
    "image_url": "/images/logos/cfr.png",
    "embedding_text": "The CFR Cyber Operations Tracker is an extensive database designed to provide a detailed overview of publicly known state-sponsored cyber incidents that have occurred since 2005. This dataset is structured in a tabular format, with rows representing individual cyber incidents and columns detailing various attributes such as the date of the incident, the nation-state responsible, the type of attack, and the targeted sectors. The collection methodology involves aggregating information from various public sources, including news articles, government reports, and cybersecurity analyses, ensuring a comprehensive coverage of significant events in the realm of cyber operations. The key variables within the dataset measure aspects such as the nature of the cyber attack, the attribution to specific threat actors, and the impact on targeted organizations or sectors. While the dataset provides valuable insights, it is important to note that data quality may vary based on the availability and reliability of sources, and there may be limitations regarding the completeness of incident reporting. Common preprocessing steps may include cleaning the data for consistency, categorizing incidents by type, and normalizing dates for analysis. Researchers can leverage this dataset to address a variety of research questions, such as examining the evolution of cyber threats over time, analyzing the motivations behind state-sponsored attacks, and assessing the effectiveness of cybersecurity measures. The dataset supports various types of analyses, including descriptive statistics to summarize incident trends, regression analyses to explore relationships between geopolitical events and cyber incidents, and machine learning techniques to predict future threats based on historical data. Typically, researchers utilize the CFR Cyber Operations Tracker in studies focused on cybersecurity policy, international relations, and the strategic implications of cyber warfare, making it a crucial resource for understanding the dynamics of state-sponsored cyber operations.",
    "tfidf_keywords": [
      "cyber incidents",
      "state-sponsored attacks",
      "threat actor attribution",
      "cybersecurity policy",
      "incident trends",
      "geopolitical events",
      "cyber warfare",
      "data aggregation",
      "cyber threat analysis",
      "publicly known incidents",
      "cyber operations",
      "incident reporting",
      "data quality",
      "machine learning",
      "descriptive statistics"
    ],
    "semantic_cluster": "cybersecurity-datasets",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "cyber warfare",
      "threat intelligence",
      "incident response",
      "national security",
      "data analysis"
    ],
    "canonical_topics": [
      "policy-evaluation",
      "statistics",
      "machine-learning",
      "data-engineering"
    ]
  },
  {
    "name": "World Inequality Database",
    "description": "Income and wealth inequality data for 100+ countries by Piketty, Saez, and Zucman",
    "category": "Dataset Aggregators",
    "url": "https://wid.world",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "inequality",
      "wealth",
      "income",
      "Piketty"
    ],
    "best_for": "Income and wealth distribution research with free visualization tools",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "inequality",
      "wealth",
      "income"
    ],
    "summary": "The World Inequality Database provides comprehensive data on income and wealth inequality across more than 100 countries, compiled by renowned economists Piketty, Saez, and Zucman. Researchers can utilize this dataset to analyze trends in inequality, assess the impact of policy changes, and explore socioeconomic disparities.",
    "use_cases": [
      "Analyzing trends in income inequality over time",
      "Comparing wealth distribution across different countries",
      "Evaluating the effects of taxation policies on inequality",
      "Studying the relationship between economic growth and inequality"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the World Inequality Database?",
      "How can I access income inequality data?",
      "What countries are included in the World Inequality Database?",
      "Who are the authors of the World Inequality Database?",
      "What types of inequality data does the World Inequality Database provide?",
      "How is wealth inequality measured in the World Inequality Database?",
      "What research can be conducted using the World Inequality Database?",
      "Where can I find data on wealth distribution by country?"
    ],
    "domain_tags": [
      "economics",
      "social-sciences"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0004,
    "image_url": "/images/logos/wid.png",
    "embedding_text": "The World Inequality Database (WID) is a rich repository of data focused on income and wealth inequality, curated by prominent economists Thomas Piketty, Emmanuel Saez, and Gabriel Zucman. This dataset encompasses information from over 100 countries, providing a detailed view of how income and wealth are distributed across different populations. The data structure typically includes rows representing individual countries or regions, while columns capture various metrics related to income and wealth, such as Gini coefficients, income shares, and wealth distribution percentiles. The collection methodology involves aggregating data from national accounts, tax records, and household surveys, ensuring a comprehensive and accurate representation of inequality metrics. Researchers can leverage this dataset to address critical questions regarding the dynamics of inequality, the effectiveness of policy interventions, and the socio-economic factors contributing to disparities. Common preprocessing steps may include data cleaning, normalization, and the handling of missing values. The dataset supports a range of analytical techniques, including regression analysis, machine learning models, and descriptive statistics, making it a versatile tool for economists and social scientists. However, users should be aware of potential limitations, such as variations in data collection methods across countries and the challenges of comparing inequality metrics over time. Overall, the World Inequality Database serves as a foundational resource for understanding and analyzing the complex landscape of global inequality.",
    "tfidf_keywords": [
      "Gini coefficient",
      "income distribution",
      "wealth inequality",
      "economic disparity",
      "data aggregation",
      "household surveys",
      "national accounts",
      "policy evaluation",
      "socioeconomic factors",
      "income shares",
      "inequality metrics",
      "data normalization",
      "regression analysis",
      "machine learning",
      "descriptive statistics"
    ],
    "semantic_cluster": "inequality-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "income-distribution",
      "economic-policy",
      "socioeconomic-status",
      "data-analysis",
      "public-economics"
    ],
    "canonical_topics": [
      "econometrics",
      "policy-evaluation",
      "consumer-behavior",
      "labor-economics"
    ]
  },
  {
    "name": "Creator Economy Reports",
    "description": "Survey-based earnings breakdowns by platform (YouTube, TikTok, Instagram, Twitch). Influencer Marketing Factory research",
    "category": "Creator Economy",
    "url": "https://theinfluencermarketingfactory.com/creator-economy/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "creator economy",
      "earnings",
      "influencers",
      "surveys"
    ],
    "best_for": "Learning creator economy analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "creator economy",
      "influencer marketing"
    ],
    "summary": "The Creator Economy Reports provide insights into earnings breakdowns for influencers across various platforms such as YouTube, TikTok, Instagram, and Twitch. This dataset can be utilized to analyze trends in influencer earnings and the impact of platform choice on revenue generation.",
    "use_cases": [
      "Comparative analysis of influencer earnings across platforms",
      "Trend analysis of creator economy earnings over time",
      "Impact assessment of platform features on influencer revenue",
      "Survey analysis to understand influencer demographics and earnings"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the earnings breakdowns for influencers on YouTube?",
      "How do TikTok earnings compare to Instagram?",
      "What platforms generate the highest revenue for influencers?",
      "What trends can be identified in influencer earnings across different platforms?",
      "How do surveys inform our understanding of the creator economy?",
      "What is the impact of platform choice on influencer earnings?",
      "What are the key factors influencing earnings in the creator economy?",
      "How does the influencer marketing landscape vary by platform?"
    ],
    "domain_tags": [
      "marketing",
      "digital media"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0004,
    "image_url": "/images/logos/theinfluencermarketingfactory.png",
    "embedding_text": "The Creator Economy Reports dataset comprises survey-based earnings breakdowns for influencers operating on major social media platforms including YouTube, TikTok, Instagram, and Twitch. This dataset is structured in a tabular format, where each row represents an individual influencer's earnings data, and the columns include variables such as platform type, earnings figures, and demographic information about the influencers. The collection methodology involves conducting surveys among influencers to gather accurate earnings data, ensuring a comprehensive understanding of the creator economy landscape. While the dataset provides valuable insights, it is important to note that the data quality may vary based on the honesty and accuracy of self-reported earnings from influencers. Common preprocessing steps may include data cleaning to handle missing values and normalization of earnings figures for comparative analysis. Researchers can leverage this dataset to address various research questions, such as identifying which platforms yield the highest earnings for influencers, analyzing trends in influencer earnings over time, and understanding the demographic factors that influence earnings potential. The dataset supports a range of analyses, including regression analysis to explore relationships between platform choice and earnings, as well as descriptive statistics to summarize earnings distributions across different platforms. Overall, the Creator Economy Reports dataset serves as a valuable resource for researchers and marketers alike, providing insights into the evolving landscape of influencer marketing and the financial dynamics of the creator economy.",
    "tfidf_keywords": [
      "influencer earnings",
      "creator economy",
      "platform comparison",
      "survey data",
      "digital marketing",
      "earnings breakdown",
      "influencer demographics",
      "social media platforms",
      "revenue generation",
      "influencer marketing"
    ],
    "semantic_cluster": "creator-economy-insights",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "influencer marketing",
      "digital media",
      "earnings analysis",
      "social media strategy",
      "market trends"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "marketing",
      "econometrics"
    ]
  },
  {
    "name": "Synthetic Patient Wait Time Records",
    "description": "Hospital wait time records for analyzing patient flow and queue dynamics in healthcare settings.",
    "category": "Healthcare",
    "url": "https://www.kaggle.com/datasets/bharathreddybollu/hospital-wait-time-data",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "queueing",
      "wait-times",
      "hospital",
      "synthetic",
      "patient-flow",
      "Kaggle"
    ],
    "best_for": "Queueing theory - wait time prediction, capacity planning for target service levels",
    "image_url": "/images/datasets/synthetic-patient-wait-time-records.png",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Synthetic Patient Wait Time Records dataset contains hospital wait time records designed for analyzing patient flow and queue dynamics in healthcare settings. Researchers can utilize this dataset to model and predict patient wait times, assess the efficiency of healthcare services, and identify bottlenecks in patient processing.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "hospital wait time dataset",
      "synthetic patient flow data",
      "queue dynamics in healthcare",
      "analyze hospital wait times",
      "patient wait time records",
      "Kaggle synthetic datasets",
      "hospital efficiency analysis"
    ],
    "use_cases": [
      "Modeling patient wait times",
      "Assessing healthcare service efficiency",
      "Identifying bottlenecks in patient processing"
    ],
    "embedding_text": "The Synthetic Patient Wait Time Records dataset is a structured collection of hospital wait time records specifically designed for the analysis of patient flow and queue dynamics within healthcare environments. This dataset typically consists of rows representing individual patient visits and columns detailing various attributes such as wait times, service times, and possibly demographic information. The data structure allows for a comprehensive examination of how patients navigate through healthcare facilities, providing insights into operational efficiency and patient experience. The collection methodology for this dataset may involve simulations or synthetic data generation techniques, ensuring that the records reflect realistic scenarios without compromising patient privacy. As such, the dataset is valuable for researchers interested in healthcare analytics, as it provides a controlled environment to test hypotheses related to patient wait times and hospital efficiency. Key variables in this dataset likely include wait time, service time, and patient demographics, each of which plays a crucial role in understanding patient flow dynamics. However, as with any synthetic dataset, there are limitations regarding the generalizability of findings to real-world scenarios, as the data may not capture all the complexities of actual patient interactions. Common preprocessing steps might include normalization of wait times, handling of missing values, and transformation of categorical variables into numerical formats for analysis. Researchers can leverage this dataset to address various questions, such as how different factors influence patient wait times, the effectiveness of interventions aimed at reducing wait times, and the overall impact of wait times on patient satisfaction. The types of analyses supported by this dataset include regression modeling, machine learning applications, and descriptive statistics, making it a versatile resource for both exploratory and confirmatory research in healthcare settings. Typically, researchers utilize this dataset to conduct studies that aim to improve healthcare delivery by identifying key drivers of patient wait times and proposing data-driven solutions to enhance operational efficiency.",
    "domain_tags": [
      "healthcare"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "tfidf_keywords": [
      "patient flow",
      "queue dynamics",
      "wait time analysis",
      "healthcare efficiency",
      "synthetic data",
      "operational bottlenecks",
      "service time",
      "healthcare analytics",
      "data preprocessing",
      "regression modeling"
    ],
    "semantic_cluster": "healthcare-analytics",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "queueing theory",
      "healthcare operations",
      "data simulation",
      "patient experience",
      "service delivery"
    ],
    "canonical_topics": [
      "healthcare",
      "statistics",
      "machine-learning"
    ],
    "model_score": 0.0004
  },
  {
    "name": "2015 Flight Delays and Cancellations",
    "description": "5M+ US flights from 2015 with departure/arrival times and delay information. Model runways as single-server queues.",
    "category": "Transportation & Mobility",
    "url": "https://www.kaggle.com/datasets/usdot/flight-delays",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "queueing",
      "flights",
      "delays",
      "aviation",
      "DOT",
      "Kaggle"
    ],
    "best_for": "Queueing theory - runway queueing, delay propagation cascades, system stability analysis",
    "image_url": "/images/datasets/2015-flight-delays-and-cancellations.jpg",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "transportation",
      "data-analysis"
    ],
    "summary": "The 2015 Flight Delays and Cancellations dataset contains over 5 million records of US flights from the year 2015, detailing departure and arrival times along with delay information. This dataset can be utilized to model airport runways as single-server queues, enabling analysis of flight delays and cancellations to improve operational efficiency.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the common causes of flight delays in 2015?",
      "How can we model airport runways as queues using flight data?",
      "What patterns can be observed in flight cancellations?",
      "How do weather conditions affect flight delays?",
      "What is the average delay time for flights departing from major US airports?",
      "How does the time of day influence flight delays?",
      "What are the trends in flight delays over the year 2015?",
      "How can machine learning be applied to predict flight delays?"
    ],
    "use_cases": [
      "Analyzing the impact of weather on flight delays",
      "Modeling airport operations using queueing theory",
      "Identifying trends in flight cancellations",
      "Evaluating the efficiency of different airlines based on delay metrics"
    ],
    "embedding_text": "The 2015 Flight Delays and Cancellations dataset is a comprehensive collection of over 5 million records of flights within the United States for the year 2015. This dataset includes crucial information such as departure and arrival times, as well as detailed delay information, making it an invaluable resource for researchers and analysts in the field of transportation and mobility. The data is structured in a tabular format, with each row representing an individual flight and columns detailing various attributes such as flight number, airline, origin and destination airports, scheduled times, actual departure and arrival times, and delay durations. The collection methodology for this dataset involves aggregating flight data from the U.S. Department of Transportation (DOT), which ensures a high level of accuracy and reliability. However, like any dataset, it has its limitations, including potential inaccuracies in reported delay times and missing entries for certain flights. Researchers can utilize this dataset to address a variety of research questions, such as analyzing the factors contributing to flight delays, evaluating the performance of different airlines, and understanding the impact of time of day or weather conditions on flight operations. Common preprocessing steps may include handling missing values, normalizing time formats, and aggregating data for specific analyses. The dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics. Researchers typically leverage this dataset to improve operational efficiencies in airport management, develop predictive models for flight delays, and conduct policy evaluations related to aviation regulations.",
    "domain_tags": [
      "transportation",
      "aviation"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2015",
    "geographic_scope": "United States",
    "size_category": "massive",
    "benchmark_usage": [
      "Modeling runways as single-server queues"
    ],
    "tfidf_keywords": [
      "flight delays",
      "cancellations",
      "queueing theory",
      "aviation operations",
      "data analysis",
      "predictive modeling",
      "transportation efficiency",
      "delay metrics",
      "airline performance",
      "DOT data"
    ],
    "semantic_cluster": "aviation-analytics",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "queueing-theory",
      "predictive-modeling",
      "transportation-analysis",
      "data-visualization",
      "statistical-analysis"
    ],
    "canonical_topics": [
      "machine-learning",
      "statistics",
      "data-engineering",
      "consumer-behavior",
      "policy-evaluation"
    ],
    "model_score": 0.0004
  },
  {
    "name": "NEMSIS Public Release Research Dataset",
    "description": "49M+ EMS activations nationally with minute-level timestamps. The definitive dataset for US emergency medical services research.",
    "category": "Healthcare",
    "url": "https://nemsis.org/using-ems-data/request-research-data/",
    "docs_url": "https://nemsis.org/technical-resources/version-3/version-3-data-dictionaries/",
    "github_url": null,
    "tags": [
      "EMS",
      "emergency",
      "ambulance",
      "national",
      "healthcare"
    ],
    "best_for": "National EMS patterns, response time benchmarking, capacity planning",
    "image_url": "/images/datasets/nemsis-public-release-research-dataset.jpg",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "healthcare",
      "emergency services",
      "data analysis"
    ],
    "summary": "The NEMSIS Public Release Research Dataset contains over 49 million emergency medical service (EMS) activations across the United States, complete with minute-level timestamps. This dataset serves as a comprehensive resource for researchers investigating various aspects of emergency medical services, including response times, patient outcomes, and service utilization patterns.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the NEMSIS Public Release Research Dataset?",
      "How can I access the NEMSIS dataset for EMS research?",
      "What types of analyses can be performed using the NEMSIS dataset?",
      "What variables are included in the NEMSIS Public Release Research Dataset?",
      "How does the NEMSIS dataset support emergency medical services research?",
      "What are the limitations of the NEMSIS Public Release Research Dataset?",
      "What insights can be gained from analyzing EMS activations in the NEMSIS dataset?",
      "How is the NEMSIS dataset structured and what data does it contain?"
    ],
    "use_cases": [
      "Analyzing response times of EMS across different regions",
      "Studying the impact of EMS interventions on patient outcomes",
      "Evaluating trends in emergency service utilization over time",
      "Identifying factors influencing EMS activation rates"
    ],
    "embedding_text": "The NEMSIS Public Release Research Dataset is a comprehensive collection of over 49 million emergency medical service (EMS) activations across the United States, featuring minute-level timestamps that provide detailed insights into the timing and nature of EMS responses. This dataset is structured in a tabular format, with rows representing individual EMS activations and columns capturing a variety of variables, including timestamps, geographic locations, types of services rendered, and patient demographics. The data is collected through the National Emergency Medical Services Information System (NEMSIS), which aggregates information from EMS agencies nationwide, ensuring a robust and representative dataset for research purposes. Researchers can utilize this dataset to explore critical questions surrounding EMS performance, such as response times, patient outcomes, and service delivery patterns. Key variables in the dataset may include timestamps of activation and response, geographic identifiers, patient age and gender, and the nature of the medical emergency. While the dataset offers a wealth of information, researchers should be aware of potential limitations, such as variations in data reporting practices across different states and agencies, which may affect data consistency and quality. Common preprocessing steps may involve cleaning the data to handle missing values, standardizing variable formats, and aggregating data for specific analyses. The NEMSIS dataset supports a range of analytical approaches, including regression analysis, machine learning, and descriptive statistics, enabling researchers to draw meaningful conclusions about the effectiveness and efficiency of EMS systems. Overall, the NEMSIS Public Release Research Dataset is an invaluable resource for those studying emergency medical services, providing a foundation for evidence-based improvements in healthcare delivery.",
    "tfidf_keywords": [
      "emergency medical services",
      "NEMSIS",
      "EMS activations",
      "response times",
      "patient outcomes",
      "data quality",
      "service utilization",
      "geographic identifiers",
      "data preprocessing",
      "healthcare analytics",
      "temporal analysis",
      "demographic variables",
      "data aggregation",
      "statistical analysis",
      "machine learning"
    ],
    "semantic_cluster": "healthcare-data-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "healthcare analytics",
      "emergency response",
      "data quality assessment",
      "statistical modeling",
      "temporal data analysis"
    ],
    "canonical_topics": [
      "healthcare",
      "statistics",
      "data-engineering"
    ],
    "domain_tags": [
      "healthcare"
    ],
    "data_modality": "tabular",
    "geographic_scope": "United States",
    "size_category": "massive",
    "model_score": 0.0004
  },
  {
    "name": "CoasterQueues",
    "description": "10-minute interval wait time data for 48 theme parks worldwide. CC-BY licensed CSV downloads.",
    "category": "Entertainment & Media",
    "url": "https://coasterqueues.com/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "theme-park",
      "wait-times",
      "attractions",
      "CC-BY"
    ],
    "best_for": "Theme park demand patterns, attraction capacity planning, historical wait analysis",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "entertainment",
      "consumer-behavior"
    ],
    "summary": "CoasterQueues provides 10-minute interval wait time data for 48 theme parks worldwide, allowing users to analyze visitor patterns and optimize their experiences. This dataset can be utilized for various analyses related to theme park operations and customer behavior.",
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the average wait times at theme parks?",
      "How do wait times vary by day of the week?",
      "What factors influence wait times at attractions?",
      "Can wait time data help improve visitor experience?",
      "How does the wait time data compare across different parks?",
      "What trends can be identified in theme park wait times?"
    ],
    "use_cases": [
      "Analyzing peak wait times to improve visitor flow",
      "Comparing wait times across different parks to identify best practices",
      "Studying the impact of special events on wait times",
      "Modeling visitor behavior based on wait time data"
    ],
    "embedding_text": "CoasterQueues is a comprehensive dataset that captures 10-minute interval wait time data for 48 theme parks globally. This dataset is structured in a tabular format, with rows representing individual time intervals and columns detailing specific variables such as park name, attraction name, and wait time in minutes. The data is collected through various methodologies, likely involving automated systems or manual reporting at each theme park to ensure accuracy and timeliness. The key variables in this dataset include the park name, attraction name, and the recorded wait times, which provide insights into visitor experiences and operational efficiency. However, users should be aware of potential limitations in data quality, such as variations in reporting practices among parks and the impact of external factors like weather or special events on wait times. Common preprocessing steps may involve cleaning the data to handle missing values or outliers and aggregating wait times for broader analysis. Researchers can leverage this dataset to address various research questions, such as understanding how wait times fluctuate throughout the day or how they are affected by different seasons. The dataset supports a range of analyses, including regression modeling to predict wait times based on historical data, machine learning techniques to identify patterns, and descriptive statistics to summarize visitor experiences. Typically, researchers utilize this dataset to enhance operational strategies for theme parks, improve customer satisfaction, and inform marketing efforts based on visitor behavior.",
    "tfidf_keywords": [
      "wait-time",
      "theme-park",
      "visitor-experience",
      "data-collection",
      "operational-efficiency",
      "peak-times",
      "attraction-analysis",
      "customer-behavior",
      "time-series-data",
      "data-quality",
      "preprocessing",
      "regression-modeling",
      "machine-learning",
      "descriptive-statistics"
    ],
    "semantic_cluster": "theme-park-analytics",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "visitor-flow",
      "customer-satisfaction",
      "operational-strategy",
      "data-visualization",
      "behavioral-analysis"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "statistics",
      "data-engineering"
    ],
    "domain_tags": [
      "entertainment"
    ],
    "data_modality": "time-series",
    "size_category": "medium",
    "model_score": 0.0004
  },
  {
    "name": "Fliggy Transfers",
    "description": "Transfer-related data (flights, ground transport) from Fliggy",
    "category": "Travel & Hospitality",
    "url": "https://tianchi.aliyun.com/dataset/140721",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "travel",
      "transfers",
      "transportation"
    ],
    "best_for": "Learning travel & hospitality analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "Fliggy Transfers provides comprehensive transfer-related data, including information on flights and ground transportation sourced from Fliggy. This dataset can be utilized for various analyses in the travel and hospitality sector, enabling insights into consumer behavior and transportation trends.",
    "use_cases": [
      "Analyzing consumer preferences for different transfer options",
      "Evaluating the impact of transfer availability on travel decisions",
      "Investigating pricing strategies for transportation services",
      "Identifying trends in travel-related transfers over time"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the transfer options available on Fliggy?",
      "How do flight transfers impact consumer choices?",
      "What trends can be observed in ground transportation data?",
      "How does Fliggy's transfer data compare to other travel platforms?",
      "What are the pricing patterns for transfers on Fliggy?",
      "How can Fliggy's transfer data inform travel planning?"
    ],
    "domain_tags": [
      "travel",
      "transportation"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0003,
    "embedding_text": "The Fliggy Transfers dataset encompasses a rich array of transfer-related data, specifically focusing on flights and ground transportation options available through the Fliggy platform. This dataset is structured in a tabular format, consisting of rows representing individual transfer instances and columns detailing various attributes such as transfer type, pricing, duration, and user ratings. The data is collected from Fliggy's operational records, ensuring a comprehensive representation of the transfer services offered. Researchers can expect to find key variables that measure aspects like transfer efficiency, customer satisfaction, and pricing dynamics. While the dataset is robust, potential limitations include data completeness and the variability of user-generated content, which may affect the consistency of ratings and reviews. Preprocessing steps may involve cleaning the data to handle missing values and normalizing pricing information for comparative analyses. This dataset supports a range of analytical approaches, including regression analysis to identify factors influencing consumer choices, machine learning techniques for predictive modeling of transfer preferences, and descriptive statistics to summarize usage patterns. Researchers typically leverage this dataset to explore questions related to consumer behavior in the travel sector, the effectiveness of pricing strategies, and the overall impact of transfer options on travel experiences. By analyzing the Fliggy Transfers data, insights can be gained into how transportation choices affect travel planning and consumer satisfaction, contributing to a deeper understanding of the travel and hospitality landscape.",
    "tfidf_keywords": [
      "transfer-options",
      "consumer-preferences",
      "pricing-strategies",
      "travel-behavior",
      "ground-transportation",
      "flight-transfers",
      "user-ratings",
      "data-completeness",
      "predictive-modeling",
      "descriptive-statistics"
    ],
    "semantic_cluster": "travel-data-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "transportation-economics",
      "market-analysis",
      "data-preprocessing",
      "travel-industry"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "marketplaces",
      "pricing",
      "statistics",
      "data-engineering"
    ]
  },
  {
    "name": "Vietnam Supermarket",
    "description": "Sales and inventory snapshot data from Vietnamese supermarket",
    "category": "Grocery & Supermarkets",
    "url": "https://www.kaggle.com/datasets/tienanh2003/sales-and-inventory-snapshot-data",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "supermarket",
      "Vietnam",
      "inventory"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Vietnam Supermarket dataset provides a comprehensive snapshot of sales and inventory data from a Vietnamese supermarket. Researchers can utilize this dataset to analyze consumer purchasing behavior, inventory management practices, and pricing strategies within the grocery sector.",
    "use_cases": [
      "Analyzing consumer purchasing patterns over time",
      "Evaluating the effectiveness of pricing strategies",
      "Studying inventory turnover rates",
      "Forecasting future sales based on historical data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the sales trends in Vietnamese supermarkets?",
      "How does inventory management affect supermarket sales in Vietnam?",
      "What consumer behaviors can be observed from supermarket sales data?",
      "How do pricing strategies impact sales in Vietnamese grocery stores?",
      "What are the seasonal variations in supermarket inventory in Vietnam?",
      "How can sales data from Vietnamese supermarkets inform supply chain decisions?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Vietnam",
    "size_category": "medium",
    "model_score": 0.0003,
    "image_url": "/images/datasets/vietnam-supermarket.png",
    "embedding_text": "The Vietnam Supermarket dataset is a valuable resource for researchers and analysts interested in the grocery retail sector. This dataset comprises sales and inventory snapshot data collected from a Vietnamese supermarket, offering insights into consumer behavior and inventory management practices. The data is structured in a tabular format, with rows representing individual transactions or inventory records and columns capturing key variables such as product ID, sales volume, inventory levels, pricing, and timestamps. The collection methodology likely involves point-of-sale systems and inventory tracking tools used by the supermarket, ensuring that the data reflects real-time sales and stock levels. While the dataset provides a rich source of information, users should be aware of potential limitations, such as data quality issues stemming from incomplete records or discrepancies in inventory tracking. Common preprocessing steps may include data cleaning to handle missing values, normalization of pricing data, and aggregation of sales figures for analysis. Researchers can leverage this dataset to address various research questions, such as understanding seasonal sales trends, evaluating the impact of promotional pricing on consumer purchases, and analyzing inventory turnover rates. The dataset supports a range of analytical techniques, including regression analysis, machine learning models, and descriptive statistics, making it a versatile tool for both academic and industry applications. Typical studies utilizing this dataset may focus on consumer behavior analysis, pricing optimization strategies, and inventory management efficiency, contributing to a deeper understanding of the dynamics within the Vietnamese grocery market.",
    "tfidf_keywords": [
      "sales-trends",
      "inventory-management",
      "consumer-behavior",
      "pricing-strategies",
      "seasonal-variations",
      "supply-chain",
      "forecasting",
      "data-cleaning",
      "regression-analysis",
      "machine-learning",
      "descriptive-statistics",
      "promotional-pricing",
      "inventory-turnover",
      "grocery-retail",
      "Vietnam"
    ],
    "semantic_cluster": "retail-data-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "pricing",
      "inventory-management",
      "sales-forecasting",
      "data-preprocessing"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "pricing",
      "forecasting",
      "statistics",
      "data-engineering"
    ]
  },
  {
    "name": "Montgomery Liquor",
    "description": "Warehouse and retail liquor sales from Montgomery County, Maryland",
    "category": "Grocery & Supermarkets",
    "url": "https://data.montgomerycountymd.gov/Community-Recreation/Warehouse-and-Retail-Sales/v76h-r7br",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "liquor",
      "retail",
      "government data"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Montgomery Liquor dataset provides insights into warehouse and retail liquor sales specifically from Montgomery County, Maryland. Researchers can utilize this data to analyze consumer purchasing patterns, pricing strategies, and the impact of local regulations on liquor sales.",
    "use_cases": [
      "Analyzing the impact of local policies on liquor sales.",
      "Examining consumer behavior trends in liquor purchases.",
      "Comparing pricing strategies among different retailers.",
      "Investigating the correlation between sales and local events."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the liquor sales trends in Montgomery County?",
      "How do retail prices of liquor vary across different stores?",
      "What is the impact of government regulations on liquor sales?",
      "How do consumer preferences change over time in liquor purchases?",
      "What demographic factors influence liquor sales in Montgomery County?",
      "How can we analyze the seasonal variations in liquor sales?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Montgomery County, Maryland",
    "size_category": "medium",
    "model_score": 0.0003,
    "embedding_text": "The Montgomery Liquor dataset is a comprehensive collection of data related to warehouse and retail liquor sales in Montgomery County, Maryland. This dataset is structured in a tabular format, consisting of rows representing individual sales transactions and columns capturing various attributes such as product type, sale price, quantity sold, and date of transaction. The data is collected from both government sources and retail outlets, providing a rich source of information for analysis. Researchers can explore key variables such as sales volume, pricing trends, and consumer preferences, which are crucial for understanding market dynamics in the liquor industry. The dataset's coverage is specific to Montgomery County, allowing for localized analysis of liquor sales patterns. However, researchers should be aware of potential limitations in data quality, such as missing values or variations in reporting standards among different retailers. Common preprocessing steps may include data cleaning, normalization of price data, and handling of missing entries. This dataset supports various types of analyses, including regression analysis to identify factors influencing sales, machine learning models for predictive analytics, and descriptive statistics to summarize sales trends. Researchers typically use this dataset to address questions related to consumer behavior, pricing strategies, and the effects of local regulations on sales, making it a valuable resource for both academic and commercial inquiries.",
    "tfidf_keywords": [
      "liquor sales",
      "consumer preferences",
      "pricing strategies",
      "government regulations",
      "retail analysis",
      "sales trends",
      "local market dynamics",
      "data quality",
      "preprocessing",
      "sales volume"
    ],
    "semantic_cluster": "retail-sales-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "pricing",
      "market-analysis",
      "data-collection",
      "sales-trends"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "pricing",
      "econometrics"
    ]
  },
  {
    "name": "Expedia Hotel",
    "description": "Hotel booking and search data from Expedia",
    "category": "Travel & Hospitality",
    "url": "https://www.kaggle.com/datasets/vijeetnigam26/expedia-hotel",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "hotels",
      "bookings",
      "travel search"
    ],
    "best_for": "Learning travel & hospitality analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Expedia Hotel dataset contains hotel booking and search data from Expedia, allowing users to analyze trends in hotel bookings, consumer preferences, and pricing strategies. Researchers can utilize this dataset to explore various aspects of the travel and hospitality industry, including demand forecasting and market analysis.",
    "use_cases": [
      "Analyzing consumer behavior in hotel bookings",
      "Forecasting demand for hotel rooms",
      "Evaluating pricing strategies in the hospitality sector"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Expedia Hotel dataset?",
      "How can I access hotel booking data from Expedia?",
      "What insights can be derived from Expedia's hotel search data?",
      "Where can I find datasets related to hotel bookings?",
      "What are the trends in hotel bookings on Expedia?",
      "How does pricing affect hotel bookings on Expedia?"
    ],
    "domain_tags": [
      "travel",
      "hospitality"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0003,
    "image_url": "/images/datasets/expedia-hotel.png",
    "embedding_text": "The Expedia Hotel dataset is a comprehensive collection of hotel booking and search data sourced from Expedia, one of the leading online travel agencies. This dataset is structured in a tabular format, consisting of rows representing individual hotel bookings and search queries, with columns detailing various attributes such as hotel ID, booking date, check-in and check-out dates, customer demographics, pricing information, and search parameters. The data collection methodology involves aggregating user interactions and transactions on the Expedia platform, ensuring a rich source of information for analysis. While the dataset provides valuable insights into the travel and hospitality industry, it is important to note potential limitations, such as data quality issues arising from incomplete records or user privacy concerns. Common preprocessing steps may include data cleaning, normalization, and handling missing values to prepare the dataset for analysis. Researchers can leverage this dataset to address a variety of research questions, such as understanding consumer preferences, analyzing pricing trends, and forecasting future demand for hotel accommodations. The dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it a versatile tool for data scientists and researchers in the field. Typically, researchers utilize this dataset to conduct market analysis, evaluate the effectiveness of promotional strategies, and explore the dynamics of consumer behavior in the travel sector.",
    "tfidf_keywords": [
      "hotel-booking",
      "consumer-preferences",
      "pricing-strategies",
      "demand-forecasting",
      "market-analysis",
      "travel-industry",
      "online-travel-agency",
      "data-aggregation",
      "user-interactions",
      "transaction-data"
    ],
    "semantic_cluster": "travel-data-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "pricing",
      "marketplaces",
      "data-analysis",
      "demand-forecasting"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "pricing",
      "marketplaces"
    ]
  },
  {
    "name": "CMS Medicare & Medicaid Data",
    "description": "Public use files from Centers for Medicare & Medicaid Services including claims data, provider statistics, and program enrollment",
    "category": "Insurance & Actuarial",
    "url": "https://data.cms.gov/",
    "docs_url": "https://www.cms.gov/Research-Statistics-Data-and-Systems/Research-Statistics-Data-and-Systems",
    "github_url": null,
    "tags": [
      "medicare",
      "medicaid",
      "claims-data",
      "healthcare",
      "government-programs"
    ],
    "best_for": "Healthcare policy research, reimbursement modeling, and population health analysis",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "healthcare",
      "government-programs"
    ],
    "summary": "The CMS Medicare & Medicaid Data provides public use files from the Centers for Medicare & Medicaid Services, encompassing claims data, provider statistics, and program enrollment information. Researchers can utilize this dataset to analyze healthcare trends, evaluate the performance of government programs, and assess provider efficiency.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the CMS Medicare & Medicaid Data?",
      "How can I access CMS claims data?",
      "What types of statistics are included in the CMS Medicare & Medicaid Data?",
      "What insights can be derived from provider statistics in CMS data?",
      "How does CMS data support healthcare program evaluations?",
      "What are the main variables in the CMS Medicare & Medicaid dataset?",
      "What research questions can be addressed using CMS Medicare & Medicaid Data?",
      "Where can I find public use files from the Centers for Medicare & Medicaid Services?"
    ],
    "use_cases": [
      "Analyzing trends in Medicare and Medicaid claims over time.",
      "Evaluating the effectiveness of healthcare policies and programs.",
      "Assessing provider performance and patient outcomes.",
      "Investigating demographic differences in program enrollment."
    ],
    "domain_tags": [
      "healthcare",
      "insurance"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0003,
    "embedding_text": "The CMS Medicare & Medicaid Data is a comprehensive dataset provided by the Centers for Medicare & Medicaid Services (CMS) that includes public use files containing claims data, provider statistics, and program enrollment information. This dataset is structured in a tabular format, typically consisting of rows representing individual claims or records, and columns that detail various attributes such as patient demographics, service types, provider identifiers, and payment amounts. The data is collected through the CMS's extensive administrative processes, which compile information from healthcare providers and beneficiaries participating in Medicare and Medicaid programs. Researchers and analysts can leverage this dataset to explore a wide range of research questions, including the evaluation of healthcare access and quality, the analysis of cost trends, and the assessment of program effectiveness. Key variables within the dataset may include patient age, gender, diagnosis codes, service dates, and payment amounts, which collectively facilitate a thorough examination of healthcare utilization patterns. However, users should be aware of potential limitations regarding data quality, such as missing values or inaccuracies in provider reporting. Common preprocessing steps may involve cleaning the data, handling missing values, and transforming variables for analysis. The dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it a versatile resource for healthcare researchers. Typically, researchers use this data to conduct studies that inform policy decisions, improve healthcare delivery, and enhance the understanding of health outcomes across different populations.",
    "tfidf_keywords": [
      "claims-data",
      "provider-statistics",
      "program-enrollment",
      "healthcare-access",
      "cost-analysis",
      "Medicare",
      "Medicaid",
      "healthcare-quality",
      "demographic-analysis",
      "program-effectiveness",
      "utilization-patterns",
      "data-cleaning",
      "regression-modeling",
      "machine-learning",
      "descriptive-statistics"
    ],
    "semantic_cluster": "healthcare-data-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "healthcare-policy",
      "data-analysis",
      "public-health",
      "health-economics",
      "statistical-modeling"
    ],
    "canonical_topics": [
      "healthcare",
      "policy-evaluation",
      "econometrics"
    ]
  },
  {
    "name": "Rossmann Store Sales",
    "description": "1,115 Rossmann drug stores historical sales data",
    "category": "Grocery & Supermarkets",
    "url": "https://www.kaggle.com/competitions/rossmann-store-sales",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "retail",
      "Germany",
      "Kaggle competition"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Rossmann Store Sales dataset contains historical sales data from 1,115 Rossmann drug stores in Germany. It provides insights into retail sales patterns, allowing for analyses related to consumer behavior, sales forecasting, and inventory management.",
    "use_cases": [
      "Sales forecasting for retail stores",
      "Analyzing consumer purchasing trends",
      "Optimizing inventory management",
      "Evaluating the impact of promotions on sales"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Rossmann Store Sales dataset?",
      "How can I analyze retail sales data?",
      "What insights can be gained from Rossmann sales data?",
      "What variables are included in the Rossmann dataset?",
      "How does seasonality affect sales in the Rossmann dataset?",
      "What machine learning techniques can be applied to the Rossmann sales data?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Germany",
    "size_category": "medium",
    "model_score": 0.0003,
    "image_url": "/images/datasets/rossmann-store-sales.png",
    "embedding_text": "The Rossmann Store Sales dataset is a comprehensive collection of historical sales data from 1,115 Rossmann drug stores located across Germany. This dataset is structured in a tabular format, containing various rows and columns that represent different aspects of store sales. Key variables include store identifiers, sales figures, customer footfall, and promotional activities. Researchers and analysts can utilize this dataset to explore a range of research questions, such as the impact of promotions on sales, seasonal trends in consumer behavior, and the effectiveness of inventory management strategies. The data collection methodology involved aggregating sales data from the stores over a specified period, ensuring a robust representation of retail dynamics. However, like any dataset, it may have limitations, including missing values or anomalies that could affect analysis outcomes. Common preprocessing steps may include handling missing data, normalizing sales figures, and transforming categorical variables into numerical formats for machine learning applications. The dataset supports various types of analyses, including regression analysis, machine learning modeling, and descriptive statistics, making it a valuable resource for both academic research and practical applications in the retail sector. Researchers typically use this dataset to develop predictive models for sales forecasting, assess consumer behavior patterns, and optimize operational strategies within the retail environment.",
    "tfidf_keywords": [
      "sales-forecasting",
      "consumer-behavior",
      "inventory-management",
      "promotional-analysis",
      "time-series",
      "regression-modeling",
      "data-preprocessing",
      "retail-analytics",
      "footfall-analysis",
      "seasonality"
    ],
    "semantic_cluster": "retail-analytics",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "time-series-analysis",
      "predictive-modeling",
      "data-visualization",
      "retail-economics",
      "statistical-analysis"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "forecasting",
      "machine-learning",
      "statistics",
      "pricing"
    ]
  },
  {
    "name": "Ecuador Grocery (Favorita)",
    "description": "Unit sales data with store/item metadata and oil prices from Ecuador",
    "category": "Grocery & Supermarkets",
    "url": "https://www.kaggle.com/competitions/favorita-grocery-sales-forecasting",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "grocery",
      "sales forecasting",
      "Ecuador",
      "Kaggle"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "grocery",
      "sales forecasting",
      "Ecuador"
    ],
    "summary": "The Ecuador Grocery (Favorita) dataset contains unit sales data along with store and item metadata, as well as oil prices from Ecuador. This dataset can be utilized for sales forecasting, market analysis, and understanding consumer behavior in the grocery sector.",
    "use_cases": [
      "Analyzing the impact of oil prices on grocery sales trends.",
      "Forecasting future sales based on historical data.",
      "Evaluating consumer purchasing behavior in the Ecuadorian grocery market."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the unit sales data for grocery items in Ecuador?",
      "How do oil prices affect grocery sales in Ecuador?",
      "What metadata is available for stores and items in the Ecuador Grocery dataset?",
      "Can I use this dataset for sales forecasting in the grocery sector?",
      "What insights can be gained from analyzing grocery sales data in Ecuador?",
      "Where can I find the Ecuador Grocery dataset for analysis?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Ecuador",
    "size_category": "medium",
    "model_score": 0.0003,
    "image_url": "/images/datasets/ecuador-grocery-favorita.jpg",
    "embedding_text": "The Ecuador Grocery (Favorita) dataset is a rich source of unit sales data that encompasses various aspects of grocery retail in Ecuador. It includes detailed store and item metadata, allowing researchers and analysts to explore the dynamics of grocery sales in relation to external factors such as oil prices. The dataset is structured in a tabular format, featuring rows that represent individual sales transactions and columns that capture key variables such as item identifiers, store locations, sales volumes, and corresponding oil prices. This structure facilitates straightforward data manipulation and analysis using tools like pandas in Python. The collection methodology for this dataset is based on actual sales records from grocery stores, ensuring a high level of accuracy and relevance to the Ecuadorian market. However, users should be aware of potential limitations in data quality, such as missing values or discrepancies in item categorization, which may require preprocessing steps like data cleaning and normalization. Key variables in the dataset include sales volume, item prices, and store locations, which can be used to measure sales performance and consumer preferences. Researchers can leverage this dataset to address a variety of research questions, such as the correlation between oil prices and grocery sales, seasonal trends in consumer purchasing behavior, and the effectiveness of pricing strategies. The dataset supports various types of analyses, including regression analysis, machine learning models, and descriptive statistics, making it a versatile tool for both academic research and practical applications in the grocery retail sector. By analyzing this dataset, researchers can gain valuable insights into market trends, consumer behavior, and the overall economic landscape of grocery shopping in Ecuador.",
    "tfidf_keywords": [
      "unit sales",
      "grocery retail",
      "oil prices",
      "consumer behavior",
      "sales forecasting",
      "Ecuador",
      "store metadata",
      "item metadata",
      "data quality",
      "data preprocessing",
      "market analysis",
      "historical data",
      "sales trends",
      "pricing strategies"
    ],
    "semantic_cluster": "grocery-sales-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "sales forecasting",
      "consumer behavior",
      "market analysis",
      "data preprocessing",
      "regression analysis"
    ],
    "canonical_topics": [
      "forecasting",
      "consumer-behavior",
      "econometrics",
      "pricing"
    ]
  },
  {
    "name": "LIAR Fact-Checking",
    "description": "12.8K fact-checked political statements with speaker metadata and 6-way truthfulness labels. Politifact benchmark",
    "category": "Content Moderation",
    "url": "https://www.cs.ucsb.edu/~william/data/liar_dataset.zip",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "fact-checking",
      "politics",
      "misinformation",
      "NLP",
      "Politifact"
    ],
    "best_for": "Learning content moderation analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "politics",
      "fact-checking",
      "misinformation"
    ],
    "summary": "The LIAR Fact-Checking dataset contains 12.8K fact-checked political statements, each associated with speaker metadata and six truthfulness labels. This dataset can be utilized for natural language processing tasks, particularly in analyzing political discourse and misinformation.",
    "use_cases": [
      "Analyzing the accuracy of political statements over time.",
      "Training machine learning models to detect misinformation.",
      "Evaluating the effectiveness of fact-checking organizations.",
      "Studying the relationship between speaker characteristics and truthfulness."
    ],
    "audience": [
      "Curious-browser",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What is the LIAR Fact-Checking dataset?",
      "How can I access the LIAR dataset for NLP tasks?",
      "What are the truthfulness labels in the LIAR dataset?",
      "What types of political statements are included in the LIAR dataset?",
      "How does the LIAR dataset compare to other fact-checking datasets?",
      "What speaker metadata is available in the LIAR dataset?",
      "How can I use the LIAR dataset for machine learning?",
      "What research questions can be addressed using the LIAR dataset?"
    ],
    "domain_tags": [
      "politics"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "benchmark_usage": [
      "Politifact benchmark"
    ],
    "model_score": 0.0003,
    "embedding_text": "The LIAR Fact-Checking dataset is a comprehensive collection of 12.8K fact-checked political statements, designed to facilitate research in the field of natural language processing (NLP) and misinformation detection. Each entry in the dataset includes detailed speaker metadata, such as the identity of the speaker and their political affiliation, as well as a six-way truthfulness label that categorizes the statements into various levels of accuracy. The data structure consists of rows representing individual statements and columns that capture key variables, including the statement text, speaker information, and truthfulness labels. This rich schema allows researchers to explore various dimensions of political discourse and misinformation. The dataset was collected from a range of political speeches, debates, and public statements, ensuring a diverse representation of political viewpoints. However, researchers should be aware of potential limitations, such as the subjective nature of truthfulness assessments and the evolving landscape of political communication. Common preprocessing steps may include text normalization, tokenization, and the application of NLP techniques to extract features relevant for analysis. The LIAR dataset supports a variety of analytical approaches, including regression analysis, machine learning classification tasks, and descriptive statistics. Researchers typically use this dataset to address questions related to the prevalence of misinformation, the impact of fact-checking on public perception, and the dynamics of political communication. By leveraging the LIAR dataset, scholars can gain insights into the mechanisms of misinformation and contribute to the broader discourse on political accountability and media literacy.",
    "tfidf_keywords": [
      "fact-checking",
      "political statements",
      "truthfulness labels",
      "speaker metadata",
      "misinformation detection",
      "natural language processing",
      "NLP",
      "data collection methodology",
      "political discourse",
      "data quality"
    ],
    "semantic_cluster": "nlp-for-economics",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "natural-language-processing",
      "misinformation",
      "political-communication",
      "data-quality",
      "text-analysis"
    ],
    "canonical_topics": [
      "natural-language-processing",
      "machine-learning",
      "consumer-behavior"
    ]
  },
  {
    "name": "NetEase Music (INFORMS)",
    "description": "Data from NetEase Cloud Music for INFORMS competition",
    "category": "Entertainment & Media",
    "url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3554826",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "music",
      "streaming",
      "INFORMS",
      "China"
    ],
    "best_for": "Learning entertainment & media analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "music",
      "streaming",
      "China"
    ],
    "summary": "The NetEase Music dataset comprises data from NetEase Cloud Music, specifically curated for the INFORMS competition. This dataset allows researchers and analysts to explore various aspects of music streaming behaviors, user preferences, and market dynamics in the Chinese music industry.",
    "use_cases": [
      "Analyzing user streaming behavior",
      "Exploring trends in music preferences",
      "Comparing music consumption across different demographics"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What data is available in the NetEase Music dataset?",
      "How can I analyze streaming trends using this dataset?",
      "What insights can be gained from user preferences in NetEase Cloud Music?",
      "What are the key variables in the NetEase Music dataset?",
      "How does the dataset support analysis of the Chinese music market?",
      "What research questions can be addressed with the NetEase Music data?",
      "What preprocessing steps are needed for this dataset?",
      "How can I visualize music streaming data from NetEase?"
    ],
    "domain_tags": [
      "entertainment",
      "media"
    ],
    "data_modality": "tabular",
    "geographic_scope": "China",
    "size_category": "medium",
    "model_score": 0.0003,
    "embedding_text": "The NetEase Music dataset is a comprehensive collection of data derived from NetEase Cloud Music, specifically designed for the INFORMS competition. This dataset provides a rich resource for analyzing music streaming behaviors and user preferences within the rapidly evolving Chinese music industry. The data structure is primarily tabular, consisting of rows and columns that capture various attributes related to music tracks, user interactions, and streaming metrics. Key variables may include track identifiers, user demographics, streaming frequency, and engagement metrics, all of which are essential for understanding consumer behavior in the music streaming sector. The collection methodology involves aggregating user-generated data from the NetEase Cloud Music platform, ensuring a diverse representation of user interactions and preferences. However, researchers should be aware of potential limitations in data quality, such as biases in user representation or incomplete records. Common preprocessing steps might include data cleaning, normalization, and transformation to facilitate analysis. This dataset supports a range of analytical techniques, including regression analysis, machine learning models, and descriptive statistics, allowing researchers to explore various research questions related to music consumption patterns, user engagement, and market dynamics. By leveraging this dataset, analysts can gain insights into the factors influencing music streaming choices and the overall landscape of the Chinese music industry.",
    "image_url": "/images/logos/ssrn.png",
    "tfidf_keywords": [
      "streaming-behavior",
      "user-preferences",
      "music-consumption",
      "data-collection",
      "engagement-metrics",
      "Chinese-music-market",
      "data-preprocessing",
      "regression-analysis",
      "machine-learning",
      "descriptive-statistics"
    ],
    "semantic_cluster": "music-streaming-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "market-analysis",
      "data-visualization",
      "user-engagement",
      "music-industry"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "machine-learning",
      "data-engineering"
    ]
  },
  {
    "name": "Fashion-MNIST",
    "description": "70,000 28x28 grayscale images of 10 fashion categories from Zalando",
    "category": "Fashion & Apparel",
    "url": "https://github.com/zalandoresearch/fashion-mnist",
    "docs_url": null,
    "github_url": "https://github.com/zalandoresearch/fashion-mnist",
    "tags": [
      "image classification",
      "benchmark",
      "deep learning"
    ],
    "best_for": "Learning fashion & apparel analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Fashion-MNIST dataset consists of 70,000 grayscale images, each measuring 28x28 pixels, representing 10 different fashion categories sourced from Zalando. This dataset serves as a benchmark for image classification tasks, particularly in the field of deep learning, allowing researchers and practitioners to develop and evaluate their models effectively.",
    "use_cases": [
      "Training deep learning models for image classification",
      "Benchmarking new image classification algorithms",
      "Exploring transfer learning techniques in computer vision",
      "Evaluating model performance on fashion-related image data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Fashion-MNIST dataset?",
      "How can I use Fashion-MNIST for image classification?",
      "What are the categories in the Fashion-MNIST dataset?",
      "Where can I find the Fashion-MNIST dataset?",
      "What is the size of the Fashion-MNIST dataset?",
      "How to preprocess images from the Fashion-MNIST dataset?",
      "What machine learning models can be applied to Fashion-MNIST?",
      "What are the benchmarks for Fashion-MNIST?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "image",
    "size_category": "medium",
    "benchmark_usage": [
      "Common uses include benchmarking image classification algorithms"
    ],
    "model_score": 0.0003,
    "image_url": "/images/datasets/fashion-mnist.png",
    "embedding_text": "The Fashion-MNIST dataset is a widely recognized resource in the field of machine learning and computer vision, consisting of 70,000 grayscale images, each with a resolution of 28x28 pixels. These images represent 10 distinct categories of fashion items, including T-shirts, trousers, pullovers, dresses, coats, sandals, shirts, sneakers, bags, and ankle boots. The dataset was created as a drop-in replacement for the original MNIST dataset, which contains handwritten digits, to provide a more challenging benchmark for image classification tasks. The images are labeled with their corresponding categories, making it suitable for supervised learning tasks. Researchers and practitioners typically utilize this dataset to train and evaluate various deep learning models, particularly convolutional neural networks (CNNs), which are well-suited for image data. The dataset's structure is straightforward, with each image represented as a 28x28 matrix of pixel values ranging from 0 to 255, indicating the intensity of the grayscale color. The dataset is divided into a training set of 60,000 images and a test set of 10,000 images, allowing for effective model training and validation. Due to its size and simplicity, Fashion-MNIST is often used as a starting point for those new to deep learning and image classification, providing a practical introduction to the concepts and techniques involved. Common preprocessing steps include normalizing pixel values, augmenting images to improve model robustness, and reshaping data for input into neural networks. Researchers can address various research questions using this dataset, such as exploring the impact of different model architectures on classification accuracy, investigating the effectiveness of data augmentation techniques, and comparing the performance of traditional machine learning algorithms against deep learning methods. The Fashion-MNIST dataset supports a range of analyses, including regression, machine learning, and descriptive statistics, making it a versatile tool for both academic research and practical applications in the fashion industry.",
    "tfidf_keywords": [
      "image classification",
      "deep learning",
      "convolutional neural networks",
      "benchmark dataset",
      "fashion categories",
      "data augmentation",
      "pixel normalization",
      "supervised learning",
      "computer vision",
      "training set",
      "test set",
      "Zalando",
      "grayscale images",
      "fashion items",
      "model evaluation"
    ],
    "semantic_cluster": "image-classification-benchmark",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "computer-vision",
      "machine-learning",
      "image-processing",
      "deep-learning",
      "data-augmentation"
    ],
    "canonical_topics": [
      "machine-learning",
      "computer-vision",
      "experimentation"
    ]
  },
  {
    "name": "Stack Overflow Developer Survey",
    "description": "49K+ annual responses with salaries, tech adoption, and developer analytics",
    "category": "Labor Markets",
    "url": "https://survey.stackoverflow.co/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "developers",
      "salaries",
      "survey",
      "tech"
    ],
    "best_for": "Learning labor markets analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Stack Overflow Developer Survey dataset contains over 49,000 annual responses from developers worldwide, providing insights into salaries, technology adoption, and developer analytics. Researchers can utilize this dataset to analyze trends in the tech industry, understand salary disparities, and explore the adoption of various programming languages and tools.",
    "use_cases": [
      "Analyzing salary trends across different programming languages and technologies.",
      "Exploring the impact of education and experience on developer salaries.",
      "Assessing the adoption rates of new technologies among developers.",
      "Investigating job satisfaction factors in the tech industry."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the average salaries for developers in different regions?",
      "How does technology adoption vary among different developer demographics?",
      "What programming languages are most popular among developers?",
      "What factors influence job satisfaction in the tech industry?",
      "How do salaries differ based on years of experience?",
      "What is the relationship between education level and salary in tech?",
      "How has developer sentiment changed over the years?",
      "What technologies are emerging in the developer community?"
    ],
    "domain_tags": [
      "technology",
      "labor markets"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0003,
    "image_url": "/images/logos/stackoverflow.png",
    "embedding_text": "The Stack Overflow Developer Survey dataset is a comprehensive collection of responses from over 49,000 developers, gathered annually to provide insights into various aspects of the tech industry. The dataset includes a wide range of variables, such as salaries, technology adoption rates, and developer demographics, making it a valuable resource for researchers and industry analysts alike. The data is structured in a tabular format, with rows representing individual survey responses and columns capturing key variables such as job title, years of experience, education level, and preferred programming languages. This rich dataset allows for a multitude of analyses, including regression modeling, machine learning applications, and descriptive statistics. Researchers can use this dataset to address critical questions regarding salary disparities, technology trends, and the overall landscape of the developer community. The collection methodology involves an online survey distributed through the Stack Overflow platform, ensuring a diverse and representative sample of the developer population. However, researchers should be aware of potential limitations, such as self-selection bias and the variability in response rates across different regions and demographics. Common preprocessing steps may include data cleaning, handling missing values, and normalizing salary figures to account for inflation. Overall, the Stack Overflow Developer Survey dataset serves as a foundational resource for understanding the dynamics of labor markets in the tech sector and can inform policy decisions, educational programs, and industry practices.",
    "tfidf_keywords": [
      "developer-salaries",
      "technology-adoption",
      "programming-languages",
      "job-satisfaction",
      "demographic-analysis",
      "salary-disparities",
      "developer-sentiment",
      "education-level",
      "experience-years",
      "tech-industry-trends"
    ],
    "semantic_cluster": "labor-market-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "labor-economics",
      "consumer-behavior",
      "statistics",
      "data-engineering",
      "machine-learning"
    ],
    "canonical_topics": [
      "labor-economics",
      "statistics",
      "consumer-behavior"
    ]
  },
  {
    "name": "Dressipi Fashion (RecSys 2022)",
    "description": "Session interactions and item features from styling service",
    "category": "Fashion & Apparel",
    "url": "http://www.recsyschallenge.com/2022/dataset.html",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "fashion",
      "RecSys",
      "styling",
      "sessions"
    ],
    "best_for": "Learning fashion & apparel analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Dressipi Fashion dataset consists of session interactions and item features from a styling service, providing insights into consumer preferences and behaviors in the fashion industry. Researchers can utilize this dataset to analyze user engagement, develop recommendation systems, and explore trends in fashion consumption.",
    "use_cases": [
      "Analyzing user engagement with fashion items",
      "Developing personalized recommendation systems",
      "Exploring trends in consumer preferences",
      "Evaluating the effectiveness of styling services"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the session interactions in the Dressipi Fashion dataset?",
      "How can item features from Dressipi be used for recommendation systems?",
      "What insights can be gained from analyzing consumer behavior in the fashion industry?",
      "How does the Dressipi dataset help in understanding styling preferences?",
      "What are the key variables in the Dressipi Fashion dataset?",
      "How can session data inform fashion retail strategies?",
      "What types of analyses can be performed with the Dressipi dataset?",
      "What are the limitations of the Dressipi Fashion dataset?"
    ],
    "domain_tags": [
      "fashion"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0003,
    "image_url": "/images/logos/recsyschallenge.png",
    "embedding_text": "The Dressipi Fashion dataset is a rich resource for understanding consumer interactions with fashion items through session data and item features collected from a styling service. This dataset is structured in a tabular format, comprising rows that represent individual session interactions and columns that detail various item features such as style attributes, user engagement metrics, and session timestamps. The data collection methodology involves tracking user sessions on the styling platform, capturing interactions with various fashion items, and recording associated features that define each item. While the dataset does not specify temporal or geographic coverage, it is assumed to reflect contemporary fashion trends and consumer behavior patterns relevant to the styling service's operational context. Key variables in the dataset include session IDs, item IDs, user demographic information (if available), and interaction metrics such as clicks, views, and purchases, which measure user engagement and preferences. However, researchers should be aware of potential limitations in data quality, such as incomplete session records or biases in user demographics that may affect the generalizability of findings. Common preprocessing steps may include data cleaning to handle missing values, normalization of item features, and transformation of session data into a format suitable for analysis. The dataset supports various types of analyses, including regression analyses to identify factors influencing consumer choices, machine learning models for predicting user preferences, and descriptive statistics to summarize user engagement trends. Researchers typically use this dataset to address questions related to consumer behavior, the effectiveness of styling recommendations, and the dynamics of fashion consumption, making it a valuable tool for both academic and industry applications.",
    "tfidf_keywords": [
      "session-interactions",
      "item-features",
      "consumer-preferences",
      "recommendation-systems",
      "user-engagement",
      "fashion-consumption",
      "styling-service",
      "data-collection-methodology",
      "fashion-trends",
      "user-behavior-analysis"
    ],
    "semantic_cluster": "fashion-recommendation-systems",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "consumer-behavior",
      "recommendation-systems",
      "user-engagement",
      "data-analysis",
      "fashion-trends"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "recommendation-systems",
      "machine-learning"
    ]
  },
  {
    "name": "AWS Open Data Registry",
    "description": "300+ petabytes across hundreds of datasets - Common Crawl, satellite imagery, genomics",
    "category": "Dataset Aggregators",
    "url": "https://registry.opendata.aws",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "cloud",
      "petabyte",
      "Common Crawl",
      "satellite",
      "genomics"
    ],
    "best_for": "Large-scale data - Common Crawl (300B+ web pages), satellite imagery, genomics",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The AWS Open Data Registry provides access to over 300 petabytes of diverse datasets, including Common Crawl web data, satellite imagery, and genomic information. Researchers can utilize this extensive repository for various analyses, including machine learning, data mining, and large-scale data processing.",
    "use_cases": [
      "Analyzing web traffic patterns using Common Crawl data.",
      "Conducting research on genomic variations using genomic datasets.",
      "Utilizing satellite imagery for environmental monitoring.",
      "Building machine learning models with large-scale datasets."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What datasets are available in the AWS Open Data Registry?",
      "How can I access satellite imagery from AWS Open Data?",
      "What is the size of the datasets in the AWS Open Data Registry?",
      "Can I find genomic data in the AWS Open Data Registry?",
      "How does Common Crawl data contribute to research?",
      "What types of analyses can be performed with AWS Open Data?",
      "What are the common applications of satellite imagery datasets?",
      "How to utilize AWS Open Data for machine learning projects?"
    ],
    "domain_tags": [
      "cloud",
      "genomics",
      "satellite"
    ],
    "data_modality": "mixed",
    "size_category": "massive",
    "model_score": 0.0003,
    "image_url": "/images/logos/opendata.png",
    "embedding_text": "The AWS Open Data Registry serves as a comprehensive repository for a vast array of datasets, exceeding 300 petabytes in total. It encompasses a variety of data types, including web data from Common Crawl, which captures a significant portion of the internet, satellite imagery that provides insights into environmental changes, and genomic data that supports research in biology and medicine. The datasets are structured in various formats, allowing for flexibility in data handling and analysis. Researchers can leverage these datasets for a myriad of purposes, from machine learning applications to large-scale data processing tasks. The collection methodology involves aggregating publicly available datasets and making them accessible through the AWS cloud infrastructure, ensuring ease of use and scalability. While the data quality is generally high, users should be aware of potential limitations such as data completeness and the need for preprocessing to align datasets for specific analyses. Common preprocessing steps may include data cleaning, normalization, and transformation to prepare the data for analytical tasks. Researchers typically use this data to address questions related to web behavior, environmental trends, and biological research, employing techniques such as regression analysis, machine learning, and descriptive statistics. The diverse nature of the datasets supports a wide range of analyses, making the AWS Open Data Registry a valuable resource for data scientists and researchers across various fields.",
    "tfidf_keywords": [
      "Common Crawl",
      "satellite imagery",
      "genomics",
      "large-scale data",
      "data processing",
      "machine learning",
      "data mining",
      "cloud computing",
      "environmental monitoring",
      "data aggregation",
      "public datasets",
      "data quality",
      "preprocessing",
      "data analysis",
      "data accessibility"
    ],
    "semantic_cluster": "big-data-repositories",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "data-engineering",
      "machine-learning",
      "cloud-computing",
      "big-data",
      "data-analytics"
    ],
    "canonical_topics": [
      "machine-learning",
      "data-engineering",
      "statistics"
    ]
  },
  {
    "name": "Multi-Region MMM eCommerce Dataset",
    "description": "Marketing mix modeling data for 100 brands across Google, Meta, and TikTok channels in multiple regions",
    "category": "Marketing Mix",
    "url": "https://figshare.com/articles/dataset/Multi-Region_MMM_Data/21629584",
    "docs_url": "https://figshare.com/articles/dataset/Multi-Region_MMM_Data/21629584",
    "github_url": null,
    "tags": [
      "MMM",
      "marketing mix",
      "ecommerce",
      "multi-channel"
    ],
    "best_for": "Marketing mix modeling with cross-channel and cross-region data",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "marketing-mix",
      "digital-advertising"
    ],
    "summary": "The Multi-Region MMM eCommerce Dataset provides comprehensive marketing mix modeling data across various digital channels for 100 brands. This dataset enables researchers and analysts to evaluate the effectiveness of marketing strategies and optimize advertising spend across platforms like Google, Meta, and TikTok.",
    "use_cases": [
      "Evaluating the ROI of different digital advertising channels",
      "Optimizing marketing budgets across platforms",
      "Analyzing consumer behavior in response to marketing strategies",
      "Forecasting sales based on marketing spend"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Multi-Region MMM eCommerce Dataset?",
      "How can I analyze marketing mix data for e-commerce?",
      "What brands are included in the Multi-Region MMM eCommerce Dataset?",
      "What channels does the Multi-Region MMM eCommerce Dataset cover?",
      "How to use marketing mix modeling for digital advertising?",
      "What insights can be gained from the Multi-Region MMM eCommerce Dataset?",
      "How to optimize marketing spend using MMM data?",
      "What are the key variables in the Multi-Region MMM eCommerce Dataset?"
    ],
    "domain_tags": [
      "retail",
      "e-commerce"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "varies",
    "geographic_scope": "Multiple regions",
    "size_category": "medium",
    "model_score": 0.0003,
    "image_url": "/images/logos/figshare.png",
    "embedding_text": "The Multi-Region MMM eCommerce Dataset is a rich source of marketing mix modeling data that encompasses various digital channels, specifically focusing on platforms such as Google, Meta, and TikTok. This dataset includes data for 100 brands, providing a diverse range of marketing strategies and consumer interactions. Each entry in the dataset is structured in a tabular format, containing rows that represent different marketing campaigns or time periods, and columns that capture key variables such as advertising spend, impressions, clicks, and conversions. The data is collected through various methodologies, likely involving direct integration with advertising platforms and possibly surveys or web scraping to gather comprehensive insights on marketing performance.\n\nKey variables in the dataset measure the effectiveness of marketing efforts across different channels, allowing for in-depth analysis of how various factors influence consumer behavior and sales outcomes. Researchers can utilize this dataset to address critical research questions such as the impact of advertising spend on sales, the comparative effectiveness of different marketing channels, and the overall return on investment (ROI) for marketing initiatives.\n\nCommon preprocessing steps may include cleaning the data for missing values, normalizing advertising spend across channels, and aggregating data to the appropriate level for analysis. The dataset supports a variety of analytical approaches, including regression analysis, machine learning techniques, and descriptive statistics, making it a versatile tool for both academic and practical applications in marketing research.\n\nResearchers typically use this dataset to conduct studies that aim to optimize marketing strategies, evaluate the effectiveness of advertising campaigns, and understand consumer behavior in the context of digital marketing. The insights derived from analyses of this dataset can inform decision-making processes for marketers, helping them allocate resources more effectively and enhance overall marketing performance.",
    "tfidf_keywords": [
      "marketing-mix-modeling",
      "digital-advertising",
      "advertising-spend",
      "consumer-behavior",
      "ROI",
      "multi-channel",
      "campaign-analysis",
      "e-commerce-strategies",
      "data-collection-methodology",
      "performance-evaluation",
      "regression-analysis",
      "sales-forecasting",
      "data-preprocessing",
      "advertising-effectiveness",
      "channel-optimization"
    ],
    "semantic_cluster": "marketing-mix-modeling",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "causal-inference",
      "consumer-behavior",
      "advertising-effectiveness",
      "econometrics",
      "data-analysis"
    ],
    "canonical_topics": [
      "marketing-mix",
      "consumer-behavior",
      "econometrics",
      "machine-learning",
      "statistics"
    ]
  },
  {
    "name": "UCI Machine Learning Repository",
    "description": "688 curated benchmark datasets since 1987 - gold standard for ML research",
    "category": "Dataset Aggregators",
    "url": "https://archive.ics.uci.edu",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "benchmarks",
      "classic",
      "ML",
      "academic"
    ],
    "best_for": "Benchmark datasets with extensive documentation for reproducible ML research",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The UCI Machine Learning Repository is a collection of 688 curated benchmark datasets that have been made available since 1987, serving as a gold standard for machine learning research. Researchers and practitioners can utilize these datasets for various machine learning tasks, including classification, regression, and clustering, facilitating the development and evaluation of algorithms.",
    "use_cases": [
      "Evaluating the performance of machine learning algorithms",
      "Conducting comparative studies on different machine learning techniques",
      "Teaching machine learning concepts using real-world data",
      "Developing prototypes for machine learning applications"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What datasets are available in the UCI Machine Learning Repository?",
      "How can I access benchmark datasets for machine learning research?",
      "What types of machine learning tasks can be performed with UCI datasets?",
      "Where can I find classic datasets for academic research in machine learning?",
      "What is the history of the UCI Machine Learning Repository?",
      "How do I use UCI datasets for my machine learning projects?"
    ],
    "domain_tags": [
      "academic"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0003,
    "image_url": "/images/logos/uci.png",
    "embedding_text": "The UCI Machine Learning Repository is a comprehensive collection of 688 benchmark datasets that have been meticulously curated since 1987, making it a pivotal resource for machine learning researchers and practitioners. The datasets cover a wide array of domains and are structured in a tabular format, consisting of rows and columns that represent observations and variables, respectively. Each dataset typically includes key variables that measure specific attributes relevant to the domain of study, such as features for classification tasks or target variables for regression analyses. The repository serves as a gold standard for evaluating machine learning algorithms, providing a common ground for researchers to benchmark their models against established datasets. The collection methodology involves gathering data from various sources, ensuring a diverse representation of real-world scenarios. While the datasets are generally of high quality, researchers should be aware of potential limitations, such as missing values or biases inherent in the data collection process. Common preprocessing steps may include normalization, handling missing data, and feature selection to prepare the datasets for analysis. Researchers can address a multitude of research questions using these datasets, ranging from predictive modeling to exploratory data analysis. The UCI repository supports various types of analyses, including regression, classification, and clustering, making it an invaluable tool for both academic and practical applications in machine learning. Typically, researchers utilize these datasets to develop, test, and refine machine learning algorithms, contributing to the advancement of the field.",
    "benchmark_usage": [
      "Commonly used as a reference point for algorithm performance"
    ],
    "tfidf_keywords": [
      "benchmark-datasets",
      "machine-learning",
      "classification",
      "regression",
      "clustering",
      "data-preprocessing",
      "algorithm-evaluation",
      "academic-research",
      "data-collection-methodology",
      "real-world-data",
      "performance-benchmarking",
      "feature-selection",
      "predictive-modeling",
      "exploratory-data-analysis"
    ],
    "semantic_cluster": "machine-learning-benchmarks",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "machine-learning",
      "data-preprocessing",
      "algorithm-evaluation",
      "predictive-modeling",
      "classification",
      "regression",
      "clustering",
      "benchmarking"
    ],
    "canonical_topics": [
      "machine-learning",
      "statistics",
      "data-engineering"
    ]
  },
  {
    "name": "BLS JOLTS",
    "description": "Monthly job openings, hires, separations by industry since 2000. Bureau of Labor Statistics time series",
    "category": "Labor Markets",
    "url": "https://www.bls.gov/jlt/data.htm",
    "docs_url": "https://www.bls.gov/jlt/jltover.htm",
    "github_url": null,
    "tags": [
      "jobs",
      "labor",
      "openings",
      "hires",
      "BLS"
    ],
    "best_for": "Learning labor markets analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "time-series-analysis"
    ],
    "topic_tags": [],
    "summary": "The BLS JOLTS dataset provides detailed monthly statistics on job openings, hires, and separations across various industries since the year 2000. Researchers and analysts can utilize this dataset to examine labor market trends, assess employment dynamics, and inform policy decisions related to workforce development.",
    "use_cases": [
      "Analyzing trends in job openings over time",
      "Comparing hires and separations across different industries",
      "Evaluating the impact of economic policies on employment",
      "Forecasting future labor market conditions"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the monthly job openings by industry from BLS JOLTS?",
      "How can I analyze hires and separations in the labor market?",
      "What trends can be observed in job openings since 2000?",
      "How does the BLS JOLTS data inform labor market policies?",
      "What industries have the highest job openings according to BLS JOLTS?",
      "How do hires and separations vary across different sectors?"
    ],
    "domain_tags": [
      "labor markets"
    ],
    "data_modality": "time-series",
    "temporal_coverage": "2000-present",
    "size_category": "medium",
    "model_score": 0.0003,
    "image_url": "/images/datasets/bls-jolts.png",
    "embedding_text": "The BLS JOLTS dataset, maintained by the Bureau of Labor Statistics, offers a comprehensive view of the U.S. labor market through its monthly records of job openings, hires, and separations by industry. This dataset has been collected since the year 2000, providing a rich temporal coverage that allows for longitudinal analysis of employment trends. The data structure consists of rows representing monthly observations and columns detailing key variables such as the number of job openings, hires, and separations, categorized by industry. The collection methodology involves surveys conducted by the Bureau of Labor Statistics, ensuring a high level of reliability and accuracy in the reported figures. However, users should be aware of potential limitations, such as response biases and the challenges of capturing informal employment. Common preprocessing steps may include handling missing values, normalizing data for comparative analysis, and aggregating data by industry or time period for more focused insights. Researchers often utilize this dataset to address critical questions regarding labor market dynamics, such as the relationship between job openings and unemployment rates, the effects of economic downturns on hiring practices, and the overall health of the labor market. The dataset supports various types of analyses, including regression models to identify trends and correlations, machine learning techniques for predictive modeling, and descriptive statistics for summarizing employment patterns. Overall, the BLS JOLTS dataset serves as a vital resource for economists, policymakers, and data scientists interested in understanding and analyzing labor market behavior.",
    "geographic_scope": "United States",
    "tfidf_keywords": [
      "job openings",
      "hires",
      "separations",
      "labor market",
      "Bureau of Labor Statistics",
      "employment trends",
      "economic policies",
      "workforce development",
      "time series analysis",
      "industry comparison"
    ],
    "semantic_cluster": "labor-market-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "labor-economics",
      "econometrics",
      "time-series-analysis",
      "employment-dynamics",
      "policy-evaluation"
    ],
    "canonical_topics": [
      "labor-economics",
      "econometrics",
      "policy-evaluation"
    ]
  },
  {
    "name": "NYC Shopping",
    "description": "Large sales dataset from New York City retail",
    "category": "Grocery & Supermarkets",
    "url": "https://www.kaggle.com/datasets/pigment/big-sales-data",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "NYC",
      "retail",
      "large-scale"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The NYC Shopping dataset is a comprehensive collection of sales data from retail establishments in New York City. This dataset allows researchers and analysts to explore consumer purchasing patterns, evaluate pricing strategies, and understand market dynamics in a major urban retail environment.",
    "use_cases": [
      "Analyzing consumer purchasing trends",
      "Evaluating the impact of pricing changes",
      "Studying the effects of promotional campaigns",
      "Understanding seasonal variations in sales"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the sales trends in NYC retail?",
      "How do pricing strategies affect consumer behavior in NYC?",
      "What products are most popular among NYC shoppers?",
      "How does seasonality impact sales in NYC grocery stores?",
      "What demographic factors influence shopping habits in NYC?",
      "How can regression analysis be applied to NYC retail data?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "New York City",
    "size_category": "medium",
    "model_score": 0.0003,
    "image_url": "/images/datasets/nyc-shopping.png",
    "embedding_text": "The NYC Shopping dataset is a large-scale collection of sales data sourced from various retail establishments across New York City. This dataset is structured in a tabular format, consisting of multiple rows and columns that capture a wide range of variables related to retail transactions. Key variables may include transaction dates, product categories, sales amounts, and customer demographics, among others. The collection methodology involves aggregating sales data from point-of-sale systems in grocery and supermarket environments, ensuring a comprehensive representation of consumer behavior in an urban setting. Researchers can leverage this dataset to address a variety of research questions, such as analyzing purchasing trends over time, evaluating the effectiveness of pricing strategies, and understanding the impact of promotions on sales. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics. However, users should be aware of potential limitations regarding data quality, such as missing values or inconsistencies in reporting across different retailers. Common preprocessing steps may include data cleaning, normalization, and feature engineering to prepare the dataset for analysis. Overall, the NYC Shopping dataset serves as a valuable resource for researchers and analysts interested in the dynamics of retail in one of the world's largest metropolitan areas.",
    "tfidf_keywords": [
      "sales-trends",
      "consumer-purchasing",
      "pricing-strategies",
      "promotional-campaigns",
      "seasonality",
      "regression-analysis",
      "urban-retail",
      "data-aggregation",
      "point-of-sale",
      "market-dynamics"
    ],
    "semantic_cluster": "urban-retail-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "pricing",
      "market-analysis",
      "sales-forecasting",
      "promotional-strategies"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "pricing",
      "marketplaces"
    ]
  },
  {
    "name": "Common Crawl",
    "description": "250TB/month web crawl. 9.5 PB archive since 2008. Product listings, pricing, economic text at web scale",
    "category": "Social & Web",
    "url": "https://commoncrawl.org/",
    "docs_url": "https://commoncrawl.org/the-data/get-started/",
    "github_url": null,
    "tags": [
      "web crawl",
      "text",
      "pricing",
      "petabyte-scale"
    ],
    "best_for": "Learning social & web analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "Common Crawl is a massive web dataset that provides a comprehensive archive of web pages collected since 2008. Researchers can leverage this dataset to analyze product listings, pricing trends, and economic text at a large scale, facilitating insights into consumer behavior and market dynamics.",
    "use_cases": [
      "Analyzing pricing strategies across various e-commerce platforms",
      "Studying consumer behavior through product listing data",
      "Investigating trends in economic text over time"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What insights can be derived from the Common Crawl dataset?",
      "How can I analyze pricing trends using Common Crawl data?",
      "What are the key variables in the Common Crawl dataset?",
      "How does Common Crawl support research in e-commerce?",
      "What methodologies can be applied to analyze data from Common Crawl?",
      "What types of economic text are available in the Common Crawl archive?",
      "How can I access the Common Crawl dataset for analysis?",
      "What are the limitations of using Common Crawl data for research?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "text",
    "size_category": "massive",
    "model_score": 0.0003,
    "image_url": "/images/logos/commoncrawl.png",
    "embedding_text": "Common Crawl is a vast dataset that encompasses a web crawl of approximately 250TB per month, resulting in a cumulative archive of 9.5 PB since its inception in 2008. The dataset primarily consists of text data extracted from a diverse array of web pages, including product listings and economic texts. The data structure typically comprises rows representing individual web pages and columns detailing various attributes such as URLs, HTML content, and metadata. The collection methodology involves automated web crawling processes that systematically gather and index web content from across the internet. This extensive coverage allows researchers to explore a wide range of topics, particularly in the fields of e-commerce and consumer behavior. Key variables within the dataset include product names, prices, descriptions, and other relevant metadata that can be used to measure market trends and consumer preferences. However, researchers should be aware of potential data quality issues, such as incomplete data, duplicates, and variations in web page formats. Common preprocessing steps include cleaning the text data, removing duplicates, and standardizing formats for analysis. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile resource for researchers. Typical research questions might involve examining how pricing strategies vary across different platforms or analyzing shifts in consumer behavior over time. Overall, Common Crawl serves as a critical resource for those looking to conduct in-depth studies in economics, marketing, and data science.",
    "tfidf_keywords": [
      "web-crawl",
      "economic-text",
      "product-listings",
      "pricing-trends",
      "consumer-behavior",
      "data-quality",
      "preprocessing",
      "automated-collection",
      "market-analysis",
      "text-analysis"
    ],
    "semantic_cluster": "web-data-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "data-mining",
      "text-analysis",
      "market-research",
      "web-scraping",
      "big-data"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "pricing",
      "data-engineering"
    ]
  },
  {
    "name": "NATO Defence Expenditure",
    "description": "Standardized defense spending data for all NATO members enabling alliance burden-sharing analysis and 2% GDP target tracking",
    "category": "Defense Economics",
    "url": "https://www.nato.int/cps/en/natohq/topics_49198.htm",
    "docs_url": "https://www.nato.int/nato_static_fl2014/assets/pdf/2024/3/pdf/240314-def-exp-2023-en.pdf",
    "github_url": null,
    "tags": [
      "NATO",
      "alliance",
      "burden-sharing",
      "Europe"
    ],
    "best_for": "Analyzing NATO burden-sharing and 2% GDP spending targets",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The NATO Defence Expenditure dataset provides standardized defense spending data for all NATO members, facilitating analysis of alliance burden-sharing and tracking the 2% GDP target. Researchers can utilize this dataset to explore trends in defense spending, compare expenditures across member countries, and assess compliance with NATO guidelines.",
    "use_cases": [
      "Analyzing compliance with NATO's 2% GDP defense spending guideline.",
      "Comparing defense expenditures across different NATO countries.",
      "Investigating the impact of defense spending on national security.",
      "Assessing trends in defense spending over time."
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the NATO Defence Expenditure dataset?",
      "How can I analyze NATO members' defense spending?",
      "What trends can be observed in NATO defense expenditures?",
      "How does NATO spending relate to GDP targets?",
      "What are the implications of burden-sharing among NATO members?",
      "How can I visualize NATO defense expenditure data?"
    ],
    "domain_tags": [
      "defense",
      "economics"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1949-present",
    "geographic_scope": "NATO member countries",
    "size_category": "medium",
    "model_score": 0.0003,
    "image_url": "/images/logos/nato.png",
    "embedding_text": "The NATO Defence Expenditure dataset is a comprehensive collection of standardized defense spending data for all NATO member nations, aimed at facilitating rigorous analysis of alliance burden-sharing and tracking adherence to the 2% GDP defense spending target established by NATO. The dataset is structured in a tabular format, with rows representing individual NATO member countries and columns detailing various aspects of defense expenditure, including total spending, percentage of GDP, and year-on-year changes. This structured approach allows researchers to easily manipulate and analyze the data using common data analysis tools such as Python's Pandas library or R. The collection methodology for this dataset typically involves aggregating data from national defense budgets, reports from NATO, and other reliable governmental sources, ensuring a high level of accuracy and consistency across the dataset. However, users should be aware of potential limitations, such as variations in how different countries report their defense spending and the impact of exchange rate fluctuations on comparative analyses. Common preprocessing steps may include normalizing data for inflation, handling missing values, and converting currency units to ensure comparability. The dataset supports a variety of analytical approaches, including regression analysis to identify factors influencing defense spending, machine learning techniques for predictive modeling, and descriptive statistics for summarizing trends. Researchers often utilize this dataset to address critical research questions related to national security policy, fiscal responsibility among NATO members, and the socio-economic implications of defense spending. By examining the data, analysts can uncover insights into how defense expenditures correlate with economic indicators and international relations, making it a valuable resource for policymakers, economists, and defense analysts alike.",
    "benchmark_usage": [
      "Burden-sharing analysis",
      "GDP target tracking"
    ],
    "tfidf_keywords": [
      "defense spending",
      "NATO",
      "burden-sharing",
      "GDP target",
      "expenditure analysis",
      "national security",
      "comparative analysis",
      "economic indicators",
      "fiscal policy",
      "data normalization",
      "predictive modeling",
      "regression analysis",
      "defense budgets",
      "international relations"
    ],
    "semantic_cluster": "defense-economics-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "public finance",
      "international relations",
      "defense policy",
      "economic analysis",
      "fiscal responsibility"
    ],
    "canonical_topics": [
      "econometrics",
      "policy-evaluation",
      "finance"
    ]
  },
  {
    "name": "Bandcamp Music Sales",
    "description": "Music sales data (digital/physical) from Bandcamp platform",
    "category": "Entertainment & Media",
    "url": "https://components.one/datasets/bandcamp-sales",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "music",
      "sales",
      "independent artists"
    ],
    "best_for": "Learning entertainment & media analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Bandcamp Music Sales dataset provides insights into music sales data from the Bandcamp platform, encompassing both digital and physical sales. Researchers can analyze trends in independent music sales, understand consumer behavior, and explore pricing strategies for artists.",
    "use_cases": [
      "Analyzing sales trends over time",
      "Comparing digital vs. physical sales",
      "Exploring pricing strategies for independent artists",
      "Investigating consumer preferences in music purchasing"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the trends in Bandcamp music sales?",
      "How do independent artists price their music on Bandcamp?",
      "What factors influence music sales on Bandcamp?",
      "How do digital and physical sales compare on Bandcamp?",
      "What genres perform best on Bandcamp?",
      "How does consumer behavior vary on Bandcamp?"
    ],
    "domain_tags": [
      "entertainment",
      "media"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0003,
    "image_url": "/images/logos/components.png",
    "embedding_text": "The Bandcamp Music Sales dataset is a comprehensive collection of music sales data sourced from the Bandcamp platform, which is known for its support of independent artists. The dataset includes various sales metrics, encompassing both digital and physical sales, allowing for a multifaceted analysis of the independent music market. The data is structured in a tabular format, with rows representing individual sales transactions and columns capturing key variables such as artist name, album title, genre, sale type (digital or physical), sale price, and date of sale. This structure facilitates straightforward data manipulation and analysis using tools such as pandas in Python.\n\nThe collection methodology primarily involves aggregating sales data directly from the Bandcamp platform, ensuring that the dataset reflects real-time sales trends and consumer behavior. While the dataset does not specify temporal or geographic coverage, it is inferred that the data encompasses a range of time periods and potentially global sales, given Bandcamp's international reach.\n\nKey variables in the dataset measure essential aspects of music sales, including the number of units sold, revenue generated, and the types of products sold. These variables are crucial for understanding market dynamics and artist performance on the platform. However, researchers should be aware of potential limitations in data quality, such as incomplete records or variations in reporting practices across different artists.\n\nCommon preprocessing steps may include cleaning the data to handle missing values, normalizing sale prices for inflation, and categorizing music genres for more nuanced analysis. Researchers can leverage this dataset to address various research questions, such as identifying trends in music sales over time, examining the impact of pricing strategies on sales volume, and exploring consumer preferences across different genres.\n\nThe dataset supports a range of analytical approaches, including regression analysis to model sales trends, machine learning techniques for predictive modeling, and descriptive statistics to summarize sales performance. Researchers typically use this dataset to gain insights into the independent music market, inform marketing strategies, and contribute to academic discussions on consumer behavior in the digital age.",
    "tfidf_keywords": [
      "Bandcamp",
      "music sales",
      "independent artists",
      "digital sales",
      "physical sales",
      "consumer behavior",
      "pricing strategies",
      "sales trends",
      "genre analysis",
      "revenue generation"
    ],
    "semantic_cluster": "music-sales-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "pricing",
      "marketplaces",
      "e-commerce",
      "data-analysis"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "marketplaces",
      "pricing",
      "econometrics"
    ]
  },
  {
    "name": "IMF Data",
    "description": "International macroeconomic forecasts, BOP, and financial statistics for 195 countries",
    "category": "Dataset Aggregators",
    "url": "https://data.imf.org",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "macro",
      "international",
      "forecasts",
      "BOP",
      "IFS"
    ],
    "best_for": "World Economic Outlook forecasts and International Financial Statistics",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The IMF Data provides comprehensive international macroeconomic forecasts, balance of payments (BOP), and financial statistics for 195 countries. Researchers and analysts can utilize this dataset to assess global economic trends, compare financial indicators across nations, and inform policy decisions based on macroeconomic conditions.",
    "use_cases": [
      "Analyzing the impact of global economic policies on national economies",
      "Comparing financial statistics across different countries",
      "Forecasting economic trends based on historical data",
      "Evaluating balance of payments data for economic stability assessments"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the latest IMF macroeconomic forecasts?",
      "How can I access financial statistics for 195 countries?",
      "What does the IMF Data include regarding BOP?",
      "Where can I find international financial statistics?",
      "What are the key macroeconomic indicators provided by the IMF?",
      "How does the IMF Data support economic research?"
    ],
    "domain_tags": [
      "finance",
      "economics"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0003,
    "embedding_text": "The IMF Data is a rich repository of international macroeconomic forecasts, balance of payments (BOP), and financial statistics that spans 195 countries. This dataset is structured in a tabular format, with rows representing different countries and columns detailing various macroeconomic indicators such as GDP, inflation rates, and trade balances. The collection methodology involves aggregating data from multiple sources, including national statistical offices, central banks, and international financial organizations, ensuring a comprehensive and reliable dataset for analysis. Researchers can leverage this dataset to explore key variables that measure economic performance, such as GDP growth rates, current account balances, and foreign direct investment inflows. While the dataset is robust, users should be aware of potential limitations, such as differences in data collection methodologies across countries and the availability of historical data. Common preprocessing steps may include handling missing values, normalizing data for comparative analysis, and transforming variables for specific econometric models. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile tool for economists and data scientists alike. Researchers typically use the IMF Data to address questions related to economic growth, stability, and the effects of policy changes on national and global economies.",
    "geographic_scope": "195 countries",
    "tfidf_keywords": [
      "macroeconomic-forecasts",
      "balance-of-payments",
      "financial-statistics",
      "GDP",
      "inflation",
      "trade-balance",
      "foreign-direct-investment",
      "economic-performance",
      "data-aggregation",
      "international-financial-data"
    ],
    "semantic_cluster": "international-economic-data",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "econometrics",
      "financial-analysis",
      "policy-evaluation",
      "global-economics",
      "data-visualization"
    ],
    "canonical_topics": [
      "econometrics",
      "finance",
      "policy-evaluation",
      "forecasting"
    ]
  },
  {
    "name": "Chicago Rideshare Data",
    "description": "Trip-level data for all Transportation Network Provider (Uber/Lyft) trips in Chicago since 2018. Includes ~57 million trips annually with origins, destinations, and fares.",
    "category": "Transportation Economics & Technology",
    "url": "https://data.cityofchicago.org/Transportation/Transportation-Network-Providers-Trips/m6dm-c72p",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "rideshare",
      "Chicago",
      "trip-data",
      "urban-mobility",
      "Uber",
      "Lyft"
    ],
    "best_for": "Ridesharing market analysis, platform competition, and urban transportation research",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "urban-mobility",
      "rideshare",
      "data-analysis"
    ],
    "summary": "The Chicago Rideshare Data provides comprehensive trip-level information for all Transportation Network Provider trips in Chicago, including Uber and Lyft, since 2018. This dataset allows for in-depth analysis of urban mobility patterns, fare structures, and transportation economics.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the trends in rideshare usage in Chicago since 2018?",
      "How do fares vary across different neighborhoods in Chicago?",
      "What is the average trip distance for Uber and Lyft in Chicago?",
      "How does rideshare usage correlate with public transportation availability?",
      "What time of day sees the highest rideshare demand in Chicago?",
      "How do Uber and Lyft fares compare in Chicago over time?"
    ],
    "use_cases": [
      "Analyzing fare trends over time to understand pricing strategies.",
      "Examining the impact of rideshare services on public transportation usage.",
      "Investigating trip patterns to inform urban planning and policy decisions."
    ],
    "domain_tags": [
      "transportation",
      "urban-planning"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2018-present",
    "geographic_scope": "Chicago",
    "size_category": "massive",
    "model_score": 0.0003,
    "image_url": "/images/logos/cityofchicago.png",
    "embedding_text": "The Chicago Rideshare Data is a rich dataset that encompasses trip-level information for all Transportation Network Provider (TNP) trips, specifically focusing on Uber and Lyft, within the city of Chicago since 2018. This dataset includes approximately 57 million trips annually, providing a comprehensive view of urban mobility dynamics. Each entry in the dataset typically includes variables such as trip start and end times, origins and destinations, fare amounts, and possibly additional metadata on the service used. The data is structured in a tabular format, allowing for straightforward manipulation and analysis using data science tools like pandas. The collection methodology involves aggregating trip data from TNPs, ensuring a broad representation of rideshare activity across various neighborhoods and times of day. Researchers can utilize this dataset to explore a variety of questions related to transportation economics, such as fare fluctuations, demand patterns, and the relationship between rideshare services and public transit systems. Common preprocessing steps may include data cleaning to handle missing values, normalization of fare data, and temporal aggregation to analyze trends over specific periods. The dataset supports various types of analyses, including regression modeling to predict fare prices, machine learning algorithms for demand forecasting, and descriptive statistics to summarize trip characteristics. However, users should be aware of potential limitations, such as data quality issues stemming from incomplete trip records or variations in reporting standards among TNPs. Overall, the Chicago Rideshare Data serves as a vital resource for researchers and policymakers aiming to understand and improve urban transportation systems.",
    "tfidf_keywords": [
      "rideshare",
      "urban-mobility",
      "fare-structure",
      "trip-patterns",
      "transportation-network-providers",
      "data-analysis",
      "public-transportation",
      "demand-forecasting",
      "urban-planning",
      "Chicago"
    ],
    "semantic_cluster": "urban-mobility-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "transportation-economics",
      "urban-planning",
      "data-science",
      "machine-learning",
      "regression-analysis"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "econometrics",
      "machine-learning",
      "transportation-economics"
    ],
    "benchmark_usage": [
      "Trip analysis",
      "Fare analysis",
      "Urban mobility studies"
    ]
  },
  {
    "name": "Hate Speech Data Catalogue",
    "description": "50+ hate speech datasets across languages compiled at hatespeechdata.com. Meta-resource for content moderation research",
    "category": "Content Moderation",
    "url": "https://hatespeechdata.com/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "hate speech",
      "catalogue",
      "multilingual",
      "meta-dataset"
    ],
    "best_for": "Learning content moderation analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Hate Speech Data Catalogue is a comprehensive collection of over 50 datasets related to hate speech across multiple languages. This resource serves as a meta-dataset for researchers focused on content moderation, enabling them to analyze and develop strategies for identifying and mitigating hate speech in various contexts.",
    "use_cases": [
      "Analyzing trends in hate speech across different languages",
      "Developing machine learning models for hate speech detection",
      "Comparative studies of hate speech prevalence in various cultures",
      "Evaluating the effectiveness of content moderation strategies"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What datasets are available for hate speech research?",
      "How can I access multilingual hate speech datasets?",
      "What is the Hate Speech Data Catalogue?",
      "Where can I find resources for content moderation research?",
      "What are the key datasets for studying hate speech?",
      "How does the Hate Speech Data Catalogue support content moderation?"
    ],
    "domain_tags": [
      "content moderation"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0003,
    "embedding_text": "The Hate Speech Data Catalogue is a meticulously curated collection of over 50 datasets that encompass various aspects of hate speech across multiple languages, sourced from hatespeechdata.com. This catalogue serves as a vital meta-resource for researchers engaged in content moderation, providing a comprehensive overview of available datasets that can be utilized for a wide range of research inquiries. The datasets included in this catalogue vary in structure, typically consisting of rows and columns that represent individual instances of hate speech, categorized by language, context, and severity. Each dataset may contain key variables such as the type of hate speech, the target group, and the context in which the speech occurs, allowing for nuanced analysis and understanding of the phenomenon. Researchers can leverage this catalogue to address critical research questions related to the prevalence, impact, and mitigation of hate speech in digital environments. The data collection methodology varies across datasets, with some sourced from social media platforms, online forums, and news articles, while others may be generated through surveys or expert annotations. This diversity in data sources enhances the richness of the catalogue, although it also introduces variability in data quality and potential limitations, such as biases inherent in the sources or the subjective nature of hate speech classification. Common preprocessing steps may include text normalization, tokenization, and the removal of non-relevant content to prepare the datasets for analysis. The Hate Speech Data Catalogue supports various types of analyses, including regression, machine learning, and descriptive statistics, making it a versatile tool for researchers aiming to explore the complexities of hate speech. By utilizing this catalogue, researchers can develop and refine algorithms for hate speech detection, evaluate the effectiveness of content moderation practices, and contribute to the broader discourse on online safety and community standards.",
    "tfidf_keywords": [
      "hate speech",
      "content moderation",
      "multilingual datasets",
      "meta-dataset",
      "social media analysis",
      "text normalization",
      "machine learning",
      "data quality",
      "text classification",
      "preprocessing",
      "hate speech detection",
      "digital environments",
      "research methodology",
      "algorithm development",
      "community standards"
    ],
    "semantic_cluster": "hate-speech-research",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "natural-language-processing",
      "machine-learning",
      "text-analysis",
      "social-media-research",
      "content-moderation-strategies"
    ],
    "canonical_topics": [
      "natural-language-processing",
      "machine-learning",
      "policy-evaluation"
    ]
  },
  {
    "name": "SIPRI Military Expenditure Database",
    "description": "Comprehensive annual military spending data covering all countries since 1949 in local currency, constant/current USD, and GDP shares",
    "category": "Defense Economics",
    "url": "https://www.sipri.org/databases/milex",
    "docs_url": "https://www.sipri.org/databases/milex/sources-and-methods",
    "github_url": null,
    "tags": [
      "military spending",
      "defense budgets",
      "international",
      "SIPRI"
    ],
    "best_for": "Cross-country defense spending analysis and burden-sharing studies",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The SIPRI Military Expenditure Database provides comprehensive annual military spending data for all countries since 1949. Researchers can utilize this dataset to analyze trends in military budgets, compare defense expenditures across nations, and assess the impact of military spending on economic indicators.",
    "use_cases": [
      "Analyzing the relationship between military spending and economic growth.",
      "Comparing military expenditures across different countries and regions.",
      "Studying the impact of military budgets on social spending.",
      "Evaluating trends in defense budgets over time."
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the trends in military spending globally since 1949?",
      "How does military expenditure correlate with GDP in different countries?",
      "What are the defense budgets of the top military spenders?",
      "How has military spending changed in response to global conflicts?",
      "What is the distribution of military expenditure across different regions?",
      "How do military budgets impact economic growth?"
    ],
    "domain_tags": [
      "Defense Economics"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1949-present",
    "geographic_scope": "global",
    "size_category": "medium",
    "model_score": 0.0003,
    "image_url": "/images/logos/sipri.png",
    "embedding_text": "The SIPRI Military Expenditure Database is a robust resource for researchers interested in defense economics and military spending trends. This dataset encompasses annual military expenditure data for all countries from 1949 to the present, providing insights into how nations allocate resources towards defense. The data is structured in a tabular format, with rows representing individual countries and years, and columns detailing military spending figures in local currency, constant and current USD, and as a share of GDP. The collection methodology involves gathering data from national budgets, defense ministries, and international organizations, ensuring a comprehensive and reliable dataset. Key variables include total military expenditure, GDP, and year, allowing for various analyses such as trend analysis, comparative studies, and econometric modeling. Researchers often preprocess the data to account for inflation, currency conversion, and normalization to GDP, enabling more accurate comparisons. This dataset supports a range of analyses, including regression, machine learning, and descriptive statistics, making it a valuable tool for understanding the implications of military spending on economic and social factors. Common research questions addressed with this dataset include the effects of military expenditure on economic growth, the relationship between defense budgets and social spending, and the impact of geopolitical events on military budgets. Overall, the SIPRI Military Expenditure Database is essential for anyone studying the intersection of defense and economics.",
    "tfidf_keywords": [
      "military expenditure",
      "defense budgets",
      "GDP share",
      "international spending",
      "economic impact",
      "defense economics",
      "budget analysis",
      "historical trends",
      "global defense",
      "SIPRI"
    ],
    "semantic_cluster": "defense-economics-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "economic indicators",
      "public finance",
      "international relations",
      "defense policy",
      "budgetary analysis"
    ],
    "canonical_topics": [
      "econometrics",
      "policy-evaluation",
      "finance"
    ]
  },
  {
    "name": "PUDL (Public Utility Data Liberation)",
    "description": "Cleaned and integrated dataset combining EIA, FERC, and EPA energy data into a unified database",
    "category": "Energy",
    "url": "https://catalyst.coop/pudl/",
    "docs_url": "https://catalystcoop-pudl.readthedocs.io/",
    "github_url": "https://github.com/catalyst-cooperative/pudl",
    "tags": [
      "integrated",
      "cleaned",
      "EIA",
      "FERC",
      "EPA"
    ],
    "best_for": "Research requiring integrated energy data without extensive cleaning",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The PUDL dataset is a cleaned and integrated resource that combines energy data from the EIA, FERC, and EPA into a unified database. Researchers can utilize this dataset to analyze energy consumption patterns, regulatory impacts, and environmental outcomes in the energy sector.",
    "use_cases": [
      "Analyzing trends in energy consumption over time.",
      "Evaluating the impact of regulatory changes on energy production.",
      "Assessing environmental outcomes related to energy use.",
      "Comparing energy data across different states or regions."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the PUDL dataset?",
      "How can I access the PUDL energy data?",
      "What types of analyses can be performed with the PUDL dataset?",
      "What sources contribute to the PUDL dataset?",
      "What are the key variables in the PUDL dataset?",
      "How is the PUDL dataset structured?",
      "What are the limitations of the PUDL dataset?",
      "How can I use the PUDL dataset for energy research?"
    ],
    "domain_tags": [
      "energy"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1994-present",
    "geographic_scope": "United States",
    "size_category": "medium",
    "model_score": 0.0003,
    "image_url": "/images/datasets/pudl-public-utility-data-liberation.png",
    "embedding_text": "The PUDL (Public Utility Data Liberation) dataset is a comprehensive and meticulously cleaned resource that integrates energy data from three prominent sources: the Energy Information Administration (EIA), the Federal Energy Regulatory Commission (FERC), and the Environmental Protection Agency (EPA). This dataset is designed to facilitate research and analysis in the energy sector by providing a unified database that combines various energy-related metrics and variables. The structure of the PUDL dataset is primarily tabular, consisting of rows and columns that represent different observations and variables related to energy production, consumption, and regulatory compliance. Key variables within the dataset include metrics such as energy generation by source, emissions data, and regulatory filings, which are crucial for understanding the dynamics of energy markets and their environmental impacts. The collection methodology for the PUDL dataset involves aggregating and harmonizing data from the aforementioned sources, ensuring that the information is not only accurate but also consistent across different reporting frameworks. This integration process addresses common challenges in energy data analysis, such as discrepancies in reporting standards and data formats. Researchers utilizing the PUDL dataset can address a variety of research questions, including the effects of policy changes on energy production, the relationship between energy consumption and environmental outcomes, and the comparative analysis of energy generation across different regions. The dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it a versatile tool for both academic and industry research. However, like any dataset, the PUDL dataset has its limitations. Researchers should be aware of potential data quality issues, such as missing values or inconsistencies in reporting, which may require preprocessing steps such as data cleaning and normalization before analysis. Overall, the PUDL dataset serves as a valuable resource for those interested in exploring the multifaceted aspects of energy economics, policy evaluation, and environmental impact assessment.",
    "tfidf_keywords": [
      "energy-data",
      "EIA",
      "FERC",
      "EPA",
      "data-integration",
      "energy-consumption",
      "regulatory-impact",
      "environmental-outcomes",
      "data-cleaning",
      "energy-production",
      "energy-markets",
      "emissions-data",
      "policy-evaluation"
    ],
    "semantic_cluster": "energy-data-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "energy-economics",
      "policy-evaluation",
      "data-integration",
      "environmental-impact",
      "regulatory-analysis"
    ],
    "canonical_topics": [
      "econometrics",
      "policy-evaluation",
      "forecasting",
      "statistics"
    ]
  },
  {
    "name": "SIPRI Arms Transfers Database",
    "description": "Most comprehensive public source on international transfers of major conventional weapons since 1950",
    "category": "Defense Economics",
    "url": "https://www.sipri.org/databases/armstransfers",
    "docs_url": "https://www.sipri.org/databases/armstransfers/sources-and-methods",
    "github_url": null,
    "tags": [
      "arms trade",
      "weapons transfers",
      "international",
      "SIPRI"
    ],
    "best_for": "Analyzing global arms trade patterns and supplier-recipient relationships",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The SIPRI Arms Transfers Database is the most comprehensive public source on international transfers of major conventional weapons since 1950. Researchers can utilize this dataset to analyze trends in arms trade, assess the impact of weapons transfers on international relations, and study the economic implications of defense spending.",
    "use_cases": [
      "Analyzing the correlation between arms transfers and conflict outbreaks.",
      "Studying the economic effects of military expenditure on national economies.",
      "Investigating the role of arms trade in international diplomacy.",
      "Assessing the impact of arms embargoes on recipient countries."
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the trends in international arms transfers since 1950?",
      "How do arms transfers impact global security dynamics?",
      "What countries are the largest exporters and importers of conventional weapons?",
      "How has the arms trade evolved over the decades?",
      "What are the economic implications of military spending on arms imports?",
      "How do different regions compare in terms of arms transfers?"
    ],
    "domain_tags": [
      "defense",
      "economics"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1950-present",
    "geographic_scope": "Global",
    "size_category": "medium",
    "model_score": 0.0003,
    "image_url": "/images/logos/sipri.png",
    "embedding_text": "The SIPRI Arms Transfers Database serves as a vital resource for understanding the global landscape of arms trade, providing detailed records of international transfers of major conventional weapons since 1950. This dataset is structured in a tabular format, consisting of rows that represent individual arms transfer transactions, with columns detailing various attributes such as the exporting and importing countries, types of weapons involved, and the volume of transfers. The data is collected from a variety of reliable sources, including government reports, international organizations, and media articles, ensuring a comprehensive coverage of arms transfers across different nations and time periods. Key variables in this dataset include the names of exporting and importing countries, weapon categories (such as aircraft, naval vessels, and missiles), and the year of transfer, which collectively allow researchers to analyze trends and patterns in global arms trade. However, users should be aware of potential limitations in data quality, such as discrepancies in reporting standards among countries and the challenges of capturing illicit arms transfers. Common preprocessing steps may involve cleaning the data for consistency, handling missing values, and aggregating transfers by country or weapon type for more straightforward analysis. Researchers can leverage this dataset to address various research questions, such as the relationship between arms transfers and conflict escalation, the economic impact of military spending, and the geopolitical implications of arms trade policies. The dataset supports a range of analytical techniques, including regression analysis, machine learning models, and descriptive statistics, making it a versatile tool for scholars and policymakers alike. By utilizing the SIPRI Arms Transfers Database, researchers can gain insights into the complexities of international security and the economic factors driving the arms trade.",
    "tfidf_keywords": [
      "arms trade",
      "conventional weapons",
      "international transfers",
      "SIPRI",
      "military expenditure",
      "geopolitical implications",
      "defense economics",
      "arms embargoes",
      "conflict analysis",
      "economic impact"
    ],
    "semantic_cluster": "arms-trade-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "defense economics",
      "international relations",
      "conflict studies",
      "security studies",
      "economic impact analysis"
    ],
    "canonical_topics": [
      "econometrics",
      "policy-evaluation",
      "finance",
      "industrial-organization"
    ]
  },
  {
    "name": "FEMA NFIP Claims & Policies",
    "description": "National Flood Insurance Program data with 2M+ claims since 1978 and policy-level information for flood risk modeling",
    "category": "Insurance & Actuarial",
    "url": "https://www.fema.gov/openfema-data-page/fima-nfip-redacted-claims-v2",
    "docs_url": "https://www.fema.gov/about/openfema/data-sets",
    "github_url": null,
    "tags": [
      "flood-insurance",
      "catastrophe",
      "claims-data",
      "natural-disasters",
      "property-insurance"
    ],
    "best_for": "Catastrophe modeling, flood risk analysis, and climate-related insurance research",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "insurance",
      "flood-risk",
      "data-analysis"
    ],
    "summary": "The FEMA NFIP Claims & Policies dataset contains over 2 million claims and policy-level information from the National Flood Insurance Program since 1978. This dataset can be utilized for flood risk modeling, analysis of insurance claims, and understanding the impact of natural disasters on property insurance.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the trends in FEMA NFIP claims over the years?",
      "How does flood insurance coverage vary by region?",
      "What factors influence the amount of claims filed under the NFIP?",
      "How can this dataset be used to model flood risk?",
      "What is the historical data on flood insurance policies in the US?",
      "How do natural disasters impact property insurance claims?"
    ],
    "use_cases": [
      "Analyzing trends in flood insurance claims over time.",
      "Modeling flood risk based on historical claims data.",
      "Assessing the effectiveness of flood insurance policies.",
      "Evaluating the economic impact of natural disasters on property insurance."
    ],
    "domain_tags": [
      "insurance",
      "natural-disasters"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1978-present",
    "size_category": "massive",
    "model_score": 0.0003,
    "image_url": "/images/datasets/fema-nfip-claims-policies.png",
    "embedding_text": "The FEMA NFIP Claims & Policies dataset is a comprehensive resource for researchers and analysts interested in the dynamics of flood insurance in the United States. This dataset encompasses over 2 million claims and policy-level information collected since 1978, providing a rich historical perspective on flood-related insurance activities. The data structure is primarily tabular, with rows representing individual claims and columns detailing various attributes such as claim amounts, policy types, and geographic locations. Key variables include claim amounts, policy coverage details, and dates of claims, which are crucial for understanding trends and patterns in flood insurance. The dataset is collected through the National Flood Insurance Program, which has been operational since 1968, and it draws from a variety of sources including insurance companies and federal records. Coverage is extensive, both temporally and geographically, allowing for in-depth analyses of flood insurance across different regions and time periods. However, researchers should be aware of potential limitations in data quality, such as reporting inconsistencies or gaps in coverage for certain areas. Common preprocessing steps may include cleaning the data for missing values, normalizing claim amounts for inflation, and categorizing policies based on coverage types. This dataset supports a range of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it a versatile tool for addressing various research questions related to flood risk and insurance. Researchers typically use this dataset to explore the economic implications of flooding, assess the effectiveness of insurance policies, and develop predictive models for future flood events.",
    "tfidf_keywords": [
      "flood-insurance",
      "claims-data",
      "natural-disasters",
      "policy-level-information",
      "risk-modeling",
      "insurance-claims",
      "economic-impact",
      "temporal-analysis",
      "geographic-coverage",
      "data-quality",
      "preprocessing",
      "claim-amounts",
      "policy-types",
      "historical-data"
    ],
    "semantic_cluster": "flood-risk-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "insurance-economics",
      "risk-assessment",
      "natural-disaster-response",
      "economic-analysis",
      "policy-evaluation"
    ],
    "canonical_topics": [
      "insurance",
      "econometrics",
      "policy-evaluation"
    ],
    "geographic_scope": "United States",
    "benchmark_usage": [
      "Flood risk modeling",
      "Insurance claims analysis"
    ]
  },
  {
    "name": "OECD Data",
    "description": "Harmonized indicators for 38 member countries - gold standard for advanced economy comparisons",
    "category": "Dataset Aggregators",
    "url": "https://data-explorer.oecd.org",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "OECD",
      "international",
      "harmonized",
      "SDMX",
      "API"
    ],
    "best_for": "Harmonized cross-country comparisons for advanced economies",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The OECD Data provides harmonized indicators for 38 member countries, serving as a gold standard for advanced economy comparisons. Researchers can utilize this dataset to analyze economic performance, compare indicators across nations, and inform policy decisions.",
    "use_cases": [
      "Comparing GDP growth rates among OECD countries",
      "Analyzing labor market indicators across member nations",
      "Evaluating the impact of economic policies using harmonized data"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the harmonized indicators provided by OECD?",
      "How can I compare economic performance across OECD countries?",
      "What data does OECD provide for advanced economy comparisons?",
      "Where can I find international economic indicators?",
      "How to access OECD data via API?",
      "What is the significance of SDMX in OECD data?"
    ],
    "domain_tags": [
      "economics",
      "international"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0003,
    "image_url": "/images/logos/oecd.png",
    "embedding_text": "The OECD Data is a comprehensive collection of harmonized indicators that facilitate comparisons among the 38 member countries of the Organisation for Economic Co-operation and Development (OECD). This dataset is considered a gold standard for researchers and policymakers seeking to analyze economic performance across advanced economies. The data is structured in a tabular format, typically consisting of rows representing different countries and columns denoting various economic indicators such as GDP, unemployment rates, inflation rates, and other relevant metrics. Each variable in the dataset is carefully defined to ensure consistency and comparability across nations, allowing for robust analysis and interpretation. The collection methodology employed by the OECD involves rigorous data gathering from national statistical offices, ensuring high data quality and reliability. However, users should be aware of potential limitations, such as differences in data collection practices among countries and the possibility of missing values for certain indicators. Common preprocessing steps may include data cleaning, normalization, and transformation to prepare the data for analysis. Researchers can leverage this dataset to address a variety of research questions, including the impact of economic policies on growth, the relationship between labor market dynamics and economic performance, and cross-country comparisons of social indicators. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile tool for economic research. Typically, researchers utilize OECD Data in empirical studies to draw insights about economic trends, inform policy recommendations, and contribute to the broader understanding of global economic dynamics.",
    "geographic_scope": "OECD member countries",
    "tfidf_keywords": [
      "harmonized-indicators",
      "OECD",
      "economic-performance",
      "GDP-comparisons",
      "labor-market-indicators",
      "SDMX",
      "international-economics",
      "policy-analysis",
      "data-quality",
      "cross-country-comparisons"
    ],
    "semantic_cluster": "international-economic-comparisons",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "economic-indicators",
      "policy-evaluation",
      "data-collection-methods",
      "comparative-economics",
      "international-statistics"
    ],
    "canonical_topics": [
      "econometrics",
      "policy-evaluation",
      "consumer-behavior"
    ]
  },
  {
    "name": "LIAR",
    "description": "12.8K fact-checked political statements with speaker metadata",
    "category": "Content Moderation",
    "url": "https://sites.cs.ucsb.edu/~william/software.html",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "fact-checking",
      "politics",
      "misinformation"
    ],
    "best_for": "Learning content moderation analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "politics",
      "fact-checking",
      "misinformation"
    ],
    "summary": "The LIAR dataset contains 12.8K fact-checked political statements along with speaker metadata, making it a valuable resource for analyzing political discourse and misinformation. Researchers can utilize this dataset to study the accuracy of political statements and the impact of misinformation on public opinion.",
    "use_cases": [
      "Analyzing the accuracy of political statements",
      "Studying the spread of misinformation in political contexts",
      "Evaluating the effectiveness of fact-checking initiatives",
      "Investigating speaker biases in political discourse"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the LIAR dataset?",
      "How can I access the LIAR dataset?",
      "What types of analyses can be performed with the LIAR dataset?",
      "What are the key features of the LIAR dataset?",
      "How does the LIAR dataset help in fact-checking?",
      "What is the size of the LIAR dataset?",
      "What metadata is included in the LIAR dataset?",
      "What research questions can be addressed using the LIAR dataset?"
    ],
    "domain_tags": [
      "politics"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0003,
    "embedding_text": "The LIAR dataset is a comprehensive collection of 12.8K fact-checked political statements, enriched with metadata about the speakers. This dataset is structured in a tabular format, with rows representing individual statements and columns detailing various attributes such as the statement text, speaker identity, and fact-checking outcomes. The collection methodology involves gathering statements from a variety of political contexts, ensuring a diverse representation of political discourse. Researchers can leverage this dataset to explore critical research questions regarding the accuracy of political statements, the prevalence of misinformation, and the role of fact-checking in shaping public perception. Key variables in the dataset include the statement text, speaker information, and the fact-checking verdict, which collectively measure the truthfulness of political claims. While the dataset offers rich insights, it may have limitations regarding the representativeness of the statements and potential biases in the fact-checking process. Common preprocessing steps may include text normalization, categorization of statements, and analysis of speaker demographics. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics. Researchers typically use the LIAR dataset in studies focused on political communication, misinformation, and the effectiveness of fact-checking efforts, making it a vital tool for understanding the dynamics of political discourse in contemporary society.",
    "tfidf_keywords": [
      "fact-checking",
      "political statements",
      "misinformation",
      "speaker metadata",
      "discourse analysis",
      "political communication",
      "truthfulness",
      "public perception",
      "data collection",
      "statement categorization"
    ],
    "semantic_cluster": "political-misinformation-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "political communication",
      "media studies",
      "public opinion",
      "information dissemination",
      "data analysis"
    ],
    "canonical_topics": [
      "natural-language-processing",
      "consumer-behavior",
      "policy-evaluation"
    ]
  },
  {
    "name": "DB1B Airline Origin and Destination Survey",
    "description": "10% random sample of all US airline tickets with origin, destination, fare, and itinerary details. Quarterly since 1993. The gold standard for airline pricing research.",
    "category": "Transportation Economics & Technology",
    "url": "https://www.transtats.bts.gov/DatabaseInfo.asp?QO_VQ=EFI",
    "docs_url": "https://www.transtats.bts.gov/Fields.asp?gnoession_VQ=FHK",
    "github_url": null,
    "tags": [
      "airlines",
      "fares",
      "routes",
      "BTS",
      "pricing"
    ],
    "best_for": "Airline pricing research, competition analysis, and route economics",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "transportation",
      "economics",
      "pricing"
    ],
    "summary": "The DB1B Airline Origin and Destination Survey provides a 10% random sample of all US airline tickets, including details on origin, destination, fare, and itinerary. This dataset is invaluable for researchers studying airline pricing dynamics and route efficiency.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the DB1B Airline Origin and Destination Survey?",
      "How can I access the DB1B dataset for airline pricing research?",
      "What variables are included in the DB1B Airline dataset?",
      "What time period does the DB1B Airline dataset cover?",
      "How is the DB1B dataset collected?",
      "What are common analyses performed using the DB1B dataset?",
      "What insights can be gained from the DB1B Airline Origin and Destination Survey?",
      "How does the DB1B dataset support research in transportation economics?"
    ],
    "use_cases": [
      "Analyzing fare trends over time to understand pricing strategies.",
      "Examining route efficiency and demand patterns in the airline industry.",
      "Investigating the impact of external factors on airline pricing.",
      "Conducting comparative studies of pricing across different airlines."
    ],
    "domain_tags": [
      "transportation"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "Quarterly since 1993",
    "geographic_scope": "United States",
    "size_category": "medium",
    "benchmark_usage": [
      "Airline pricing research",
      "Route optimization studies"
    ],
    "model_score": 0.0003,
    "embedding_text": "The DB1B Airline Origin and Destination Survey is a comprehensive dataset that captures a 10% random sample of all airline tickets issued in the United States. This dataset has been collected quarterly since 1993, making it a rich resource for analyzing trends in airline pricing, fare structures, and route performance over time. The data includes key variables such as origin and destination airports, fare amounts, and itinerary details, allowing researchers to explore various aspects of the airline industry. The collection methodology involves aggregating ticket data from the Bureau of Transportation Statistics (BTS), ensuring a high level of accuracy and reliability. Researchers utilizing this dataset can address a variety of research questions, such as how airline pricing strategies evolve, the effects of regulatory changes on fares, and the impact of economic conditions on travel behavior. Common preprocessing steps may include cleaning the data for missing values, aggregating fare data by route or time period, and transforming variables for analysis. The dataset supports a range of analytical techniques, including regression analysis, machine learning models, and descriptive statistics, making it suitable for both exploratory and confirmatory research. Overall, the DB1B dataset serves as a foundational tool for scholars and practitioners interested in transportation economics and the dynamics of airline pricing.",
    "tfidf_keywords": [
      "airline pricing",
      "fare analysis",
      "route efficiency",
      "Bureau of Transportation Statistics",
      "ticket data",
      "transportation economics",
      "itinerary details",
      "pricing strategies",
      "economic impact",
      "travel behavior"
    ],
    "semantic_cluster": "airline-pricing-research",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "econometrics",
      "consumer-behavior",
      "pricing",
      "transportation",
      "data-analysis"
    ],
    "canonical_topics": [
      "econometrics",
      "pricing",
      "consumer-behavior",
      "transportation",
      "data-engineering"
    ]
  },
  {
    "name": "LastFM-1B",
    "description": "1 billion listening events with long-term user histories. Music recommendation and listening behavior research",
    "category": "Entertainment & Media",
    "url": "http://www.cp.jku.at/datasets/LFM-1b/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "music",
      "listening",
      "recommendations",
      "large-scale",
      "LastFM"
    ],
    "best_for": "Learning entertainment & media analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "music",
      "listening",
      "recommendations"
    ],
    "summary": "The LastFM-1B dataset comprises 1 billion listening events, capturing long-term user histories in music consumption. It is designed for research into music recommendation systems and listening behavior, allowing researchers to analyze patterns and preferences in music consumption.",
    "use_cases": [
      "Analyzing user listening patterns",
      "Developing music recommendation algorithms",
      "Studying the impact of user history on music preferences"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the LastFM-1B dataset?",
      "How can I analyze music listening behavior?",
      "What insights can be gained from 1 billion listening events?",
      "How does user history affect music recommendations?",
      "What are the patterns in music consumption?",
      "How can I use LastFM data for recommendation systems?"
    ],
    "domain_tags": [
      "entertainment",
      "media"
    ],
    "data_modality": "tabular",
    "size_category": "massive",
    "model_score": 0.0003,
    "image_url": "/images/logos/jku.png",
    "embedding_text": "The LastFM-1B dataset is a comprehensive collection of 1 billion listening events that provide a rich resource for understanding music consumption behaviors and preferences. This dataset is structured in a tabular format, consisting of rows representing individual listening events and columns capturing various attributes such as user ID, track ID, timestamp, and potentially other metadata related to the music tracks. The collection methodology involves aggregating user listening histories from the LastFM platform, which is a popular music recommendation service. The dataset captures long-term user engagement with music, making it particularly valuable for researchers interested in the dynamics of music preferences over time. Key variables in the dataset include user identifiers, track identifiers, and timestamps, which allow for the analysis of listening trends and the development of recommendation algorithms. However, researchers should be aware of potential limitations, such as biases in user engagement or missing data for less popular tracks. Common preprocessing steps may include data cleaning, normalization of timestamps, and encoding of categorical variables to prepare the dataset for analysis. The LastFM-1B dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics, enabling researchers to explore questions related to user behavior, recommendation effectiveness, and the influence of historical listening patterns on current preferences. Researchers typically leverage this dataset to build and evaluate music recommendation systems, conduct exploratory data analysis, and investigate the factors that drive music consumption in different demographics. Overall, the LastFM-1B dataset serves as a foundational resource for advancing the understanding of music recommendation systems and user listening behavior.",
    "tfidf_keywords": [
      "listening-events",
      "user-histories",
      "music-recommendation",
      "behavioral-analysis",
      "music-consumption",
      "data-aggregation",
      "recommendation-algorithms",
      "user-engagement",
      "temporal-patterns",
      "data-preprocessing"
    ],
    "semantic_cluster": "music-recommendation-systems",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "recommendation-systems",
      "user-behavior",
      "data-mining",
      "machine-learning",
      "music-analysis"
    ],
    "canonical_topics": [
      "recommendation-systems",
      "consumer-behavior",
      "machine-learning"
    ]
  },
  {
    "name": "YouTube Engagement Dataset",
    "description": "5M videos with watch percentage, engagement maps, Freebase topic labels. Video-level engagement metrics for content research",
    "category": "Entertainment & Media",
    "url": "https://github.com/avalanchesiqi/youtube-engagement",
    "docs_url": null,
    "github_url": "https://github.com/avalanchesiqi/youtube-engagement",
    "tags": [
      "YouTube",
      "engagement",
      "video",
      "watch time",
      "topics"
    ],
    "best_for": "Learning entertainment & media analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "YouTube",
      "engagement",
      "video",
      "watch time",
      "topics"
    ],
    "summary": "The YouTube Engagement Dataset contains video-level engagement metrics for 5 million videos, including watch percentage and engagement maps. This dataset is valuable for content research, allowing researchers to analyze viewer behavior and engagement trends across various topics.",
    "use_cases": [
      "Analyzing viewer engagement trends over time",
      "Comparing engagement metrics across different video topics",
      "Investigating the impact of video length on watch percentage",
      "Exploring the relationship between engagement maps and viewer retention"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the engagement metrics for YouTube videos?",
      "How can I analyze watch time across different video topics?",
      "What insights can be gained from YouTube engagement maps?",
      "How does video length affect viewer engagement on YouTube?",
      "What are the trends in viewer engagement for popular YouTube topics?",
      "How can I use video-level metrics for content research?"
    ],
    "domain_tags": [
      "entertainment",
      "media"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0003,
    "image_url": "/images/datasets/youtube-engagement-dataset.png",
    "embedding_text": "The YouTube Engagement Dataset is a comprehensive collection of video-level engagement metrics derived from 5 million YouTube videos. This dataset includes critical variables such as watch percentage, which measures the proportion of a video that viewers watch, and engagement maps that visualize viewer interaction with the content. The dataset is structured in a tabular format, where each row corresponds to a unique video, and columns represent various engagement metrics and Freebase topic labels. The collection methodology involves scraping publicly available data from YouTube, ensuring a diverse representation of content across different genres and topics. While the dataset provides a wealth of information for content research, it is essential to consider potential limitations, such as the variability in data quality due to differences in video length, content type, and audience engagement patterns. Researchers can leverage this dataset to address various research questions, including the impact of video characteristics on viewer retention and the effectiveness of different engagement strategies. Common preprocessing steps may include cleaning the data to handle missing values and normalizing engagement metrics for comparative analysis. The dataset supports various types of analyses, including regression analysis to identify factors influencing engagement and descriptive analysis to summarize viewer behavior trends. Overall, the YouTube Engagement Dataset serves as a valuable resource for researchers and content creators aiming to understand and enhance viewer engagement on the platform.",
    "tfidf_keywords": [
      "watch percentage",
      "engagement metrics",
      "viewer retention",
      "content research",
      "engagement maps",
      "video characteristics",
      "audience interaction",
      "data scraping",
      "YouTube analytics",
      "Freebase topic labels"
    ],
    "semantic_cluster": "youtube-engagement-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "viewer behavior",
      "content strategy",
      "video marketing",
      "audience analytics",
      "digital media research"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "data-engineering",
      "product-analytics"
    ]
  },
  {
    "name": "Indonesian Fashion",
    "description": "Fashion items for image classification tasks from Indonesia",
    "category": "Fashion & Apparel",
    "url": "https://www.kaggle.com/datasets/latifahhukma/fashion-campus",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "fashion",
      "image classification",
      "Indonesia"
    ],
    "best_for": "Learning fashion & apparel analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Indonesian Fashion dataset comprises various fashion items sourced from Indonesia, specifically curated for image classification tasks. This dataset can be utilized to train machine learning models for recognizing and categorizing fashion items, enhancing applications in e-commerce and consumer insights.",
    "use_cases": [
      "Training image classification models for fashion recognition.",
      "Analyzing consumer preferences in Indonesian fashion.",
      "Developing recommendation systems for fashion e-commerce."
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What fashion items are included in the Indonesian Fashion dataset?",
      "How can I use the Indonesian Fashion dataset for image classification?",
      "What types of fashion categories are represented in the Indonesian Fashion dataset?",
      "Is the Indonesian Fashion dataset suitable for machine learning projects?",
      "What are the potential applications of the Indonesian Fashion dataset?",
      "How can I access the Indonesian Fashion dataset for research purposes?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "image",
    "geographic_scope": "Indonesia",
    "size_category": "medium",
    "model_score": 0.0003,
    "image_url": "/images/datasets/indonesian-fashion.png",
    "embedding_text": "The Indonesian Fashion dataset is a curated collection of fashion items specifically designed for image classification tasks. This dataset provides a rich resource for researchers and practitioners in the field of machine learning and computer vision, particularly those focusing on fashion-related applications. The data structure typically consists of images of various fashion items, each labeled with relevant categories that facilitate supervised learning tasks. The collection methodology involves sourcing images from various online platforms, ensuring a diverse representation of fashion styles and trends prevalent in Indonesia. Coverage is primarily geographic, focusing on Indonesian fashion, which allows for localized insights into consumer behavior and preferences. Key variables in the dataset include item images and their corresponding labels, which measure the classification accuracy of machine learning models. Data quality is generally high, although limitations may arise from variations in image quality and the potential for overlapping categories. Common preprocessing steps include image resizing, normalization, and augmentation to enhance model robustness. Researchers can leverage this dataset to address questions related to fashion trends, consumer preferences, and the effectiveness of image classification algorithms. The dataset supports various types of analyses, including regression analysis, machine learning model training, and descriptive statistics to summarize findings. Typically, researchers utilize this dataset to develop and evaluate models that can accurately classify fashion items, contributing to advancements in e-commerce and consumer insights.",
    "tfidf_keywords": [
      "image classification",
      "fashion recognition",
      "machine learning",
      "e-commerce",
      "consumer insights",
      "data preprocessing",
      "image augmentation",
      "fashion trends",
      "Indonesia",
      "computer vision"
    ],
    "semantic_cluster": "fashion-image-classification",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "computer-vision",
      "machine-learning",
      "consumer-behavior",
      "data-preprocessing",
      "image-augmentation"
    ],
    "canonical_topics": [
      "machine-learning",
      "computer-vision",
      "consumer-behavior"
    ]
  },
  {
    "name": "HateXplain",
    "description": "20K social media posts with human rationales across 10 hate speech target categories. Explainable AI for content moderation",
    "category": "Content Moderation",
    "url": "https://github.com/hate-alert/HateXplain",
    "docs_url": null,
    "github_url": "https://github.com/hate-alert/HateXplain",
    "tags": [
      "hate speech",
      "explainability",
      "NLP",
      "content moderation",
      "annotations"
    ],
    "best_for": "Learning content moderation analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "HateXplain is a dataset comprising 20,000 social media posts annotated with human rationales across ten categories of hate speech. This dataset is designed for researchers and practitioners in the field of content moderation, particularly those interested in explainable AI methodologies.",
    "use_cases": [
      "Training models for hate speech detection",
      "Evaluating explainability in AI systems",
      "Conducting sentiment analysis on hate speech",
      "Developing content moderation tools"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the HateXplain dataset?",
      "How can I use HateXplain for content moderation?",
      "What are the hate speech categories in HateXplain?",
      "What kind of analyses can be performed with HateXplain?",
      "How does HateXplain support explainable AI?",
      "What kind of social media posts are included in HateXplain?"
    ],
    "domain_tags": [
      "social-media",
      "AI",
      "content-moderation"
    ],
    "data_modality": "text",
    "size_category": "medium",
    "model_score": 0.0003,
    "image_url": "/images/datasets/hatexplain.png",
    "embedding_text": "The HateXplain dataset is a comprehensive collection of 20,000 social media posts that have been meticulously annotated to provide insights into hate speech across ten distinct target categories. Each entry in the dataset includes not only the text of the post but also human rationales that explain the reasoning behind the classification of each post as hate speech or not. This rich annotation makes HateXplain particularly valuable for researchers and practitioners interested in developing explainable AI systems for content moderation. The dataset's structure consists of rows representing individual social media posts and columns that capture various attributes, including the post text, the identified hate speech category, and the corresponding human rationale. The collection methodology involved sourcing posts from various social media platforms, ensuring a diverse representation of hate speech instances. While the dataset does not explicitly mention temporal or geographic coverage, it is designed to reflect a wide range of social media interactions, making it relevant for contemporary studies in hate speech detection and moderation. Key variables in the dataset include the text of the post, the category of hate speech, and the rationale provided by annotators, which collectively facilitate a deeper understanding of the nuances of hate speech in online discourse. Researchers utilizing HateXplain can address critical questions regarding the effectiveness of AI models in identifying hate speech and the interpretability of these models' decisions. Common preprocessing steps may include text normalization, tokenization, and the application of natural language processing techniques to prepare the data for analysis. The dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it a versatile resource for academic and industry research. Overall, HateXplain serves as a foundational tool for advancing the field of content moderation through the lens of explainable AI, enabling researchers to explore the complexities of hate speech and develop more effective moderation strategies.",
    "tfidf_keywords": [
      "hate speech",
      "explainable AI",
      "social media posts",
      "content moderation",
      "human rationales",
      "NLP",
      "annotations",
      "hate speech categories",
      "AI transparency",
      "text classification"
    ],
    "semantic_cluster": "explainable-ai-content-moderation",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "natural-language-processing",
      "machine-learning",
      "content-moderation",
      "explainability",
      "social-media-analysis"
    ],
    "canonical_topics": [
      "natural-language-processing",
      "machine-learning",
      "policy-evaluation"
    ]
  },
  {
    "name": "Social Blade",
    "description": "Public subscriber/follower counts and growth metrics across YouTube, Twitch, Instagram, Twitter, TikTok",
    "category": "Creator Economy",
    "url": "https://socialblade.com/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "YouTube",
      "Twitch",
      "Instagram",
      "followers",
      "growth metrics"
    ],
    "best_for": "Learning creator economy analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "creator-economy"
    ],
    "summary": "The Social Blade dataset provides public subscriber and follower counts along with growth metrics for various social media platforms including YouTube, Twitch, Instagram, Twitter, and TikTok. Researchers and analysts can utilize this data to understand trends in social media growth, analyze influencer performance, and study the dynamics of audience engagement across different platforms.",
    "use_cases": [
      "Analyzing the growth trends of YouTube channels over time.",
      "Comparing follower growth across different social media platforms.",
      "Studying the impact of social media metrics on influencer marketing.",
      "Evaluating audience engagement metrics for Twitch streamers."
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the subscriber counts for popular YouTube channels?",
      "How do follower counts on Instagram compare to those on Twitter?",
      "What growth metrics can be analyzed for Twitch streamers?",
      "How has TikTok's user engagement changed over time?",
      "What are the trends in social media follower counts?",
      "How can I access public growth metrics for social media platforms?",
      "What platforms does Social Blade cover?",
      "How do social media metrics influence marketing strategies?"
    ],
    "domain_tags": [
      "social-media",
      "marketing"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0003,
    "embedding_text": "The Social Blade dataset is a comprehensive collection of public subscriber and follower counts, as well as growth metrics, across major social media platforms such as YouTube, Twitch, Instagram, Twitter, and TikTok. This dataset is structured in a tabular format, with rows representing individual accounts or channels and columns detailing various metrics such as subscriber counts, follower counts, and growth rates over time. The data is collected from publicly available information on these platforms, ensuring a broad coverage of popular content creators and influencers. Key variables in this dataset include metrics like total subscribers, follower counts, and growth percentages, which measure the popularity and engagement levels of social media accounts. Researchers can leverage this dataset to address a variety of research questions, such as identifying trends in social media growth, understanding audience engagement, and analyzing the effectiveness of marketing strategies based on influencer metrics. Common preprocessing steps may include cleaning the data for missing values, normalizing metrics for comparative analysis, and aggregating data over specific time periods for trend analysis. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a valuable resource for both academic and industry research. Researchers typically use this dataset to study the dynamics of the creator economy, evaluate the performance of social media influencers, and explore the relationship between social media metrics and marketing outcomes.",
    "tfidf_keywords": [
      "subscriber-counts",
      "follower-growth",
      "social-media-metrics",
      "audience-engagement",
      "influencer-performance",
      "YouTube-analytics",
      "Twitch-growth",
      "Instagram-followers",
      "TikTok-engagement",
      "Twitter-metrics",
      "creator-economy",
      "social-media-analysis",
      "public-data",
      "growth-metrics"
    ],
    "semantic_cluster": "social-media-metrics",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "social-media-analysis",
      "influencer-marketing",
      "audience-engagement",
      "digital-marketing",
      "creator-economy"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "marketplaces",
      "data-engineering"
    ]
  },
  {
    "name": "UK Land Registry Price Paid",
    "description": "4.3GB of UK property sales transactions going back decades, messy real-world government data",
    "category": "Real Estate",
    "url": "https://www.gov.uk/government/collections/price-paid-data",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "real estate",
      "UK",
      "government data",
      "large-scale",
      "transactions",
      "messy data"
    ],
    "best_for": "Learning real estate analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "data-cleaning",
      "regression-analysis"
    ],
    "topic_tags": [
      "real estate",
      "government data",
      "property transactions"
    ],
    "summary": "The UK Land Registry Price Paid dataset contains extensive records of property sales transactions across the UK, offering insights into market trends and property valuations over time. Researchers and analysts can utilize this dataset to conduct various analyses, including price trends, market comparisons, and economic assessments related to real estate.",
    "use_cases": [
      "Analyzing trends in property prices over time",
      "Comparing property values across different regions",
      "Assessing the impact of government policies on real estate markets"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the historical property prices in the UK?",
      "How do property prices vary by region in the UK?",
      "What trends can be identified in UK property sales over the decades?",
      "How does government data on property transactions inform market analysis?",
      "What are the common characteristics of high-value property sales in the UK?",
      "How can I analyze the impact of economic factors on property prices in the UK?"
    ],
    "domain_tags": [
      "real estate"
    ],
    "data_modality": "tabular",
    "geographic_scope": "UK",
    "size_category": "large",
    "model_score": 0.0003,
    "image_url": "/images/datasets/uk-land-registry-price-paid.png",
    "embedding_text": "The UK Land Registry Price Paid dataset is a comprehensive collection of property sales transactions that spans several decades, providing a rich source of information for researchers and analysts interested in the real estate market. This dataset is structured in a tabular format, containing rows that represent individual property transactions and columns that capture various attributes such as transaction date, property type, price, and location. The data is sourced from the UK Land Registry, which collects information on property sales as part of its mandate to maintain accurate records of property ownership and transactions in the UK. Coverage of the dataset includes a wide range of property types, from residential homes to commercial buildings, and it encompasses transactions across all regions of the UK, making it a valuable resource for geographic and demographic analysis. Key variables in the dataset include the sale price, which measures the monetary value of each transaction, and property type, which categorizes properties into types such as detached houses, flats, and terraced houses. While the dataset provides a wealth of information, it is important to note that the data quality can vary, with some entries being incomplete or containing errors due to the messy nature of real-world data collection. Common preprocessing steps may include data cleaning to handle missing values, normalization of price data, and filtering out outliers that could skew analysis results. Researchers can leverage this dataset to address a variety of research questions, such as identifying trends in property values over time, examining the effects of economic conditions on real estate prices, and exploring the relationship between property characteristics and sale prices. The dataset supports various types of analyses, including regression analysis to model price determinants, machine learning techniques for predictive modeling, and descriptive statistics to summarize key trends. Overall, the UK Land Registry Price Paid dataset serves as a foundational resource for studies in real estate economics, market analysis, and policy evaluation, enabling researchers to derive insights that can inform both academic research and practical applications in the real estate sector.",
    "tfidf_keywords": [
      "property transactions",
      "UK Land Registry",
      "real estate market",
      "price trends",
      "data cleaning",
      "economic analysis",
      "property valuation",
      "geographic analysis",
      "transaction data",
      "data quality"
    ],
    "semantic_cluster": "real-estate-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "market analysis",
      "property valuation",
      "economic indicators",
      "data preprocessing",
      "time-series analysis"
    ],
    "canonical_topics": [
      "econometrics",
      "consumer-behavior",
      "pricing"
    ]
  },
  {
    "name": "ETHOS Hate Speech",
    "description": "998 online comments labeled for hate speech detection in English. Binary and multi-label annotations",
    "category": "Content Moderation",
    "url": "https://zenodo.org/records/4459923",
    "docs_url": null,
    "github_url": "https://github.com/intelligence-csd-auth-gr/Ethos-Hate-Speech-Dataset",
    "tags": [
      "hate speech",
      "NLP",
      "annotations",
      "English"
    ],
    "best_for": "Learning content moderation analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The ETHOS Hate Speech dataset consists of 998 online comments that have been labeled for the purpose of hate speech detection in the English language. This dataset provides both binary and multi-label annotations, making it suitable for various natural language processing tasks focused on identifying and classifying hate speech in text.",
    "use_cases": [
      "Training machine learning models for hate speech detection",
      "Evaluating the performance of NLP algorithms in classifying online comments"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the ETHOS Hate Speech dataset?",
      "How can I use the ETHOS dataset for NLP tasks?",
      "What are the annotations available in the ETHOS Hate Speech dataset?",
      "Where can I find datasets for hate speech detection?",
      "What types of comments are included in the ETHOS Hate Speech dataset?",
      "How is hate speech defined in the ETHOS dataset?"
    ],
    "domain_tags": [
      "content moderation"
    ],
    "data_modality": "text",
    "size_category": "medium",
    "model_score": 0.0003,
    "image_url": "/images/datasets/ethos-hate-speech.png",
    "embedding_text": "The ETHOS Hate Speech dataset is a carefully curated collection of 998 online comments specifically labeled for the detection of hate speech in the English language. This dataset is structured in a tabular format, where each row represents a unique comment, and the columns contain various attributes including the text of the comment and its associated labels. The labels are designed to indicate whether a comment contains hate speech, and they can be binary (hate speech or not) or multi-label, allowing for a nuanced classification of comments that may exhibit multiple forms of hate speech. The collection methodology for this dataset involved sourcing comments from various online platforms, ensuring a diverse representation of language and contexts in which hate speech may occur. As such, it provides a valuable resource for researchers and practitioners in the field of natural language processing (NLP) who are focused on developing algorithms to detect and mitigate hate speech online. The key variables in this dataset include the comment text and the corresponding labels, which measure the presence and type of hate speech. It is important to note that while the dataset is comprehensive, it may have limitations in terms of the representativeness of the comments and the subjective nature of hate speech classification. Common preprocessing steps for utilizing this dataset may include text normalization, tokenization, and the removal of stop words, which are essential for preparing the text for analysis. Researchers can use this dataset to address various research questions, such as understanding the linguistic features that characterize hate speech or evaluating the effectiveness of different machine learning models in classifying hate speech. The dataset supports a range of analyses, including regression analysis, machine learning model training, and descriptive statistics, making it a versatile tool for those studying hate speech in online discourse. Typically, researchers leverage this dataset to train and validate models aimed at automating the detection of hate speech, thereby contributing to efforts in content moderation and the promotion of healthier online communication.",
    "tfidf_keywords": [
      "hate speech",
      "NLP",
      "annotations",
      "text classification",
      "binary labels",
      "multi-label",
      "online comments",
      "text normalization",
      "machine learning",
      "data preprocessing",
      "linguistic features",
      "content moderation",
      "algorithm evaluation"
    ],
    "semantic_cluster": "nlp-for-hate-speech",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "text classification",
      "natural language processing",
      "machine learning",
      "content moderation",
      "sentiment analysis"
    ],
    "canonical_topics": [
      "natural-language-processing",
      "machine-learning",
      "experimentation"
    ]
  },
  {
    "name": "HateDay",
    "description": "Global representative sample of real-world hate speech across languages. 2024 benchmark for content moderation",
    "category": "Content Moderation",
    "url": "https://arxiv.org/abs/2404.06465",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "hate speech",
      "multilingual",
      "benchmark",
      "content moderation"
    ],
    "best_for": "Learning content moderation analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "HateDay is a global dataset that provides a representative sample of real-world hate speech across multiple languages. It serves as a benchmark for content moderation efforts, enabling researchers and practitioners to analyze and improve hate speech detection systems.",
    "use_cases": [
      "Developing hate speech detection algorithms",
      "Benchmarking content moderation tools",
      "Analyzing trends in hate speech across languages"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the HateDay dataset?",
      "How can HateDay be used for content moderation?",
      "What languages are represented in the HateDay dataset?",
      "What are the key features of the HateDay dataset?",
      "How does HateDay contribute to hate speech research?",
      "What methodologies can be applied to analyze HateDay data?"
    ],
    "domain_tags": [
      "content moderation"
    ],
    "data_modality": "text",
    "size_category": "medium",
    "model_score": 0.0003,
    "image_url": "/images/datasets/hateday.png",
    "embedding_text": "The HateDay dataset is a comprehensive collection designed to provide a global representative sample of real-world hate speech across various languages. It is particularly valuable for researchers and practitioners in the field of content moderation, as it offers a benchmark for evaluating the effectiveness of hate speech detection algorithms. The dataset is structured in a tabular format, containing rows that represent individual instances of hate speech and columns that capture key variables such as the text of the speech, language, and possibly the context in which it was used. The collection methodology involves aggregating data from diverse sources to ensure a wide-ranging representation of hate speech occurrences, making it suitable for various analytical approaches. Researchers can utilize this dataset to address critical questions regarding the prevalence and characteristics of hate speech across different cultures and languages. Common preprocessing steps may include text normalization, tokenization, and the removal of non-relevant content to prepare the data for analysis. The dataset supports various types of analyses, including regression, machine learning, and descriptive statistics, enabling users to derive insights into the dynamics of hate speech and its impact on society. Despite its strengths, users should be aware of potential data quality issues, such as bias in the sources of hate speech or limitations in the representation of certain languages. Overall, HateDay serves as a vital resource for advancing the understanding of hate speech and improving content moderation practices.",
    "benchmark_usage": [
      "Content moderation research",
      "Hate speech detection benchmarking"
    ],
    "tfidf_keywords": [
      "hate speech",
      "content moderation",
      "multilingual",
      "benchmark",
      "text analysis",
      "natural language processing",
      "data collection",
      "machine learning",
      "algorithm evaluation",
      "speech detection"
    ],
    "semantic_cluster": "hate-speech-detection",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "natural-language-processing",
      "machine-learning",
      "text-analysis",
      "data-collection",
      "algorithm-evaluation"
    ],
    "canonical_topics": [
      "natural-language-processing",
      "machine-learning",
      "policy-evaluation"
    ]
  },
  {
    "name": "Predicting Poverty Replication Data",
    "description": "Satellite imagery and survey data from Jean et al. (Science 2016) for predicting poverty in African countries using deep learning.",
    "category": "Geospatial",
    "url": "https://github.com/nealjean/predicting-poverty",
    "docs_url": null,
    "github_url": "https://github.com/nealjean/predicting-poverty",
    "tags": [
      "satellite",
      "poverty",
      "deep-learning",
      "Africa",
      "development"
    ],
    "best_for": "Replicating and extending satellite-based poverty prediction research",
    "model_score": 0.0003,
    "image_url": "/images/datasets/predicting-poverty-replication-data.png",
    "difficulty": "intermediate",
    "prerequisites": [
      "deep-learning",
      "geospatial-analysis"
    ],
    "topic_tags": [
      "geospatial",
      "poverty",
      "machine-learning"
    ],
    "summary": "The Predicting Poverty Replication Data combines satellite imagery and survey data to facilitate the prediction of poverty levels in African countries using advanced deep learning techniques. Researchers can leverage this dataset to explore the relationship between environmental factors and socioeconomic indicators, enabling targeted interventions for poverty alleviation.",
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Predicting Poverty Replication Data?",
      "How can satellite imagery be used to predict poverty?",
      "What deep learning techniques are applicable to this dataset?",
      "What are the key variables in the Predicting Poverty Replication Data?",
      "How does this dataset contribute to poverty research in Africa?",
      "What methodologies were used to collect the data?",
      "What are the limitations of the Predicting Poverty Replication Data?",
      "How can this dataset be used for development studies?"
    ],
    "use_cases": [
      "Analyzing the correlation between satellite imagery features and poverty levels.",
      "Developing predictive models to assess poverty in unmeasured regions.",
      "Evaluating the effectiveness of development programs based on poverty predictions.",
      "Investigating the impact of environmental changes on socioeconomic conditions."
    ],
    "embedding_text": "The Predicting Poverty Replication Data is a rich dataset that combines satellite imagery and survey data, specifically designed to assist researchers in predicting poverty levels across various African nations. This dataset is grounded in the research conducted by Jean et al. (Science 2016), which emphasizes the use of deep learning techniques to analyze complex geospatial data. The structure of the dataset typically includes rows representing different geographic locations or survey respondents, with columns detailing various variables such as socioeconomic indicators, environmental features derived from satellite images, and other relevant metrics. The collection methodology involves integrating high-resolution satellite imagery with ground-truth survey data, which provides a comprehensive view of the factors influencing poverty. Key variables within the dataset may include income levels, education, access to resources, and environmental characteristics, all of which are crucial for understanding the multifaceted nature of poverty. However, researchers should be aware of potential limitations regarding data quality, such as the accuracy of survey responses and the resolution of satellite imagery. Common preprocessing steps may involve normalizing data, handling missing values, and transforming variables to suit analytical models. This dataset supports a range of analyses, including regression modeling, machine learning applications, and descriptive statistics, enabling researchers to address critical research questions related to poverty dynamics and the effectiveness of interventions. Typically, researchers utilize this dataset to build predictive models, assess the impact of environmental changes on poverty, and inform policy decisions aimed at poverty alleviation in Africa.",
    "domain_tags": [
      "development",
      "geospatial"
    ],
    "data_modality": "mixed",
    "geographic_scope": "Africa",
    "size_category": "medium",
    "tfidf_keywords": [
      "satellite-imagery",
      "poverty-prediction",
      "deep-learning",
      "geospatial-analysis",
      "socioeconomic-indicators",
      "data-integration",
      "environmental-factors",
      "predictive-modeling",
      "development-interventions",
      "data-quality"
    ],
    "semantic_cluster": "poverty-prediction-methods",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "machine-learning",
      "geospatial-analysis",
      "socioeconomic-research",
      "data-integration",
      "predictive-modeling"
    ],
    "canonical_topics": [
      "machine-learning",
      "geospatial",
      "econometrics",
      "policy-evaluation"
    ]
  },
  {
    "name": "ER Wait Time",
    "description": "Simulated emergency room data with wait times, patient outcomes, and satisfaction scores for healthcare queueing analysis.",
    "category": "Healthcare",
    "url": "https://www.kaggle.com/datasets/rivalytics/er-wait-time",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "queueing",
      "emergency-room",
      "wait-times",
      "healthcare",
      "simulation",
      "Kaggle"
    ],
    "best_for": "Queueing theory - ER capacity planning, priority queue modeling, patient flow optimization",
    "image_url": "/images/datasets/er-wait-time.png",
    "difficulty": "beginner",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [],
    "summary": "The ER Wait Time dataset provides simulated emergency room data, including wait times, patient outcomes, and satisfaction scores, which can be utilized for healthcare queueing analysis. Researchers can leverage this dataset to explore factors affecting wait times and patient satisfaction in emergency care settings.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the average wait times in emergency rooms?",
      "How do patient outcomes correlate with wait times?",
      "What factors influence patient satisfaction in emergency care?",
      "How can simulation data improve healthcare queueing?",
      "What trends can be observed in emergency room wait times?",
      "What is the impact of wait times on patient outcomes?"
    ],
    "use_cases": [
      "Analyzing the impact of wait times on patient satisfaction",
      "Simulating different queueing scenarios to optimize emergency room operations",
      "Evaluating patient outcomes based on varying wait times"
    ],
    "embedding_text": "The ER Wait Time dataset is a comprehensive collection of simulated emergency room data designed for healthcare queueing analysis. This dataset encompasses various key variables, including wait times, patient outcomes, and satisfaction scores, which are essential for understanding the dynamics of emergency care. The data is structured in a tabular format, consisting of rows representing individual patient encounters and columns detailing specific attributes such as wait time duration, outcome measures, and satisfaction ratings. The collection methodology involves simulation techniques that replicate real-world emergency room scenarios, providing a rich resource for researchers and practitioners alike. While the dataset does not specify temporal or geographic coverage, it offers valuable insights into the factors influencing patient experiences in emergency settings. Key variables within the dataset measure critical aspects of emergency care, including the duration of wait times and their correlation with patient satisfaction and outcomes. However, researchers should be aware of potential limitations in data quality, as simulated data may not fully capture the complexities of real-world emergency care. Common preprocessing steps may include cleaning the data for missing values, normalizing scores, and transforming variables for analysis. Researchers can utilize this dataset to address various research questions, such as the relationship between wait times and patient satisfaction or the effectiveness of different queueing strategies. The dataset supports a range of analyses, including regression analysis, machine learning applications, and descriptive statistics. Typically, researchers employ this dataset to inform policy decisions, optimize emergency room operations, and enhance patient care strategies.",
    "domain_tags": [
      "healthcare"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "tfidf_keywords": [
      "emergency-room",
      "wait-times",
      "patient-outcomes",
      "satisfaction-scores",
      "healthcare-queueing",
      "simulation-data",
      "queueing-theory",
      "healthcare-analytics",
      "patient-experience",
      "operational-efficiency"
    ],
    "semantic_cluster": "healthcare-queueing-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "queueing-theory",
      "healthcare-analytics",
      "patient-experience",
      "operational-efficiency",
      "simulation-modeling"
    ],
    "canonical_topics": [
      "healthcare",
      "statistics",
      "machine-learning"
    ],
    "model_score": 0.0003
  },
  {
    "name": "Hospital Emergency Dataset",
    "description": "Patient care and operational efficiency data from hospital emergency departments for healthcare operations research.",
    "category": "Healthcare",
    "url": "https://www.kaggle.com/datasets/xavierberge/hospital-emergency-dataset",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "queueing",
      "emergency",
      "hospital",
      "operations",
      "patient-flow",
      "Kaggle"
    ],
    "best_for": "Queueing theory - bed requirements, treatment time distributions, boarding delay analysis",
    "image_url": "/images/datasets/hospital-emergency-dataset.jpg",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Hospital Emergency Dataset provides comprehensive data on patient care and operational efficiency within hospital emergency departments. This dataset can be utilized for healthcare operations research, enabling analysis of patient flow, resource allocation, and emergency response strategies.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What data is available in the Hospital Emergency Dataset?",
      "How can I analyze patient flow in emergency departments?",
      "What operational efficiency metrics are included in the dataset?",
      "How does the Hospital Emergency Dataset support healthcare operations research?",
      "What are the key variables in the Hospital Emergency Dataset?",
      "How can I use the dataset for regression analysis?",
      "What insights can be gained from emergency department patient care data?",
      "Where can I find the Hospital Emergency Dataset for research?"
    ],
    "use_cases": [
      "Analyzing patient wait times in emergency departments",
      "Evaluating the impact of staffing levels on patient outcomes",
      "Studying trends in emergency department visits over time",
      "Optimizing resource allocation in hospital emergency services"
    ],
    "embedding_text": "The Hospital Emergency Dataset is a valuable resource for researchers focused on healthcare operations, particularly in the context of emergency departments. This dataset encompasses various dimensions of patient care and operational efficiency, structured in a tabular format that allows for straightforward analysis. The data typically includes rows representing individual patient visits and columns detailing key variables such as patient demographics, wait times, treatment outcomes, and resource utilization. The collection methodology for this dataset often involves aggregating data from hospital information systems, ensuring a comprehensive view of emergency department operations. However, researchers should be aware of potential limitations, such as data quality issues stemming from incomplete records or variations in reporting practices across different hospitals. Common preprocessing steps may include data cleaning, normalization, and handling of missing values to prepare the dataset for analysis. Researchers can leverage this dataset to address critical questions regarding patient flow, operational bottlenecks, and the effectiveness of interventions aimed at improving emergency care. The dataset supports a variety of analytical approaches, including regression analysis, machine learning, and descriptive statistics, making it a versatile tool for both exploratory and confirmatory research in the healthcare domain. By utilizing the Hospital Emergency Dataset, researchers can gain insights that inform policy decisions, enhance patient care strategies, and ultimately contribute to improved healthcare outcomes.",
    "domain_tags": [
      "healthcare"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "tfidf_keywords": [
      "patient-flow",
      "emergency-department",
      "operational-efficiency",
      "resource-allocation",
      "wait-times",
      "treatment-outcomes",
      "healthcare-operations",
      "data-quality",
      "preprocessing",
      "regression-analysis",
      "machine-learning",
      "descriptive-statistics",
      "healthcare-research",
      "data-aggregation",
      "hospital-information-systems"
    ],
    "semantic_cluster": "healthcare-operations-research",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "healthcare",
      "operations-research",
      "patient-care",
      "resource-management",
      "data-analysis"
    ],
    "canonical_topics": [
      "healthcare",
      "statistics",
      "data-engineering"
    ],
    "model_score": 0.0003
  },
  {
    "name": "Hospital Triage and Patient History",
    "description": "Triage data with timestamps and patient history for modeling priority queues in emergency medicine.",
    "category": "Healthcare",
    "url": "https://www.kaggle.com/datasets/maalona/hospital-triage-and-patient-history-data",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "queueing",
      "triage",
      "patient-flow",
      "timestamps",
      "priority-queue",
      "Kaggle"
    ],
    "best_for": "Queueing theory - priority queue modeling (high acuity preempts), arrival rate estimation by acuity",
    "image_url": "/images/datasets/hospital-triage-and-patient-history.jpg",
    "difficulty": "beginner",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [],
    "summary": "The Hospital Triage and Patient History dataset contains triage data with timestamps and patient histories, which can be utilized for modeling priority queues in emergency medicine. Researchers can analyze patient flow and develop algorithms to improve triage efficiency and outcomes.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the structure of the Hospital Triage and Patient History dataset?",
      "How can I use triage data to model patient flow in emergency medicine?",
      "What variables are included in the Hospital Triage dataset?",
      "What analyses can be performed on patient history data?",
      "How does timestamp data impact triage decision-making?",
      "What are the common preprocessing steps for triage datasets?"
    ],
    "use_cases": [
      "Modeling patient flow in emergency departments",
      "Developing algorithms for prioritizing patient treatment",
      "Analyzing the impact of triage decisions on patient outcomes"
    ],
    "embedding_text": "The Hospital Triage and Patient History dataset is a crucial resource for researchers and practitioners in the field of emergency medicine. This dataset encompasses a structured collection of triage data, including timestamps and detailed patient histories, which are essential for modeling and analyzing priority queues in emergency settings. The data is typically organized in a tabular format, with rows representing individual patient encounters and columns capturing various attributes such as patient demographics, presenting complaints, triage scores, timestamps of key events, and treatment outcomes. Each variable within the dataset serves a specific purpose; for instance, timestamps allow for the analysis of patient wait times and flow through the emergency department, while triage scores help prioritize patients based on the severity of their conditions. The collection methodology for this dataset often involves direct extraction from hospital information systems or electronic health records, ensuring that the data reflects real-world scenarios encountered in emergency care. However, researchers should be aware of potential limitations, such as missing data or variations in triage protocols across different institutions, which may affect the generalizability of findings. Common preprocessing steps may include handling missing values, normalizing timestamps, and encoding categorical variables for analysis. This dataset supports various types of analyses, including regression modeling to predict patient outcomes, machine learning techniques for classification of triage categories, and descriptive statistics to summarize patient demographics and flow patterns. Researchers typically utilize this dataset to address critical questions regarding the efficiency of triage processes, the impact of timely interventions on patient outcomes, and the overall optimization of emergency department operations.",
    "domain_tags": [
      "healthcare"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "tfidf_keywords": [
      "triage",
      "patient-flow",
      "priority-queue",
      "timestamps",
      "emergency-medicine",
      "data-collection",
      "patient-history",
      "wait-times",
      "treatment-outcomes",
      "regression-modeling",
      "machine-learning",
      "data-preprocessing",
      "healthcare-analytics",
      "emergency-department",
      "queueing-theory"
    ],
    "semantic_cluster": "healthcare-analytics",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "queueing-theory",
      "healthcare-analytics",
      "predictive-modeling",
      "patient-outcomes",
      "emergency-care",
      "data-mining",
      "algorithm-development",
      "operational-efficiency",
      "statistical-analysis"
    ],
    "canonical_topics": [
      "healthcare",
      "machine-learning",
      "statistics",
      "data-engineering"
    ],
    "model_score": 0.0003
  },
  {
    "name": "DataMOCCA US Bank Database",
    "description": "220M calls across 2.5 years from multiple call center sites with skills-based routing. Accessible via SEEStat software.",
    "category": "Operations & Service",
    "url": "https://ie.technion.ac.il/serveng/References/DataMOCCA.pdf",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "call-center",
      "skills-routing",
      "multi-site",
      "large-scale"
    ],
    "best_for": "Multi-site pooling effects, skills-based routing analysis, large-scale staffing",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The DataMOCCA US Bank Database contains 220 million calls recorded over 2.5 years from various call center locations utilizing skills-based routing. This dataset allows for in-depth analysis of call center operations, customer interactions, and service efficiency, making it a valuable resource for operational research and service optimization.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What insights can be gained from the DataMOCCA US Bank Database?",
      "How can skills-based routing impact call center efficiency?",
      "What are the trends in call volume over 2.5 years?",
      "How does multi-site operation affect customer service outcomes?",
      "What analysis can be performed using the DataMOCCA dataset?",
      "How can I access the DataMOCCA US Bank Database?",
      "What variables are included in the DataMOCCA call center dataset?",
      "What methodologies are suitable for analyzing call center data?"
    ],
    "use_cases": [
      "Analyzing call volume trends over time",
      "Evaluating the effectiveness of skills-based routing",
      "Assessing customer satisfaction based on call outcomes",
      "Optimizing staffing based on call patterns"
    ],
    "embedding_text": "The DataMOCCA US Bank Database is a comprehensive dataset comprising 220 million calls collected over a span of 2.5 years from multiple call center sites. This dataset is structured in a tabular format, with rows representing individual calls and columns capturing various attributes related to each call, such as call duration, time of day, agent performance metrics, and customer satisfaction ratings. The collection methodology involves recording calls from call centers that utilize skills-based routing, ensuring that calls are directed to agents with the appropriate expertise. This approach not only enhances the customer experience but also allows for a detailed analysis of operational efficiency across different sites. The dataset's temporal coverage spans 2.5 years, providing a rich source of longitudinal data that can reveal trends and patterns in call center operations. Geographic coverage is specific to the United States, making it particularly relevant for studies focused on American customer service practices. Key variables in the dataset include call duration, wait times, agent IDs, and customer feedback scores, which measure various aspects of the call experience. Researchers can utilize this dataset to address a range of research questions, such as the impact of skills-based routing on call resolution times, the relationship between call volume and customer satisfaction, and the effectiveness of different operational strategies across multiple sites. Common preprocessing steps may include cleaning the data to handle missing values, normalizing call durations, and categorizing call outcomes for analysis. The dataset supports various types of analyses, including regression analysis to identify factors influencing customer satisfaction, machine learning models to predict call outcomes, and descriptive statistics to summarize call center performance. Researchers typically use this dataset to inform operational improvements, enhance customer service strategies, and contribute to the broader understanding of call center dynamics.",
    "tfidf_keywords": [
      "call-center",
      "skills-routing",
      "customer-satisfaction",
      "operational-efficiency",
      "call-volume",
      "agent-performance",
      "longitudinal-data",
      "multi-site",
      "data-collection-methodology",
      "service-optimization",
      "customer-interaction",
      "call-duration",
      "wait-times",
      "data-preprocessing",
      "predictive-modeling"
    ],
    "semantic_cluster": "call-center-analytics",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "customer-service",
      "operational-research",
      "data-analysis",
      "service-quality",
      "performance-metrics"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "operations",
      "statistics"
    ],
    "domain_tags": [
      "operations",
      "service"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2.5 years",
    "geographic_scope": "United States",
    "size_category": "large",
    "model_score": 0.0003
  },
  {
    "name": "Bosch Production Line Performance",
    "description": "1.18M parts tracked across 52 workstations with 1,156 timestamp features. One of the largest real manufacturing datasets.",
    "category": "Manufacturing",
    "url": "https://www.kaggle.com/c/bosch-production-line-performance",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "manufacturing",
      "production-line",
      "timestamps",
      "quality",
      "Kaggle"
    ],
    "best_for": "Tandem queue analysis, bottleneck identification, work-in-progress dynamics",
    "image_url": "/images/datasets/bosch-production-line-performance.png",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "manufacturing",
      "production-line",
      "quality"
    ],
    "summary": "The Bosch Production Line Performance dataset consists of 1.18 million parts tracked across 52 workstations, featuring 1,156 timestamp variables. This extensive dataset allows researchers to analyze production efficiency, quality control, and temporal trends in manufacturing processes.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the Bosch Production Line Performance dataset?",
      "How can I analyze production efficiency using Bosch dataset?",
      "What are the key features in the Bosch manufacturing dataset?",
      "Where can I find the Bosch Production Line dataset?",
      "What insights can be gained from Bosch production data?",
      "How to preprocess Bosch dataset for analysis?",
      "What machine learning techniques can be applied to Bosch dataset?",
      "What are the limitations of the Bosch Production Line dataset?"
    ],
    "use_cases": [
      "Analyzing production efficiency over time",
      "Identifying quality issues in manufacturing processes",
      "Developing predictive models for production outcomes",
      "Exploring the impact of workstation performance on overall production"
    ],
    "embedding_text": "The Bosch Production Line Performance dataset is a comprehensive collection of data that tracks 1.18 million parts across 52 workstations, featuring an impressive array of 1,156 timestamp variables. This dataset is one of the largest publicly available datasets in the manufacturing domain, providing a rich resource for researchers and practitioners interested in analyzing production line performance. The data structure is primarily tabular, with rows representing individual parts and columns capturing various attributes related to their production, including timestamps that detail the timing of each operation at the workstations. The dataset is designed to facilitate a wide range of analyses, from basic descriptive statistics to advanced machine learning applications. Researchers can leverage this dataset to address critical questions related to production efficiency, quality control, and operational bottlenecks. Key variables include timestamps that measure the duration of processes, which can be analyzed to identify trends and anomalies in production. However, users should be aware of potential data quality issues, such as missing values or inconsistencies that may arise from the manufacturing process. Common preprocessing steps may include data cleaning, normalization, and feature engineering to prepare the dataset for analysis. The dataset supports various analytical techniques, including regression analysis, machine learning algorithms, and time-series analysis, making it a versatile tool for exploring the dynamics of manufacturing processes. Researchers typically use this dataset to develop predictive models that can forecast production outcomes, optimize workstation performance, and enhance overall production quality. By utilizing this dataset, analysts can gain valuable insights into the operational efficiency of manufacturing lines, ultimately contributing to improved productivity and reduced costs.",
    "tfidf_keywords": [
      "production-efficiency",
      "quality-control",
      "timestamp-analysis",
      "manufacturing-processes",
      "predictive-modeling",
      "operational-bottlenecks",
      "data-cleaning",
      "feature-engineering",
      "time-series-analysis",
      "workstation-performance",
      "anomaly-detection",
      "data-normalization",
      "descriptive-statistics",
      "machine-learning",
      "data-quality"
    ],
    "semantic_cluster": "manufacturing-performance-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "production-optimization",
      "data-analysis",
      "machine-learning",
      "time-series-analysis",
      "quality-assurance"
    ],
    "canonical_topics": [
      "machine-learning",
      "statistics",
      "industrial-organization"
    ],
    "domain_tags": [
      "manufacturing"
    ],
    "data_modality": "tabular",
    "size_category": "massive",
    "model_score": 0.0003
  },
  {
    "name": "Telecom Italia Big Data Challenge",
    "description": "Milan call detail records aggregated by 10-minute intervals and geographic grid squares. Ideal for Erlang B/C validation.",
    "category": "Telecommunications",
    "url": "https://dandelion.eu/datamine/open-big-data/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "telecom",
      "CDR",
      "Milan",
      "cellular",
      "grid"
    ],
    "best_for": "Telecommunications traffic engineering, Erlang formula validation, spatial demand",
    "image_url": "/images/logos/dandelion.png",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "telecommunications",
      "big-data",
      "data-analysis"
    ],
    "summary": "The Telecom Italia Big Data Challenge dataset comprises call detail records from Milan, aggregated into 10-minute intervals and organized by geographic grid squares. This dataset is particularly useful for validating Erlang B/C models, which are essential in telecommunications for understanding call traffic and network capacity.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the call detail records for Milan?",
      "How can I validate Erlang B/C models using this dataset?",
      "What insights can be derived from aggregated call data?",
      "How is call traffic distributed across different geographic areas in Milan?",
      "What patterns can be observed in cellular usage over time?",
      "How can I analyze the impact of grid square locations on call volume?"
    ],
    "use_cases": [
      "Analyzing call traffic patterns in urban areas",
      "Validating telecommunications models like Erlang B/C",
      "Studying the impact of geographic factors on cellular usage",
      "Exploring temporal trends in call detail records"
    ],
    "embedding_text": "The Telecom Italia Big Data Challenge dataset consists of call detail records (CDRs) collected from the city of Milan, Italy. This dataset is structured in a tabular format, where each row represents a 10-minute interval of aggregated call data, and the columns include various variables such as the number of calls, duration, and geographic grid square identifiers. The collection methodology involves aggregating raw call data from cellular networks, which is then processed to ensure that it is anonymized and suitable for analysis. The dataset is particularly valuable for telecommunications research, as it allows for the exploration of call traffic patterns and the validation of Erlang B/C models, which are crucial for understanding network capacity and performance. Key variables in the dataset include the total number of calls made within each time interval, the average call duration, and the geographic identifiers that correspond to specific grid squares in Milan. Researchers using this dataset can address a variety of research questions, such as how call traffic varies by time of day or how geographic factors influence cellular usage. Common preprocessing steps may include data cleaning, normalization, and transformation to prepare the data for analysis. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile resource for both academic and industry research. Researchers typically leverage this dataset to study urban telecommunications dynamics, validate theoretical models, and derive actionable insights for network optimization.",
    "tfidf_keywords": [
      "call detail records",
      "Erlang B/C",
      "telecommunications",
      "aggregated data",
      "grid squares",
      "urban analysis",
      "network capacity",
      "temporal patterns",
      "cellular usage",
      "data preprocessing"
    ],
    "semantic_cluster": "telecom-data-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "network-traffic-analysis",
      "urban-telecommunications",
      "data-aggregation",
      "Erlang-modeling",
      "big-data-analytics"
    ],
    "canonical_topics": [
      "telecommunications",
      "machine-learning",
      "statistics"
    ],
    "domain_tags": [
      "telecommunications"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Milan",
    "size_category": "medium",
    "benchmark_usage": [
      "Erlang B/C validation"
    ],
    "model_score": 0.0003
  },
  {
    "name": "SOA Mortality Tables",
    "description": "Society of Actuaries mortality tables and experience studies used as industry standards for life insurance pricing",
    "category": "Insurance & Actuarial",
    "url": "https://mort.soa.org/",
    "docs_url": "https://www.soa.org/resources/experience-studies/",
    "github_url": null,
    "tags": [
      "mortality-tables",
      "life-insurance",
      "actuarial",
      "experience-studies",
      "annuities"
    ],
    "best_for": "Life insurance product development, reserving, and mortality assumption setting",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "insurance",
      "actuarial-science",
      "mortality"
    ],
    "summary": "The SOA Mortality Tables provide standardized mortality rates and experience studies utilized in the life insurance industry for pricing and risk assessment. These tables are essential for actuaries and insurance professionals to evaluate life expectancy and inform policy pricing strategies.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the SOA Mortality Tables?",
      "How are mortality tables used in life insurance pricing?",
      "What data is included in the Society of Actuaries mortality tables?",
      "How can actuaries utilize experience studies for risk assessment?",
      "What is the significance of mortality tables in actuarial science?",
      "Where can I find Society of Actuaries mortality tables?",
      "What methodologies are used to create mortality tables?",
      "How do mortality tables impact annuity pricing?"
    ],
    "use_cases": [
      "Evaluating life insurance policy pricing based on mortality rates.",
      "Conducting risk assessments for life insurance products.",
      "Analyzing trends in mortality rates over time.",
      "Developing annuity pricing models using mortality data."
    ],
    "domain_tags": [
      "insurance",
      "actuarial"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/logos/soa.png",
    "embedding_text": "The SOA Mortality Tables are a vital resource for actuaries and insurance professionals, providing a comprehensive set of mortality rates and experience studies that serve as industry standards for life insurance pricing. These tables are structured in a tabular format, typically consisting of rows representing different age groups and columns detailing mortality rates, life expectancies, and other relevant variables. The data is collected from various sources, including national vital statistics and actuarial research, ensuring a high level of accuracy and reliability. However, users should be aware of potential limitations, such as changes in population health trends and demographic shifts that may affect the applicability of the data over time. Common preprocessing steps may include normalization of data, handling missing values, and adjusting for demographic factors. Researchers can utilize the SOA Mortality Tables to address a range of questions, such as estimating the probability of death at various ages, analyzing the impact of health interventions on mortality rates, and developing predictive models for life insurance pricing. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics. Actuaries typically use this data in studies to inform pricing strategies, assess risk, and evaluate the financial implications of mortality trends on insurance products.",
    "benchmark_usage": [
      "Industry standards for life insurance pricing"
    ],
    "tfidf_keywords": [
      "mortality-rates",
      "life-expectancy",
      "actuarial-science",
      "risk-assessment",
      "insurance-pricing",
      "experience-studies",
      "demographic-analysis",
      "predictive-modeling",
      "data-normalization",
      "population-health",
      "mortality-trends",
      "actuarial-research",
      "policy-pricing",
      "statistical-analysis"
    ],
    "semantic_cluster": "mortality-data-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "actuarial-modeling",
      "risk-management",
      "life-insurance",
      "statistical-methods",
      "demography"
    ],
    "canonical_topics": [
      "finance",
      "healthcare",
      "statistics",
      "econometrics",
      "consumer-behavior"
    ]
  },
  {
    "name": "Amazon Last Mile",
    "description": "9,184 historical routes across 5 US metro areas",
    "category": "Logistics & Supply Chain",
    "url": "https://registry.opendata.aws/amazon-last-mile-challenges/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "routing",
      "last-mile",
      "Amazon",
      "AWS"
    ],
    "best_for": "Learning logistics & supply chain analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "logistics"
    ],
    "summary": "The Amazon Last Mile dataset contains 9,184 historical routes across five major US metropolitan areas, providing insights into last-mile delivery logistics. Researchers can analyze routing efficiency, consumer behavior, and the impact of delivery routes on service quality.",
    "use_cases": [
      "Analyzing the efficiency of delivery routes in urban logistics.",
      "Studying the impact of routing on delivery times and customer satisfaction.",
      "Exploring patterns in last-mile delivery across different metropolitan areas."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the historical last-mile delivery routes for Amazon?",
      "How can I analyze routing efficiency in urban areas?",
      "What insights can be gained from Amazon's last-mile logistics?",
      "How do delivery routes vary across different US metro areas?",
      "What factors influence last-mile delivery performance?",
      "How can I visualize Amazon's delivery routes?"
    ],
    "domain_tags": [
      "logistics",
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "US",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/logos/opendata.png",
    "embedding_text": "The Amazon Last Mile dataset is a comprehensive collection of 9,184 historical delivery routes specifically focused on last-mile logistics in five major US metropolitan areas. This dataset is structured in a tabular format, with each row representing a unique delivery route, and columns detailing various attributes such as route distance, delivery time, and geographic coordinates. The collection methodology likely involved aggregating data from Amazon's logistics operations, possibly including GPS tracking and delivery logs, although specific data sources are not disclosed. The dataset covers urban areas, providing a rich context for analyzing last-mile delivery dynamics. Key variables include route efficiency metrics, delivery times, and geographic locations, which are crucial for understanding the complexities of urban logistics and consumer behavior. While the dataset offers valuable insights, researchers should be aware of potential limitations such as data quality issues arising from incomplete or inconsistent records. Common preprocessing steps may include cleaning the data, handling missing values, and normalizing route distances. This dataset supports a variety of research questions, including those related to the optimization of delivery routes, the impact of geographic factors on delivery efficiency, and the analysis of consumer behavior in relation to delivery services. Researchers typically employ regression analysis, machine learning techniques, and descriptive statistics to derive meaningful insights from the data. The dataset serves as a vital resource for studies focused on logistics optimization, consumer behavior analysis, and the broader implications of e-commerce on urban infrastructure.",
    "tfidf_keywords": [
      "last-mile logistics",
      "delivery routes",
      "urban delivery",
      "routing efficiency",
      "consumer behavior",
      "geographic analysis",
      "logistics optimization",
      "delivery performance",
      "urban infrastructure",
      "e-commerce logistics",
      "route distance",
      "delivery time",
      "GPS tracking",
      "data aggregation"
    ],
    "semantic_cluster": "logistics-optimization",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "routing algorithms",
      "urban logistics",
      "delivery optimization",
      "consumer behavior analysis",
      "geospatial analysis"
    ],
    "canonical_topics": [
      "logistics",
      "consumer-behavior",
      "optimization"
    ]
  },
  {
    "name": "Meta (Facebook) Research",
    "description": "1.1B+ public FB/IG posts with engagement metrics",
    "category": "Social & Web",
    "url": "https://fort.fb.com/researcher-datasets",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "social media",
      "Facebook",
      "Instagram",
      "engagement"
    ],
    "best_for": "Learning social & web analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "social media",
      "consumer-behavior"
    ],
    "summary": "The Meta (Facebook) Research dataset comprises over 1.1 billion public posts from Facebook and Instagram, complete with engagement metrics. This extensive dataset allows researchers to analyze social media interactions, understand consumer behavior, and explore trends in engagement across different demographics and time periods.",
    "use_cases": [
      "Analyzing trends in social media engagement over time.",
      "Studying the impact of different types of content on user interaction.",
      "Exploring demographic differences in social media usage.",
      "Investigating the relationship between post characteristics and engagement metrics."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the engagement metrics for public Facebook posts?",
      "How do Instagram posts perform in terms of user interaction?",
      "What trends can be observed in social media engagement over time?",
      "How does user engagement differ between Facebook and Instagram?",
      "What demographic factors influence social media engagement?",
      "How can public posts be analyzed for consumer behavior insights?"
    ],
    "domain_tags": [
      "social media"
    ],
    "data_modality": "tabular",
    "size_category": "massive",
    "model_score": 0.0002,
    "image_url": "/images/logos/fb.png",
    "embedding_text": "The Meta (Facebook) Research dataset is a comprehensive collection of over 1.1 billion public posts from Facebook and Instagram, designed to facilitate in-depth analysis of social media engagement metrics. This dataset includes a variety of variables such as post content, engagement metrics (likes, shares, comments), and timestamps, allowing researchers to explore the dynamics of social media interactions. The data is structured in a tabular format, with rows representing individual posts and columns capturing key attributes such as post type, engagement statistics, and user demographics where available. The collection methodology involves scraping publicly available posts from Facebook and Instagram, ensuring that the dataset reflects a wide range of user-generated content across different topics and interests. However, researchers should be aware of potential limitations regarding data quality, including issues related to the representativeness of the sample, as well as the impact of platform algorithms on visibility and engagement. Common preprocessing steps may include data cleaning, normalization of engagement metrics, and categorization of post types for more nuanced analysis. This dataset supports various types of analyses, including regression models to identify factors influencing engagement, machine learning techniques for predictive modeling, and descriptive statistics to summarize user interactions. Researchers typically leverage this dataset to address questions related to consumer behavior, content effectiveness, and the evolving landscape of social media interactions. By analyzing engagement patterns, scholars can gain insights into how different demographics interact with content, the effectiveness of marketing strategies, and the overall impact of social media on public discourse.",
    "tfidf_keywords": [
      "engagement-metrics",
      "social-media-analysis",
      "consumer-behavior",
      "post-characteristics",
      "demographic-factors",
      "content-performance",
      "user-interaction",
      "trend-analysis",
      "public-posts",
      "Instagram-engagement"
    ],
    "semantic_cluster": "social-media-engagement",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "social-media-marketing",
      "data-mining",
      "consumer-insights",
      "behavioral-analysis",
      "digital-engagement"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "data-engineering",
      "statistics"
    ]
  },
  {
    "name": "Kaggle Datasets",
    "description": "50,000+ public datasets with free GPU notebooks and active ML community of 23M members",
    "category": "Dataset Aggregators",
    "url": "https://www.kaggle.com/datasets",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "ML",
      "competitions",
      "notebooks",
      "community"
    ],
    "best_for": "Applied machine learning with financial, economic, and pricing datasets",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Kaggle Datasets is a vast repository of over 50,000 public datasets, accompanied by free GPU notebooks and a vibrant machine learning community of 23 million members. This platform enables users to explore, analyze, and share datasets, fostering collaboration and innovation in data science and machine learning.",
    "use_cases": [
      "Exploring datasets for machine learning projects",
      "Participating in Kaggle competitions",
      "Collaborating with other data scientists",
      "Utilizing GPU notebooks for data analysis"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the best datasets for machine learning?",
      "How can I find public datasets for data analysis?",
      "What datasets are available for competitions on Kaggle?",
      "Where can I access free GPU notebooks for data science?",
      "How to engage with the Kaggle community?",
      "What types of datasets are available on Kaggle?",
      "How to use Kaggle datasets for machine learning projects?",
      "What are the popular datasets in the Kaggle community?"
    ],
    "domain_tags": [
      "ML",
      "Data Science"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/datasets/kaggle-datasets.png",
    "embedding_text": "Kaggle Datasets serves as a comprehensive platform for data scientists and machine learning practitioners, offering access to over 50,000 public datasets that span a wide range of topics and domains. The datasets are structured in various formats, including tabular data, images, and text, allowing for diverse analytical approaches. Users can leverage free GPU notebooks to perform computations and analyses directly on the platform, which enhances the accessibility and usability of the datasets. The collection methodology for these datasets varies, as they are contributed by a global community of data enthusiasts, researchers, and professionals, ensuring a rich and diverse array of data sources. While the datasets cover numerous topics, the quality and completeness can vary, and users are encouraged to assess the data quality and limitations before applying them to their projects. Common preprocessing steps may include handling missing values, normalizing data, and feature engineering, depending on the specific dataset and the intended analysis. Researchers typically use Kaggle Datasets to address a variety of research questions, from predictive modeling and machine learning applications to exploratory data analysis and hypothesis testing. The platform supports a wide range of analytical techniques, including regression analysis, classification, clustering, and more, making it a valuable resource for both novice and experienced data scientists. Overall, Kaggle Datasets not only provides a wealth of data but also fosters a collaborative environment where users can share insights, techniques, and results, thus contributing to the advancement of the data science field.",
    "tfidf_keywords": [
      "public datasets",
      "machine learning",
      "data analysis",
      "Kaggle competitions",
      "GPU notebooks",
      "data science community",
      "data exploration",
      "collaboration",
      "data quality",
      "feature engineering"
    ],
    "semantic_cluster": "data-science-community",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "data-exploration",
      "machine-learning",
      "collaborative-learning",
      "data-sharing",
      "predictive-modeling"
    ],
    "canonical_topics": [
      "machine-learning",
      "data-engineering",
      "consumer-behavior"
    ]
  },
  {
    "name": "Twitch Streaming Dataset",
    "description": "16 days of viewer counts, stream metadata, game categories from Oct 2017. Live streaming platform dynamics",
    "category": "Entertainment & Media",
    "url": "https://github.com/mingt2019/Twitch-Dataset",
    "docs_url": null,
    "github_url": "https://github.com/mingt2019/Twitch-Dataset",
    "tags": [
      "Twitch",
      "live streaming",
      "viewership",
      "gaming"
    ],
    "best_for": "Learning entertainment & media analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "gaming",
      "live streaming",
      "viewership"
    ],
    "summary": "The Twitch Streaming Dataset encompasses 16 days of viewer counts, stream metadata, and game categories from October 2017. This dataset allows researchers and analysts to explore the dynamics of live streaming platforms, focusing on viewer engagement and game popularity.",
    "use_cases": [
      "Analyzing trends in viewer engagement over time",
      "Comparing viewership across different game categories",
      "Investigating the impact of stream metadata on viewer counts",
      "Exploring the dynamics of live streaming platforms"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the viewer counts for different games on Twitch?",
      "How does stream metadata affect viewer engagement?",
      "What trends can be observed in Twitch streaming from October 2017?",
      "Which game categories had the highest viewership during the dataset period?",
      "How can I analyze viewer behavior on Twitch?",
      "What insights can be drawn from Twitch streaming data?",
      "How does live streaming impact gaming popularity?",
      "What are the dynamics of viewer counts on Twitch?"
    ],
    "domain_tags": [
      "entertainment",
      "media"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2017-10",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/datasets/twitch-streaming-dataset.png",
    "embedding_text": "The Twitch Streaming Dataset provides a comprehensive view of the dynamics of live streaming on the popular platform Twitch, capturing 16 days of detailed viewer counts, stream metadata, and game categories from October 2017. The dataset is structured in a tabular format, with rows representing individual streams and columns capturing key variables such as viewer counts, stream titles, game categories, and other metadata. This rich dataset allows for a variety of analyses, including time-series analysis to observe trends in viewer engagement, regression analyses to understand the factors influencing viewership, and descriptive statistics to summarize the data. The collection methodology involved aggregating data from Twitch's public API, ensuring a robust representation of streaming activity during the specified period. Key variables include viewer counts, which measure the number of concurrent viewers for each stream, and game categories, which classify the streams into different gaming genres. Researchers can use this dataset to address questions related to viewer behavior, the popularity of different games, and the overall dynamics of the live streaming ecosystem. Common preprocessing steps may include cleaning the data to handle missing values, normalizing viewer counts, and categorizing streams based on game genres. The dataset supports a range of analyses, from simple descriptive statistics to more complex machine learning models aimed at predicting viewer engagement based on stream characteristics. Overall, the Twitch Streaming Dataset serves as a valuable resource for researchers and analysts interested in understanding the factors that drive viewer engagement on live streaming platforms.",
    "tfidf_keywords": [
      "Twitch",
      "live streaming",
      "viewer counts",
      "stream metadata",
      "game categories",
      "engagement dynamics",
      "streaming trends",
      "gaming popularity",
      "data analysis",
      "viewership insights"
    ],
    "semantic_cluster": "streaming-data-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "viewer engagement",
      "streaming platforms",
      "data analysis",
      "gaming trends",
      "audience behavior"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "experimentation",
      "machine-learning",
      "statistics",
      "data-engineering"
    ]
  },
  {
    "name": "AirBnb (Inside Airbnb)",
    "description": "6M+ listings, 190M+ reviews with pricing and amenities",
    "category": "Real Estate",
    "url": "http://insideairbnb.com/explore",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Airbnb",
      "rentals",
      "pricing",
      "global"
    ],
    "best_for": "Learning real estate analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The AirBnb dataset contains over 6 million listings and more than 190 million reviews, providing insights into pricing and amenities across various global locations. Researchers can utilize this data to analyze trends in rental markets, consumer preferences, and pricing strategies.",
    "use_cases": [
      "Analyzing the impact of amenities on rental prices",
      "Examining consumer behavior through review sentiment analysis",
      "Comparing AirBnb pricing strategies across different regions",
      "Studying the correlation between rental prices and occupancy rates"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the average rental prices on AirBnb?",
      "How do amenities affect AirBnb pricing?",
      "What trends can be observed in AirBnb reviews over time?",
      "How does location influence AirBnb rental prices?",
      "What are the most common amenities offered in AirBnb listings?",
      "How do AirBnb listings compare to traditional hotel pricing?",
      "What demographic factors influence AirBnb rental choices?",
      "How has the AirBnb market changed in the last decade?"
    ],
    "domain_tags": [
      "real estate"
    ],
    "data_modality": "tabular",
    "size_category": "massive",
    "model_score": 0.0002,
    "image_url": "/images/logos/insideairbnb.png",
    "embedding_text": "The AirBnb dataset, sourced from Inside Airbnb, encompasses a vast collection of over 6 million listings and more than 190 million reviews, making it a rich resource for understanding the dynamics of the rental market. The data is structured in a tabular format, with rows representing individual listings and reviews, and columns capturing various attributes such as pricing, amenities, location, and user feedback. Key variables include rental prices, types of amenities offered, review ratings, and geographic locations, which collectively provide a comprehensive view of the rental landscape. The collection methodology involves scraping publicly available data from the AirBnb platform, ensuring a broad coverage of listings across different regions and demographics. However, it is essential to note that the dataset may have limitations regarding data quality, including potential inaccuracies in user-generated content and variations in reporting standards across different regions. Common preprocessing steps include cleaning the data to handle missing values, standardizing formats for pricing and amenities, and conducting exploratory data analysis to identify trends and patterns. Researchers can leverage this dataset to address various research questions, such as the impact of amenities on pricing, trends in consumer preferences reflected in reviews, and the influence of geographic factors on rental prices. The dataset supports a range of analyses, including regression modeling, machine learning applications, and descriptive statistics. Typically, researchers utilize this dataset to inform studies on market dynamics, consumer behavior, and pricing strategies in the evolving landscape of short-term rentals.",
    "tfidf_keywords": [
      "AirBnb",
      "rental-pricing",
      "consumer-preferences",
      "amenities-analysis",
      "review-sentiment",
      "market-trends",
      "occupancy-rates",
      "geographic-influence",
      "data-scraping",
      "listing-attributes",
      "user-generated-content",
      "data-cleaning",
      "exploratory-data-analysis",
      "machine-learning",
      "regression-modeling"
    ],
    "semantic_cluster": "marketplace-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "pricing",
      "marketplaces",
      "data-engineering",
      "sentiment-analysis"
    ],
    "canonical_topics": [
      "pricing",
      "consumer-behavior",
      "marketplaces",
      "data-engineering"
    ]
  },
  {
    "name": "Google Trends Datastore",
    "description": "Search interest data for nowcasting. Economic indicators, demand prediction, event detection",
    "category": "Social & Web",
    "url": "https://googletrends.github.io/data/",
    "docs_url": "https://developers.google.com/search/apis/trends",
    "github_url": null,
    "tags": [
      "search trends",
      "nowcasting",
      "research",
      "time-series"
    ],
    "best_for": "Learning social & web analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Google Trends Datastore provides search interest data that can be leveraged for nowcasting economic indicators, predicting demand, and detecting events. Researchers can utilize this dataset to analyze trends over time and make informed decisions based on public interest.",
    "use_cases": [
      "Analyzing consumer interest over time",
      "Predicting demand for products based on search trends",
      "Detecting events that influence economic indicators"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the latest search trends for economic indicators?",
      "How can Google Trends data be used for demand prediction?",
      "What events can be detected using search interest data?",
      "How does public interest in certain topics change over time?",
      "What are the implications of search trends on consumer behavior?",
      "How can researchers utilize Google Trends for economic analysis?"
    ],
    "domain_tags": [
      "retail",
      "finance"
    ],
    "data_modality": "time-series",
    "size_category": "medium",
    "model_score": 0.0002,
    "embedding_text": "The Google Trends Datastore is a rich repository of search interest data that serves as a valuable resource for researchers and analysts in the fields of economics and social sciences. This dataset is structured in a time-series format, where each row represents a specific time point, and columns include variables such as search terms, geographic location, and interest over time. The collection methodology involves aggregating search queries from Google, providing insights into public interest and behavior. While the dataset does not specify temporal or geographic coverage, it is widely recognized for its ability to reflect real-time trends in various sectors. Key variables within the dataset measure the frequency of searches for particular terms, which can be indicative of consumer sentiment and market dynamics. However, researchers should be aware of potential limitations, such as data quality issues stemming from variations in search volume and the influence of external events on search behavior. Common preprocessing steps may include normalizing data, handling missing values, and filtering out irrelevant search terms. The Google Trends Datastore can address a variety of research questions, including those related to consumer behavior, market trends, and economic forecasting. Analysts can employ various types of analyses, from regression models to machine learning techniques, to derive insights from the data. Researchers typically use this dataset to support studies that require an understanding of how public interest correlates with economic indicators, thereby facilitating informed decision-making in business and policy contexts.",
    "benchmark_usage": [
      "Nowcasting economic indicators",
      "Demand prediction",
      "Event detection"
    ],
    "tfidf_keywords": [
      "nowcasting",
      "search interest",
      "demand prediction",
      "event detection",
      "time-series analysis",
      "consumer sentiment",
      "market dynamics",
      "data normalization",
      "search volume",
      "public interest"
    ],
    "semantic_cluster": "economic-trend-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "forecasting",
      "market-analysis",
      "data-visualization",
      "time-series-forecasting"
    ],
    "canonical_topics": [
      "forecasting",
      "consumer-behavior",
      "econometrics"
    ]
  },
  {
    "name": "Zenodo",
    "description": "CERN-operated research data repository with DOI citations, accepts all file types up to 50GB",
    "category": "Dataset Aggregators",
    "url": "https://zenodo.org",
    "docs_url": "https://help.zenodo.org",
    "github_url": null,
    "tags": [
      "DOI",
      "CERN",
      "open science",
      "preservation"
    ],
    "best_for": "Publishing and preserving research data with persistent DOI citations",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Zenodo is a research data repository operated by CERN that allows researchers to upload and share their datasets, providing DOI citations for easy referencing. It supports a wide range of file types up to 50GB, making it a versatile tool for open science and data preservation.",
    "use_cases": [
      "Storing research data",
      "Sharing datasets with the academic community",
      "Citing datasets in research papers"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is Zenodo?",
      "How to upload datasets to Zenodo?",
      "What file types does Zenodo accept?",
      "How does Zenodo support open science?",
      "What are the benefits of using Zenodo for data preservation?",
      "How to cite datasets from Zenodo?"
    ],
    "domain_tags": [
      "open science"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/logos/zenodo.png",
    "embedding_text": "Zenodo is a comprehensive research data repository operated by CERN, designed to facilitate the sharing and preservation of research data across various disciplines. It allows researchers to upload datasets of diverse formats, accommodating file sizes up to 50GB, which is particularly beneficial for large-scale research projects. The platform is equipped with a robust DOI (Digital Object Identifier) citation system, ensuring that datasets can be easily referenced in academic publications. This feature enhances the visibility and accessibility of research outputs, promoting open science practices. Zenodo supports a wide range of file types, making it a versatile tool for researchers looking to share their work with the global academic community. The repository is structured to handle various data modalities, including tabular data, images, and other file formats, thus catering to a broad spectrum of research needs. Researchers utilizing Zenodo can expect to engage in a variety of analyses, from descriptive statistics to more complex machine learning applications, depending on the nature of their datasets. The platform's emphasis on data preservation aligns with the growing demand for transparency and reproducibility in research, making it an essential resource for scholars across fields. However, users should be aware of potential limitations regarding data quality and the need for appropriate preprocessing steps before analysis. Common preprocessing may include data cleaning, normalization, and formatting to ensure compatibility with analytical tools. Zenodo not only serves as a repository but also as a collaborative space where researchers can discover and utilize datasets shared by others, fostering a culture of knowledge sharing and innovation in research. Overall, Zenodo stands out as a pivotal resource in the landscape of open science, providing essential tools for data management, sharing, and citation.",
    "tfidf_keywords": [
      "DOI",
      "open science",
      "data preservation",
      "CERN",
      "research data",
      "dataset sharing",
      "file types",
      "repository",
      "academic citations",
      "data management"
    ],
    "semantic_cluster": "open-science-repositories",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "data-sharing",
      "research-data-management",
      "open-access",
      "academic-publishing",
      "data-citation"
    ],
    "canonical_topics": [
      "data-engineering",
      "statistics",
      "consumer-behavior"
    ]
  },
  {
    "name": "YouTube-8M",
    "description": "8M videos with video-level features for large-scale video understanding. Google Research benchmark for video classification",
    "category": "Entertainment & Media",
    "url": "https://research.google.com/youtube8m/",
    "docs_url": "https://research.google.com/youtube8m/download.html",
    "github_url": null,
    "tags": [
      "YouTube",
      "video",
      "classification",
      "benchmark",
      "Google Research"
    ],
    "best_for": "Learning entertainment & media analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "YouTube-8M is a large-scale dataset comprising 8 million videos, each with associated video-level features designed for video understanding. This dataset serves as a benchmark for video classification tasks, allowing researchers and practitioners to develop and evaluate their video classification models.",
    "use_cases": [
      "Developing video classification models",
      "Evaluating machine learning algorithms for video understanding",
      "Benchmarking video analysis techniques",
      "Conducting research in computer vision"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What features are available in the YouTube-8M dataset?",
      "How can I use YouTube-8M for video classification?",
      "What benchmarks exist for video understanding with YouTube-8M?",
      "What types of analyses can be performed on YouTube-8M?",
      "How does YouTube-8M support machine learning research?",
      "What are the key variables in the YouTube-8M dataset?",
      "What is the significance of the YouTube-8M dataset in video classification?",
      "How can I access the YouTube-8M dataset for research?"
    ],
    "domain_tags": [
      "entertainment",
      "media"
    ],
    "data_modality": "mixed",
    "size_category": "massive",
    "benchmark_usage": [
      "Video classification",
      "Large-scale video understanding"
    ],
    "model_score": 0.0002,
    "image_url": "/images/logos/google.png",
    "embedding_text": "The YouTube-8M dataset is a comprehensive collection of 8 million YouTube videos, each annotated with a variety of video-level features that facilitate large-scale video understanding. The dataset is structured with rows representing individual videos and columns capturing features such as video IDs, labels, and other metadata. This rich schema allows researchers to explore various aspects of video content and classification. The collection methodology involves scraping publicly available videos from YouTube, ensuring a diverse representation of content across different genres and topics. While the dataset does not explicitly mention temporal or geographic coverage, it encompasses a wide range of videos uploaded over several years, reflecting the dynamic nature of online video content. Key variables in the dataset include video IDs, labels indicating the content of the videos, and features that describe the visual and audio elements of the videos. Researchers have noted some limitations in data quality, particularly concerning the accuracy of labels and the presence of noise in the video features. Common preprocessing steps may include filtering out low-quality videos, normalizing features, and splitting the dataset into training and testing subsets. The YouTube-8M dataset supports a variety of research questions related to video classification, such as identifying the most effective features for classification tasks and evaluating the performance of different machine learning algorithms. Analyses can range from regression and machine learning techniques to descriptive statistics, making it a versatile resource for both academic and industry research. Researchers typically leverage this dataset to benchmark their models against established performance metrics, contributing to advancements in the field of computer vision and video analysis.",
    "tfidf_keywords": [
      "video-classification",
      "large-scale",
      "benchmark",
      "Google-Research",
      "video-understanding",
      "machine-learning",
      "computer-vision",
      "feature-extraction",
      "dataset",
      "YouTube"
    ],
    "semantic_cluster": "video-classification-benchmarking",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "computer-vision",
      "machine-learning",
      "video-analysis",
      "feature-extraction",
      "deep-learning"
    ],
    "canonical_topics": [
      "machine-learning",
      "computer-vision",
      "video-classification"
    ]
  },
  {
    "name": "Drone Delivery",
    "description": "Drone delivery logistics and operations dataset",
    "category": "Logistics & Supply Chain",
    "url": "https://tianchi.aliyun.com/dataset/89726",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "drones",
      "delivery",
      "autonomous"
    ],
    "best_for": "Learning logistics & supply chain analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "logistics",
      "supply-chain",
      "autonomous-delivery"
    ],
    "summary": "The Drone Delivery dataset provides insights into the logistics and operations involved in drone delivery systems. Researchers can analyze delivery efficiency, operational challenges, and the impact of autonomous technologies on supply chains.",
    "use_cases": [
      "Analyzing delivery times and efficiency",
      "Evaluating operational challenges in drone logistics",
      "Studying the impact of autonomous delivery on supply chains"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the logistics of drone delivery?",
      "How do drones impact supply chain efficiency?",
      "What operational challenges are faced in drone delivery?",
      "What data is available on autonomous delivery systems?",
      "How can drone delivery be optimized?",
      "What are the key metrics for evaluating drone delivery operations?"
    ],
    "domain_tags": [
      "logistics",
      "transportation"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0002,
    "embedding_text": "The Drone Delivery dataset is a comprehensive collection of data related to the logistics and operations of drone delivery systems. It encompasses various aspects of drone delivery, including operational metrics, delivery times, and logistical challenges faced by companies implementing these technologies. The dataset is structured in a tabular format, consisting of rows representing individual delivery instances and columns detailing various attributes such as delivery distance, time taken, drone specifications, and environmental conditions. Researchers can leverage this dataset to explore a multitude of research questions, such as the efficiency of drone deliveries compared to traditional methods, the impact of different variables on delivery success rates, and the overall feasibility of widespread drone adoption in urban environments. The data collection methodology typically involves gathering information from operational drone delivery services, pilot studies, and simulations, ensuring a rich and diverse dataset. However, users should be aware of potential limitations, including data quality issues stemming from varying operational standards and the evolving nature of drone technology. Common preprocessing steps may include cleaning the data for inconsistencies, normalizing variables for comparative analysis, and potentially augmenting the dataset with external data sources for a more comprehensive analysis. The dataset supports various types of analyses, including regression analysis to identify key factors influencing delivery success, machine learning models for predictive analytics, and descriptive statistics to summarize operational performance. Researchers often utilize this dataset in studies focused on optimizing delivery routes, assessing the economic viability of drone logistics, and understanding consumer behavior in response to autonomous delivery options.",
    "tfidf_keywords": [
      "drone-delivery",
      "logistics-optimization",
      "autonomous-systems",
      "delivery-efficiency",
      "operational-challenges",
      "supply-chain-management",
      "data-analysis",
      "predictive-modeling",
      "urban-logistics",
      "transportation-innovation"
    ],
    "semantic_cluster": "drone-logistics-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "logistics-optimization",
      "autonomous-vehicles",
      "supply-chain-management",
      "transportation-innovation",
      "data-analysis"
    ],
    "canonical_topics": [
      "machine-learning",
      "consumer-behavior",
      "logistics"
    ]
  },
  {
    "name": "EPA CEMS (Continuous Emissions Monitoring)",
    "description": "Hourly emissions and generation data from U.S. power plants since 1995",
    "category": "Energy",
    "url": "https://campd.epa.gov/",
    "docs_url": "https://www.epa.gov/power-sector/about-continuous-emissions-monitoring-system-cems",
    "github_url": null,
    "tags": [
      "emissions",
      "hourly",
      "CO2",
      "SO2",
      "NOx"
    ],
    "best_for": "Analyzing power plant emissions patterns and environmental impacts",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The EPA CEMS dataset provides hourly emissions and generation data from U.S. power plants dating back to 1995. This dataset can be utilized for analyzing trends in emissions over time, understanding the impact of regulations on power generation, and conducting environmental assessments.",
    "use_cases": [
      "Analyzing the impact of environmental regulations on emissions.",
      "Comparing emissions across different power plants.",
      "Studying the correlation between power generation and emissions.",
      "Evaluating trends in emissions over time."
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the hourly emissions from U.S. power plants?",
      "How has CO2 emissions changed since 1995?",
      "What is the generation data for power plants in the U.S.?",
      "How do SO2 and NOx emissions compare across different states?",
      "What trends can be observed in emissions data over the years?",
      "How does power generation relate to emissions levels?"
    ],
    "domain_tags": [
      "energy"
    ],
    "data_modality": "time-series",
    "temporal_coverage": "1995-present",
    "geographic_scope": "United States",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/logos/epa.png",
    "embedding_text": "The EPA CEMS (Continuous Emissions Monitoring System) dataset is a comprehensive collection of hourly emissions and generation data from U.S. power plants, established in 1995. This dataset encompasses a variety of key variables, including emissions of carbon dioxide (CO2), sulfur dioxide (SO2), and nitrogen oxides (NOx), alongside power generation metrics. The data is structured in a tabular format, where each row represents an hourly record for a specific power plant, and the columns include identifiers for the plant, timestamps, and various emissions measurements. The collection methodology involves continuous monitoring at power plants, ensuring high-frequency data capture that reflects real-time operational conditions. Researchers and analysts utilize this dataset to explore a range of research questions, such as the effectiveness of environmental policies, the relationship between energy production and emissions, and the identification of trends in air quality over time. Common preprocessing steps may include handling missing values, normalizing emissions data, and aggregating hourly data into daily or monthly summaries for broader analyses. The dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it a valuable resource for environmental studies, policy evaluation, and energy economics. However, users should be aware of potential limitations, such as variations in monitoring practices across states and the influence of external factors on emissions data. Overall, the EPA CEMS dataset serves as a critical tool for understanding the dynamics of emissions in the energy sector and informing policy decisions aimed at reducing environmental impacts.",
    "tfidf_keywords": [
      "emissions-monitoring",
      "power-generation",
      "environmental-regulations",
      "air-quality",
      "CO2",
      "SO2",
      "NOx",
      "data-analysis",
      "time-series",
      "energy-sector",
      "policy-evaluation",
      "environmental-impact",
      "regression-analysis",
      "data-preprocessing"
    ],
    "semantic_cluster": "energy-emissions-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "environmental-economics",
      "energy-policy",
      "data-analysis",
      "air-quality-monitoring",
      "regression-modeling"
    ],
    "canonical_topics": [
      "policy-evaluation",
      "environmental-economics",
      "statistics",
      "forecasting",
      "machine-learning"
    ]
  },
  {
    "name": "Facebook URL Shares",
    "description": "38M URLs with 10T exposure numbers, fact-checking flags, interaction types (2017-2019). Social Science One initiative",
    "category": "Social & Web",
    "url": "https://socialscience.one/our-facebook-partnership",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Facebook",
      "URL sharing",
      "misinformation",
      "fact-checking",
      "social media"
    ],
    "best_for": "Learning social & web analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Facebook URL Shares dataset contains 38 million URLs along with their exposure numbers and fact-checking flags, covering interaction types from 2017 to 2019. This dataset can be utilized to analyze social media dynamics, misinformation trends, and the effectiveness of fact-checking mechanisms.",
    "use_cases": [
      "Analyzing the impact of fact-checking on misinformation spread.",
      "Investigating trends in social media engagement over time.",
      "Examining the relationship between URL exposure and user interactions.",
      "Studying the effectiveness of different fact-checking strategies."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the most shared URLs on Facebook from 2017 to 2019?",
      "How does fact-checking influence URL sharing on Facebook?",
      "What types of interactions are most common for shared URLs?",
      "How can exposure numbers be correlated with misinformation flags?",
      "What trends can be observed in URL sharing over the specified years?",
      "How does the sharing of URLs vary by interaction type?"
    ],
    "domain_tags": [
      "social media",
      "misinformation"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2017-2019",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/datasets/facebook-url-shares.jpg",
    "embedding_text": "The Facebook URL Shares dataset is a comprehensive collection of 38 million URLs, meticulously compiled to reflect the dynamics of social media sharing on Facebook from 2017 to 2019. This dataset is part of the Social Science One initiative, which aims to provide researchers with access to rich data for analyzing social phenomena. The dataset includes key variables such as exposure numbers, which quantify the reach of each URL, and fact-checking flags that indicate whether a URL has been subjected to verification processes. The interaction types captured in the dataset provide insights into how users engage with shared content, allowing for a nuanced understanding of social media behavior. Researchers can leverage this dataset to explore various research questions, such as the effectiveness of fact-checking in curbing misinformation or the patterns of user engagement with different types of content. The data is structured in a tabular format, with rows representing individual URLs and columns capturing variables like exposure numbers, interaction types, and fact-checking status. Common preprocessing steps may include cleaning the data to remove duplicates, handling missing values, and transforming variables for analysis. Given the dataset's focus on social media interactions, it supports a range of analytical approaches, including regression analysis, machine learning models, and descriptive statistics. Researchers typically use this dataset to investigate the interplay between social media sharing, misinformation, and user engagement, contributing valuable insights to the fields of social science and digital communication.",
    "tfidf_keywords": [
      "URL sharing",
      "exposure numbers",
      "fact-checking",
      "interaction types",
      "social media dynamics",
      "misinformation trends",
      "data collection methodology",
      "social phenomena",
      "user engagement",
      "data preprocessing"
    ],
    "semantic_cluster": "social-media-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "misinformation",
      "social media analytics",
      "user behavior",
      "data-driven research",
      "fact-checking mechanisms"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "social media",
      "data-engineering",
      "policy-evaluation"
    ]
  },
  {
    "name": "MSOM Pharma Manufacturing (2024)",
    "description": "Continuous pharmaceutical manufacturing data from MSD. Real production processes for operations management research",
    "category": "Logistics & Supply Chain",
    "url": "https://pubsonline.informs.org/page/msom/data-driven-challenge",
    "docs_url": "https://pubsonline.informs.org/doi/10.1287/msom.2024.0860",
    "github_url": null,
    "tags": [
      "manufacturing",
      "operations",
      "INFORMS",
      "2024",
      "real-world",
      "competition"
    ],
    "best_for": "Learning logistics & supply chain analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The MSOM Pharma Manufacturing dataset provides continuous data on real pharmaceutical manufacturing processes from MSD, aimed at operations management research. Researchers can utilize this dataset to analyze production efficiency, operational challenges, and process optimization in the pharmaceutical sector.",
    "use_cases": [
      "Analyzing production efficiency in pharmaceutical manufacturing.",
      "Identifying operational challenges in real-world production processes.",
      "Optimizing manufacturing processes based on data insights."
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the MSOM Pharma Manufacturing dataset?",
      "How can I access continuous pharmaceutical manufacturing data from MSD?",
      "What insights can be gained from real production processes in pharma?",
      "What are the operational management applications of this dataset?",
      "How does continuous manufacturing data impact supply chain logistics?",
      "What research questions can be addressed using the MSOM Pharma Manufacturing dataset?"
    ],
    "domain_tags": [
      "healthcare",
      "pharmaceutical"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2024",
    "size_category": "medium",
    "model_score": 0.0002,
    "embedding_text": "The MSOM Pharma Manufacturing dataset is a comprehensive collection of continuous data reflecting real-world pharmaceutical manufacturing processes from MSD. This dataset is specifically designed for operations management research, providing insights into the intricacies of production workflows within the pharmaceutical industry. The data structure typically consists of rows representing individual production runs and columns detailing various operational metrics such as production time, yield rates, and equipment utilization. Each variable captures critical aspects of the manufacturing process, allowing researchers to analyze performance and identify areas for improvement. The collection methodology involves direct observation and recording of production activities, ensuring that the data reflects actual operational conditions. However, researchers should be aware of potential limitations related to data quality, such as missing values or inconsistencies that may arise from manual data entry or equipment malfunctions. Common preprocessing steps may include data cleaning, normalization, and transformation to prepare the dataset for analysis. This dataset supports a range of analytical techniques, including regression analysis, machine learning models, and descriptive statistics, enabling researchers to address various research questions related to production efficiency, operational challenges, and process optimization. Researchers typically leverage this dataset to conduct studies that inform best practices in pharmaceutical manufacturing, ultimately contributing to enhanced operational performance and competitiveness in the industry.",
    "tfidf_keywords": [
      "continuous-manufacturing",
      "pharmaceutical-operations",
      "production-efficiency",
      "real-world-data",
      "manufacturing-processes",
      "operations-management",
      "data-analysis",
      "process-optimization",
      "yield-rates",
      "equipment-utilization",
      "production-workflows",
      "data-quality",
      "preprocessing",
      "machine-learning",
      "regression-analysis"
    ],
    "semantic_cluster": "pharmaceutical-manufacturing",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "operations-management",
      "data-analysis",
      "process-optimization",
      "manufacturing-systems",
      "supply-chain-management"
    ],
    "canonical_topics": [
      "healthcare",
      "industrial-organization",
      "statistics",
      "data-engineering"
    ]
  },
  {
    "name": "LaDe Last-Mile Delivery",
    "description": "10.6M+ packages, 619k trajectories with GPS data",
    "category": "Logistics & Supply Chain",
    "url": "https://arxiv.org/html/2306.10675v2",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "last-mile",
      "GPS",
      "trajectories",
      "large-scale"
    ],
    "best_for": "Learning logistics & supply chain analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "logistics",
      "supply-chain",
      "data-analysis"
    ],
    "summary": "The LaDe Last-Mile Delivery dataset contains over 10.6 million packages and 619,000 trajectories with GPS data, providing a rich resource for analyzing last-mile logistics. Researchers can utilize this dataset to explore patterns in delivery routes, optimize logistics operations, and understand consumer behavior in the context of e-commerce.",
    "use_cases": [
      "Analyzing delivery route efficiency",
      "Optimizing logistics operations",
      "Studying consumer behavior in e-commerce"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the LaDe Last-Mile Delivery dataset?",
      "How can GPS data improve last-mile delivery efficiency?",
      "What insights can be gained from analyzing package trajectories?",
      "What are the common challenges in last-mile logistics?",
      "How does last-mile delivery impact consumer satisfaction?",
      "What are the key variables in the LaDe dataset?"
    ],
    "domain_tags": [
      "logistics",
      "e-commerce"
    ],
    "data_modality": "tabular",
    "size_category": "massive",
    "model_score": 0.0002,
    "image_url": "/images/logos/arxiv.png",
    "embedding_text": "The LaDe Last-Mile Delivery dataset is a comprehensive collection of over 10.6 million packages and 619,000 trajectories, all enriched with GPS data. This dataset is structured in a tabular format, where each row represents a unique package delivery instance, and columns include variables such as package ID, delivery time, GPS coordinates, and trajectory information. The data was collected through a combination of GPS tracking technologies and logistics management systems, ensuring a high level of detail and accuracy in the recorded trajectories. The dataset covers a wide range of last-mile delivery scenarios, making it suitable for various analyses in logistics and supply chain management. Key variables include delivery times, distances traveled, and route efficiency metrics, which can be used to measure the performance of delivery operations. However, researchers should be aware of potential limitations, such as data quality issues related to GPS inaccuracies or missing data points. Common preprocessing steps may include cleaning the data for inconsistencies, normalizing GPS coordinates, and aggregating trajectories for analysis. This dataset supports a variety of analytical approaches, including regression analysis, machine learning models, and descriptive statistics. Researchers typically use the LaDe dataset to address questions related to delivery efficiency, consumer behavior patterns, and the optimization of logistics processes. By leveraging this rich dataset, analysts can gain valuable insights into the dynamics of last-mile delivery, ultimately contributing to improved operational strategies and enhanced customer satisfaction.",
    "tfidf_keywords": [
      "last-mile delivery",
      "GPS trajectories",
      "logistics optimization",
      "delivery efficiency",
      "consumer behavior",
      "route analysis",
      "supply chain management",
      "data preprocessing",
      "trajectory data",
      "package tracking"
    ],
    "semantic_cluster": "logistics-data-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "logistics optimization",
      "supply chain analysis",
      "data visualization",
      "transportation modeling",
      "consumer behavior analysis"
    ],
    "canonical_topics": [
      "machine-learning",
      "consumer-behavior",
      "data-engineering",
      "statistics",
      "optimization"
    ]
  },
  {
    "name": "Wikipedia Full Database Dump",
    "description": "Complete Wikipedia content and metadata in SQL/XML format, includes all revisions and edit history",
    "category": "Social & Web",
    "url": "https://dumps.wikimedia.org/",
    "docs_url": "https://en.wikipedia.org/wiki/Wikipedia:Database_download",
    "github_url": null,
    "tags": [
      "database dump",
      "SQL",
      "large-scale",
      "text",
      "real-world",
      "encyclopedic"
    ],
    "best_for": "Learning social & web analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Wikipedia Full Database Dump contains the entire content and metadata of Wikipedia in SQL/XML format, including all revisions and edit history. Researchers can utilize this dataset for various analyses such as text mining, historical research, and understanding the evolution of knowledge over time.",
    "use_cases": [
      "Text mining for linguistic analysis",
      "Historical research on the evolution of topics",
      "Analysis of edit patterns and contributor behavior"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is included in the Wikipedia Full Database Dump?",
      "How can I access the complete Wikipedia content in SQL format?",
      "What types of analyses can be performed using the Wikipedia database dump?",
      "What metadata is available in the Wikipedia Full Database Dump?",
      "How does the Wikipedia database dump handle revisions and edit history?",
      "What are the benefits of using the Wikipedia Full Database Dump for research?"
    ],
    "domain_tags": [
      "education",
      "technology"
    ],
    "data_modality": "text",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/logos/wikimedia.png",
    "embedding_text": "The Wikipedia Full Database Dump is a comprehensive dataset that encompasses the entirety of Wikipedia's content and metadata, formatted in SQL/XML. This dataset includes all revisions and edit history, providing a rich resource for researchers interested in linguistic analysis, historical research, and the dynamics of knowledge evolution. The data structure consists of tables that represent articles, revisions, and user contributions, allowing for detailed examination of changes over time. Researchers can explore various key variables such as article titles, content, revision timestamps, and user IDs, which measure aspects of content creation and modification. The collection methodology involves periodic dumps of Wikipedia's database, ensuring that users have access to the most current and complete data available. However, potential limitations include the sheer volume of data, which may require significant computational resources for processing and analysis. Common preprocessing steps may include cleaning text data, normalizing revisions, and filtering for specific topics or timeframes. This dataset supports a variety of analyses, including descriptive statistics, regression modeling, and machine learning applications. Researchers typically use this dataset to address questions related to content reliability, user engagement, and the impact of edits on information dissemination. The versatility of the Wikipedia Full Database Dump makes it a valuable asset for a wide range of academic inquiries and practical applications.",
    "tfidf_keywords": [
      "Wikipedia",
      "database dump",
      "SQL",
      "XML",
      "revisions",
      "edit history",
      "text mining",
      "linguistic analysis",
      "knowledge evolution",
      "content creation"
    ],
    "semantic_cluster": "text-data-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "text-mining",
      "data-collection",
      "revision-history",
      "linguistic-analysis",
      "historical-research"
    ],
    "canonical_topics": [
      "natural-language-processing",
      "data-engineering",
      "statistics"
    ]
  },
  {
    "name": "Stack Overflow Data Dump",
    "description": "Full Q&A archive + annual developer survey (49K+ responses). Salaries, tech adoption, developer analytics",
    "category": "Social & Web",
    "url": "https://archive.org/details/stackexchange",
    "docs_url": "https://survey.stackoverflow.co/",
    "github_url": null,
    "tags": [
      "developers",
      "salaries",
      "tech",
      "survey",
      "Q&A"
    ],
    "best_for": "Learning social & web analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Stack Overflow Data Dump is a comprehensive archive of questions and answers from the Stack Overflow platform, along with an annual developer survey that includes over 49,000 responses. This dataset provides insights into developer salaries, technology adoption trends, and various analytics related to the developer community, making it a valuable resource for understanding the tech landscape.",
    "use_cases": [
      "Analyzing salary trends based on technology stacks.",
      "Investigating the relationship between developer experience and job satisfaction.",
      "Exploring common questions and answers to improve community engagement.",
      "Assessing technology adoption rates among different developer demographics."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What insights can I gain from the Stack Overflow Data Dump?",
      "How do developer salaries vary across different technologies?",
      "What are the trends in tech adoption among developers?",
      "How can I analyze developer responses from the annual survey?",
      "What types of questions are most frequently asked on Stack Overflow?",
      "How can I visualize the data from the Stack Overflow Data Dump?",
      "What demographic information is available in the developer survey?",
      "How can I use this dataset for machine learning projects?"
    ],
    "domain_tags": [
      "technology",
      "software-development",
      "education"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/logos/archive.png",
    "embedding_text": "The Stack Overflow Data Dump serves as a rich resource for researchers and practitioners interested in the software development landscape. It encompasses a full archive of questions and answers from the Stack Overflow platform, which is one of the largest online communities for developers. The dataset includes a variety of variables, such as question titles, body content, tags, and user information, allowing for extensive analysis of developer behavior and community interactions. Additionally, the annual developer survey, which features over 49,000 responses, provides insights into salaries, technology adoption, and demographic information. Researchers can leverage this dataset to address a multitude of research questions, such as examining the impact of different programming languages on salary, understanding the factors that influence technology adoption, and identifying trends in developer preferences over time. The data is structured in a tabular format, making it suitable for various analytical methods including regression analysis, machine learning, and descriptive statistics. Common preprocessing steps may include data cleaning, normalization, and transformation to prepare the data for analysis. However, users should be aware of potential limitations, such as response bias in survey data and the representativeness of the sample. Overall, the Stack Overflow Data Dump is an invaluable tool for anyone looking to gain insights into the developer community and the evolving technology landscape.",
    "tfidf_keywords": [
      "developer-survey",
      "salary-analysis",
      "technology-adoption",
      "community-engagement",
      "programming-languages",
      "data-visualization",
      "developer-demographics",
      "Q&A-archive",
      "software-development-trends",
      "user-interaction"
    ],
    "semantic_cluster": "developer-community-insights",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "data-analysis",
      "survey-research",
      "community-dynamics",
      "technology-trends",
      "developer-analytics"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "labor-economics",
      "data-engineering",
      "statistics",
      "machine-learning"
    ]
  },
  {
    "name": "French Motor TPL (freMTPL2)",
    "description": "French motor third-party liability insurance dataset with 678K policies and claims - the standard benchmark for insurance ML papers",
    "category": "Insurance & Actuarial",
    "url": "https://cran.r-project.org/package=CASdatasets",
    "docs_url": "https://freakonometrics.github.io/CASdatasets/",
    "github_url": null,
    "tags": [
      "motor-insurance",
      "claims-frequency",
      "pricing",
      "benchmark",
      "actuarial"
    ],
    "best_for": "Insurance pricing models, GLM vs ML comparisons, and actuarial research",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "insurance",
      "actuarial",
      "pricing"
    ],
    "summary": "The French Motor TPL dataset provides a comprehensive collection of motor third-party liability insurance policies and claims, consisting of 678,000 records. This dataset serves as a standard benchmark for machine learning applications in the insurance sector, enabling researchers to analyze claims frequency, pricing strategies, and actuarial assessments.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the French Motor TPL dataset?",
      "How can I analyze claims frequency in motor insurance?",
      "What machine learning techniques can be applied to insurance datasets?",
      "What are the key variables in the French Motor TPL dataset?",
      "How does the French Motor TPL dataset serve as a benchmark for insurance ML?",
      "What preprocessing steps are needed for the French Motor TPL dataset?",
      "What insights can be gained from analyzing motor insurance claims?",
      "How is pricing determined in motor insurance using this dataset?"
    ],
    "use_cases": [
      "Analyzing claims frequency trends",
      "Developing pricing models for motor insurance",
      "Benchmarking machine learning algorithms in insurance",
      "Conducting actuarial assessments based on policy data"
    ],
    "domain_tags": [
      "insurance"
    ],
    "data_modality": "tabular",
    "geographic_scope": "France",
    "size_category": "massive",
    "benchmark_usage": [
      "Standard benchmark for insurance ML papers"
    ],
    "model_score": 0.0002,
    "image_url": "/images/logos/r-project.png",
    "embedding_text": "The French Motor TPL (freMTPL2) dataset is a significant resource in the field of insurance and actuarial science, comprising 678,000 records of motor third-party liability insurance policies and claims. This dataset is structured in a tabular format, with rows representing individual insurance policies and claims, and columns capturing various attributes such as policyholder demographics, claim amounts, and policy details. Researchers and practitioners in the field of insurance can utilize this dataset to conduct a variety of analyses, including regression modeling, machine learning applications, and descriptive statistics. The dataset is particularly valuable for benchmarking purposes, as it provides a standard reference point for evaluating the performance of different machine learning algorithms in the context of insurance. The collection methodology for this dataset involves aggregating data from various insurance providers in France, ensuring a comprehensive representation of the motor insurance market. Key variables within the dataset include policyholder age, claim frequency, and claim amounts, which can be used to measure risk and inform pricing strategies. However, researchers should be aware of potential data quality issues, such as missing values or inconsistencies in reporting. Common preprocessing steps may include data cleaning, normalization, and feature engineering to prepare the dataset for analysis. The French Motor TPL dataset supports a range of research questions, such as identifying factors that influence claims frequency, evaluating the effectiveness of pricing models, and assessing the impact of demographic variables on insurance claims. Overall, this dataset serves as a crucial tool for researchers aiming to advance the understanding of motor insurance dynamics and improve predictive modeling in the insurance industry.",
    "tfidf_keywords": [
      "third-party liability",
      "motor insurance",
      "claims frequency",
      "actuarial science",
      "pricing models",
      "machine learning benchmark",
      "insurance policies",
      "data preprocessing",
      "risk assessment",
      "policyholder demographics"
    ],
    "semantic_cluster": "insurance-analytics",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "risk modeling",
      "predictive analytics",
      "insurance pricing",
      "claims analysis",
      "data science in insurance"
    ],
    "canonical_topics": [
      "machine-learning",
      "econometrics",
      "pricing",
      "consumer-behavior"
    ]
  },
  {
    "name": "Dominicks Soft Drinks",
    "description": "Weekly scanner data on soft drink purchases from Dominick's Finer Foods",
    "category": "Grocery & Supermarkets",
    "url": "https://www.chicagobooth.edu/research/kilts/research-data/dominicks",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "scanner data",
      "soft drinks",
      "Chicago Booth"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The dataset provides weekly scanner data on soft drink purchases from Dominick's Finer Foods, allowing researchers to analyze consumer purchasing patterns and trends in the soft drink market. It can be utilized for various analyses, including market basket analysis and sales forecasting.",
    "use_cases": [
      "Analyzing the impact of promotions on soft drink sales.",
      "Studying consumer behavior and preferences in soft drink purchases.",
      "Forecasting future sales trends based on historical data."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the weekly sales trends for soft drinks at Dominick's?",
      "How do consumer preferences for soft drinks vary over time?",
      "What factors influence soft drink purchases in grocery stores?",
      "How can scanner data be used to analyze pricing strategies?",
      "What is the market share of different soft drink brands at Dominick's?",
      "How do promotions affect soft drink sales in supermarkets?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Chicago",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/datasets/dominicks-soft-drinks.jpg",
    "embedding_text": "The Dominick's Soft Drinks dataset consists of weekly scanner data that captures soft drink purchases made at Dominick's Finer Foods, a prominent grocery chain. This dataset is structured in a tabular format, where each row represents a unique transaction, and columns include variables such as product identifiers, quantities sold, prices, and timestamps of purchases. The data is collected through point-of-sale systems at Dominick's locations, ensuring a comprehensive capture of consumer purchasing behavior over time. The dataset is particularly valuable for researchers interested in the grocery retail sector, as it provides insights into consumer preferences, brand performance, and the effectiveness of marketing strategies. Key variables include product categories, sales volume, and pricing, which can be analyzed to understand market dynamics and consumer trends. However, researchers should be aware of potential limitations, such as data quality issues related to incomplete transactions or variations in store-level reporting. Common preprocessing steps may include data cleaning, normalization of prices, and handling missing values. This dataset can address various research questions, such as identifying seasonal trends in soft drink consumption, evaluating the impact of pricing changes on sales, and exploring the relationship between promotional activities and consumer purchasing behavior. Analyses supported by this dataset range from descriptive statistics to regression modeling and machine learning applications, making it a versatile resource for both academic and commercial research. Researchers typically use this dataset to inform marketing strategies, optimize inventory management, and enhance understanding of consumer behavior in the beverage market.",
    "tfidf_keywords": [
      "scanner data",
      "consumer purchasing behavior",
      "soft drink market",
      "market basket analysis",
      "sales forecasting",
      "promotional impact",
      "brand performance",
      "retail analytics",
      "transaction data",
      "price elasticity"
    ],
    "semantic_cluster": "consumer-behavior-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "market basket analysis",
      "sales forecasting",
      "consumer behavior",
      "pricing strategies",
      "retail analytics"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "pricing",
      "marketplaces"
    ]
  },
  {
    "name": "Iowa Liquor",
    "description": "Monthly Class E liquor sales data with volume and pricing from Iowa",
    "category": "Grocery & Supermarkets",
    "url": "https://data.iowa.gov/Sales-Distribution/Iowa-Liquor-Sales/m3tr-qhgy",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "liquor",
      "government data",
      "Iowa"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Iowa Liquor dataset provides monthly Class E liquor sales data, detailing both volume and pricing from the state of Iowa. This dataset can be utilized to analyze consumer purchasing patterns, pricing strategies, and the economic impact of liquor sales in the region.",
    "use_cases": [
      "Analyzing the impact of pricing strategies on sales volume.",
      "Investigating seasonal trends in liquor consumption.",
      "Examining the effects of regulatory changes on sales figures."
    ],
    "audience": [
      "Curious-browser",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "What are the monthly liquor sales trends in Iowa?",
      "How do pricing changes affect liquor sales volume in Iowa?",
      "What is the impact of government regulations on liquor sales in Iowa?",
      "How does consumer behavior vary across different liquor categories in Iowa?",
      "What are the seasonal patterns in Iowa's liquor sales?",
      "How can we visualize liquor sales data from Iowa?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Iowa",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/logos/iowa.png",
    "embedding_text": "The Iowa Liquor dataset comprises monthly Class E liquor sales data, capturing both the volume of sales and the corresponding pricing information from the state of Iowa. This dataset is structured in a tabular format, with rows representing individual monthly sales records and columns detailing key variables such as sales volume, pricing, and the specific liquor categories. The data is collected from government sources, ensuring a level of reliability and accuracy, although users should be aware of potential limitations such as reporting inconsistencies or data entry errors. The dataset primarily covers sales data, allowing researchers to explore various dimensions of consumer behavior and economic activity related to liquor sales. Key variables include sales volume, which measures the quantity of liquor sold, and pricing, which reflects the retail price at which the liquor was sold. Researchers can utilize this dataset to address a range of research questions, such as the impact of pricing strategies on sales volume, seasonal trends in liquor consumption, and the effects of regulatory changes on sales figures. Common preprocessing steps may include data cleaning to handle missing values or outliers, as well as normalization of pricing data for comparative analysis. The dataset supports various types of analyses, including regression analysis to identify relationships between pricing and sales volume, as well as descriptive statistics to summarize trends over time. Researchers typically use this dataset to gain insights into consumer purchasing patterns, evaluate the effectiveness of marketing strategies, and assess the economic implications of liquor sales in Iowa.",
    "tfidf_keywords": [
      "liquor sales",
      "pricing strategies",
      "consumer behavior",
      "government data",
      "Iowa",
      "monthly sales",
      "sales volume",
      "regulatory impact",
      "seasonal trends",
      "economic analysis"
    ],
    "semantic_cluster": "liquor-sales-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "pricing",
      "economic-impact",
      "data-visualization",
      "sales-analysis"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "pricing",
      "econometrics"
    ]
  },
  {
    "name": "Israeli Grocery",
    "description": "Grocery purchase data from Israel",
    "category": "Grocery & Supermarkets",
    "url": "https://www.kaggle.com/datasets/arielpazsawicki/kimonaim",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "grocery",
      "Israel",
      "purchases"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Israeli Grocery dataset provides insights into grocery purchase behaviors in Israel. Researchers can analyze consumer spending patterns, pricing strategies, and market trends within the grocery sector.",
    "use_cases": [
      "Analyzing consumer spending patterns",
      "Evaluating pricing strategies",
      "Identifying popular grocery items",
      "Studying seasonal purchasing trends"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the grocery purchasing trends in Israel?",
      "How do prices fluctuate in the Israeli grocery market?",
      "What consumer behaviors can be observed from grocery purchase data?",
      "How does seasonality affect grocery purchases in Israel?",
      "What are the most popular grocery items in Israel?",
      "How do demographic factors influence grocery spending in Israel?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Israel",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/datasets/israeli-grocery.png",
    "embedding_text": "The Israeli Grocery dataset is a comprehensive collection of grocery purchase data from Israel, designed to provide insights into consumer behavior and market dynamics within the grocery sector. The dataset is structured in a tabular format, consisting of rows representing individual transactions and columns capturing various attributes such as item categories, prices, quantities, and timestamps of purchases. This structure allows for detailed analysis of purchasing trends and consumer preferences over time. The data is collected from various grocery stores across Israel, ensuring a diverse representation of consumer behavior in different regions and demographics. Key variables in the dataset include item descriptions, purchase amounts, and customer demographics, which can be used to measure spending patterns and preferences among different consumer segments. However, researchers should be aware of potential limitations in data quality, such as incomplete records or variations in data collection methods across stores. Common preprocessing steps may include data cleaning, normalization of item categories, and handling missing values to prepare the dataset for analysis. The dataset supports a variety of analytical approaches, including regression analysis, machine learning models, and descriptive statistics, enabling researchers to address questions related to consumer behavior, pricing strategies, and market trends. By leveraging this dataset, researchers can gain valuable insights into the grocery market in Israel, informing business strategies and policy decisions.",
    "tfidf_keywords": [
      "consumer-spending",
      "grocery-pricing",
      "purchase-patterns",
      "market-trends",
      "item-categories",
      "demographic-analysis",
      "seasonality",
      "retail-analytics",
      "transaction-data",
      "consumer-preferences"
    ],
    "semantic_cluster": "grocery-market-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "pricing-strategies",
      "market-trends",
      "retail-analytics",
      "demographic-analysis"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "pricing",
      "econometrics"
    ]
  },
  {
    "name": "YFCC100M",
    "description": "100M Flickr photos/videos with metadata under Creative Commons. Yahoo/Flickr dataset for multimedia research",
    "category": "Entertainment & Media",
    "url": "https://multimediacommons.wordpress.com/yfcc100m-core-dataset/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Flickr",
      "photos",
      "video",
      "multimedia",
      "Creative Commons"
    ],
    "best_for": "Learning entertainment & media analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The YFCC100M dataset consists of 100 million Flickr photos and videos, accompanied by metadata that is available under Creative Commons licensing. This extensive dataset is ideal for multimedia research, allowing researchers to explore various aspects of visual content, including image and video analysis, user engagement, and social media dynamics.",
    "use_cases": [
      "Analyzing user engagement with multimedia content on social platforms.",
      "Studying trends in visual media over time using the dataset."
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the YFCC100M dataset?",
      "How can I access the YFCC100M Flickr dataset?",
      "What types of media are included in the YFCC100M dataset?",
      "What research can be conducted using the YFCC100M dataset?",
      "Where can I find metadata for the YFCC100M dataset?",
      "What are the Creative Commons licenses associated with YFCC100M?"
    ],
    "domain_tags": [
      "entertainment",
      "media"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/datasets/yfcc100m.png",
    "embedding_text": "The YFCC100M dataset is a substantial collection of 100 million Flickr photos and videos, enriched with metadata that is made available under Creative Commons licenses. This dataset serves as a valuable resource for researchers and practitioners interested in multimedia analysis, social media trends, and visual content engagement. The data structure includes a variety of rows and columns that capture essential information about each media item, such as user-generated tags, descriptions, and upload dates, allowing for a comprehensive exploration of user interactions and content popularity. The collection methodology involves scraping publicly available data from the Flickr platform, ensuring a diverse representation of user-generated content. While the dataset does not specify temporal or geographic coverage, it encompasses a wide range of multimedia content that reflects global user contributions. Key variables in the dataset include image and video identifiers, user IDs, timestamps, and associated metadata, which facilitate various analyses, including descriptive statistics, regression modeling, and machine learning applications. Researchers often preprocess the data to clean and standardize the metadata, remove duplicates, and filter content based on specific criteria such as licensing or media type. Common research questions addressed using the YFCC100M dataset include examining the impact of user engagement on media popularity, exploring trends in visual content over time, and analyzing the effectiveness of different tagging strategies. The dataset supports a variety of analytical approaches, from exploratory data analysis to advanced machine learning techniques, making it a versatile tool for multimedia research. Researchers typically leverage this dataset to gain insights into user behavior, content dissemination, and the evolving landscape of digital media.",
    "tfidf_keywords": [
      "Flickr",
      "metadata",
      "Creative Commons",
      "user engagement",
      "multimedia analysis",
      "social media dynamics",
      "visual content",
      "image analysis",
      "video analysis",
      "user-generated content",
      "data scraping",
      "content popularity",
      "tagging strategies",
      "exploratory data analysis",
      "machine learning"
    ],
    "semantic_cluster": "multimedia-research",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "social-media-analysis",
      "image-processing",
      "video-content-analysis",
      "user-engagement-studies",
      "data-mining"
    ],
    "canonical_topics": [
      "machine-learning",
      "computer-vision",
      "consumer-behavior",
      "data-engineering",
      "statistics"
    ]
  },
  {
    "name": "Alibaba Industrial Dump (150GB)",
    "description": "Large-scale industrial dataset from Alibaba (150GB)",
    "category": "E-Commerce",
    "url": "https://tianchi.aliyun.com/dataset/81505",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "large-scale",
      "industrial",
      "Alibaba"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Alibaba Industrial Dump is a large-scale dataset containing various industrial data sourced from Alibaba. It can be utilized for analyzing consumer behavior, pricing strategies, and market trends within the e-commerce sector.",
    "use_cases": [
      "Analyzing consumer purchasing patterns",
      "Evaluating pricing strategies",
      "Identifying market trends",
      "Conducting regression analysis on sales data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What insights can be derived from the Alibaba Industrial Dump?",
      "How can I analyze consumer behavior using Alibaba's dataset?",
      "What pricing strategies can be evaluated with this dataset?",
      "What are the key trends in e-commerce reflected in the Alibaba Industrial Dump?",
      "How does the dataset support regression analysis?",
      "What preprocessing steps are needed for the Alibaba Industrial Dump?",
      "What machine learning models can be applied to this dataset?",
      "How can the dataset inform industrial market research?"
    ],
    "domain_tags": [
      "e-commerce"
    ],
    "data_modality": "tabular",
    "size_category": "large",
    "model_score": 0.0002,
    "embedding_text": "The Alibaba Industrial Dump is a comprehensive dataset that encompasses a wide array of industrial data provided by Alibaba, aimed at facilitating research and analysis in the e-commerce sector. The dataset is structured in a tabular format, comprising numerous rows and columns that represent various variables pertinent to industrial activities. Each row typically corresponds to a unique transaction or event, while the columns capture key attributes such as product categories, pricing information, consumer demographics, and transaction timestamps. The collection methodology for this dataset involves aggregating data from Alibaba's extensive e-commerce platform, which processes millions of transactions daily. This ensures a rich and diverse dataset that reflects real-world consumer behavior and market dynamics. However, researchers should be aware of potential limitations in data quality, such as missing values or inconsistencies that may arise from the vast scale of data collection. Common preprocessing steps include data cleaning, normalization, and feature engineering to prepare the dataset for analysis. Researchers can leverage the Alibaba Industrial Dump to address a variety of research questions, including those related to consumer behavior, pricing optimization, and market trend analysis. The dataset supports various types of analyses, including regression, machine learning, and descriptive statistics, making it a versatile resource for data scientists and economists alike. Typically, researchers utilize this dataset to uncover insights that can inform business strategies, enhance consumer engagement, and optimize pricing models in the competitive e-commerce landscape.",
    "tfidf_keywords": [
      "consumer-behavior",
      "pricing-strategies",
      "market-trends",
      "e-commerce",
      "data-cleaning",
      "feature-engineering",
      "regression-analysis",
      "machine-learning",
      "data-quality",
      "transaction-data"
    ],
    "semantic_cluster": "e-commerce-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "consumer-behavior",
      "pricing",
      "market-analysis",
      "data-preprocessing",
      "machine-learning"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "pricing",
      "marketplaces",
      "machine-learning",
      "data-engineering"
    ]
  },
  {
    "name": "M5Product",
    "description": "5 modalities (image, text, table, video, audio), 6M+ samples for multimodal learning",
    "category": "E-Commerce",
    "url": "https://xiaodongsuper.github.io/M5Product_dataset/index.html",
    "docs_url": "https://xiaodongsuper.github.io/M5Product_dataset/index.html",
    "github_url": null,
    "tags": [
      "multimodal",
      "product data",
      "images",
      "video"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The M5Product dataset comprises over 6 million samples across five modalities: image, text, table, video, and audio, specifically designed for multimodal learning in the e-commerce sector. Researchers and practitioners can utilize this dataset to train models that analyze product data, enhancing understanding of consumer behavior and improving product recommendations.",
    "use_cases": [
      "Training machine learning models for product recommendation systems",
      "Analyzing consumer behavior through multimodal data",
      "Improving product search algorithms using image and text data",
      "Developing marketing strategies based on product performance insights"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the M5Product dataset?",
      "How can I use multimodal data for product analysis?",
      "What types of samples are included in the M5Product dataset?",
      "Where can I find multimodal datasets for e-commerce?",
      "What are the applications of the M5Product dataset in machine learning?",
      "How does the M5Product dataset support multimodal learning?",
      "What modalities are present in the M5Product dataset?",
      "What insights can be gained from analyzing the M5Product dataset?"
    ],
    "domain_tags": [
      "e-commerce"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0002,
    "embedding_text": "The M5Product dataset is a comprehensive resource for researchers and practitioners in the field of e-commerce and multimodal learning. It contains over 6 million samples, encompassing five distinct modalities: image, text, table, video, and audio. This rich dataset structure allows for diverse applications in machine learning, particularly in training models that can analyze and interpret product data across different formats. The data schema includes various variables that represent product features, descriptions, and multimedia content, enabling a multifaceted approach to understanding consumer interactions with products. Collection methodologies for the M5Product dataset are not explicitly detailed, but it is likely sourced from various e-commerce platforms, aggregating product information and multimedia assets to create a robust dataset for analysis. Researchers can leverage this dataset to address a range of research questions, such as how different modalities influence consumer purchasing decisions or how to optimize product visibility through effective recommendation systems. Common preprocessing steps may include data cleaning, normalization of text and image data, and feature extraction to prepare the dataset for analysis. The dataset supports various types of analyses, including regression, machine learning, and descriptive statistics, making it a versatile tool for exploring the dynamics of e-commerce. However, users should be aware of potential limitations in data quality, such as missing values or inconsistencies in product descriptions. Overall, the M5Product dataset serves as a valuable asset for advancing research in multimodal learning and enhancing the understanding of consumer behavior in the digital marketplace.",
    "tfidf_keywords": [
      "multimodal-learning",
      "product-recommendation",
      "consumer-behavior",
      "machine-learning",
      "data-collection-methodology",
      "image-analysis",
      "text-processing",
      "e-commerce-data",
      "audio-visual-content",
      "data-preprocessing",
      "feature-extraction",
      "product-analytics",
      "market-strategy",
      "search-optimization"
    ],
    "semantic_cluster": "multimodal-e-commerce",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "machine-learning",
      "consumer-behavior",
      "data-preprocessing",
      "feature-extraction",
      "recommendation-systems"
    ],
    "canonical_topics": [
      "machine-learning",
      "consumer-behavior",
      "product-analytics",
      "recommendation-systems",
      "data-engineering"
    ]
  },
  {
    "name": "JD-pretrain-data",
    "description": "Encoded search queries and item data for intent detection",
    "category": "E-Commerce",
    "url": "https://github.com/jdcomsearch/jd-pretrain-data",
    "docs_url": null,
    "github_url": "https://github.com/jdcomsearch/jd-pretrain-data",
    "tags": [
      "search",
      "intent detection",
      "embeddings"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The JD-pretrain-data dataset contains encoded search queries and item data specifically designed for intent detection in e-commerce applications. Researchers and practitioners can utilize this dataset to train models that improve search relevance and user experience by understanding user intent more effectively.",
    "use_cases": [
      "Improving search algorithms",
      "Enhancing user experience in e-commerce",
      "Training intent detection models"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is JD-pretrain-data?",
      "How can I use JD-pretrain-data for intent detection?",
      "What types of queries are included in JD-pretrain-data?",
      "What insights can be derived from JD-pretrain-data?",
      "How does JD-pretrain-data improve e-commerce search?",
      "What are the applications of JD-pretrain-data in machine learning?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/datasets/jd-pretrain-data.png",
    "embedding_text": "The JD-pretrain-data dataset is a structured collection of encoded search queries and item data that serves as a foundational resource for intent detection in e-commerce environments. This dataset is designed to facilitate the development and training of machine learning models that can accurately interpret user intent based on search behavior. The data is organized in a tabular format, with rows representing individual search queries and corresponding items, and columns detailing various attributes such as query text, item identifiers, and encoded features that capture the semantic meaning of the queries. The collection methodology for this dataset involves aggregating search queries from e-commerce platforms, encoding them to preserve their semantic structure while ensuring privacy and compliance with data protection regulations. The dataset does not specify temporal or geographic coverage, making it versatile for various applications across different e-commerce contexts. Key variables in the dataset include the search query itself, item IDs, and encoded features that represent user intent, which can be leveraged to measure the effectiveness of search algorithms and user engagement. Data quality is maintained through rigorous preprocessing steps, including normalization of query text and encoding, although limitations may arise from the inherent biases in user-generated search data. Researchers can utilize this dataset to address research questions related to user behavior, search optimization, and model performance in intent detection. The types of analyses supported by this dataset include regression analyses to evaluate the impact of search features on user engagement, machine learning applications for classifying user intent, and descriptive analyses to uncover patterns in search behavior. Typically, researchers employ this dataset in studies aimed at enhancing search functionalities, improving recommendation systems, and understanding consumer behavior in digital marketplaces.",
    "tfidf_keywords": [
      "intent-detection",
      "e-commerce",
      "search-queries",
      "user-intent",
      "machine-learning",
      "data-encoding",
      "search-relevance",
      "consumer-behavior",
      "item-data",
      "search-optimization"
    ],
    "semantic_cluster": "intent-detection-e-commerce",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "natural-language-processing",
      "machine-learning",
      "consumer-behavior",
      "recommendation-systems",
      "search-engine-optimization"
    ],
    "canonical_topics": [
      "machine-learning",
      "natural-language-processing",
      "consumer-behavior",
      "recommendation-systems",
      "data-engineering"
    ]
  },
  {
    "name": "Alibaba Clickstream 2018",
    "description": "Clickstream data from Alibaba platforms (2018)",
    "category": "E-Commerce",
    "url": "https://tianchi.aliyun.com/dataset/56",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "clickstream",
      "Alibaba",
      "user behavior"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Alibaba Clickstream 2018 dataset contains detailed clickstream data from Alibaba platforms, capturing user interactions and behaviors throughout the year. Researchers can utilize this dataset to analyze consumer behavior patterns, optimize marketing strategies, and improve user experience on e-commerce platforms.",
    "use_cases": [
      "Analyzing user behavior trends on Alibaba platforms",
      "Optimizing product recommendations based on clickstream data",
      "Evaluating the effectiveness of marketing campaigns",
      "Studying the impact of website design on user engagement"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Alibaba Clickstream 2018 dataset?",
      "How can I access Alibaba clickstream data from 2018?",
      "What insights can be gained from Alibaba's user behavior data?",
      "What are the key variables in the Alibaba Clickstream dataset?",
      "How does Alibaba's clickstream data inform e-commerce strategies?",
      "What types of analyses can be performed on Alibaba clickstream data?"
    ],
    "domain_tags": [
      "e-commerce"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2018",
    "size_category": "medium",
    "model_score": 0.0002,
    "embedding_text": "The Alibaba Clickstream 2018 dataset is a comprehensive collection of user interaction data from Alibaba's e-commerce platforms, capturing a wide array of clickstream events throughout the year. This dataset is structured in a tabular format, with rows representing individual user interactions and columns detailing various attributes such as timestamps, user IDs, product IDs, and engagement metrics. The data is collected through Alibaba's internal tracking systems, which log user actions as they navigate through the platform, providing a rich source of information for understanding consumer behavior. Key variables in the dataset include user identifiers, session durations, product categories, and click counts, which together facilitate a nuanced analysis of user engagement and preferences. Researchers can leverage this dataset to address a variety of research questions, such as identifying patterns in user behavior, assessing the impact of marketing strategies, and exploring the relationship between user engagement and sales performance. Common preprocessing steps may include data cleaning to handle missing values, normalization of engagement metrics, and transformation of categorical variables for analysis. The dataset supports a range of analytical techniques, including regression analysis, machine learning models, and descriptive statistics, making it a versatile resource for both academic and commercial research. However, researchers should be aware of potential limitations, such as biases in user representation and the challenges of inferring causation from observational data. Overall, the Alibaba Clickstream 2018 dataset serves as a valuable tool for those looking to explore the dynamics of online consumer behavior in the rapidly evolving e-commerce landscape.",
    "tfidf_keywords": [
      "clickstream-analysis",
      "user-engagement",
      "e-commerce-strategies",
      "consumer-behavior",
      "data-preprocessing",
      "marketing-optimization",
      "session-duration",
      "product-recommendation",
      "user-interaction",
      "data-collection-methodology"
    ],
    "semantic_cluster": "e-commerce-user-behavior",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "user-experience",
      "data-analytics",
      "marketing-research",
      "behavioral-economics",
      "predictive-modeling"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "data-engineering",
      "marketplaces",
      "experimentation",
      "machine-learning"
    ]
  },
  {
    "name": "MobilityData GTFS Catalog",
    "description": "Curated directory of 1,327+ GTFS feeds from transit agencies globally with quality metrics, update frequency, and standardized metadata.",
    "category": "Transportation Economics & Technology",
    "url": "https://database.mobilitydata.org/",
    "docs_url": "https://mobilitydata.org/",
    "github_url": null,
    "tags": [
      "transit",
      "GTFS",
      "open-data",
      "schedules",
      "catalog"
    ],
    "best_for": "Finding and comparing transit data quality across cities and agencies",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "transportation",
      "economics",
      "technology"
    ],
    "summary": "The MobilityData GTFS Catalog is a comprehensive directory containing over 1,327 General Transit Feed Specification (GTFS) feeds from transit agencies around the world. This dataset provides valuable insights into public transit schedules and operational metrics, enabling users to analyze transit patterns and improve transportation planning.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the MobilityData GTFS Catalog?",
      "How many GTFS feeds are included in the MobilityData GTFS Catalog?",
      "What quality metrics are available in the MobilityData GTFS Catalog?",
      "How can I access transit schedules from the MobilityData GTFS Catalog?",
      "What types of data are included in the GTFS feeds?",
      "How frequently is the MobilityData GTFS Catalog updated?",
      "What are the benefits of using the MobilityData GTFS Catalog for transit analysis?",
      "Where can I find GTFS feeds from global transit agencies?"
    ],
    "use_cases": [
      "Analyzing public transit accessibility in urban areas",
      "Evaluating the impact of transit schedule changes on ridership",
      "Comparing the performance of different transit agencies",
      "Identifying trends in public transportation usage over time"
    ],
    "domain_tags": [
      "transportation",
      "public transit"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/logos/mobilitydata.png",
    "embedding_text": "The MobilityData GTFS Catalog serves as a curated repository of over 1,327 General Transit Feed Specification (GTFS) feeds, which are essential for understanding public transit systems globally. Each feed contains structured data that represents various aspects of transit services, including schedules, routes, stops, and fare information. The catalog is designed to facilitate access to this data, allowing researchers, developers, and transit agencies to leverage the information for a wide range of applications. The data structure typically includes rows representing individual transit trips, with columns detailing variables such as trip IDs, route IDs, stop IDs, arrival and departure times, and other relevant metrics. The collection methodology involves aggregating feeds from transit agencies worldwide, ensuring that the data is standardized and up-to-date. Users can expect to find quality metrics and update frequencies associated with each feed, which can aid in assessing the reliability and timeliness of the data. While the catalog provides a wealth of information, users should be aware of potential limitations, such as variations in data quality across different agencies and the need for preprocessing steps to clean and format the data for analysis. Common preprocessing may include handling missing values, normalizing time formats, and merging datasets for comprehensive analysis. Researchers can utilize this dataset to address questions related to transit efficiency, accessibility, and service optimization. The dataset supports various types of analyses, including descriptive statistics, regression modeling, and machine learning applications, enabling users to derive insights that can inform transportation policy and planning decisions. Overall, the MobilityData GTFS Catalog is a vital resource for anyone interested in the intersection of transportation economics and technology, providing a foundation for innovative research and practical applications in the field.",
    "tfidf_keywords": [
      "GTFS",
      "transit feeds",
      "public transportation",
      "schedules",
      "quality metrics",
      "update frequency",
      "transit agencies",
      "data standardization",
      "accessibility analysis",
      "transportation planning",
      "ridership evaluation",
      "route optimization",
      "data preprocessing",
      "urban mobility",
      "transit patterns"
    ],
    "semantic_cluster": "transit-data-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "public transit systems",
      "transportation planning",
      "data visualization",
      "urban studies",
      "mobility analytics"
    ],
    "canonical_topics": [
      "transportation",
      "consumer-behavior",
      "policy-evaluation"
    ]
  },
  {
    "name": "NREL NSRDB (National Solar Radiation Database)",
    "description": "High-resolution solar irradiance data covering the Americas with 30-minute temporal resolution",
    "category": "Energy",
    "url": "https://nsrdb.nrel.gov/",
    "docs_url": "https://nsrdb.nrel.gov/data-sets/api-instructions.html",
    "github_url": null,
    "tags": [
      "solar",
      "irradiance",
      "renewable",
      "weather",
      "API"
    ],
    "best_for": "Solar energy potential assessment and renewable energy forecasting",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The NREL NSRDB provides high-resolution solar irradiance data with a 30-minute temporal resolution, covering the Americas. This dataset can be utilized for various analyses related to solar energy production, weather modeling, and renewable energy research.",
    "use_cases": [
      "Analyzing solar energy potential for a specific region",
      "Modeling weather patterns and their impact on solar energy generation",
      "Conducting research on renewable energy trends",
      "Developing applications that utilize solar irradiance data via API"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the NREL NSRDB?",
      "How can I access solar irradiance data?",
      "What are the applications of the National Solar Radiation Database?",
      "Where can I find high-resolution solar data for the Americas?",
      "What temporal resolution does the NREL NSRDB provide?",
      "How is solar irradiance measured in the NREL NSRDB?",
      "What types of analyses can be performed with solar irradiance data?",
      "What are the key features of the NREL NSRDB?"
    ],
    "domain_tags": [
      "energy",
      "renewable"
    ],
    "data_modality": "time-series",
    "temporal_coverage": "1998-present",
    "geographic_scope": "Americas",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/datasets/nrel-nsrdb-national-solar-radiation-database.png",
    "embedding_text": "The National Renewable Energy Laboratory (NREL) National Solar Radiation Database (NSRDB) is a comprehensive dataset that provides high-resolution solar irradiance data across the Americas, with measurements taken at a 30-minute temporal resolution. This dataset is crucial for researchers, policymakers, and industry professionals interested in solar energy applications. The data structure typically includes variables such as time stamps, geographic coordinates, and solar irradiance values measured in watts per square meter. The collection methodology involves advanced satellite and ground-based measurements, ensuring high accuracy and reliability. While the dataset covers a wide geographic area, it is essential to consider the potential limitations in data quality, such as gaps in data due to cloud cover or equipment malfunctions. Common preprocessing steps may include data cleaning, normalization, and temporal alignment to prepare the data for analysis. Researchers can leverage this dataset to address various research questions, such as evaluating the feasibility of solar energy projects, understanding the impact of weather on solar generation, and optimizing solar panel placements. The types of analyses supported by this dataset range from regression modeling to machine learning applications, making it a versatile resource for advancing solar energy research and development.",
    "tfidf_keywords": [
      "solar-irradiance",
      "renewable-energy",
      "NREL",
      "high-resolution",
      "temporal-resolution",
      "data-collection",
      "satellite-measurements",
      "ground-based-measurements",
      "energy-potential",
      "weather-modeling",
      "API-access",
      "data-quality",
      "preprocessing",
      "solar-energy-applications",
      "geographic-coverage"
    ],
    "semantic_cluster": "solar-energy-data",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "renewable-energy",
      "solar-energy",
      "data-analysis",
      "weather-patterns",
      "energy-modeling"
    ],
    "canonical_topics": [
      "forecasting",
      "statistics",
      "machine-learning",
      "energy",
      "policy-evaluation"
    ]
  },
  {
    "name": "DoD National Defense Budget Estimates (Green Book)",
    "description": "Detailed U.S. defense spending by program element, military department, and appropriation from FY1945 to present",
    "category": "Defense Economics",
    "url": "https://comptroller.defense.gov/Budget-Materials/",
    "docs_url": "https://comptroller.defense.gov/Portals/45/Documents/defbudget/fy2024/FY2024_Green_Book.pdf",
    "github_url": null,
    "tags": [
      "US defense",
      "budget",
      "Pentagon",
      "appropriations"
    ],
    "best_for": "Detailed analysis of U.S. military spending by program and service branch",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The DoD National Defense Budget Estimates (Green Book) provides detailed insights into U.S. defense spending across various program elements, military departments, and appropriations from FY1945 to the present. Researchers and analysts can utilize this dataset to explore trends in defense budgeting, analyze spending patterns, and assess the implications of fiscal decisions on national security.",
    "use_cases": [
      "Analyzing historical trends in defense spending over decades.",
      "Comparing budget allocations across different military departments.",
      "Assessing the impact of budget changes on military capabilities.",
      "Evaluating the relationship between defense spending and economic indicators."
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the historical trends in U.S. defense spending?",
      "How does defense spending vary by military department?",
      "What are the appropriations for different program elements in the defense budget?",
      "How has the U.S. defense budget changed since FY1945?",
      "What are the implications of defense budget allocations on military readiness?",
      "How can we analyze the impact of defense spending on the economy?",
      "What are the key drivers of changes in the defense budget?",
      "How does the Pentagon's budget allocation reflect national security priorities?"
    ],
    "domain_tags": [
      "Defense",
      "Economics"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "FY1945 to present",
    "geographic_scope": "United States",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/logos/defense.png",
    "embedding_text": "The DoD National Defense Budget Estimates, commonly referred to as the Green Book, is a comprehensive dataset that details the U.S. defense spending landscape. This dataset encompasses a wide array of variables, including program elements, military departments, and appropriations, providing a structured view of how financial resources are allocated within the Department of Defense (DoD). The data is organized in a tabular format, with rows representing different fiscal years and columns detailing various aspects of the budget, such as total spending, departmental allocations, and specific program expenditures. Researchers interested in defense economics can leverage this dataset to conduct a variety of analyses, including historical trend analysis, comparative studies of departmental budgets, and assessments of the economic implications of defense spending. The data is collected through official government reporting and budgetary processes, ensuring a high level of accuracy and reliability. However, users should be aware of potential limitations, such as changes in accounting practices over time and the impact of inflation on budget figures. Common preprocessing steps may include normalizing data for inflation, handling missing values, and restructuring the dataset for specific analytical needs. This dataset supports a range of analytical methods, from descriptive statistics to more complex regression analyses and machine learning applications. Researchers typically use the Green Book to address questions related to fiscal policy, military readiness, and the broader economic impacts of defense spending, making it a vital resource for scholars and analysts in the field of defense economics.",
    "tfidf_keywords": [
      "defense budget",
      "appropriations",
      "military spending",
      "program elements",
      "fiscal policy",
      "national security",
      "economic impact",
      "historical trends",
      "budget allocation",
      "Department of Defense",
      "defense economics",
      "financial resources",
      "spending patterns",
      "budget analysis"
    ],
    "semantic_cluster": "defense-budget-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "fiscal policy",
      "defense economics",
      "budget analysis",
      "national security",
      "economic impact"
    ],
    "canonical_topics": [
      "econometrics",
      "policy-evaluation",
      "finance"
    ]
  },
  {
    "name": "Yelp Dataset",
    "description": "Business attributes, reviews, user data, and check-ins",
    "category": "Social & Web",
    "url": "https://www.yelp.com/dataset",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "reviews",
      "businesses",
      "local",
      "NLP"
    ],
    "best_for": "Learning social & web analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "consumer-behavior",
      "local-business",
      "NLP"
    ],
    "summary": "The Yelp Dataset contains a wealth of information about businesses, user reviews, and check-in data. Researchers can utilize this dataset to analyze consumer behavior, local business dynamics, and sentiment analysis through natural language processing techniques.",
    "use_cases": [
      "Analyzing consumer sentiment towards local businesses",
      "Examining the relationship between business attributes and user ratings",
      "Studying trends in user engagement through check-ins",
      "Exploring the impact of reviews on business performance"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the most reviewed businesses in the Yelp dataset?",
      "How do user ratings correlate with the number of reviews?",
      "What trends can be observed in check-in data over time?",
      "How does sentiment in reviews vary by business category?",
      "What are common attributes of highly rated businesses?",
      "How can NLP be applied to analyze user reviews?"
    ],
    "domain_tags": [
      "retail",
      "hospitality",
      "food-service"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/datasets/yelp-dataset.jpg",
    "embedding_text": "The Yelp Dataset is a comprehensive collection of data that encompasses various aspects of local businesses, user reviews, and check-in activities. It is structured in a tabular format, containing rows that represent individual businesses, users, and reviews, with columns detailing attributes such as business name, location, categories, user ratings, review text, and timestamps of check-ins. This dataset is particularly valuable for researchers and data scientists interested in understanding consumer behavior and local market dynamics. The data is typically collected from the Yelp platform, which aggregates user-generated content and business information, making it a rich source for analysis. Coverage includes a diverse range of businesses across various geographic locations, although specific temporal and geographic details are not explicitly mentioned. Key variables within the dataset include user ratings, which measure customer satisfaction, review text that provides insights into consumer sentiment, and check-in data that reflects user engagement with businesses. However, researchers should be aware of potential limitations, such as biases in user-generated content and the representativeness of the data. Common preprocessing steps may include text normalization for review analysis, handling missing values, and encoding categorical variables for machine learning applications. The dataset supports a variety of research questions, such as exploring the factors that influence consumer ratings, the impact of reviews on business success, and trends in user engagement over time. Analyses can range from descriptive statistics to more complex regression models and machine learning techniques, making it a versatile tool for academic studies and business applications. Researchers often employ this dataset to conduct sentiment analysis, market trend studies, and to develop recommendation systems, thereby contributing to a deeper understanding of the local business landscape and consumer preferences.",
    "tfidf_keywords": [
      "user-generated-content",
      "sentiment-analysis",
      "business-attributes",
      "check-in-data",
      "local-market-dynamics",
      "consumer-satisfaction",
      "NLP-techniques",
      "review-text",
      "data-preprocessing",
      "business-performance"
    ],
    "semantic_cluster": "consumer-behavior-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "sentiment-analysis",
      "market-research",
      "data-mining",
      "natural-language-processing",
      "user-engagement"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "natural-language-processing",
      "marketplaces"
    ]
  },
  {
    "name": "DataCo Supply Chain",
    "description": "Synthetic supply chain dataset covering sales and returns",
    "category": "Logistics & Supply Chain",
    "url": "https://tianchi.aliyun.com/dataset/89959",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "supply chain",
      "synthetic",
      "returns"
    ],
    "best_for": "Learning logistics & supply chain analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "supply chain",
      "returns",
      "synthetic data"
    ],
    "summary": "The DataCo Supply Chain dataset is a synthetic dataset that simulates various aspects of a supply chain, including sales and returns. It can be utilized for analyzing supply chain dynamics, optimizing inventory management, and understanding consumer return behavior.",
    "use_cases": [
      "Analyzing sales trends in supply chains",
      "Optimizing return processes",
      "Simulating supply chain scenarios",
      "Evaluating the impact of returns on inventory"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the DataCo Supply Chain dataset?",
      "How can I analyze sales and returns in supply chains?",
      "What insights can be gained from synthetic supply chain data?",
      "How does synthetic data help in supply chain analysis?",
      "What are the key variables in the DataCo Supply Chain dataset?",
      "How can I visualize supply chain data?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0002,
    "embedding_text": "The DataCo Supply Chain dataset is a synthetic dataset designed to provide insights into the dynamics of supply chains, specifically focusing on sales and returns. This dataset typically consists of multiple rows and columns, where each row represents a unique transaction or event within the supply chain, and the columns include variables such as product ID, transaction date, sales amount, return status, and customer ID. The synthetic nature of the data allows researchers and practitioners to explore various scenarios without the constraints of real-world data limitations. The collection methodology for this dataset involves generating data based on statistical models that reflect realistic supply chain behaviors, ensuring that the dataset maintains a level of complexity that mirrors actual supply chain operations. However, as a synthetic dataset, it is important to note that while it can provide valuable insights, it may not capture all the nuances of real-world data, leading to potential limitations in the applicability of findings derived from it. Common preprocessing steps for this dataset may include data cleaning, normalization, and transformation to prepare the data for analysis. Researchers can utilize this dataset to address various research questions, such as understanding the factors influencing return rates, analyzing the impact of returns on overall sales, and evaluating inventory management strategies. The types of analyses supported by this dataset include regression analysis, machine learning models, and descriptive statistics, making it a versatile tool for both exploratory and confirmatory research in the field of logistics and supply chain management. Typically, researchers use this dataset to simulate different supply chain scenarios, test hypotheses regarding consumer behavior, and develop strategies for optimizing supply chain operations.",
    "tfidf_keywords": [
      "supply chain",
      "synthetic data",
      "returns analysis",
      "inventory management",
      "consumer behavior",
      "sales trends",
      "transaction data",
      "data simulation",
      "operational efficiency",
      "logistics optimization"
    ],
    "semantic_cluster": "supply-chain-analytics",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "inventory-management",
      "consumer-behavior",
      "data-simulation",
      "operational-efficiency",
      "logistics"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "statistics",
      "machine-learning",
      "optimization"
    ]
  },
  {
    "name": "Wikipedia Pageviews",
    "description": "296B views/year since 2007. Hourly pageview data for all Wikimedia projects. attention metrics at scale",
    "category": "Social & Web",
    "url": "https://dumps.wikimedia.org/other/pageviews/",
    "docs_url": "https://dumps.wikimedia.org/other/pageviews/readme.html",
    "github_url": null,
    "tags": [
      "Wikipedia",
      "pageviews",
      "attention",
      "time-series",
      "large-scale"
    ],
    "best_for": "Learning social & web analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "Wikipedia",
      "pageviews",
      "attention",
      "time-series",
      "large-scale"
    ],
    "summary": "The Wikipedia Pageviews dataset provides hourly pageview data for all Wikimedia projects, offering insights into user engagement and attention metrics at a large scale. Researchers can analyze trends over time and explore factors influencing pageviews across various topics.",
    "use_cases": [
      "Trend analysis of pageviews over time",
      "Comparative analysis of pageviews across different Wikimedia projects",
      "Predictive modeling of future pageviews",
      "Exploring the impact of external events on pageviews"
    ],
    "audience": [
      "Curious-browser",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "What are the trends in Wikipedia pageviews over time?",
      "How do pageviews vary by topic?",
      "What factors influence pageview spikes?",
      "Can we predict future pageviews based on historical data?",
      "How do different Wikimedia projects compare in terms of pageviews?",
      "What is the relationship between pageviews and user engagement?"
    ],
    "domain_tags": [
      "media",
      "education"
    ],
    "data_modality": "time-series",
    "temporal_coverage": "2007-2023",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/logos/wikimedia.png",
    "embedding_text": "The Wikipedia Pageviews dataset is a comprehensive collection of hourly pageview data for all Wikimedia projects, capturing a staggering 296 billion views annually since 2007. This dataset is structured in a tabular format, with rows representing individual hourly records and columns detailing key variables such as project name, timestamp, and the number of pageviews. The data is collected directly from Wikimedia's servers, ensuring high accuracy and reliability. However, researchers should be aware of potential limitations, such as missing data during server outages or discrepancies in pageview counts due to bot activity. Common preprocessing steps include aggregating data to daily or weekly totals, handling missing values, and normalizing pageview counts for comparative analysis. This dataset supports a variety of research questions, including the exploration of temporal trends in pageviews, the impact of specific events on user engagement, and the comparative analysis of different Wikimedia projects. Analysts can employ various methodologies, including regression analysis, machine learning techniques, and descriptive statistics, to derive insights from the data. Researchers typically use this dataset to study user behavior, assess the impact of content changes, and evaluate the effectiveness of outreach efforts in driving traffic to Wikipedia pages.",
    "tfidf_keywords": [
      "pageviews",
      "Wikimedia",
      "user engagement",
      "time-series analysis",
      "data aggregation",
      "predictive modeling",
      "trend analysis",
      "external events",
      "content impact",
      "data preprocessing"
    ],
    "semantic_cluster": "web-traffic-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "user-engagement",
      "time-series-analysis",
      "data-visualization",
      "web-analytics",
      "trend-analysis"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "data-engineering",
      "statistics"
    ]
  },
  {
    "name": "Pushshift Reddit Archive",
    "description": "5.6B comments, 651M posts since 2005. Full Reddit history for social/economic research. 100+ papers published",
    "category": "Social & Web",
    "url": "https://arxiv.org/abs/2001.08435",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Reddit",
      "social media",
      "comments",
      "NLP",
      "large-scale"
    ],
    "best_for": "Learning social & web analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "natural-language-processing"
    ],
    "topic_tags": [
      "social media",
      "text analysis",
      "economic research"
    ],
    "summary": "The Pushshift Reddit Archive is a comprehensive dataset containing 5.6 billion comments and 651 million posts from Reddit since 2005. It serves as a valuable resource for social and economic research, enabling researchers to analyze trends, sentiments, and behaviors within the Reddit community.",
    "use_cases": [
      "Sentiment analysis of Reddit comments",
      "Trend analysis of social issues over time",
      "NLP applications for text classification",
      "Economic behavior analysis based on user discussions"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What is the Pushshift Reddit Archive?",
      "How can I access Reddit data for analysis?",
      "What types of analyses can be performed on Reddit comments?",
      "What insights can be gained from Reddit posts?",
      "How has Reddit content evolved over time?",
      "What are the key trends in social media discussions?"
    ],
    "domain_tags": [
      "social media",
      "economics"
    ],
    "data_modality": "text",
    "size_category": "massive",
    "benchmark_usage": [
      "Used in over 100 published papers for social and economic research"
    ],
    "model_score": 0.0002,
    "image_url": "/images/datasets/pushshift-reddit-archive.png",
    "embedding_text": "The Pushshift Reddit Archive is an extensive dataset that captures the full history of Reddit, comprising 5.6 billion comments and 651 million posts since its inception in 2005. This dataset is structured primarily as a text corpus, with each entry representing a comment or post made by users on the platform. The data is organized into rows and columns, where each row corresponds to a unique comment or post, and columns typically include variables such as user ID, timestamp, subreddit, comment text, and metadata related to the engagement of the post (e.g., upvotes, downvotes). The collection methodology involves scraping Reddit\u2019s public API and storing the data in a structured format, allowing for efficient querying and analysis. Researchers can leverage this dataset to explore various social and economic phenomena, such as public sentiment on current events, the impact of social media on consumer behavior, and the dynamics of online discussions. Key variables in the dataset include the content of comments and posts, which can be analyzed for sentiment, topic modeling, and user engagement metrics. However, researchers should be aware of potential data quality issues, such as missing values or biases in user-generated content. Common preprocessing steps may include text normalization, tokenization, and the removal of stop words to prepare the data for analysis. The dataset supports a wide range of analyses, including regression modeling, machine learning applications, and descriptive statistics. Researchers typically use this dataset to address research questions related to social trends, economic behavior, and the influence of online discourse on real-world events. Overall, the Pushshift Reddit Archive is a powerful tool for understanding the complexities of social interactions and economic discussions in the digital age.",
    "temporal_coverage": "2005-2023",
    "tfidf_keywords": [
      "Reddit",
      "comments",
      "posts",
      "social media",
      "NLP",
      "text analysis",
      "user engagement",
      "sentiment analysis",
      "data scraping",
      "public discourse"
    ],
    "semantic_cluster": "social-media-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "natural-language-processing",
      "sentiment-analysis",
      "social-network-analysis",
      "text-mining",
      "behavioral-economics"
    ],
    "canonical_topics": [
      "natural-language-processing",
      "consumer-behavior",
      "econometrics"
    ]
  },
  {
    "name": "USAspending.gov",
    "description": "User-friendly interface to federal spending data with bulk downloads in CSV/JSON and visualization tools",
    "category": "Defense Economics",
    "url": "https://www.usaspending.gov/",
    "docs_url": "https://api.usaspending.gov/",
    "github_url": "https://github.com/fedspendingtransparency/usaspending-api",
    "tags": [
      "spending",
      "contracts",
      "grants",
      "government"
    ],
    "best_for": "Accessible exploration of federal defense spending without FPDS complexity",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "USAspending.gov provides a user-friendly interface to access federal spending data, allowing users to explore various spending categories such as contracts and grants. The platform offers bulk downloads in CSV and JSON formats, along with visualization tools to help users analyze and interpret the data effectively.",
    "use_cases": [
      "Analyzing trends in federal spending over time",
      "Comparing spending across different government contracts and grants"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is USAspending.gov?",
      "How can I access federal spending data?",
      "What types of data are available on USAspending.gov?",
      "Can I download data from USAspending.gov in CSV format?",
      "What visualization tools does USAspending.gov offer?",
      "How does USAspending.gov categorize federal spending?",
      "What are the main features of USAspending.gov?",
      "How can I analyze federal contracts and grants using USAspending.gov?"
    ],
    "domain_tags": [
      "government"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2000-present",
    "geographic_scope": "United States",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/datasets/usaspendinggov.png",
    "embedding_text": "USAspending.gov serves as a comprehensive platform for accessing federal spending data, designed to enhance transparency and facilitate public engagement with government financial information. The dataset primarily consists of tabular data structured in rows and columns, where each row represents a unique spending entry, and columns include variables such as spending category, amount, recipient, and date. The collection methodology involves aggregating data from various federal agencies, ensuring a wide coverage of spending activities across different sectors. While the dataset does not explicitly mention temporal or geographic coverage, it encompasses a broad range of federal spending activities, making it a valuable resource for researchers and analysts interested in government finance. Key variables within the dataset measure aspects of federal contracts and grants, providing insights into how taxpayer money is allocated and spent. However, users should be aware of potential limitations regarding data quality, as discrepancies may exist between reported figures and actual expenditures. Common preprocessing steps may include data cleaning, normalization, and transformation to prepare the data for analysis. Researchers can leverage this dataset to address a variety of research questions, such as examining the impact of federal spending on local economies, analyzing spending patterns across different government sectors, or evaluating the effectiveness of grant programs. The dataset supports various types of analyses, including descriptive statistics, regression analysis, and machine learning applications, allowing users to derive meaningful insights from the data. Typically, researchers utilize USAspending.gov in studies focused on public policy evaluation, economic impact assessments, and accountability in government spending.",
    "tfidf_keywords": [
      "federal-spending",
      "government-contracts",
      "grants",
      "data-visualization",
      "transparency",
      "public-finance",
      "spending-analysis",
      "budget-allocation",
      "data-access",
      "CSV-downloads",
      "government-data",
      "spending-patterns",
      "economic-impact",
      "policy-evaluation"
    ],
    "semantic_cluster": "government-spending-data",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "public-policy",
      "economic-analysis",
      "data-visualization",
      "government-accountability",
      "budgeting"
    ],
    "canonical_topics": [
      "policy-evaluation",
      "econometrics",
      "finance"
    ],
    "benchmark_usage": [
      "Exploring federal spending data",
      "Visualizing government contracts and grants"
    ]
  },
  {
    "name": "National Transit Database (NTD)",
    "description": "Definitive source for US transit statistics since 2002. Ridership, operating expenses, capital expenses, safety incidents for all federally-funded transit agencies.",
    "category": "Transportation Economics & Technology",
    "url": "https://www.transit.dot.gov/ntd",
    "docs_url": "https://www.transit.dot.gov/ntd/ntd-data",
    "github_url": null,
    "tags": [
      "transit",
      "ridership",
      "financials",
      "safety",
      "US"
    ],
    "best_for": "Transit system performance analysis, cost benchmarking, and ridership trends",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "transportation",
      "economics",
      "public policy"
    ],
    "summary": "The National Transit Database (NTD) serves as the definitive source for US transit statistics, providing comprehensive data on ridership, operating expenses, capital expenses, and safety incidents across federally-funded transit agencies. Researchers and policymakers can utilize this dataset to analyze trends in public transportation, assess financial performance, and evaluate safety measures in transit systems.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the latest ridership statistics from the National Transit Database?",
      "How do operating expenses vary among different US transit agencies?",
      "What trends can be observed in capital expenses for public transit since 2002?",
      "How many safety incidents have been reported in federally-funded transit agencies?",
      "What is the overall financial performance of US transit systems?",
      "How does ridership correlate with operating expenses in public transit?",
      "What are the key metrics available in the National Transit Database?",
      "How has public transit ridership changed over the years?"
    ],
    "use_cases": [
      "Analyzing trends in public transit ridership over time.",
      "Evaluating the financial health of transit agencies through operating and capital expenses.",
      "Assessing the impact of safety incidents on public perception and ridership.",
      "Comparing financial and operational metrics across different transit agencies."
    ],
    "domain_tags": [
      "transportation",
      "public policy"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2002-present",
    "geographic_scope": "United States",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/datasets/national-transit-database-ntd.png",
    "embedding_text": "The National Transit Database (NTD) is a comprehensive dataset that serves as the definitive source for US transit statistics, established in 2002. It encompasses a wide array of data, including ridership figures, operating expenses, capital expenses, and safety incidents for all federally-funded transit agencies across the United States. The data is structured in a tabular format, with rows representing individual transit agencies and columns detailing various metrics such as total ridership, operational costs, capital investments, and the number of reported safety incidents. This dataset is invaluable for researchers, policymakers, and transit authorities, providing insights into the performance and safety of public transportation systems. The collection methodology involves gathering data from transit agencies that receive federal funding, ensuring a standardized approach to reporting. Coverage includes temporal data from 2002 to the present, allowing for longitudinal studies on trends and changes in public transit. Geographic coverage is nationwide, focusing on transit systems across the United States. Key variables in the dataset measure critical aspects of transit operations, including ridership trends, financial health through operating and capital expenses, and safety metrics that can inform policy decisions. While the NTD is a rich source of information, researchers should be aware of potential limitations, such as variations in reporting practices among agencies and the impact of external factors on ridership and expenses. Common preprocessing steps may include data cleaning to address missing values, normalization of financial figures, and aggregation of data for comparative analysis. The NTD supports various types of analyses, including regression models to explore relationships between ridership and financial metrics, machine learning applications for predictive modeling, and descriptive statistics to summarize trends over time. Researchers typically use this dataset to address research questions related to the efficiency and effectiveness of public transit systems, the economic impact of transit on communities, and the evaluation of safety measures in public transportation. Overall, the National Transit Database is a critical resource for understanding the dynamics of public transit in the United States and informing future policy and operational decisions.",
    "benchmark_usage": [
      "Commonly used for transportation policy analysis and economic evaluations."
    ],
    "tfidf_keywords": [
      "ridership",
      "operating expenses",
      "capital expenses",
      "safety incidents",
      "federally-funded transit",
      "public transportation",
      "transit agencies",
      "financial performance",
      "data collection methodology",
      "temporal trends",
      "geographic coverage",
      "data quality",
      "preprocessing steps",
      "longitudinal studies",
      "policy evaluation"
    ],
    "semantic_cluster": "transportation-data-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "public transportation",
      "economic evaluation",
      "safety analysis",
      "financial metrics",
      "policy analysis"
    ],
    "canonical_topics": [
      "econometrics",
      "policy-evaluation",
      "consumer-behavior"
    ]
  },
  {
    "name": "Amazon Reviews 2023",
    "description": "233 million product reviews across all Amazon categories with user IDs, timestamps, ratings, and review text. The largest public e-commerce review dataset for recommendation research.",
    "category": "MarTech & Customer Analytics",
    "url": "https://amazon-reviews-2023.github.io/",
    "docs_url": "https://amazon-reviews-2023.github.io/",
    "github_url": "https://github.com/hyp1231/AmazonReviews2023",
    "tags": [
      "recommendations",
      "e-commerce",
      "reviews",
      "NLP",
      "sentiment"
    ],
    "best_for": "Large-scale recommendation systems, sentiment analysis, and cross-domain recommendations",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "NLP",
      "sentiment"
    ],
    "summary": "The Amazon Reviews 2023 dataset contains 233 million product reviews from various Amazon categories, featuring user IDs, timestamps, ratings, and review text. This extensive dataset is ideal for conducting recommendation research, sentiment analysis, and exploring consumer behavior in e-commerce.",
    "use_cases": [
      "Sentiment analysis of product reviews to gauge customer satisfaction.",
      "Recommendation system development based on user reviews and ratings.",
      "Trend analysis of consumer preferences across different product categories.",
      "Exploring the relationship between review length and rating scores."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the key trends in Amazon product reviews?",
      "How can sentiment analysis be applied to Amazon reviews?",
      "What factors influence customer ratings on Amazon?",
      "How do product reviews impact purchasing decisions?",
      "What insights can be gained from analyzing Amazon reviews?",
      "How can NLP techniques be used to analyze e-commerce reviews?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "text",
    "size_category": "massive",
    "model_score": 0.0002,
    "image_url": "/images/datasets/amazon-reviews-2023.png",
    "embedding_text": "The Amazon Reviews 2023 dataset is a comprehensive collection of 233 million product reviews spanning various categories on Amazon. This dataset includes essential variables such as user IDs, timestamps, ratings, and review text, providing a rich resource for researchers and practitioners interested in e-commerce analytics. The data is structured in a tabular format, where each row corresponds to an individual review, and the columns represent the key attributes of the reviews. Researchers can leverage this dataset to conduct various analyses, including regression, machine learning, and descriptive statistics, to uncover insights into consumer behavior and product performance. The collection methodology involves aggregating reviews from the Amazon platform, ensuring a diverse representation of products and user experiences. However, researchers should be aware of potential data quality issues, such as duplicate reviews or biased ratings, which may affect the validity of their findings. Common preprocessing steps include cleaning the text data, handling missing values, and normalizing ratings for comparative analysis. This dataset supports a wide range of research questions, from understanding the factors that influence customer ratings to exploring the impact of reviews on purchasing decisions. By utilizing natural language processing techniques, researchers can extract sentiment and thematic insights from the review text, making it a valuable tool for developing recommendation systems and enhancing customer engagement strategies.",
    "tfidf_keywords": [
      "product-reviews",
      "sentiment-analysis",
      "recommendation-systems",
      "NLP",
      "consumer-behavior",
      "e-commerce",
      "data-quality",
      "text-mining",
      "user-experience",
      "rating-scores"
    ],
    "semantic_cluster": "e-commerce-reviews-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "sentiment-analysis",
      "recommendation-systems",
      "text-mining",
      "consumer-behavior",
      "natural-language-processing"
    ],
    "canonical_topics": [
      "recommendation-systems",
      "natural-language-processing",
      "consumer-behavior"
    ],
    "benchmark_usage": [
      "Recommendation research",
      "Sentiment analysis",
      "Consumer behavior studies"
    ]
  },
  {
    "name": "World Bank Open Data",
    "description": "1,400+ development indicators for 217 economies spanning 50+ years with free API",
    "category": "Dataset Aggregators",
    "url": "https://data.worldbank.org",
    "docs_url": "https://datahelpdesk.worldbank.org",
    "github_url": null,
    "tags": [
      "development",
      "international",
      "poverty",
      "GDP",
      "API"
    ],
    "best_for": "Cross-country comparisons in development economics and poverty research",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The World Bank Open Data provides access to over 1,400 development indicators for 217 economies, covering more than 50 years of data. This dataset allows users to explore various socio-economic metrics, enabling analysis of trends in poverty, GDP, and other critical development factors through a free API.",
    "use_cases": [
      "Analyzing trends in GDP across different countries over time.",
      "Investigating the relationship between poverty levels and education indicators."
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the development indicators available in the World Bank Open Data?",
      "How can I access the World Bank Open Data API?",
      "What economies are covered in the World Bank Open Data?",
      "What is the time span of the data provided by the World Bank Open Data?",
      "How does the World Bank measure poverty in its dataset?",
      "What types of analyses can be performed using the World Bank Open Data?"
    ],
    "domain_tags": [
      "international",
      "development"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "temporal_coverage": "50+ years",
    "geographic_scope": "217 economies",
    "model_score": 0.0002,
    "image_url": "/images/datasets/world-bank-open-data.svg",
    "embedding_text": "The World Bank Open Data is a comprehensive resource that offers a wealth of information on development indicators across a wide range of economies. With over 1,400 indicators available, researchers and policymakers can access data spanning more than 50 years, making it an invaluable tool for longitudinal studies. The dataset is structured in a tabular format, where each row represents a unique observation corresponding to a specific indicator for a particular economy and year. The columns typically include variables such as country codes, indicator names, values, and years, allowing for easy filtering and analysis. The data is collected through various methodologies, including surveys, administrative records, and other official statistics, ensuring a robust foundation for the indicators provided. However, users should be aware of potential limitations, such as differences in data collection methods across countries and changes in definitions over time, which may affect comparability. Common preprocessing steps might include handling missing values, normalizing data, and converting formats for analysis. Researchers often utilize this dataset to address critical questions related to economic development, such as the impact of education on economic growth or the effectiveness of poverty alleviation programs. The dataset supports a variety of analytical techniques, including regression analysis, machine learning, and descriptive statistics, making it versatile for different research needs. Overall, the World Bank Open Data serves as a crucial resource for anyone interested in understanding global development trends and their implications.",
    "tfidf_keywords": [
      "development-indicators",
      "socio-economic-metrics",
      "poverty-analysis",
      "GDP-trends",
      "longitudinal-data",
      "international-statistics",
      "data-collection-methodologies",
      "economic-research",
      "policy-analysis",
      "API-access"
    ],
    "semantic_cluster": "economic-development-data",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "economic-development",
      "poverty-research",
      "socio-economic-analysis",
      "international-economics",
      "data-analysis"
    ],
    "canonical_topics": [
      "econometrics",
      "statistics",
      "policy-evaluation"
    ]
  },
  {
    "name": "Web Log Dataset",
    "description": "Web server traffic logs for analyzing request patterns and server queue dynamics.",
    "category": "Technology & Infrastructure",
    "url": "https://www.kaggle.com/datasets/shawon10/web-log-dataset",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "queueing",
      "web-logs",
      "traffic",
      "requests",
      "server",
      "Kaggle"
    ],
    "best_for": "Queueing theory - traffic analysis, identify bursts deviating from Poisson, capacity planning",
    "image_url": "/images/datasets/web-log-dataset.png",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Web Log Dataset comprises web server traffic logs that can be utilized to analyze request patterns and server queue dynamics. Researchers can leverage this dataset to understand user behavior, optimize server performance, and improve web application responsiveness.",
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the request patterns in the Web Log Dataset?",
      "How can server queue dynamics be analyzed using web logs?",
      "What insights can be gained from web server traffic logs?",
      "How does traffic volume affect server performance?",
      "What are the common requests logged in web server traffic?",
      "How can this dataset be used for performance optimization?"
    ],
    "use_cases": [
      "Analyzing user behavior on a website",
      "Optimizing server performance based on traffic patterns",
      "Identifying peak traffic times and their impact on server load"
    ],
    "embedding_text": "The Web Log Dataset consists of detailed web server traffic logs that capture the interactions between users and web applications. The data structure typically includes rows representing individual requests, with columns detailing various attributes such as timestamps, request types, response times, and user identifiers. This dataset is invaluable for researchers and practitioners looking to analyze request patterns and server queue dynamics. The collection methodology usually involves logging requests made to a web server, capturing essential metrics that reflect user engagement and server performance. While the dataset does not specify temporal or geographic coverage, it is generally applicable to any web-based application environment. Key variables in the dataset measure aspects such as request frequency, response times, and user session durations, which can provide insights into user behavior and system efficiency. However, known limitations may include incomplete logs due to server outages or misconfigurations, which can affect data quality. Common preprocessing steps may involve cleaning the data to remove irrelevant entries, normalizing timestamps, and aggregating requests by time intervals to facilitate analysis. Researchers can address various research questions with this dataset, such as identifying trends in user behavior, understanding the impact of traffic on server load, and optimizing resource allocation based on usage patterns. The dataset supports various types of analyses, including regression analysis to model relationships between request patterns and server performance, machine learning techniques for predictive modeling, and descriptive statistics to summarize user interactions. Typically, researchers use this dataset to inform decisions related to web application design, server infrastructure improvements, and user experience enhancements.",
    "domain_tags": [
      "technology"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "tfidf_keywords": [
      "web server logs",
      "request patterns",
      "server queue dynamics",
      "traffic analysis",
      "user behavior",
      "response times",
      "performance optimization",
      "session duration",
      "data preprocessing",
      "log analysis"
    ],
    "semantic_cluster": "web-traffic-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "web analytics",
      "performance monitoring",
      "user experience",
      "data visualization",
      "traffic forecasting"
    ],
    "canonical_topics": [
      "data-engineering",
      "consumer-behavior",
      "machine-learning",
      "statistics"
    ],
    "model_score": 0.0002
  },
  {
    "name": "Disney California Adventure Wait Times",
    "description": "Historical ride wait time data from Disney California Adventure for theme park queueing analysis.",
    "category": "Entertainment & Media",
    "url": "https://www.kaggle.com/datasets/tivory27/disney-california-adventure-wait-times",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "queueing",
      "theme-park",
      "wait-times",
      "Disney",
      "attractions",
      "Kaggle"
    ],
    "best_for": "Queueing theory - back out arrival rates from wait times, FastPass priority queue modeling",
    "image_url": "/images/datasets/disney-california-adventure-wait-times.png",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Disney California Adventure Wait Times dataset provides historical data on ride wait times at Disney California Adventure, making it valuable for theme park queueing analysis. Researchers can utilize this dataset to understand visitor behavior, optimize ride operations, and enhance overall guest experience.",
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the historical wait times for rides at Disney California Adventure?",
      "How can I analyze queueing patterns at theme parks?",
      "What factors influence ride wait times at Disney?",
      "Can I use Disney California Adventure wait times for predictive modeling?",
      "What is the average wait time for attractions at Disney California Adventure?",
      "How do wait times vary by season at Disney California Adventure?"
    ],
    "use_cases": [
      "Analyzing peak wait times for rides to improve visitor experience.",
      "Modeling wait time predictions based on historical data.",
      "Evaluating the impact of special events on ride wait times."
    ],
    "embedding_text": "The Disney California Adventure Wait Times dataset consists of historical records detailing the wait times for various attractions within the Disney California Adventure theme park. This dataset is structured in a tabular format, where each row represents a specific time point and includes columns for ride names, wait times, and possibly timestamps. The data is collected through direct observation or automated systems that log wait times at regular intervals throughout the day. The key variables typically include ride identifiers, wait time measurements, and timestamps, which allow for temporal analysis of visitor behavior and ride popularity. Researchers can leverage this dataset to explore various questions such as the average wait times for specific attractions, the effects of different times of day or seasons on wait times, and the overall efficiency of ride operations. Common preprocessing steps may involve cleaning the data to handle missing values, normalizing wait times for comparative analysis, and aggregating data to analyze trends over time. The dataset supports a range of analyses, including descriptive statistics to summarize wait times, regression models to identify factors influencing wait times, and machine learning techniques to predict future wait times based on historical patterns. Researchers typically use this dataset to inform operational decisions within the park, enhance guest experiences by optimizing ride schedules, and contribute to academic studies on consumer behavior in entertainment settings.",
    "domain_tags": [
      "entertainment"
    ],
    "data_modality": "time-series",
    "size_category": "medium",
    "tfidf_keywords": [
      "queueing-analysis",
      "ride-wait-times",
      "theme-park-operations",
      "visitor-behavior",
      "predictive-modeling",
      "data-collection-methodology",
      "temporal-analysis",
      "ride-popularity",
      "operational-efficiency",
      "guest-experience"
    ],
    "semantic_cluster": "theme-park-analytics",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "queueing-theory",
      "visitor-experience",
      "data-analysis",
      "predictive-analytics",
      "operational-research"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "experimentation",
      "statistics"
    ],
    "model_score": 0.0002
  },
  {
    "name": "Flight Delay Dataset 2018-2022",
    "description": "Recent flight delay data covering 2018-2022 for analyzing aviation queueing and delay patterns.",
    "category": "Transportation & Mobility",
    "url": "https://www.kaggle.com/datasets/robikscube/flight-delay-dataset-20182022",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "queueing",
      "flights",
      "delays",
      "aviation",
      "recent",
      "Kaggle"
    ],
    "best_for": "Queueing theory - recent delay patterns, airport congestion analysis, network effects",
    "image_url": "/images/datasets/flight-delay-dataset-2018-2022.png",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Flight Delay Dataset 2018-2022 provides recent data on flight delays from 2018 to 2022, enabling users to analyze patterns in aviation queueing and delays. This dataset can be utilized for various analyses, including predictive modeling and trend analysis in the aviation sector.",
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the trends in flight delays from 2018 to 2022?",
      "How can I analyze aviation queueing patterns using flight delay data?",
      "What factors contribute to flight delays in recent years?",
      "Where can I find datasets on flight delays for analysis?",
      "What is the impact of weather on flight delays?",
      "How do flight delays vary by airline and airport?"
    ],
    "use_cases": [
      "Analyzing the impact of weather conditions on flight delays.",
      "Studying the effectiveness of airline policies on reducing delays.",
      "Identifying peak travel times and their correlation with delays."
    ],
    "embedding_text": "The Flight Delay Dataset 2018-2022 is a comprehensive collection of recent flight delay data that spans a five-year period, providing insights into aviation queueing and delay patterns. This dataset is structured in a tabular format, typically containing rows representing individual flights and columns detailing various attributes such as flight number, departure and arrival times, delay duration, weather conditions, and other relevant variables. The data is collected from aviation authorities and airlines, ensuring a robust and reliable source for analysis. Researchers can leverage this dataset to explore temporal trends in flight delays, assess the impact of different factors such as weather and operational decisions on delays, and develop predictive models to enhance operational efficiency in the aviation industry. Common preprocessing steps may include handling missing values, normalizing time formats, and aggregating data for specific analyses. The dataset supports a range of analyses, including regression modeling, machine learning applications, and descriptive statistics. Researchers typically use this dataset to address questions related to the causes of flight delays, evaluate the effectiveness of interventions aimed at reducing delays, and understand the broader implications of delays on passenger experience and airline operations.",
    "domain_tags": [
      "transportation",
      "aviation"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2018-2022",
    "size_category": "medium",
    "tfidf_keywords": [
      "flight delays",
      "aviation queueing",
      "predictive modeling",
      "weather impact",
      "operational efficiency",
      "delay duration",
      "airline policies",
      "temporal trends",
      "data preprocessing",
      "statistical analysis"
    ],
    "semantic_cluster": "aviation-delay-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "predictive-modeling",
      "statistical-analysis",
      "time-series-analysis",
      "data-preprocessing",
      "operational-research"
    ],
    "canonical_topics": [
      "statistics",
      "forecasting",
      "machine-learning",
      "consumer-behavior",
      "transportation"
    ],
    "model_score": 0.0002
  },
  {
    "name": "Flight Delay and Cancellation 2019-2023",
    "description": "5 years of flight delay and cancellation data including COVID-era disruptions for aviation operations research.",
    "category": "Transportation & Mobility",
    "url": "https://www.kaggle.com/datasets/patrickzel/flight-delay-and-cancellation-dataset-2019-2023",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "queueing",
      "flights",
      "delays",
      "cancellations",
      "COVID",
      "Kaggle"
    ],
    "best_for": "Queueing theory - system recovery analysis, capacity constraints, arrival rate instability",
    "image_url": "/images/datasets/flight-delay-and-cancellation-2019-2023.png",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "transportation",
      "mobility",
      "aviation"
    ],
    "summary": "The Flight Delay and Cancellation dataset provides comprehensive data on flight delays and cancellations from 2019 to 2023, including disruptions caused by the COVID-19 pandemic. Researchers can use this dataset to analyze trends in aviation operations, assess the impact of external factors on flight performance, and develop predictive models for future delays.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the trends in flight delays from 2019 to 2023?",
      "How did COVID-19 impact flight cancellations?",
      "What factors contribute to flight delays?",
      "Can we predict future flight delays based on historical data?",
      "What is the average delay time for domestic vs international flights?",
      "How do weather conditions affect flight cancellations?"
    ],
    "use_cases": [
      "Analyzing the impact of COVID-19 on flight operations",
      "Developing predictive models for flight delays",
      "Comparing delay patterns across different airlines",
      "Evaluating the effectiveness of operational changes in the aviation industry"
    ],
    "embedding_text": "The Flight Delay and Cancellation dataset encompasses five years of detailed records on flight delays and cancellations, capturing the complexities of aviation operations during a transformative period that includes the COVID-19 pandemic. This dataset is structured in a tabular format, with rows representing individual flight records and columns detailing various attributes such as flight number, departure and arrival times, delay durations, cancellation status, and the reasons for delays or cancellations. The data is collected from multiple sources, including airline reports and government aviation databases, ensuring a comprehensive view of the factors affecting flight performance. Key variables include timestamps for departures and arrivals, which allow for the calculation of delay durations, as well as categorical variables indicating the status of each flight (on-time, delayed, or canceled). Researchers should be aware of potential data quality issues, such as missing values or inconsistencies in reporting, particularly during the pandemic when operational norms were disrupted. Common preprocessing steps may involve cleaning the data, handling missing values, and transforming categorical variables into numerical formats for analysis. This dataset supports a variety of research questions, such as examining the correlation between weather conditions and flight delays, assessing the impact of airline policies on cancellation rates, and exploring the effectiveness of interventions aimed at reducing delays. Analysts can employ various methodologies, including regression analysis, machine learning techniques, and descriptive statistics, to extract insights from the data. Researchers typically utilize this dataset to inform operational improvements, enhance customer service strategies, and contribute to policy discussions regarding aviation regulations and practices.",
    "domain_tags": [
      "transportation",
      "aviation"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2019-2023",
    "size_category": "medium",
    "tfidf_keywords": [
      "flight delays",
      "cancellations",
      "COVID-19 impact",
      "aviation operations",
      "predictive modeling",
      "data cleaning",
      "regression analysis",
      "airline performance",
      "operational research",
      "delay duration"
    ],
    "semantic_cluster": "aviation-data-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "predictive modeling",
      "operational research",
      "data analysis",
      "time-series analysis",
      "statistical modeling"
    ],
    "canonical_topics": [
      "causal-inference",
      "forecasting",
      "statistics",
      "data-engineering",
      "consumer-behavior"
    ],
    "benchmark_usage": [
      "Common uses include operational research, policy evaluation, and performance benchmarking in aviation."
    ],
    "model_score": 0.0002
  },
  {
    "name": "NYC EMS Incident Dispatch Data",
    "description": "4.83M+ ambulance responses with second-level timestamps for dispatch, en route, and on-scene times. Ideal for M/M/c modeling of ambulance fleets.",
    "category": "Healthcare",
    "url": "https://data.cityofnewyork.us/Public-Safety/EMS-Incident-Dispatch-Data/76xm-jjuj",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "EMS",
      "ambulance",
      "emergency",
      "dispatch",
      "NYC",
      "open-data"
    ],
    "best_for": "Ambulance response time modeling, geographic pooling analysis, fleet optimization",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "healthcare",
      "emergency-services",
      "data-analysis"
    ],
    "summary": "The NYC EMS Incident Dispatch Data comprises over 4.83 million ambulance responses, providing detailed timestamps for dispatch, en route, and on-scene times. This dataset is particularly useful for modeling ambulance fleet operations and can support various analyses in emergency response efficiency.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the response times for NYC EMS incidents?",
      "How can I model ambulance fleet operations using NYC EMS data?",
      "What trends can be observed in NYC EMS dispatch times?",
      "How does the time of day affect ambulance response times in NYC?",
      "What are the peak hours for EMS incidents in NYC?",
      "How can I analyze the efficiency of ambulance dispatch in NYC?"
    ],
    "use_cases": [
      "Analyzing response time trends for emergency medical services.",
      "Modeling ambulance fleet operations using M/M/c queuing theory.",
      "Evaluating the impact of time of day on EMS response efficiency."
    ],
    "embedding_text": "The NYC EMS Incident Dispatch Data is a comprehensive dataset that captures over 4.83 million ambulance responses, providing second-level timestamps for critical phases of emergency medical service operations, including dispatch, en route, and on-scene times. This rich dataset is structured in a tabular format, with each row representing an individual ambulance response and columns detailing various attributes such as timestamps, incident types, and locations. The data is collected through the New York City Emergency Medical Services, which records each incident in real-time, ensuring high accuracy and relevance for researchers and analysts interested in emergency response systems.\n\nKey variables in this dataset include the dispatch time, which marks when the ambulance is alerted, the en route time indicating when the ambulance begins its journey, and the on-scene time that signifies the arrival at the incident location. These variables are crucial for measuring response times and understanding the efficiency of the EMS system in NYC. However, researchers should be aware of potential limitations, such as data quality issues related to reporting delays or inaccuracies in timestamp recording.\n\nCommon preprocessing steps may include cleaning the data to handle missing values, normalizing timestamps for analysis, and categorizing incidents based on type and severity. This dataset supports a variety of analyses, including regression models to predict response times, machine learning techniques for clustering incidents, and descriptive statistics to summarize response patterns.\n\nResearchers typically use this dataset to address questions related to the efficiency of emergency medical services, such as identifying peak demand times, evaluating the impact of traffic patterns on response times, and modeling the operational capacity of ambulance fleets. The insights gained from this data can inform policy decisions and operational improvements within the healthcare sector, particularly in urban environments where timely medical response is critical.",
    "tfidf_keywords": [
      "ambulance-response",
      "dispatch-time",
      "en-route-time",
      "on-scene-time",
      "M/M/c-modeling",
      "emergency-services",
      "response-time-analysis",
      "NYC-EMS",
      "data-collection-methodology",
      "healthcare-analytics"
    ],
    "semantic_cluster": "emergency-response-analytics",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "queueing-theory",
      "time-series-analysis",
      "healthcare-operations",
      "data-quality-assessment",
      "emergency-management"
    ],
    "canonical_topics": [
      "healthcare",
      "statistics",
      "data-engineering"
    ],
    "domain_tags": [
      "healthcare"
    ],
    "data_modality": "tabular",
    "geographic_scope": "NYC",
    "size_category": "massive",
    "benchmark_usage": [
      "Modeling ambulance fleet operations",
      "Evaluating emergency response efficiency"
    ],
    "model_score": 0.0002
  },
  {
    "name": "Polish Supermarket POS Dataset",
    "description": "Transaction-level checkout data with timestamps showing start/end times, basket size, and payment method. Distinguishes staffed vs self-service.",
    "category": "E-Commerce",
    "url": "https://www.mdpi.com/2306-5729/4/2/67",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "retail",
      "checkout",
      "POS",
      "self-service",
      "transactions"
    ],
    "best_for": "Checkout queue modeling, pooling effects between queue types, service time estimation",
    "image_url": "/images/datasets/polish-supermarket-pos-dataset.jpg",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "retail"
    ],
    "summary": "The Polish Supermarket POS Dataset contains transaction-level checkout data, including timestamps, basket sizes, and payment methods. This dataset can be used to analyze consumer behavior, payment preferences, and the efficiency of staffed versus self-service checkouts.",
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Polish Supermarket POS Dataset?",
      "How can I analyze checkout transactions in retail?",
      "What insights can be gained from supermarket POS data?",
      "How do payment methods vary in supermarket transactions?",
      "What is the difference between staffed and self-service checkouts?",
      "How can I visualize basket sizes from POS data?",
      "What trends can be identified in retail checkout data?",
      "How does transaction timing affect consumer behavior?"
    ],
    "use_cases": [
      "Analyzing consumer preferences for payment methods",
      "Studying the impact of checkout types on transaction speed",
      "Investigating basket size variations across different times of day",
      "Exploring transaction patterns during promotional periods"
    ],
    "embedding_text": "The Polish Supermarket POS Dataset is a rich collection of transaction-level checkout data that captures detailed information about consumer purchases in a retail environment. This dataset includes variables such as timestamps indicating the start and end times of transactions, the size of the shopping basket, and the method of payment used by customers. The dataset distinguishes between staffed and self-service checkouts, providing valuable insights into consumer behavior and preferences in a supermarket setting. The data structure typically consists of rows representing individual transactions and columns that detail various attributes of these transactions, including time of purchase, items bought, total cost, and payment method. The collection methodology for this dataset involves direct observation and recording of transactions at supermarket points of sale, ensuring high accuracy and relevance to real-world shopping behaviors.\n\nThe dataset's temporal coverage is not explicitly mentioned, but it is likely to reflect a continuous period of transaction data collection, allowing for time-series analysis. Geographic scope is also unspecified, but it is focused on Polish supermarkets, which may influence consumer behavior patterns observed in the data. Key variables in the dataset measure aspects such as transaction duration, basket size, and payment methods, which can be analyzed to understand shopping trends and consumer preferences.\n\nData quality is generally high, given the nature of POS data, but researchers should be aware of potential limitations such as missing values or anomalies in transaction records. Common preprocessing steps may include data cleaning, normalization of timestamps, and categorization of payment methods. This dataset supports various types of analyses, including regression analysis to explore relationships between variables, machine learning techniques for predictive modeling, and descriptive statistics to summarize consumer behavior trends.\n\nResearchers typically use this dataset to address questions related to consumer behavior, such as how payment methods influence transaction speed or how basket sizes vary during different times of the day. The insights gained from this dataset can inform retail strategies, enhance customer experience, and optimize checkout processes in supermarkets.",
    "tfidf_keywords": [
      "transaction-level",
      "checkout data",
      "basket size",
      "payment method",
      "staffed checkout",
      "self-service checkout",
      "consumer behavior",
      "retail analysis",
      "POS data",
      "transaction patterns",
      "supermarket transactions",
      "data preprocessing",
      "time-series analysis",
      "data quality",
      "shopping trends"
    ],
    "semantic_cluster": "retail-consumer-behavior",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "data-analysis",
      "retail-marketing",
      "transaction-analysis",
      "payment-systems"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "data-engineering",
      "statistics"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0002
  },
  {
    "name": "IMF PortWatch",
    "description": "Daily port activity across 1,985 ports worldwide using AIS satellite data from 90,000+ vessels. Free API access.",
    "category": "Logistics & Supply Chain",
    "url": "https://portwatch.imf.org/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "ports",
      "shipping",
      "maritime",
      "global",
      "AIS",
      "API"
    ],
    "best_for": "Port congestion analysis, global supply chain monitoring, vessel queueing",
    "image_url": "/images/datasets/imf-portwatch.png",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "logistics",
      "supply-chain",
      "maritime"
    ],
    "summary": "The IMF PortWatch dataset provides daily port activity data across 1,985 ports worldwide, sourced from AIS satellite data of over 90,000 vessels. This dataset allows users to analyze shipping trends, port efficiency, and global maritime logistics.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the daily port activity across global ports?",
      "How can I access AIS satellite data for shipping analysis?",
      "What insights can be gained from the IMF PortWatch dataset?",
      "How many vessels are included in the IMF PortWatch dataset?",
      "What are the key trends in maritime logistics from this dataset?",
      "How does port activity vary across different regions?"
    ],
    "use_cases": [
      "Analyzing shipping trends over time",
      "Assessing port efficiency and congestion",
      "Studying the impact of global events on maritime logistics"
    ],
    "embedding_text": "The IMF PortWatch dataset is a comprehensive resource for analyzing daily port activity across 1,985 ports worldwide. This dataset is derived from Automatic Identification System (AIS) satellite data, which tracks over 90,000 vessels, providing a rich source of information for researchers and analysts interested in maritime logistics and shipping trends. The data structure typically includes rows representing individual port activities and columns detailing various attributes such as port location, vessel identification, activity type, and timestamps. The collection methodology leverages AIS technology, which is widely used in the maritime industry to monitor vessel movements and enhance navigation safety. While the dataset covers a broad geographic scope, it is essential to note that specific temporal coverage details are not provided. Key variables in the dataset measure aspects such as the number of vessels docked, cargo types, and port turnaround times. Researchers utilizing this dataset can expect to encounter high-quality data, although limitations may include potential gaps in data reporting or variations in AIS signal reception. Common preprocessing steps may involve cleaning the data for inconsistencies, filtering for specific ports or timeframes, and aggregating data for analysis. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile tool for addressing research questions related to shipping efficiency, port performance, and global trade dynamics. Researchers often employ this dataset to study the implications of maritime activities on economic trends, assess the impact of policy changes on shipping routes, and evaluate the effectiveness of port management strategies.",
    "tfidf_keywords": [
      "AIS",
      "port-activity",
      "shipping-trends",
      "maritime-logistics",
      "vessel-tracking",
      "data-analysis",
      "port-efficiency",
      "global-trade",
      "logistics-optimization",
      "satellite-data",
      "cargo-transport",
      "shipping-industry",
      "data-collection",
      "port-management"
    ],
    "semantic_cluster": "maritime-logistics-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "logistics",
      "supply-chain-management",
      "shipping-industry",
      "data-analysis",
      "port-management"
    ],
    "canonical_topics": [
      "econometrics",
      "data-engineering",
      "consumer-behavior"
    ],
    "domain_tags": [
      "logistics",
      "shipping"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0002
  },
  {
    "name": "OR-Library Job Shop Benchmarks",
    "description": "82+ classic job shop scheduling problem instances used as benchmarks in operations research literature.",
    "category": "Manufacturing",
    "url": "https://people.brunel.ac.uk/~mastjjb/jeb/orlib/jobshopinfo.html",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "job-shop",
      "scheduling",
      "benchmarks",
      "operations-research",
      "classic"
    ],
    "best_for": "Scheduling algorithm validation, theoretical queueing model testing",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The OR-Library Job Shop Benchmarks dataset consists of over 82 classic job shop scheduling problem instances that are widely used as benchmarks in operations research literature. Researchers can utilize this dataset to evaluate and compare various scheduling algorithms and methodologies, enhancing the understanding of job shop scheduling complexities.",
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the classic job shop scheduling problem instances?",
      "How can I evaluate scheduling algorithms using benchmarks?",
      "What is the significance of the OR-Library Job Shop Benchmarks in operations research?",
      "Where can I find job shop scheduling benchmarks for research?",
      "What methodologies are applicable to job shop scheduling problems?",
      "How do classic benchmarks influence modern scheduling techniques?",
      "What are the key challenges in job shop scheduling?",
      "How can I compare different scheduling algorithms effectively?"
    ],
    "use_cases": [
      "Evaluating the performance of new scheduling algorithms against established benchmarks.",
      "Conducting comparative studies of various optimization techniques in job shop scheduling.",
      "Analyzing the impact of different parameters on scheduling efficiency."
    ],
    "embedding_text": "The OR-Library Job Shop Benchmarks dataset is a collection of over 82 classic job shop scheduling problem instances that serve as benchmarks in the field of operations research. This dataset is structured in a tabular format, where each instance represents a unique scheduling problem characterized by various parameters such as the number of jobs, machines, and processing times. The data is typically organized into rows representing individual scheduling instances and columns detailing the specific variables associated with each instance. Researchers often source these instances from established operations research literature, ensuring a high level of credibility and relevance. The key variables within this dataset measure essential aspects of job shop scheduling, including job sequences, machine availability, and processing durations. While the dataset is robust, researchers should be aware of potential limitations, such as the specific conditions under which these benchmarks were created, which may not fully represent all real-world scenarios. Common preprocessing steps may include normalization of processing times and the transformation of job sequences into a format suitable for algorithmic evaluation. This dataset supports a variety of analyses, including regression analyses to predict scheduling efficiency, machine learning approaches to optimize scheduling decisions, and descriptive analyses to understand the characteristics of different scheduling instances. Researchers typically use this dataset to benchmark new scheduling algorithms, compare their performance against established methods, and explore the complexities inherent in job shop scheduling. By leveraging these benchmarks, scholars can gain insights into the effectiveness of various optimization strategies, ultimately contributing to advancements in the field of operations research.",
    "tfidf_keywords": [
      "job-shop",
      "scheduling",
      "benchmarks",
      "operations-research",
      "optimization",
      "algorithm-evaluation",
      "performance-comparison",
      "processing-times",
      "job-sequences",
      "manufacturing",
      "scheduling-complexities",
      "data-structure",
      "research-methodology",
      "algorithmic-evaluation"
    ],
    "semantic_cluster": "job-shop-scheduling",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "operations-research",
      "scheduling-algorithms",
      "optimization-techniques",
      "benchmarking-methods",
      "performance-evaluation"
    ],
    "canonical_topics": [
      "optimization",
      "machine-learning",
      "statistics"
    ],
    "domain_tags": [
      "manufacturing"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "benchmark_usage": [
      "Evaluating scheduling algorithms",
      "Comparing optimization techniques"
    ],
    "model_score": 0.0002
  },
  {
    "name": "Redfin Housing Market Data",
    "description": "Downloadable housing market data: home prices, sales, inventory, listings by metro/city/zip. Updated weekly from MLS",
    "category": "Real Estate",
    "url": "https://www.redfin.com/news/data-center/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "real estate",
      "housing prices",
      "large-scale",
      "real-world",
      "time series"
    ],
    "best_for": "Learning real estate analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "real estate",
      "housing market",
      "data analysis"
    ],
    "summary": "The Redfin Housing Market Data provides downloadable datasets containing comprehensive information on home prices, sales, inventory, and listings across various metro areas, cities, and zip codes. This data is updated weekly from MLS, allowing for timely analysis of housing trends and market dynamics.",
    "use_cases": [
      "Analyzing trends in home prices over time.",
      "Comparing housing inventory across different metro areas.",
      "Forecasting future housing market conditions.",
      "Investigating the impact of economic factors on housing sales."
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the latest Redfin housing market data?",
      "How can I analyze home prices using Redfin data?",
      "What are the trends in housing inventory from Redfin?",
      "Where can I download Redfin housing market datasets?",
      "What metro areas are covered in the Redfin housing data?",
      "How frequently is the Redfin housing market data updated?",
      "What types of analyses can be performed with Redfin data?",
      "What variables are included in the Redfin housing market dataset?"
    ],
    "domain_tags": [
      "real estate"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/datasets/redfin-housing-market-data.png",
    "embedding_text": "The Redfin Housing Market Data is a rich resource for researchers and analysts interested in understanding the dynamics of the housing market. This dataset encompasses a variety of variables related to home prices, sales, inventory, and listings, structured in a tabular format. Each row in the dataset represents a unique entry corresponding to a specific metro area, city, or zip code, while the columns contain key variables such as home prices, number of sales, and inventory levels. The data is collected weekly from Multiple Listing Services (MLS), ensuring that it reflects the most current market conditions. Researchers can leverage this dataset to explore a range of research questions, such as identifying trends in housing prices over time, assessing the impact of economic conditions on sales, or comparing inventory levels across different regions. The data quality is generally high, but users should be aware of potential limitations, such as variations in data reporting practices across different MLS sources. Common preprocessing steps may include cleaning the data for missing values, normalizing prices for inflation, and aggregating data to different geographic levels for analysis. The dataset supports various types of analyses, including regression models, machine learning approaches, and descriptive statistics, making it a versatile tool for both academic research and practical applications in real estate. Researchers typically utilize this data to inform policy decisions, guide investment strategies, or contribute to academic literature on housing economics.",
    "tfidf_keywords": [
      "housing prices",
      "MLS",
      "inventory levels",
      "sales data",
      "metro areas",
      "real estate trends",
      "data analysis",
      "market dynamics",
      "economic factors",
      "forecasting",
      "data preprocessing",
      "temporal analysis",
      "geographic comparison"
    ],
    "semantic_cluster": "housing-market-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "market dynamics",
      "real estate economics",
      "data visualization",
      "time series analysis",
      "economic indicators"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "econometrics",
      "forecasting",
      "statistics"
    ]
  },
  {
    "name": "OpenStreetMap Road Network",
    "description": "Open-source global road network data including road types, speeds, and connectivity. Downloadable via Overpass API or pre-processed extracts.",
    "category": "Transportation Economics & Technology",
    "url": "https://www.openstreetmap.org/",
    "docs_url": "https://wiki.openstreetmap.org/wiki/Main_Page",
    "github_url": "https://github.com/openstreetmap",
    "tags": [
      "road-network",
      "open-data",
      "global",
      "GIS",
      "routing"
    ],
    "best_for": "Road network analysis, routing, accessibility studies, and urban form research",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The OpenStreetMap Road Network dataset provides open-source global road network data, including various road types, speeds, and connectivity information. Researchers and developers can utilize this dataset for routing applications, urban planning, and transportation modeling.",
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the OpenStreetMap Road Network?",
      "How can I access OpenStreetMap road data?",
      "What types of roads are included in the OpenStreetMap dataset?",
      "How does OpenStreetMap data support routing applications?",
      "What are the connectivity features of the OpenStreetMap road network?",
      "Can I download OpenStreetMap data via Overpass API?"
    ],
    "use_cases": [
      "Routing applications for navigation systems",
      "Urban planning and infrastructure development",
      "Transportation modeling and analysis"
    ],
    "domain_tags": [
      "transportation"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/datasets/openstreetmap-road-network.png",
    "embedding_text": "The OpenStreetMap Road Network dataset is a comprehensive collection of global road network data that serves as a vital resource for various applications in transportation economics and technology. This dataset encompasses a wide range of road types, speeds, and connectivity metrics, making it suitable for a variety of analyses related to transportation systems. The data is structured in a way that allows users to access detailed information about individual roads, including attributes such as road classification (e.g., highway, residential), speed limits, and connectivity to other roads. The collection methodology relies on contributions from a global community of mappers who utilize GPS devices and aerial imagery to ensure accuracy and comprehensiveness. This grassroots approach results in a dynamic dataset that is continually updated, reflecting changes in the road network over time. While the dataset does not explicitly mention temporal coverage, it is important to note that the data is subject to frequent updates, which can impact its relevance for time-sensitive analyses. Researchers typically employ this dataset to address a range of research questions, including those related to traffic flow, urban mobility, and the impact of road infrastructure on economic activity. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, enabling researchers to derive insights into transportation patterns and behaviors. However, users should be aware of potential limitations, such as data quality variations due to the reliance on user-generated content and the need for preprocessing steps to clean and standardize the data for specific analytical purposes. Common preprocessing steps may include filtering for specific road types, normalizing speed data, and ensuring connectivity information is accurately represented. Overall, the OpenStreetMap Road Network dataset is a valuable tool for researchers and practitioners in the fields of transportation economics and technology, providing a foundation for innovative applications and studies.",
    "geographic_scope": "global",
    "tfidf_keywords": [
      "OpenStreetMap",
      "road-network",
      "routing",
      "connectivity",
      "urban-planning",
      "transportation-modeling",
      "GPS-data",
      "aerial-imagery",
      "speed-limits",
      "data-quality",
      "user-generated-content",
      "preprocessing",
      "analysis-scenarios"
    ],
    "semantic_cluster": "transportation-data-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "urban-mobility",
      "traffic-flow",
      "infrastructure-development",
      "economic-activity",
      "data-visualization"
    ],
    "canonical_topics": [
      "transportation",
      "econometrics"
    ]
  },
  {
    "name": "CASdatasets R Package",
    "description": "Collection of 40+ actuarial datasets for P&C insurance including freMTPL, ausautoBI, and loss triangles for teaching and research",
    "category": "Insurance & Actuarial",
    "url": "https://cran.r-project.org/package=CASdatasets",
    "docs_url": "https://freakonometrics.github.io/CASdatasets/",
    "github_url": "https://github.com/freakonometrics/CASdatasets",
    "tags": [
      "actuarial",
      "P&C-insurance",
      "loss-triangles",
      "claims-data",
      "teaching-datasets"
    ],
    "best_for": "Learning actuarial methods, pricing model development, and reserving exercises",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The CASdatasets R Package provides a comprehensive collection of over 40 actuarial datasets specifically designed for property and casualty (P&C) insurance. It includes datasets such as freMTPL, ausautoBI, and various loss triangles, making it an invaluable resource for teaching and research in actuarial science and insurance analytics.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What datasets are included in the CASdatasets R Package?",
      "How can I use the CASdatasets for teaching actuarial science?",
      "What are loss triangles in insurance data?",
      "Where can I find actuarial datasets for research?",
      "What is the freMTPL dataset?",
      "How can I analyze claims data using CASdatasets?",
      "What is the purpose of the CASdatasets R Package?",
      "What types of analyses can be performed with P&C insurance datasets?"
    ],
    "use_cases": [
      "Analyzing loss triangles to understand claims development patterns.",
      "Using freMTPL data for modeling and predicting motor insurance claims.",
      "Teaching actuarial concepts through real-world datasets.",
      "Conducting research on P&C insurance trends and claims behavior."
    ],
    "domain_tags": [
      "Insurance",
      "Actuarial"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/datasets/casdatasets-r-package.png",
    "embedding_text": "The CASdatasets R Package is a robust collection of over 40 actuarial datasets tailored for property and casualty (P&C) insurance, serving as a vital resource for both teaching and research purposes. This package includes notable datasets such as freMTPL, which focuses on motor third-party liability insurance, and ausautoBI, which pertains to Australian automobile bodily injury claims. Additionally, it features various loss triangles that are essential for understanding the development of insurance claims over time. The data structure typically comprises rows representing individual claims or aggregated data points, while columns may include variables such as claim amounts, dates, types of coverage, and other relevant actuarial metrics. These datasets are designed to support a wide range of analyses, including regression modeling, machine learning applications, and descriptive statistics, making them suitable for both novice and experienced data scientists. Researchers often utilize these datasets to address critical questions related to claims behavior, risk assessment, and pricing strategies in the insurance industry. Common preprocessing steps may involve cleaning the data, handling missing values, and transforming variables for analysis. However, users should be aware of potential limitations in data quality, such as biases in reporting or incomplete records. Overall, the CASdatasets R Package is a valuable tool for anyone looking to deepen their understanding of actuarial science and P&C insurance analytics.",
    "tfidf_keywords": [
      "actuarial",
      "P&C-insurance",
      "loss-triangles",
      "claims-data",
      "freMTPL",
      "ausautoBI",
      "data-analysis",
      "insurance-modeling",
      "risk-assessment",
      "predictive-modeling",
      "data-preprocessing",
      "insurance-analytics",
      "statistical-analysis",
      "data-visualization"
    ],
    "semantic_cluster": "insurance-data-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "risk-management",
      "predictive-analytics",
      "statistical-modeling",
      "data-visualization",
      "insurance-pricing"
    ],
    "canonical_topics": [
      "econometrics",
      "statistics",
      "finance",
      "consumer-behavior"
    ]
  },
  {
    "name": "EM-DAT International Disaster Database",
    "description": "Global database of 26K+ natural and technological disasters since 1900 with human and economic impact data",
    "category": "Insurance & Actuarial",
    "url": "https://www.emdat.be/",
    "docs_url": "https://doc.emdat.be/",
    "github_url": null,
    "tags": [
      "disasters",
      "catastrophe",
      "global-data",
      "economic-losses",
      "humanitarian"
    ],
    "best_for": "Global catastrophe analysis, reinsurance pricing, and disaster trend research",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The EM-DAT International Disaster Database is a comprehensive global repository of over 26,000 natural and technological disasters recorded since 1900, providing detailed information on human and economic impacts. Researchers and analysts can leverage this dataset to study the effects of disasters on populations and economies, assess risk management strategies, and inform policy decisions.",
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the EM-DAT International Disaster Database?",
      "How can I access data on global disasters?",
      "What types of disasters are included in the EM-DAT database?",
      "What economic impacts are recorded in the EM-DAT database?",
      "How has the frequency of disasters changed over time?",
      "What are the human impacts of natural disasters according to EM-DAT?"
    ],
    "use_cases": [
      "Analyzing the economic losses associated with specific disasters.",
      "Studying trends in disaster frequency and severity over time.",
      "Assessing the humanitarian impacts of disasters on affected populations.",
      "Evaluating the effectiveness of disaster response strategies."
    ],
    "domain_tags": [
      "insurance",
      "humanitarian",
      "disaster-response"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/logos/emdat.png",
    "embedding_text": "The EM-DAT International Disaster Database serves as a vital resource for researchers, policymakers, and practitioners interested in understanding the impact of disasters on human life and economic stability. This dataset encompasses a wide range of natural and technological disasters, with over 26,000 entries spanning from the year 1900 to the present. Each entry in the database includes key variables such as the type of disaster, date, location, number of casualties, and economic losses, allowing for comprehensive analyses of disaster impacts. The data is structured in a tabular format, with rows representing individual disaster events and columns detailing various attributes of these events. The collection methodology involves gathering data from multiple reputable sources, including government reports, international organizations, and academic publications, ensuring a high level of data quality and reliability. However, users should be aware of potential limitations, such as underreporting in certain regions or types of disasters, which may affect the completeness of the dataset. Common preprocessing steps may include data cleaning, normalization, and handling missing values to prepare the data for analysis. Researchers typically use this dataset to address a variety of research questions, such as the correlation between disaster frequency and economic indicators, the effectiveness of disaster preparedness measures, and the long-term impacts of disasters on affected communities. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile tool for those studying disaster impacts and risk management strategies.",
    "tfidf_keywords": [
      "natural disasters",
      "technological disasters",
      "economic impact",
      "humanitarian impact",
      "disaster frequency",
      "risk management",
      "data collection methodology",
      "disaster response",
      "temporal analysis",
      "geographic analysis"
    ],
    "semantic_cluster": "disaster-impact-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "risk assessment",
      "impact evaluation",
      "disaster management",
      "economic analysis",
      "humanitarian studies"
    ],
    "canonical_topics": [
      "econometrics",
      "policy-evaluation",
      "statistics",
      "consumer-behavior"
    ]
  },
  {
    "name": "MEPS (Medical Expenditure Panel Survey)",
    "description": "Nationally representative survey of healthcare utilization, expenditures, insurance coverage, and health status for the US civilian population",
    "category": "Insurance & Actuarial",
    "url": "https://meps.ahrq.gov/mepsweb/",
    "docs_url": "https://meps.ahrq.gov/mepsweb/data_stats/download_data_files.jsp",
    "github_url": null,
    "tags": [
      "health-insurance",
      "healthcare-costs",
      "expenditure-data",
      "survey-data",
      "panel-data"
    ],
    "best_for": "Health insurance research, healthcare cost modeling, and demand analysis",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Medical Expenditure Panel Survey (MEPS) is a nationally representative survey that collects data on healthcare utilization, expenditures, insurance coverage, and health status among the US civilian population. Researchers can use this dataset to analyze trends in healthcare costs, evaluate the impact of insurance coverage on health outcomes, and inform policy decisions.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Medical Expenditure Panel Survey?",
      "How does MEPS collect data on healthcare utilization?",
      "What types of expenditures does MEPS track?",
      "How can I analyze healthcare costs using MEPS data?",
      "What demographic information is available in the MEPS dataset?",
      "How does insurance coverage affect healthcare expenditures according to MEPS?",
      "What insights can be gained from the MEPS regarding health status in the US?",
      "Where can I access the Medical Expenditure Panel Survey data?"
    ],
    "use_cases": [
      "Analyzing the relationship between insurance coverage and healthcare expenditures.",
      "Evaluating trends in healthcare utilization over time.",
      "Assessing the impact of demographic factors on health status.",
      "Informing healthcare policy decisions based on expenditure data."
    ],
    "domain_tags": [
      "healthcare"
    ],
    "data_modality": "tabular",
    "geographic_scope": "United States",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/logos/ahrq.png",
    "embedding_text": "The Medical Expenditure Panel Survey (MEPS) is a comprehensive dataset that provides insights into the healthcare utilization, expenditures, insurance coverage, and health status of the US civilian population. The dataset is structured in a tabular format, consisting of multiple rows and columns that represent various variables related to healthcare. Each row typically corresponds to an individual respondent, while the columns capture key variables such as demographic information, types of healthcare services utilized, associated costs, and insurance coverage details. MEPS employs a rigorous collection methodology, utilizing a combination of interviews and surveys to gather data from households and healthcare providers, ensuring a nationally representative sample. The coverage of MEPS spans various demographic groups, capturing data across different age ranges, income levels, and geographic locations within the United States. Key variables within the dataset include total healthcare expenditures, types of insurance coverage, frequency of healthcare visits, and self-reported health status, which collectively enable researchers to explore a wide range of research questions. However, users should be aware of potential limitations in data quality, including self-reporting biases and variations in survey response rates. Common preprocessing steps may involve cleaning the data, handling missing values, and transforming variables for analysis. Researchers typically utilize MEPS data to conduct regression analyses, descriptive statistics, and machine learning models to uncover patterns and relationships within healthcare expenditures. This dataset serves as a valuable resource for studies aimed at understanding the dynamics of healthcare costs and the implications of insurance coverage on health outcomes.",
    "tfidf_keywords": [
      "healthcare-utilization",
      "expenditures",
      "insurance-coverage",
      "health-status",
      "US-civilian-population",
      "demographic-factors",
      "data-collection-methodology",
      "policy-evaluation",
      "cost-analysis",
      "survey-data",
      "panel-data",
      "healthcare-policy",
      "data-quality",
      "preprocessing-steps",
      "research-questions"
    ],
    "semantic_cluster": "healthcare-expenditure-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "health-economics",
      "policy-evaluation",
      "insurance-market",
      "healthcare-access",
      "cost-effectiveness-analysis"
    ],
    "canonical_topics": [
      "healthcare",
      "policy-evaluation",
      "econometrics"
    ]
  },
  {
    "name": "Revelio Labs COSMOS",
    "description": "4.1B job postings from 6.6M companies. Deduplicated, parsed, enriched workforce data (commercial/academic partnerships)",
    "category": "Labor Markets",
    "url": "https://www.reveliolabs.com/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "job postings",
      "workforce",
      "companies",
      "labor",
      "commercial"
    ],
    "best_for": "Learning labor markets analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Revelio Labs COSMOS dataset comprises 4.1 billion job postings sourced from 6.6 million companies, providing a comprehensive view of the labor market. Researchers and analysts can utilize this enriched workforce data to explore employment trends, analyze workforce dynamics, and assess the impact of commercial and academic partnerships on labor markets.",
    "use_cases": [
      "Analyzing trends in job postings over time",
      "Investigating the relationship between job postings and economic conditions",
      "Evaluating the impact of educational partnerships on workforce data",
      "Comparing job posting patterns across different industries"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What insights can be derived from 4.1B job postings?",
      "How do job postings vary across different companies?",
      "What trends can be observed in the labor market using COSMOS data?",
      "How can enriched workforce data inform academic research?",
      "What are the implications of commercial partnerships on job postings?",
      "How can labor market dynamics be analyzed with this dataset?",
      "What types of companies are represented in the COSMOS dataset?",
      "How does the volume of job postings correlate with economic indicators?"
    ],
    "domain_tags": [
      "labor",
      "commercial"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/datasets/revelio-labs-cosmos.png",
    "embedding_text": "The Revelio Labs COSMOS dataset is a rich repository of workforce data, encompassing 4.1 billion job postings from a diverse array of 6.6 million companies. This dataset is structured in a tabular format, with rows representing individual job postings and columns capturing various attributes such as job title, company name, location, and posting date. The data is deduplicated and parsed to ensure accuracy and relevance, providing researchers with a clean dataset for analysis. The collection methodology involves aggregating job postings from multiple online platforms, ensuring comprehensive coverage of the labor market. While the dataset does not specify temporal or geographic coverage, it offers a broad spectrum of job postings that can be analyzed for trends and patterns over time. Key variables within the dataset include job titles, company identifiers, and posting dates, which measure aspects of employment trends and workforce dynamics. However, users should be aware of potential limitations, such as variations in job posting practices across companies and industries, which may affect data consistency. Common preprocessing steps may include filtering for specific job titles or companies, as well as normalizing data formats for analysis. Researchers can leverage this dataset to address various research questions, such as the impact of economic conditions on job postings, the effectiveness of educational partnerships in workforce development, and the analysis of labor market trends across different sectors. The dataset supports a range of analytical approaches, including regression analysis, machine learning techniques, and descriptive statistics, making it a versatile tool for labor economics studies. Typically, researchers utilize COSMOS data to gain insights into employment trends, assess the dynamics of the labor market, and inform policy decisions based on empirical evidence.",
    "tfidf_keywords": [
      "job postings",
      "workforce data",
      "labor market",
      "deduplicated data",
      "enriched data",
      "employment trends",
      "commercial partnerships",
      "academic partnerships",
      "data aggregation",
      "company analysis"
    ],
    "semantic_cluster": "labor-market-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "labor-economics",
      "employment-trends",
      "data-analysis",
      "workforce-development",
      "economic-indicators"
    ],
    "canonical_topics": [
      "labor-economics",
      "data-engineering",
      "consumer-behavior"
    ]
  },
  {
    "name": "Instacart Market Basket Analysis",
    "description": "3 million grocery orders from 200,000 Instacart users with product details and order sequences. Released for a Kaggle competition to predict which products users will reorder.",
    "category": "MarTech & Customer Analytics",
    "url": "https://www.kaggle.com/c/instacart-market-basket-analysis/data",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "market-basket",
      "sequential",
      "grocery",
      "reorder-prediction"
    ],
    "best_for": "Learning market basket analysis, sequential recommendations, and next-basket prediction",
    "difficulty": "beginner",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Instacart Market Basket Analysis dataset contains 3 million grocery orders from 200,000 users, providing detailed product information and order sequences. This dataset is ideal for predicting which products users are likely to reorder, making it valuable for research in consumer behavior and marketing strategies.",
    "use_cases": [
      "Predicting product reorder rates based on past purchase behavior.",
      "Analyzing consumer purchasing patterns in grocery shopping.",
      "Developing marketing strategies based on user order sequences.",
      "Optimizing inventory management using order data."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Instacart Market Basket Analysis dataset?",
      "How can I use the Instacart dataset for reorder prediction?",
      "What insights can be gained from grocery order sequences?",
      "What are the key variables in the Instacart dataset?",
      "How many users contributed to the Instacart Market Basket Analysis?",
      "What types of analyses can be performed on this dataset?",
      "Where can I find the Instacart Market Basket Analysis dataset?",
      "What is the structure of the Instacart Market Basket Analysis data?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0001,
    "embedding_text": "The Instacart Market Basket Analysis dataset is a rich resource for understanding consumer purchasing behavior in the grocery sector. Comprising 3 million grocery orders from 200,000 Instacart users, this dataset offers a comprehensive view of product details and order sequences. Researchers and data scientists can leverage this dataset to explore various aspects of consumer behavior, particularly focusing on reorder prediction. The data is structured in a tabular format, typically containing rows for individual orders and columns for variables such as user ID, product ID, order timestamp, and quantities purchased. This structure allows for straightforward manipulation and analysis using tools like pandas in Python. The collection methodology involved aggregating real-world grocery orders from Instacart, ensuring a diverse range of products and user demographics. However, while the dataset is extensive, known limitations include potential biases in user behavior and the representativeness of the sample. Common preprocessing steps may include data cleaning, handling missing values, and transforming categorical variables into numerical formats suitable for machine learning algorithms. Researchers can address questions such as: What factors influence the likelihood of a product being reordered? How do seasonal trends affect grocery purchases? The dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics. Typically, researchers utilize this dataset to build predictive models, conduct market research, and inform marketing strategies aimed at enhancing customer retention and satisfaction.",
    "tfidf_keywords": [
      "market-basket-analysis",
      "grocery-orders",
      "reorder-prediction",
      "consumer-purchasing-patterns",
      "product-sequences",
      "user-behavior",
      "data-preprocessing",
      "predictive-modeling",
      "inventory-optimization",
      "marketing-strategies",
      "data-aggregation",
      "order-timestamp",
      "user-demographics",
      "machine-learning",
      "data-analytics"
    ],
    "semantic_cluster": "market-basket-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "recommendation-systems",
      "machine-learning",
      "data-analytics",
      "predictive-modeling"
    ],
    "canonical_topics": [
      "machine-learning",
      "consumer-behavior",
      "recommendation-systems",
      "data-engineering",
      "pricing"
    ]
  },
  {
    "name": "HCUP (Healthcare Cost and Utilization Project)",
    "description": "Largest collection of longitudinal hospital care data in the US with 100+ million records per year covering inpatient and emergency visits",
    "category": "Insurance & Actuarial",
    "url": "https://hcup-us.ahrq.gov/",
    "docs_url": "https://hcup-us.ahrq.gov/databases.jsp",
    "github_url": null,
    "tags": [
      "hospital-data",
      "inpatient",
      "emergency",
      "healthcare-utilization",
      "cost-data"
    ],
    "best_for": "Hospital cost analysis, readmission prediction, and healthcare ML applications",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The HCUP dataset is the largest collection of longitudinal hospital care data in the United States, encompassing over 100 million records annually that detail inpatient and emergency visits. This dataset provides valuable insights into healthcare utilization and costs, enabling researchers to analyze trends, assess healthcare quality, and inform policy decisions.",
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the HCUP dataset?",
      "How can I access hospital care data in the US?",
      "What insights can be derived from HCUP data?",
      "What are the key variables in the HCUP dataset?",
      "How does HCUP data support healthcare research?",
      "What are the limitations of HCUP data?",
      "What types of analyses can be performed with HCUP data?",
      "How is HCUP data collected?"
    ],
    "use_cases": [
      "Analyzing trends in hospital admissions over time",
      "Assessing the impact of policy changes on healthcare utilization",
      "Evaluating the cost-effectiveness of different treatment options",
      "Studying demographic differences in healthcare access and outcomes"
    ],
    "domain_tags": [
      "healthcare"
    ],
    "data_modality": "tabular",
    "geographic_scope": "United States",
    "size_category": "massive",
    "model_score": 0.0001,
    "image_url": "/images/logos/ahrq.png",
    "embedding_text": "The HCUP (Healthcare Cost and Utilization Project) dataset represents the largest collection of longitudinal hospital care data in the United States, with over 100 million records generated each year. This extensive dataset includes detailed information on inpatient and emergency visits, making it an invaluable resource for researchers, policymakers, and healthcare professionals. The data structure typically consists of rows representing individual patient visits and columns detailing various attributes such as patient demographics, diagnosis codes, treatment procedures, and hospital characteristics. The collection methodology involves gathering data from a network of hospitals across the country, ensuring a comprehensive representation of hospital care. Coverage includes a wide range of demographic variables, allowing for analyses that consider factors such as age, gender, and socioeconomic status. Key variables in the dataset measure aspects of healthcare utilization, including the frequency of visits, types of services rendered, and associated costs. While the dataset is robust, researchers should be aware of potential limitations, such as variations in data reporting practices across hospitals and the challenges of generalizing findings beyond the dataset's scope. Common preprocessing steps may include data cleaning, normalization, and handling missing values to prepare the dataset for analysis. Researchers can address a variety of research questions using HCUP data, such as examining trends in hospital admissions, evaluating the effectiveness of healthcare interventions, and analyzing disparities in access to care. The dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it a versatile tool for healthcare research. Typically, researchers utilize HCUP data in studies aimed at understanding healthcare trends, informing policy decisions, and improving patient outcomes.",
    "benchmark_usage": [
      "Healthcare cost analysis",
      "Utilization studies",
      "Policy impact assessments"
    ],
    "tfidf_keywords": [
      "longitudinal-data",
      "hospital-utilization",
      "healthcare-costs",
      "inpatient-visits",
      "emergency-care",
      "patient-demographics",
      "treatment-procedures",
      "data-collection-methodology",
      "healthcare-disparities",
      "cost-effectiveness-analysis",
      "policy-evaluation",
      "healthcare-quality",
      "data-preprocessing",
      "regression-analysis",
      "machine-learning"
    ],
    "semantic_cluster": "healthcare-utilization-data",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "healthcare-economics",
      "policy-evaluation",
      "cost-analysis",
      "healthcare-access",
      "statistical-analysis"
    ],
    "canonical_topics": [
      "healthcare",
      "econometrics",
      "policy-evaluation"
    ]
  },
  {
    "name": "Telco Customer Churn (IBM)",
    "description": "7,043 customers from a telecommunications company with 21 features including demographics, services, account information, and churn status. Industry-standard dataset for churn prediction benchmarking.",
    "category": "MarTech & Customer Analytics",
    "url": "https://www.kaggle.com/datasets/blastchar/telco-customer-churn",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "churn",
      "classification",
      "telecom",
      "customer-analytics"
    ],
    "best_for": "Learning churn prediction, classification algorithms, and retention analysis",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "customer-churn",
      "telecommunications",
      "data-analysis"
    ],
    "summary": "The Telco Customer Churn dataset consists of 7,043 customers from a telecommunications company, featuring 21 attributes that include demographics, services, account information, and churn status. This dataset serves as an industry-standard benchmark for developing and testing churn prediction models, allowing analysts to explore customer behavior and retention strategies.",
    "use_cases": [
      "Predicting customer churn based on demographic and service-related features.",
      "Analyzing the impact of different services on customer retention.",
      "Developing classification models to identify at-risk customers.",
      "Benchmarking churn prediction algorithms."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Telco Customer Churn dataset?",
      "How can I use the Telco Customer Churn dataset for classification?",
      "What features are included in the Telco Customer Churn dataset?",
      "Where can I find the Telco Customer Churn dataset?",
      "What analysis can be performed with the Telco Customer Churn dataset?",
      "How does customer churn impact telecommunications companies?"
    ],
    "domain_tags": [
      "telecommunications"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/datasets/telco-customer-churn-ibm.jpg",
    "embedding_text": "The Telco Customer Churn dataset is a comprehensive collection of data from 7,043 customers of a telecommunications company, structured in a tabular format with 21 distinct features. These features encompass a variety of customer attributes, including demographic information, the services they subscribe to, account details, and their churn status, which indicates whether they have left the service. This dataset is widely recognized as an industry-standard benchmark for churn prediction, making it an invaluable resource for data scientists and analysts aiming to understand customer behavior and improve retention strategies. The data collection methodology involved gathering information from the company's customer database, ensuring a diverse representation of customer profiles. Key variables in the dataset include customer demographics such as age, gender, and income, as well as service-related features like the type of plan, contract length, and payment method. Each of these variables plays a crucial role in analyzing customer churn and developing predictive models. However, like any dataset, it has its limitations, including potential biases in customer representation and missing values that may require preprocessing. Common preprocessing steps include handling missing data, encoding categorical variables, and normalizing numerical features. Researchers typically use this dataset to address various research questions, such as identifying the factors that contribute to customer churn, evaluating the effectiveness of retention strategies, and comparing the performance of different machine learning algorithms in predicting churn. The dataset supports a range of analyses, including regression, machine learning classification, and descriptive statistics, making it a versatile tool for both academic and practical applications in customer analytics.",
    "benchmark_usage": [
      "Churn prediction benchmarking"
    ],
    "tfidf_keywords": [
      "customer-churn",
      "classification",
      "telecommunications",
      "predictive-modeling",
      "demographics",
      "service-usage",
      "retention-strategies",
      "data-preprocessing",
      "machine-learning",
      "feature-engineering",
      "benchmarking",
      "customer-behavior",
      "churn-prediction",
      "account-information",
      "data-analysis"
    ],
    "semantic_cluster": "customer-churn-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "customer-retention",
      "predictive-analytics",
      "data-mining",
      "behavioral-analysis",
      "machine-learning"
    ],
    "canonical_topics": [
      "machine-learning",
      "consumer-behavior",
      "data-engineering",
      "statistics"
    ]
  },
  {
    "name": "FHWA Highway Statistics",
    "description": "Annual data on US highway system including vehicle miles traveled, fuel consumption, road infrastructure, and highway financing since 1945.",
    "category": "Transportation Economics & Technology",
    "url": "https://www.fhwa.dot.gov/policyinformation/statistics.cfm",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "highways",
      "VMT",
      "infrastructure",
      "federal",
      "fuel"
    ],
    "best_for": "Aggregate transportation trends, infrastructure analysis, and policy research",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The FHWA Highway Statistics dataset provides annual data on the US highway system, including metrics such as vehicle miles traveled (VMT), fuel consumption, road infrastructure, and highway financing since 1945. Researchers can utilize this dataset to analyze trends in transportation economics, assess the impact of infrastructure on economic activities, and inform policy decisions related to highway management.",
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the annual vehicle miles traveled in the US?",
      "How has fuel consumption changed over the years?",
      "What is the state of US highway infrastructure?",
      "How is highway financing structured in the US?",
      "What trends can be observed in highway statistics since 1945?",
      "How do vehicle miles traveled correlate with economic indicators?"
    ],
    "use_cases": [
      "Analyzing the relationship between fuel consumption and economic growth.",
      "Assessing the impact of highway infrastructure on regional development.",
      "Evaluating trends in transportation efficiency over time."
    ],
    "domain_tags": [
      "transportation",
      "economics"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1945-present",
    "geographic_scope": "United States",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/logos/dot.png",
    "embedding_text": "The FHWA Highway Statistics dataset is a comprehensive collection of annual data pertaining to the United States highway system, encompassing a wide array of variables that reflect the performance and usage of highways over time. This dataset includes key metrics such as vehicle miles traveled (VMT), which quantifies the total distance driven by all vehicles on US highways, and fuel consumption, which measures the amount of fuel used by vehicles operating on these roads. Additionally, it provides insights into road infrastructure, detailing the conditions and characteristics of highways, as well as highway financing, which outlines the funding mechanisms and expenditures associated with highway maintenance and development. The data is structured in a tabular format, with rows representing different years and columns corresponding to various metrics and indicators. The collection methodology involves aggregating data from multiple sources, including state transportation agencies and federal reports, ensuring a robust and reliable dataset for analysis. Coverage spans from 1945 to the present, providing a longitudinal view of transportation trends in the US. The dataset's key variables include VMT, fuel consumption rates, types of road infrastructure, and financial data related to highway funding. Researchers often face challenges related to data quality, including potential inconsistencies in reporting across states and changes in measurement standards over time. Common preprocessing steps may involve cleaning the data, handling missing values, and normalizing metrics for comparative analysis. The dataset supports a variety of research questions, such as examining the effects of highway infrastructure on economic activity, analyzing trends in transportation efficiency, and evaluating the impact of policy changes on fuel consumption. It is suitable for various types of analyses, including regression analysis, machine learning applications, and descriptive statistics. Researchers typically use this dataset to inform policy decisions, assess the effectiveness of transportation initiatives, and contribute to academic studies in transportation economics and infrastructure development.",
    "tfidf_keywords": [
      "vehicle miles traveled",
      "fuel consumption",
      "highway infrastructure",
      "highway financing",
      "transportation economics",
      "data aggregation",
      "longitudinal analysis",
      "policy evaluation",
      "infrastructure investment",
      "economic impact",
      "data quality",
      "statistical modeling",
      "transportation trends"
    ],
    "semantic_cluster": "transportation-economics",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "transportation policy",
      "infrastructure economics",
      "data analysis",
      "economic modeling",
      "public finance"
    ],
    "canonical_topics": [
      "econometrics",
      "policy-evaluation",
      "statistics",
      "consumer-behavior"
    ]
  },
  {
    "name": "Merative MarketScan",
    "description": "De-identified commercial claims from 273+ million unique patients since 1995. Includes Commercial Claims, Medicare Supplemental, and Multi-State Medicaid databases. Cited in 2,650+ peer-reviewed studies.",
    "category": "Healthcare Economics & Health-Tech",
    "url": "https://www.merative.com/real-world-evidence",
    "source": "Merative (formerly IBM Watson Health)",
    "type": "Claims Database",
    "access": "Institutional license required",
    "format": "SAS",
    "tags": [
      "Healthcare",
      "Claims",
      "Commercial",
      "Longitudinal"
    ],
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Merative MarketScan dataset provides de-identified commercial claims data from over 273 million unique patients since 1995, encompassing Commercial Claims, Medicare Supplemental, and Multi-State Medicaid databases. Researchers can utilize this extensive dataset to analyze healthcare utilization, costs, and outcomes across various demographics and time periods.",
    "use_cases": [
      "Analyzing healthcare costs and utilization trends over time.",
      "Evaluating the effectiveness of healthcare interventions.",
      "Studying demographic differences in healthcare access and outcomes.",
      "Conducting longitudinal studies on patient health trajectories."
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Merative MarketScan dataset?",
      "How can I access de-identified commercial claims data?",
      "What types of claims are included in the MarketScan dataset?",
      "What research has been conducted using the MarketScan dataset?",
      "How many unique patients are represented in the MarketScan data?",
      "What time period does the MarketScan dataset cover?",
      "What are the key variables in the MarketScan dataset?",
      "How is the MarketScan dataset used in healthcare economics research?"
    ],
    "update_frequency": "Quarterly",
    "geographic_coverage": "United States (employer-insured)",
    "domain_tags": [
      "healthcare"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1995-present",
    "size_category": "medium",
    "benchmark_usage": [
      "Cited in over 2,650 peer-reviewed studies."
    ],
    "model_score": 0.0001,
    "image_url": "/images/logos/merative.png",
    "embedding_text": "The Merative MarketScan dataset is a comprehensive collection of de-identified commercial claims data that spans over 273 million unique patients since 1995. This dataset includes a variety of claims types, such as Commercial Claims, Medicare Supplemental, and Multi-State Medicaid databases, making it a valuable resource for researchers in healthcare economics and health technology. The data structure typically consists of rows representing individual claims and columns that include variables such as patient demographics, service dates, diagnosis codes, procedure codes, and costs associated with healthcare services. The collection methodology involves aggregating claims data from various healthcare providers and insurers, ensuring that patient identities are protected through de-identification processes. Coverage of the dataset is extensive, with temporal data available from 1995 to the present, allowing for longitudinal analyses of healthcare trends over time. However, geographic scope is not explicitly mentioned, indicating that the dataset may encompass a broad range of locations without specific regional delineations. Key variables in the dataset measure aspects such as healthcare utilization, costs, and patient demographics, which are critical for understanding healthcare access and outcomes. Researchers often preprocess the data to address issues such as missing values, standardizing codes, and aggregating claims to facilitate analysis. Common research questions that can be addressed using this dataset include examining the impact of policy changes on healthcare utilization, assessing the effectiveness of specific treatments, and analyzing disparities in healthcare access among different demographic groups. The dataset supports various types of analyses, including regression analyses, machine learning applications, and descriptive statistics. Researchers typically utilize the MarketScan dataset in studies aimed at informing healthcare policy, improving patient care, and understanding the economic implications of healthcare services.",
    "tfidf_keywords": [
      "de-identified claims",
      "healthcare utilization",
      "commercial claims",
      "Medicare Supplemental",
      "Multi-State Medicaid",
      "longitudinal analysis",
      "cost analysis",
      "patient demographics",
      "healthcare outcomes",
      "claims data",
      "healthcare economics",
      "data preprocessing",
      "policy evaluation",
      "treatment effectiveness",
      "healthcare access"
    ],
    "semantic_cluster": "healthcare-economics",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "healthcare policy",
      "cost-effectiveness analysis",
      "longitudinal studies",
      "health disparities",
      "utilization review"
    ],
    "canonical_topics": [
      "healthcare",
      "econometrics",
      "policy-evaluation"
    ]
  },
  {
    "name": "Chicago Property Data",
    "description": "Property assessment values and sales data from Cook County",
    "category": "Real Estate",
    "url": "https://datacatalog.cookcountyil.gov/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "property",
      "Chicago",
      "assessments",
      "government"
    ],
    "best_for": "Learning real estate analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Chicago Property Data dataset contains property assessment values and sales data sourced from Cook County. This dataset can be utilized for various analyses, including property valuation, market trends, and investment opportunities in the real estate sector.",
    "use_cases": [
      "Analyzing trends in property values over time.",
      "Comparing assessment values with sales prices to identify discrepancies.",
      "Investigating the impact of local government policies on property assessments.",
      "Evaluating investment opportunities based on historical sales data."
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the property assessment values in Chicago?",
      "How do property sales in Cook County vary over time?",
      "What trends can be observed in Chicago's real estate market?",
      "How do property assessments correlate with actual sales prices?",
      "What are the demographic factors influencing property values in Chicago?",
      "How does the property market in Cook County compare to other regions?"
    ],
    "domain_tags": [
      "real estate"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Chicago, Cook County",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/logos/cookcountyil.png",
    "embedding_text": "The Chicago Property Data dataset provides a comprehensive view of property assessment values and sales data collected from Cook County, Illinois. This dataset is structured in a tabular format, comprising rows that represent individual properties and columns that detail various attributes such as property ID, assessment value, sale price, sale date, and property type. The data is collected from official government sources, ensuring a level of reliability and accuracy that is crucial for real estate analysis. Researchers and analysts can leverage this dataset to explore a variety of research questions related to property valuation, market dynamics, and investment opportunities. Key variables in this dataset include assessment values, which reflect the estimated worth of properties as determined by local government authorities, and sale prices, which indicate the actual transaction values during property sales. The dataset may also include categorical variables such as property type, which can be instrumental in segmenting the analysis based on residential, commercial, or industrial properties. While the dataset offers valuable insights, it is important to acknowledge potential limitations, including data quality issues that may arise from discrepancies between assessed values and market prices, as well as temporal limitations if the data is not updated regularly. Common preprocessing steps might involve cleaning the data to handle missing values, normalizing sale prices for inflation, and categorizing properties for more granular analysis. Researchers typically use this dataset to conduct regression analyses, machine learning models, and descriptive statistics to uncover trends and patterns in the real estate market. The dataset supports various analyses, including comparative studies of property values, assessments of market trends over time, and evaluations of the impact of economic factors on property sales. Overall, the Chicago Property Data serves as a valuable resource for anyone interested in understanding the intricacies of the real estate market in Chicago and Cook County.",
    "tfidf_keywords": [
      "property assessment",
      "sales data",
      "real estate analysis",
      "market trends",
      "investment opportunities",
      "Cook County",
      "property valuation",
      "government data",
      "property types",
      "transaction values",
      "data quality",
      "temporal analysis",
      "categorical variables"
    ],
    "semantic_cluster": "real-estate-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "property valuation",
      "market analysis",
      "real estate economics",
      "data visualization",
      "statistical modeling"
    ],
    "canonical_topics": [
      "econometrics",
      "consumer-behavior",
      "statistics"
    ]
  },
  {
    "name": "SNAP Facebook Ego Networks",
    "description": "4K users with social circles and anonymized node features. Stanford Network Analysis Project dataset",
    "category": "Social & Web",
    "url": "https://snap.stanford.edu/data/ego-Facebook.html",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Facebook",
      "social network",
      "ego networks",
      "SNAP",
      "Stanford"
    ],
    "best_for": "Learning social & web analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The SNAP Facebook Ego Networks dataset contains information on 4,000 users along with their social circles and anonymized node features. This dataset can be utilized for analyzing social network structures, understanding user interactions, and exploring ego network dynamics.",
    "use_cases": [
      "Analyzing the structure of social networks",
      "Exploring user interactions within ego networks",
      "Studying the impact of social connections on behavior",
      "Investigating community detection within social circles"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the SNAP Facebook Ego Networks dataset?",
      "How can I analyze social circles using the SNAP dataset?",
      "What features are included in the Facebook Ego Networks dataset?",
      "How does the SNAP dataset help in studying social networks?",
      "What are ego networks and how are they represented in this dataset?",
      "What types of analyses can be performed with the SNAP Facebook dataset?"
    ],
    "domain_tags": [
      "social network"
    ],
    "data_modality": "graph",
    "size_category": "medium",
    "model_score": 0.0001,
    "embedding_text": "The SNAP Facebook Ego Networks dataset is a rich resource for researchers interested in social network analysis. It comprises data from 4,000 users, capturing their social circles and various anonymized node features. The dataset's structure typically includes rows representing individual users and columns detailing their connections, attributes, and interactions within their networks. This allows for a comprehensive examination of ego networks, where each user is considered the 'ego' and their connections are the 'alters'. The collection methodology involves gathering data from Facebook, ensuring that user privacy is maintained through anonymization. While the dataset does not specify temporal or geographic coverage, it provides a snapshot of social interactions at a given point in time. Key variables may include user identifiers, connection strength, and demographic features, which can be instrumental in measuring social influence and network dynamics. However, researchers should be aware of potential limitations, such as biases in user representation and the challenges of interpreting anonymized data. Common preprocessing steps may involve cleaning the data, handling missing values, and transforming the network structure into a usable format for analysis. The dataset supports various types of analyses, including regression, machine learning, and descriptive statistics, making it versatile for addressing research questions related to social behavior, community structures, and the effects of social networks on individual outcomes. Researchers typically employ this dataset to explore hypotheses about social influence, network effects, and the role of ego networks in shaping user behavior in digital environments.",
    "tfidf_keywords": [
      "ego networks",
      "social circles",
      "anonymized features",
      "network analysis",
      "user interactions",
      "community detection",
      "social influence",
      "graph theory",
      "node features",
      "connection strength",
      "social network dynamics",
      "data anonymization",
      "user behavior",
      "network structure",
      "social connectivity"
    ],
    "semantic_cluster": "social-network-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "social-network-theory",
      "community-detection",
      "graph-analysis",
      "user-behavior",
      "network-visualization"
    ],
    "canonical_topics": [
      "social-network-analysis",
      "consumer-behavior",
      "machine-learning"
    ]
  },
  {
    "name": "OpenStreetMap Planet",
    "description": "84GB PBF (2TB+ uncompressed) complete world map database with full edit history, weekly updates",
    "category": "Social & Web",
    "url": "https://planet.openstreetmap.org/",
    "docs_url": "https://wiki.openstreetmap.org/wiki/Planet.osm",
    "github_url": null,
    "tags": [
      "database dump",
      "geospatial",
      "large-scale",
      "real-world",
      "PostgreSQL",
      "messy data"
    ],
    "best_for": "Learning social & web analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The OpenStreetMap Planet dataset is a comprehensive global map database that includes detailed geographical data and full edit history. Researchers and developers can utilize this dataset for a variety of applications, including urban planning, transportation analysis, and geographic information system (GIS) projects.",
    "use_cases": [
      "Analyzing urban infrastructure development",
      "Studying transportation patterns and accessibility",
      "Mapping demographic changes over time",
      "Conducting spatial analysis for environmental studies"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the OpenStreetMap Planet dataset?",
      "How can I access the OpenStreetMap Planet data?",
      "What types of analyses can be performed with OpenStreetMap data?",
      "What are the key features of the OpenStreetMap Planet dataset?",
      "How frequently is the OpenStreetMap dataset updated?",
      "What formats are available for the OpenStreetMap Planet data?",
      "What are the limitations of using OpenStreetMap data?",
      "How can OpenStreetMap data be used in urban planning?"
    ],
    "domain_tags": [
      "geospatial",
      "urban planning",
      "transportation"
    ],
    "data_modality": "mixed",
    "size_category": "massive",
    "model_score": 0.0001,
    "image_url": "/images/logos/openstreetmap.png",
    "embedding_text": "The OpenStreetMap Planet dataset is a massive and comprehensive geographical database that provides a complete world map with full edit history. It is available in a PBF format of 84GB, which expands to over 2TB when uncompressed. This dataset is structured in a way that includes various geographical features such as roads, buildings, and natural landmarks, making it an invaluable resource for researchers and developers in the fields of urban planning, transportation, and geographic information systems (GIS). The data is collected through a collaborative mapping process, where volunteers contribute information about their local areas, ensuring a high level of detail and accuracy. The dataset is updated weekly, allowing users to access the most current geographical information. Key variables in the dataset include geographic coordinates, feature types, and edit history, which measure the changes and updates made to the map over time. However, users should be aware of potential data quality issues, as the dataset may contain inconsistencies or inaccuracies due to the nature of crowd-sourced data. Common preprocessing steps may include cleaning the data to remove duplicates, standardizing formats, and filtering out irrelevant information. Researchers can use this dataset to address various research questions, such as analyzing urban infrastructure development, studying transportation patterns, and examining demographic changes over time. The dataset supports a range of analyses, including regression, machine learning, and descriptive statistics, making it a versatile tool for a wide array of research applications. Typically, researchers leverage the OpenStreetMap data in studies related to urban planning, environmental assessments, and transportation modeling, utilizing its rich geographical information to derive insights and inform decision-making.",
    "tfidf_keywords": [
      "OpenStreetMap",
      "geographical database",
      "urban planning",
      "transportation analysis",
      "GIS",
      "crowd-sourced data",
      "feature types",
      "edit history",
      "spatial analysis",
      "infrastructure development",
      "demographic changes",
      "data preprocessing",
      "data quality",
      "geospatial analysis",
      "mapping"
    ],
    "semantic_cluster": "geospatial-data-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "geographic-information-systems",
      "urban-studies",
      "spatial-economics",
      "data-visualization",
      "transportation-planning"
    ],
    "canonical_topics": [
      "geospatial",
      "data-engineering",
      "urban-planning",
      "transportation",
      "statistics"
    ]
  },
  {
    "name": "Meta Content Library",
    "description": "Full Facebook/Instagram public archive via ICPSR application. Posts, Pages, groups, events for academic research",
    "category": "Social & Web",
    "url": "https://transparency.meta.com/researchtools/meta-content-library",
    "docs_url": "https://developers.facebook.com/docs/content-library-and-api",
    "github_url": null,
    "tags": [
      "Facebook",
      "Instagram",
      "Meta",
      "social media",
      "posts"
    ],
    "best_for": "Learning social & web analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Meta Content Library is a comprehensive archive of public posts, pages, groups, and events from Facebook and Instagram, specifically curated for academic research. Researchers can utilize this dataset to analyze social media trends, user engagement, and the impact of social media on various societal aspects.",
    "use_cases": [
      "Analyzing user engagement trends on Facebook and Instagram.",
      "Studying the impact of social media posts on public opinion.",
      "Investigating the role of social media in event promotion.",
      "Exploring the dynamics of online communities through group interactions."
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What types of posts are available in the Meta Content Library?",
      "How can I access the Facebook public archive for research purposes?",
      "What social media events are included in the dataset?",
      "Can I analyze Instagram posts using the Meta Content Library?",
      "What academic research has utilized the Meta Content Library?",
      "How does the Meta Content Library support social media analysis?"
    ],
    "domain_tags": [
      "social media"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/logos/meta.png",
    "embedding_text": "The Meta Content Library serves as a vital resource for researchers interested in social media dynamics, providing a full archive of public content from Facebook and Instagram. This dataset encompasses a wide array of posts, pages, groups, and events, allowing for in-depth analysis of social media interactions and trends. The data is structured in a tabular format, where each row represents a unique post or event, and columns may include variables such as post content, user engagement metrics (likes, shares, comments), timestamps, and associated metadata (e.g., page or group identifiers). The collection methodology involves utilizing the ICPSR application to gather publicly available data, ensuring compliance with privacy regulations while maximizing the breadth of information captured. While the dataset offers a rich source of information, researchers should be aware of potential limitations, including the variability in data quality due to the nature of user-generated content and the challenges in interpreting engagement metrics without context. Common preprocessing steps may involve cleaning the text data, normalizing engagement metrics, and categorizing posts based on content type or themes. This dataset can address various research questions, such as understanding the factors influencing user engagement, the effectiveness of social media campaigns, and the role of social media in shaping public discourse. It supports a range of analytical approaches, including regression analysis, machine learning techniques, and descriptive statistics, making it a versatile tool for academic inquiry. Researchers typically leverage the Meta Content Library to conduct studies on consumer behavior, social influence, and the implications of social media on societal trends, contributing valuable insights to the field of social sciences.",
    "tfidf_keywords": [
      "social media analysis",
      "user engagement",
      "public posts",
      "Facebook archive",
      "Instagram content",
      "community dynamics",
      "event promotion",
      "content categorization",
      "engagement metrics",
      "ICPSR application"
    ],
    "semantic_cluster": "social-media-research",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "social-network-analysis",
      "user-generated-content",
      "digital-communication",
      "online-community",
      "public-opinion"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "social-media",
      "experimentation",
      "data-engineering"
    ]
  },
  {
    "name": "Zillow Research Data",
    "description": "Home values (ZHVI), rents (ZORI), inventory, and market heat indices across US metros and zip codes",
    "category": "Real Estate",
    "url": "https://www.zillow.com/research/data/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "real estate",
      "housing prices",
      "rents",
      "large-scale",
      "time series"
    ],
    "best_for": "Learning real estate analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Zillow Research Data provides comprehensive insights into home values, rents, inventory, and market heat indices across various US metropolitan areas and zip codes. This dataset can be utilized for analyzing housing market trends, forecasting future real estate values, and understanding rental market dynamics.",
    "use_cases": [
      "Analyzing trends in housing prices over time.",
      "Forecasting future rental prices based on historical data.",
      "Investigating the relationship between inventory levels and market heat indices.",
      "Comparing housing markets across different geographical locations."
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the latest home values in my zip code?",
      "How have rents changed in major US metros over the last decade?",
      "What is the current inventory of homes for sale in my area?",
      "How does the housing market heat index vary across different regions?",
      "What trends can be observed in the Zillow Research Data?",
      "How do home values compare across different US metropolitan areas?"
    ],
    "domain_tags": [
      "real estate"
    ],
    "data_modality": "time-series",
    "geographic_scope": "US",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/datasets/zillow-research-data.jpg",
    "embedding_text": "The Zillow Research Data is a rich dataset that encompasses various dimensions of the housing market, including home values (ZHVI), rents (ZORI), inventory levels, and market heat indices across the United States. This dataset is structured in a tabular format, with each row representing a specific geographic area, such as a metropolitan area or zip code, and each column containing variables that measure different aspects of the housing market. Key variables include the Zillow Home Value Index (ZHVI), which provides a measure of home values over time, and the Zillow Rent Index (ZORI), which tracks rental prices. The dataset is collected from Zillow's extensive database, which aggregates information from public records, user-generated content, and proprietary data sources, ensuring a comprehensive view of the housing market. However, users should be aware of potential limitations, such as variations in data collection methodologies across different regions and the impact of external economic factors on housing prices. Common preprocessing steps may include handling missing values, normalizing data, and aggregating data to specific time intervals for analysis. Researchers can leverage this dataset to address various research questions, such as how housing prices respond to economic changes, the impact of inventory levels on market dynamics, and regional comparisons of rental prices. The dataset supports a wide range of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a valuable resource for economists, real estate analysts, and policymakers. Typically, researchers utilize this dataset to inform housing policy decisions, conduct market analyses, and develop predictive models for real estate investments.",
    "tfidf_keywords": [
      "Zillow Home Value Index",
      "Zillow Rent Index",
      "housing market trends",
      "market heat indices",
      "real estate forecasting",
      "inventory levels",
      "US metropolitan areas",
      "rental price dynamics",
      "time series analysis",
      "economic factors"
    ],
    "semantic_cluster": "real-estate-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "housing economics",
      "market analysis",
      "real estate investment",
      "time series forecasting",
      "economic indicators"
    ],
    "canonical_topics": [
      "econometrics",
      "consumer-behavior",
      "pricing",
      "forecasting",
      "data-engineering"
    ]
  },
  {
    "name": "NHTSA FARS (Fatality Analysis Reporting System)",
    "description": "Complete census of fatal traffic crashes in the United States since 1975 with vehicle, person, and crash-level details",
    "category": "Insurance & Actuarial",
    "url": "https://www.nhtsa.gov/research-data/fatality-analysis-reporting-system-fars",
    "docs_url": "https://crashstats.nhtsa.dot.gov/",
    "github_url": null,
    "tags": [
      "traffic-safety",
      "auto-insurance",
      "crash-data",
      "fatality-data",
      "vehicle-safety"
    ],
    "best_for": "Auto insurance risk modeling, safety analysis, and claims severity research",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "traffic-safety",
      "auto-insurance",
      "crash-data"
    ],
    "summary": "The NHTSA FARS dataset provides a comprehensive census of fatal traffic crashes in the United States since 1975, detailing vehicle, person, and crash-level information. Researchers can utilize this dataset to analyze trends in traffic fatalities, assess the effectiveness of safety regulations, and develop predictive models for future incidents.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the trends in fatal traffic crashes in the US?",
      "How do vehicle types correlate with crash fatalities?",
      "What demographic factors influence traffic safety outcomes?",
      "How effective are safety regulations in reducing fatalities?",
      "What patterns can be identified in crash data over the years?",
      "How can machine learning be applied to predict traffic fatalities?"
    ],
    "use_cases": [
      "Analyzing the impact of new traffic laws on fatality rates.",
      "Identifying high-risk demographics for targeted safety campaigns.",
      "Developing predictive models for traffic accident occurrences.",
      "Evaluating the effectiveness of vehicle safety features."
    ],
    "domain_tags": [
      "insurance",
      "transportation"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1975-present",
    "geographic_scope": "United States",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/datasets/nhtsa-fars-fatality-analysis-reporting-system.png",
    "embedding_text": "The NHTSA FARS (Fatality Analysis Reporting System) dataset is a vital resource for understanding fatal traffic crashes in the United States, providing a complete census of such incidents since 1975. This dataset includes detailed information at the vehicle, person, and crash levels, making it an invaluable tool for researchers and policymakers alike. The data structure typically consists of rows representing individual fatal crashes, with columns detailing various attributes such as crash location, time, involved vehicles, and demographics of the individuals involved. Key variables include the type of vehicle, the age and gender of the individuals, and specific circumstances surrounding the crash, such as weather conditions and road types. The collection methodology involves systematic reporting from law enforcement agencies across the country, ensuring a comprehensive coverage of fatal incidents. However, researchers should be aware of potential limitations, such as underreporting in certain areas and variations in data collection practices across states. Common preprocessing steps may involve cleaning the data for missing values, standardizing variable formats, and aggregating data for specific analyses. The dataset supports a range of analyses, including regression models to identify factors influencing fatality rates, machine learning techniques for predictive modeling, and descriptive statistics to summarize trends over time. Researchers often use this dataset to address critical questions regarding traffic safety, evaluate the effectiveness of safety interventions, and inform policy decisions aimed at reducing fatalities on the roads.",
    "tfidf_keywords": [
      "fatality-analysis",
      "traffic-crash",
      "vehicle-safety",
      "demographic-factors",
      "safety-regulations",
      "predictive-modeling",
      "traffic-safety",
      "crash-data",
      "insurance-risk",
      "data-collection-methodology",
      "temporal-trends",
      "data-preprocessing",
      "machine-learning",
      "statistical-analysis"
    ],
    "semantic_cluster": "traffic-safety-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "causal-inference",
      "statistical-analysis",
      "risk-assessment",
      "policy-evaluation",
      "machine-learning"
    ],
    "canonical_topics": [
      "causal-inference",
      "statistics",
      "policy-evaluation",
      "machine-learning",
      "consumer-behavior"
    ],
    "benchmark_usage": [
      "Traffic safety analysis",
      "Insurance risk assessment"
    ]
  },
  {
    "name": "UK Biobank",
    "description": "Prospective cohort of 500,000 UK participants aged 40-69 with genetic data, imaging, and longitudinal health records. Extensive phenotyping including MRI, accelerometry, and linked hospital records.",
    "category": "Healthcare Economics & Health-Tech",
    "url": "https://www.ukbiobank.ac.uk/",
    "source": "UK Biobank",
    "type": "Cohort + Biobank",
    "access": "Application required (fees apply)",
    "format": "Various",
    "tags": [
      "Healthcare",
      "Genomics",
      "Imaging",
      "Longitudinal"
    ],
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The UK Biobank is a comprehensive dataset comprising health and genetic information from 500,000 participants aged 40-69 in the UK. Researchers can utilize this dataset for various analyses, including genetic epidemiology, health outcomes, and the impact of lifestyle factors on health.",
    "use_cases": [
      "Investigating the relationship between genetic factors and chronic diseases.",
      "Analyzing the impact of lifestyle choices on health outcomes over time.",
      "Studying the effects of imaging data on disease progression.",
      "Exploring associations between physical activity and health metrics."
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the UK Biobank dataset?",
      "How can I access genetic data from the UK Biobank?",
      "What types of health records are included in the UK Biobank?",
      "What imaging data is available in the UK Biobank?",
      "How does the UK Biobank support longitudinal health studies?",
      "What are the key variables in the UK Biobank dataset?",
      "What research can be conducted using the UK Biobank?",
      "What demographic information is available in the UK Biobank?"
    ],
    "update_frequency": "Ongoing linkage",
    "geographic_coverage": "United Kingdom",
    "domain_tags": [
      "healthcare",
      "genomics"
    ],
    "data_modality": "mixed",
    "geographic_scope": "UK",
    "size_category": "massive",
    "model_score": 0.0001,
    "image_url": "/images/datasets/uk-biobank.jpg",
    "embedding_text": "The UK Biobank is a landmark prospective cohort study that has collected extensive health-related data from 500,000 participants aged between 40 and 69 years across the United Kingdom. This dataset encompasses a rich array of variables, including genetic data, imaging results, and longitudinal health records, making it a valuable resource for researchers in the fields of healthcare economics and health technology. The data structure includes various rows representing individual participants and columns that encompass a wide range of variables, such as demographic information, health metrics, and lifestyle factors. The collection methodology involved a combination of self-reported questionnaires, physical assessments, and advanced imaging techniques such as MRI. The dataset's coverage is extensive, focusing on the UK population, and it includes diverse demographic groups, although specific temporal coverage is not explicitly mentioned. Key variables within the dataset measure health outcomes, genetic predispositions, and lifestyle choices, providing a comprehensive view of participant health. However, researchers should be aware of potential limitations, such as self-reporting biases and the need for careful preprocessing to handle missing data and standardize variables. Common preprocessing steps may include data cleaning, normalization, and transformation to prepare the dataset for analysis. The UK Biobank supports a variety of research questions, such as understanding the genetic basis of diseases, the impact of environmental factors on health, and the effectiveness of interventions over time. Analyses can range from regression models to machine learning applications and descriptive statistics, allowing for a multifaceted exploration of health-related phenomena. Researchers typically leverage this dataset in studies aimed at uncovering insights into public health, disease prevention, and the interplay between genetics and lifestyle.",
    "tfidf_keywords": [
      "prospective cohort",
      "genetic epidemiology",
      "longitudinal health records",
      "MRI imaging",
      "health outcomes",
      "lifestyle factors",
      "demographic data",
      "health metrics",
      "physical activity",
      "chronic diseases",
      "data collection methodology",
      "self-reported questionnaires",
      "participant health",
      "data preprocessing",
      "missing data"
    ],
    "semantic_cluster": "health-data-research",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "health economics",
      "epidemiology",
      "genomics",
      "public health",
      "biostatistics"
    ],
    "canonical_topics": [
      "healthcare",
      "machine-learning",
      "statistics",
      "causal-inference",
      "econometrics"
    ]
  },
  {
    "name": "MovieLens 25M",
    "description": "25 million ratings and 1 million tag applications across 62,000 movies by 162,000 users. The gold-standard benchmark for collaborative filtering research with rich metadata including genres, tags, and timestamps.",
    "category": "MarTech & Customer Analytics",
    "url": "https://grouplens.org/datasets/movielens/25m/",
    "docs_url": "https://files.grouplens.org/datasets/movielens/ml-25m-README.html",
    "github_url": null,
    "tags": [
      "recommendations",
      "collaborative-filtering",
      "movies",
      "ratings"
    ],
    "best_for": "Learning recommendation systems, matrix factorization, and collaborative filtering",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "recommendations",
      "collaborative-filtering",
      "movies",
      "ratings"
    ],
    "summary": "The MovieLens 25M dataset contains 25 million ratings and 1 million tag applications across 62,000 movies by 162,000 users, making it a rich resource for collaborative filtering research. Researchers can leverage this dataset to develop and evaluate recommendation systems, analyze user preferences, and explore the dynamics of movie ratings.",
    "use_cases": [
      "Developing recommendation algorithms based on user ratings.",
      "Analyzing trends in movie ratings over time.",
      "Exploring the impact of tags on user preferences.",
      "Evaluating collaborative filtering techniques."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the MovieLens 25M dataset?",
      "How can I access the MovieLens 25M dataset?",
      "What types of analyses can be performed with the MovieLens 25M dataset?",
      "What are the key features of the MovieLens 25M dataset?",
      "How is the MovieLens 25M dataset structured?",
      "What research questions can be addressed using the MovieLens 25M dataset?",
      "What are the limitations of the MovieLens 25M dataset?",
      "How can I use the MovieLens 25M dataset for collaborative filtering?"
    ],
    "domain_tags": [
      "entertainment"
    ],
    "data_modality": "tabular",
    "size_category": "massive",
    "benchmark_usage": [
      "Collaborative filtering research",
      "Recommendation system evaluation"
    ],
    "model_score": 0.0001,
    "image_url": "/images/datasets/movielens-25m.jpg",
    "embedding_text": "The MovieLens 25M dataset is a comprehensive collection of movie ratings and tags that serves as a benchmark for collaborative filtering research. It consists of 25 million ratings provided by 162,000 users across 62,000 movies, making it one of the largest datasets available for studying user preferences in the context of movies. The dataset includes rich metadata such as genres, tags, and timestamps, which are crucial for understanding user behavior and improving recommendation algorithms. The data is structured in a tabular format, with rows representing individual ratings and columns capturing variables such as user ID, movie ID, rating value, and timestamp of the rating. The collection methodology involves gathering ratings from users who voluntarily provide feedback on movies they have watched, ensuring a diverse set of opinions and preferences. However, researchers should be aware of potential biases in the data, as ratings may be influenced by factors such as popularity and user demographics. Common preprocessing steps include handling missing values, normalizing ratings, and encoding categorical variables. The dataset supports various types of analyses, including regression, machine learning, and descriptive statistics, allowing researchers to explore questions related to user preferences, movie popularity, and the effectiveness of different recommendation strategies. Researchers typically use the MovieLens 25M dataset to validate their algorithms, compare performance against established benchmarks, and gain insights into consumer behavior within the entertainment industry.",
    "tfidf_keywords": [
      "collaborative-filtering",
      "recommendation-systems",
      "user-preferences",
      "movie-ratings",
      "metadata",
      "data-collection",
      "rating-bias",
      "preprocessing",
      "user-behavior",
      "algorithm-evaluation"
    ],
    "semantic_cluster": "recommendation-systems-benchmarking",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "user-item-interaction",
      "machine-learning",
      "data-mining",
      "consumer-behavior",
      "algorithm-performance"
    ],
    "canonical_topics": [
      "recommendation-systems",
      "machine-learning",
      "consumer-behavior"
    ]
  },
  {
    "name": "CelesTrak",
    "description": "Well-organized TLE and OMM orbital data by satellite category including SOCRATES collision assessment tool",
    "category": "Space",
    "url": "https://celestrak.org/",
    "docs_url": "https://celestrak.org/NORAD/documentation/",
    "github_url": null,
    "tags": [
      "satellites",
      "orbital elements",
      "collision",
      "debris"
    ],
    "best_for": "Accessible satellite data organized by mission type and constellation",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "CelesTrak provides well-organized Two-Line Element (TLE) and Orbital Maneuvering Model (OMM) data categorized by satellite type. This dataset can be utilized for collision assessment, tracking satellite debris, and analyzing orbital elements for various space-related research.",
    "use_cases": [
      "Assessing potential satellite collisions using TLE data.",
      "Tracking and analyzing space debris over time.",
      "Researching orbital elements for satellite mission planning.",
      "Evaluating satellite performance based on orbital data."
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is CelesTrak orbital data?",
      "How can I access TLE data for satellites?",
      "What tools are available for collision assessment using CelesTrak?",
      "What categories of satellites are included in CelesTrak?",
      "How does CelesTrak organize satellite orbital data?",
      "What is the significance of OMM data in space research?",
      "How can I analyze satellite debris using CelesTrak data?",
      "What are the applications of TLE data in satellite tracking?"
    ],
    "domain_tags": [
      "space"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "current",
    "geographic_scope": "Global (orbital)",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/logos/celestrak.png",
    "embedding_text": "CelesTrak is a comprehensive resource for satellite data, specifically focusing on Two-Line Element (TLE) and Orbital Maneuvering Model (OMM) data. The dataset is structured in a tabular format, with rows representing individual satellites and columns detailing their orbital parameters, including inclination, eccentricity, and other key variables that define their trajectories. The data is meticulously organized by satellite categories, which facilitates easier access and analysis for researchers and enthusiasts alike. CelesTrak's collection methodology involves aggregating data from various authoritative sources, ensuring high-quality and reliable information for users. While the dataset does not explicitly mention temporal or geographic coverage, it is widely recognized for its relevance in real-time satellite tracking and collision assessment. Key variables within the dataset measure critical aspects of satellite orbits, enabling users to conduct various analyses, from simple descriptive statistics to complex regression models and machine learning applications. Researchers often utilize CelesTrak data to address questions related to satellite safety, orbital dynamics, and the growing issue of space debris. Common preprocessing steps may include data cleaning, normalization, and integration with other datasets for enhanced analysis. The dataset supports a range of analytical techniques, making it a valuable asset for both academic research and practical applications in the field of space science.",
    "tfidf_keywords": [
      "Two-Line Element",
      "orbital data",
      "collision assessment",
      "satellite tracking",
      "orbital parameters",
      "space debris",
      "Orbital Maneuvering Model",
      "satellite categories",
      "trajectory analysis",
      "data aggregation"
    ],
    "semantic_cluster": "satellite-data-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "orbital mechanics",
      "collision avoidance",
      "satellite technology",
      "space debris management",
      "trajectory optimization"
    ],
    "canonical_topics": [
      "forecasting",
      "machine-learning",
      "statistics"
    ]
  },
  {
    "name": "NHL API",
    "description": "Official NHL statistics and play-by-play data from 2010-present including shot locations, player stats, and game events",
    "category": "Sports & Athletics",
    "url": "https://api-web.nhle.com/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "hockey",
      "NHL",
      "play-by-play",
      "shot-locations",
      "player-stats"
    ],
    "best_for": "Hockey analytics, expected goals modeling, and player evaluation",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "sports",
      "data-analysis",
      "statistics"
    ],
    "summary": "The NHL API provides comprehensive official statistics and play-by-play data for NHL games from 2010 to the present. Users can analyze player performance, game events, and shot locations to gain insights into hockey games and player statistics.",
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the official NHL statistics available through the NHL API?",
      "How can I access play-by-play data for NHL games?",
      "What player stats can I retrieve from the NHL API?",
      "How do shot locations impact game outcomes in hockey?",
      "What time frame does the NHL API cover for its statistics?",
      "Can I analyze historical NHL game events using this API?"
    ],
    "use_cases": [
      "Analyzing player performance trends over multiple seasons",
      "Visualizing shot locations and their impact on game outcomes",
      "Conducting statistical analyses on game events to identify patterns",
      "Comparing player stats across different teams and seasons"
    ],
    "domain_tags": [
      "sports"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2010-present",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/logos/nhle.png",
    "embedding_text": "The NHL API is a robust resource for accessing official statistics and play-by-play data for National Hockey League games from 2010 to the present. This dataset is structured in a tabular format, where each row represents a game event, shot, or player statistic, allowing for detailed analysis of hockey performance. Key variables include player identifiers, game dates, shot locations, and event types, which provide insights into player actions and game dynamics. The data is collected directly from NHL game broadcasts and official statistics, ensuring a high level of accuracy and reliability. However, users should be aware of potential limitations, such as missing data for certain games or events, which may require preprocessing steps like data cleaning and imputation. Researchers can address various questions using this dataset, such as examining the correlation between shot locations and scoring outcomes, analyzing trends in player performance over time, or conducting comparative analyses between teams. The NHL API supports a range of analytical approaches, including regression analysis, machine learning models, and descriptive statistics, making it a versatile tool for sports analysts and enthusiasts alike. Typical use cases involve visualizing player performance metrics, exploring game strategies, and evaluating the effectiveness of different plays based on historical data.",
    "tfidf_keywords": [
      "NHL",
      "play-by-play",
      "shot locations",
      "player statistics",
      "game events",
      "hockey analytics",
      "performance trends",
      "data visualization",
      "sports statistics",
      "event analysis"
    ],
    "semantic_cluster": "sports-data-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "sports analytics",
      "data visualization",
      "performance metrics",
      "statistical analysis",
      "machine learning"
    ],
    "canonical_topics": [
      "statistics",
      "consumer-behavior"
    ]
  },
  {
    "name": "Retrosheet",
    "description": "Play-by-play data for MLB games from 1911-2024 including detailed event files, game logs, and transaction records",
    "category": "Sports & Athletics",
    "url": "https://www.retrosheet.org/",
    "docs_url": "https://www.retrosheet.org/datause.htm",
    "github_url": null,
    "tags": [
      "baseball",
      "play-by-play",
      "MLB",
      "game-logs",
      "historical"
    ],
    "best_for": "Granular game analysis, situational statistics, and historical baseball research",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Retrosheet dataset provides comprehensive play-by-play data for Major League Baseball (MLB) games spanning from 1911 to 2024. Researchers and analysts can utilize this dataset to conduct historical analyses of game events, player performance, and team strategies over time.",
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Retrosheet dataset?",
      "How can I access MLB play-by-play data?",
      "What historical baseball data is available from 1911 to 2024?",
      "Where can I find detailed event files for MLB games?",
      "What types of analyses can be performed with MLB game logs?",
      "How does player performance vary over different MLB seasons?"
    ],
    "use_cases": [
      "Analyzing trends in player performance across seasons",
      "Examining the impact of specific game events on outcomes",
      "Studying historical changes in game strategies",
      "Comparing team performance metrics over time"
    ],
    "domain_tags": [
      "sports",
      "athletics"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1911-2024",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/logos/retrosheet.png",
    "embedding_text": "The Retrosheet dataset is a rich repository of play-by-play data for Major League Baseball (MLB) games, covering an extensive temporal range from 1911 to 2024. This dataset is structured in a tabular format, consisting of rows that represent individual game events and columns that capture various attributes such as game date, teams involved, player statistics, and specific events that occurred during each game. The dataset includes detailed event files, game logs, and transaction records, making it a valuable resource for sports analysts, researchers, and enthusiasts interested in the historical performance of teams and players. The collection methodology for this dataset involves meticulous documentation of game events, often sourced from official MLB records, broadcasts, and historical archives. As such, the data quality is generally high, though users should be aware of potential limitations regarding the completeness of certain historical records, especially from earlier decades. Common preprocessing steps may include cleaning the data for consistency, handling missing values, and transforming variables for analysis. Researchers can leverage this dataset to address a variety of research questions, such as the effects of specific game events on winning probabilities, the evolution of player performance metrics over time, and the analysis of team strategies across different eras. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, enabling users to uncover insights into the dynamics of baseball games. Typically, researchers utilize this dataset in studies focused on sports analytics, historical comparisons, and performance evaluations, contributing to a deeper understanding of the game and its evolution over the years.",
    "tfidf_keywords": [
      "play-by-play",
      "MLB",
      "game logs",
      "historical data",
      "event files",
      "player performance",
      "team strategies",
      "sports analytics",
      "data quality",
      "temporal analysis",
      "performance metrics",
      "game events",
      "transaction records",
      "data preprocessing",
      "historical comparisons"
    ],
    "semantic_cluster": "sports-data-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "sports analytics",
      "historical data analysis",
      "performance evaluation",
      "time-series analysis",
      "event study"
    ],
    "canonical_topics": [
      "statistics",
      "consumer-behavior",
      "econometrics"
    ]
  },
  {
    "name": "Online Retail II (UCI)",
    "description": "1M+ transactions from a UK-based online retailer (2009-2011). Contains invoice number, stock code, description, quantity, invoice date, unit price, customer ID, and country. Standard benchmark for RFM analysis and CLV modeling.",
    "category": "MarTech & Customer Analytics",
    "url": "https://archive.ics.uci.edu/ml/datasets/Online+Retail+II",
    "docs_url": "https://archive.ics.uci.edu/ml/datasets/Online+Retail+II",
    "github_url": null,
    "tags": [
      "CLV",
      "RFM",
      "e-commerce",
      "transaction-data"
    ],
    "best_for": "Learning CLV modeling with BG/NBD, RFM segmentation, and cohort analysis",
    "difficulty": "beginner",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Online Retail II dataset comprises over 1 million transactions from a UK-based online retailer between 2009 and 2011. It includes detailed information such as invoice numbers, stock codes, descriptions, quantities, invoice dates, unit prices, customer IDs, and countries, making it a standard benchmark for RFM analysis and CLV modeling.",
    "use_cases": [
      "RFM analysis to segment customers",
      "Customer lifetime value modeling",
      "Trend analysis in e-commerce transactions",
      "Market basket analysis to identify product associations"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the key variables in the Online Retail II dataset?",
      "How can I use the Online Retail II dataset for RFM analysis?",
      "What insights can be gained from analyzing transaction data?",
      "What is the temporal coverage of the Online Retail II dataset?",
      "How does customer behavior vary by country in the dataset?",
      "What preprocessing steps are needed for the Online Retail II dataset?",
      "What types of analyses can be performed with the Online Retail II dataset?",
      "How can I model customer lifetime value using this dataset?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "temporal_coverage": "2009-2011",
    "geographic_scope": "UK",
    "benchmark_usage": [
      "RFM analysis",
      "CLV modeling"
    ],
    "model_score": 0.0001,
    "embedding_text": "The Online Retail II dataset is a comprehensive collection of over 1 million transactions recorded by a UK-based online retailer from 2009 to 2011. This dataset is structured in a tabular format, featuring various columns that capture essential transaction details such as invoice number, stock code, product description, quantity sold, invoice date, unit price, customer ID, and the country of the customer. The dataset serves as a standard benchmark for conducting RFM (Recency, Frequency, Monetary) analysis and modeling Customer Lifetime Value (CLV), making it a valuable resource for researchers and practitioners in the fields of marketing technology and customer analytics. The collection methodology involved gathering transactional data from the retailer's sales records, ensuring a rich dataset that reflects consumer purchasing behavior over a significant period. Key variables within the dataset include invoice number, which uniquely identifies each transaction; stock code, which categorizes products; description, providing product details; quantity, indicating the number of items purchased; invoice date, which records the date of the transaction; unit price, reflecting the cost of each item; customer ID, which allows for customer segmentation; and country, providing geographic context. While the dataset is robust, researchers should be aware of potential limitations, such as missing values or discrepancies in product descriptions. Common preprocessing steps may include data cleaning to handle null values, normalization of product descriptions, and conversion of date formats for time-series analysis. The dataset supports a variety of analyses, including regression modeling, machine learning applications, and descriptive statistics. Researchers typically leverage this dataset to explore questions related to consumer behavior, pricing strategies, and sales trends, providing insights that can inform marketing strategies and enhance customer engagement.",
    "tfidf_keywords": [
      "RFM analysis",
      "customer lifetime value",
      "transaction data",
      "e-commerce",
      "consumer behavior",
      "market basket analysis",
      "data preprocessing",
      "sales trends",
      "product associations",
      "quantitative analysis"
    ],
    "semantic_cluster": "customer-analytics-methods",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "customer-segmentation",
      "predictive-analytics",
      "market-analysis",
      "transactional-data",
      "behavioral-analysis"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "pricing",
      "data-engineering"
    ]
  },
  {
    "name": "Glassdoor Reviews",
    "description": "Company ratings, salary reports, interview experiences. Employer review platform data for labor analytics",
    "category": "Labor Markets",
    "url": "https://www.glassdoor.com/research/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "salaries",
      "company reviews",
      "interviews",
      "employer ratings"
    ],
    "best_for": "Learning labor markets analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "data-cleaning",
      "statistical-analysis"
    ],
    "topic_tags": [
      "labor-economics",
      "consumer-behavior",
      "employment"
    ],
    "summary": "The Glassdoor Reviews dataset provides insights into company ratings, salary reports, and interview experiences, serving as a valuable resource for labor analytics. Researchers can analyze trends in employee satisfaction, compensation, and recruitment processes across various industries.",
    "use_cases": [
      "Analyzing salary trends across different sectors.",
      "Evaluating the impact of company culture on employee retention.",
      "Investigating the relationship between interview experiences and job acceptance rates."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the average salaries reported for different job titles?",
      "How do company ratings vary by industry?",
      "What trends can be observed in interview experiences across companies?",
      "How do employee reviews correlate with company performance?",
      "What are common themes in employee feedback?",
      "How do salaries differ by geographic location?"
    ],
    "domain_tags": [
      "labor markets",
      "employment",
      "human resources"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0001,
    "embedding_text": "The Glassdoor Reviews dataset is a comprehensive collection of employer reviews, salary reports, and interview experiences, which serves as a crucial resource for labor analytics. This dataset is structured in a tabular format, typically consisting of rows representing individual reviews and columns capturing various attributes such as company name, job title, salary, review rating, and interview experiences. The data is collected from users of the Glassdoor platform, where employees voluntarily share their insights about their employers, making it a rich source of qualitative and quantitative information. Key variables in this dataset include company ratings, which measure employee satisfaction, salary reports that provide insights into compensation across different roles, and interview experiences that detail the hiring process. Researchers often preprocess this data to handle missing values, standardize salary figures, and categorize reviews for sentiment analysis. The dataset supports a range of analyses, including regression analysis to identify factors influencing employee satisfaction, machine learning models to predict job acceptance based on interview experiences, and descriptive statistics to summarize salary distributions. Common research questions addressed using this dataset include examining the correlation between company ratings and employee turnover, exploring salary disparities across demographics, and analyzing the impact of interview experiences on candidate perceptions. However, users should be aware of potential limitations, such as self-selection bias in the reviews and variations in reporting standards across different companies. Overall, the Glassdoor Reviews dataset is invaluable for researchers and practitioners seeking to understand labor market trends and employee sentiments.",
    "tfidf_keywords": [
      "employee-satisfaction",
      "salary-disparities",
      "interview-experiences",
      "company-culture",
      "labor-analytics",
      "qualitative-data",
      "quantitative-analysis",
      "self-selection-bias",
      "sentiment-analysis",
      "employee-retention"
    ],
    "semantic_cluster": "labor-market-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "employee-engagement",
      "human-capital",
      "workplace-culture",
      "job-market-trends",
      "salary-negotiation"
    ],
    "canonical_topics": [
      "labor-economics",
      "consumer-behavior",
      "statistics"
    ],
    "benchmark_usage": [
      "Understanding labor market dynamics",
      "Evaluating employer branding"
    ]
  },
  {
    "name": "NOAA Storm Events Database",
    "description": "Detailed records of significant weather events including property and crop damage estimates from 1950-present",
    "category": "Insurance & Actuarial",
    "url": "https://www.ncdc.noaa.gov/stormevents/",
    "docs_url": "https://www.ncei.noaa.gov/pub/data/swdi/stormevents/csvfiles/",
    "github_url": null,
    "tags": [
      "weather-data",
      "catastrophe",
      "natural-disasters",
      "property-damage",
      "climate"
    ],
    "best_for": "Catastrophe modeling, climate risk assessment, and property insurance pricing",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The NOAA Storm Events Database contains detailed records of significant weather events, including estimates of property and crop damage from 1950 to the present. Researchers and analysts can utilize this dataset to study the impact of natural disasters on various sectors, assess risk for insurance purposes, and inform climate-related policy decisions.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the significant weather events recorded by NOAA?",
      "How does the NOAA Storm Events Database estimate property damage?",
      "What time period does the NOAA Storm Events Database cover?",
      "What types of natural disasters are included in the NOAA Storm Events Database?",
      "How can I access the NOAA Storm Events Database?",
      "What methodologies does NOAA use to collect storm event data?",
      "What are the implications of storm events on insurance claims?",
      "How can the NOAA Storm Events Database inform climate change research?"
    ],
    "use_cases": [
      "Analyzing the economic impact of hurricanes on property values.",
      "Assessing the frequency and severity of tornadoes in different regions.",
      "Evaluating the effectiveness of disaster preparedness programs.",
      "Studying trends in climate-related damages over time."
    ],
    "domain_tags": [
      "insurance",
      "climate",
      "natural-disasters"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1950-present",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/logos/noaa.png",
    "embedding_text": "The NOAA Storm Events Database is a comprehensive resource that provides detailed records of significant weather events in the United States from 1950 to the present. This dataset includes various types of natural disasters such as hurricanes, tornadoes, floods, and severe storms, along with estimates of property and crop damage associated with these events. The data is structured in a tabular format, comprising rows that represent individual storm events and columns that capture key variables such as event type, date, location, and damage estimates. Researchers and analysts can leverage this dataset to address a multitude of research questions, including the economic impact of severe weather on local communities, the frequency and patterns of natural disasters over time, and the effectiveness of mitigation strategies. The collection methodology employed by NOAA involves gathering data from various sources, including local weather stations, emergency management agencies, and insurance reports. While the dataset is robust, it is important to note that there may be limitations in data quality, such as underreporting of damages in less populated areas or discrepancies in data collection methods across different regions. Common preprocessing steps may include cleaning the data to handle missing values, normalizing damage estimates for inflation, and categorizing events by severity. The NOAA Storm Events Database supports a range of analyses, including regression analysis to model the relationship between storm frequency and economic impact, machine learning techniques to predict future weather events, and descriptive statistics to summarize trends in storm occurrences. Researchers typically use this dataset to inform policy decisions, enhance disaster preparedness, and contribute to the broader understanding of climate change impacts on society.",
    "tfidf_keywords": [
      "NOAA",
      "storm events",
      "natural disasters",
      "property damage",
      "crop damage",
      "economic impact",
      "climate change",
      "data collection",
      "weather patterns",
      "disaster preparedness",
      "risk assessment",
      "damage estimates",
      "temporal analysis",
      "environmental impact",
      "insurance claims"
    ],
    "semantic_cluster": "climate-impact-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "disaster management",
      "economic analysis",
      "environmental science",
      "risk management",
      "climate policy"
    ],
    "canonical_topics": [
      "forecasting",
      "natural-language-processing",
      "econometrics",
      "policy-evaluation",
      "consumer-behavior"
    ],
    "benchmark_usage": [
      "Estimating property and crop damage from natural disasters"
    ]
  },
  {
    "name": "Medical Expenditure Panel Survey (MEPS)",
    "description": "Definitive U.S. data on healthcare expenditures, utilization, and insurance coverage. Surveys ~15,000 households annually with detailed spending by payer and service type. Free public use files.",
    "category": "Healthcare Economics & Health-Tech",
    "url": "https://meps.ahrq.gov/mepsweb/",
    "source": "Agency for Healthcare Research and Quality (AHRQ)",
    "type": "Survey",
    "access": "Free public use files",
    "format": "SAS/Stata/CSV",
    "tags": [
      "Healthcare",
      "Expenditures",
      "Survey",
      "Free"
    ],
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "Healthcare",
      "Expenditures",
      "Survey"
    ],
    "summary": "The Medical Expenditure Panel Survey (MEPS) provides comprehensive data on healthcare expenditures, utilization, and insurance coverage in the U.S. Researchers can utilize this dataset to analyze spending patterns, assess healthcare access, and evaluate the impact of insurance on healthcare utilization.",
    "use_cases": [
      "Analyzing trends in healthcare spending over time.",
      "Evaluating the effects of insurance coverage on healthcare access.",
      "Comparing expenditures across different demographic groups.",
      "Assessing the impact of policy changes on healthcare utilization."
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the healthcare expenditures in the U.S. according to MEPS?",
      "How does insurance coverage affect healthcare utilization?",
      "What types of services incur the highest costs in healthcare?",
      "How can I access the public use files of MEPS?",
      "What demographic factors influence healthcare spending?",
      "What is the methodology behind the Medical Expenditure Panel Survey?",
      "How many households are surveyed annually in MEPS?",
      "What variables are included in the MEPS dataset?"
    ],
    "update_frequency": "Annual",
    "geographic_coverage": "United States (national)",
    "domain_tags": [
      "healthcare"
    ],
    "data_modality": "tabular",
    "geographic_scope": "United States",
    "size_category": "medium",
    "model_score": 0.0001,
    "embedding_text": "The Medical Expenditure Panel Survey (MEPS) is a critical dataset for understanding healthcare expenditures, utilization, and insurance coverage in the United States. MEPS collects data from approximately 15,000 households annually, providing a rich source of information on healthcare spending categorized by payer and service type. The dataset is structured in a tabular format, with rows representing individual households and columns capturing variables such as total expenditures, types of services utilized, and insurance coverage details. This comprehensive data enables researchers to explore various dimensions of healthcare economics, including the relationship between insurance status and healthcare access, the distribution of healthcare costs across different demographics, and the overall trends in healthcare spending over time.\n\nThe collection methodology of MEPS involves a series of surveys that gather detailed information on healthcare use and expenditures from households. The data is collected through interviews and questionnaires, ensuring a robust representation of the U.S. population. Key variables in the dataset include total medical expenditures, types of services received (such as hospital visits, outpatient care, and prescription drugs), and demographic information about the respondents, including age, income, and insurance status. While the dataset is a valuable resource for researchers, it is essential to acknowledge some limitations, such as potential reporting biases and the challenges of capturing all healthcare expenditures accurately.\n\nCommon preprocessing steps for utilizing MEPS data may include cleaning the dataset to handle missing values, transforming categorical variables into numerical formats for analysis, and aggregating data to analyze trends over time. Researchers typically use MEPS data to address various research questions, such as evaluating the impact of insurance coverage on healthcare utilization or analyzing the factors that contribute to high healthcare expenditures. The dataset supports a range of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile tool for healthcare economists and policy analysts. Overall, MEPS serves as a foundational dataset for studies aimed at understanding the complexities of healthcare economics and informing policy decisions.",
    "tfidf_keywords": [
      "healthcare-expenditures",
      "insurance-coverage",
      "utilization-patterns",
      "demographic-analysis",
      "public-use-files",
      "expenditure-categories",
      "household-survey",
      "healthcare-access",
      "spending-trends",
      "data-collection-methodology",
      "cost-analysis",
      "healthcare-policy",
      "economic-evaluation",
      "service-utilization",
      "insurance-impact"
    ],
    "semantic_cluster": "healthcare-expenditure-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "healthcare-policy",
      "economic-evaluation",
      "insurance-economics",
      "healthcare-access",
      "demographic-analysis"
    ],
    "canonical_topics": [
      "healthcare",
      "econometrics",
      "consumer-behavior"
    ]
  },
  {
    "name": "nflverse",
    "description": "Comprehensive NFL play-by-play data from 1999-present with EPA, win probability, and player participation data",
    "category": "Sports & Athletics",
    "url": "https://github.com/nflverse/nflverse-data",
    "docs_url": "https://nflreadr.nflverse.com/",
    "github_url": "https://github.com/nflverse/nflverse-data",
    "tags": [
      "football",
      "NFL",
      "play-by-play",
      "EPA",
      "win-probability"
    ],
    "best_for": "NFL analytics, expected points analysis, and game strategy optimization",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The nflverse dataset provides comprehensive play-by-play data from NFL games spanning from 1999 to the present. It includes advanced metrics such as Expected Points Added (EPA) and win probability, along with player participation data, making it a valuable resource for analyzing team and player performance in American football.",
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the nflverse dataset?",
      "How can I access NFL play-by-play data?",
      "What metrics are included in the nflverse dataset?",
      "How does EPA relate to NFL game outcomes?",
      "What years does the nflverse dataset cover?",
      "What types of analyses can be performed with NFL play-by-play data?",
      "How is player participation tracked in the nflverse dataset?",
      "What are the key features of the nflverse dataset?"
    ],
    "use_cases": [
      "Analyzing the impact of specific plays on game outcomes using EPA.",
      "Evaluating player performance metrics over multiple seasons.",
      "Studying win probability changes throughout a game.",
      "Comparing team strategies based on play-by-play data."
    ],
    "domain_tags": [
      "sports",
      "athletics"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1999-present",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/datasets/nflverse.png",
    "embedding_text": "The nflverse dataset is a comprehensive collection of NFL play-by-play data that spans from 1999 to the present, making it an invaluable resource for sports analysts, researchers, and enthusiasts interested in American football. This dataset includes detailed information on each play, including the type of play, the players involved, and various performance metrics such as Expected Points Added (EPA) and win probability. The data is structured in a tabular format, with rows representing individual plays and columns capturing key variables such as game date, teams, player participation, and play outcomes. The collection methodology involves aggregating data from official NFL sources and play-by-play logs, ensuring high accuracy and reliability. While the dataset covers a wide temporal range, it does not specify geographic scope beyond the NFL's operational areas in the United States. Researchers can leverage this dataset to address a variety of research questions, such as the effectiveness of different offensive strategies, the correlation between player performance and team success, and the impact of game situations on play calling. Common preprocessing steps may include cleaning the data for missing values, normalizing metrics for comparative analysis, and transforming categorical variables into numerical formats for statistical modeling. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, allowing users to extract insights and trends from the rich play-by-play data. Overall, the nflverse dataset serves as a foundational tool for anyone looking to delve deeper into the analytics of NFL games and player performance.",
    "tfidf_keywords": [
      "play-by-play",
      "Expected Points Added",
      "EPA",
      "win probability",
      "NFL",
      "player participation",
      "game analysis",
      "performance metrics",
      "offensive strategies",
      "data collection",
      "sports analytics"
    ],
    "semantic_cluster": "nfl-data-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "sports analytics",
      "performance metrics",
      "data visualization",
      "statistical modeling",
      "machine learning"
    ],
    "canonical_topics": [
      "statistics",
      "machine-learning",
      "consumer-behavior"
    ]
  },
  {
    "name": "KDD Cup 2009 Customer Relationship Prediction",
    "description": "Orange Telecom CRM dataset with 50,000 customers and 230 anonymized features. Predict churn, appetency (propensity to buy), and up-selling. Classic benchmark for CRM analytics.",
    "category": "MarTech & Customer Analytics",
    "url": "https://www.kdd.org/kdd-cup/view/kdd-cup-2009/Data",
    "docs_url": "https://www.kdd.org/kdd-cup/view/kdd-cup-2009",
    "github_url": null,
    "tags": [
      "CRM",
      "churn",
      "uplift",
      "classification"
    ],
    "best_for": "Learning CRM analytics, multi-task learning, and handling messy real-world data",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "customer-relationship-management",
      "predictive-analytics"
    ],
    "summary": "The KDD Cup 2009 Customer Relationship Prediction dataset consists of 50,000 customers with 230 anonymized features, designed to predict customer churn, appetency, and up-selling opportunities. This dataset serves as a classic benchmark for CRM analytics, allowing researchers and practitioners to explore customer behavior and improve retention strategies.",
    "use_cases": [
      "Predicting customer churn",
      "Analyzing customer appetency",
      "Developing up-selling strategies"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the KDD Cup 2009 Customer Relationship Prediction dataset?",
      "How can I use the KDD dataset to predict customer churn?",
      "What features are included in the KDD Cup 2009 dataset?",
      "How does the KDD dataset help in CRM analytics?",
      "What machine learning techniques can be applied to the KDD dataset?",
      "Where can I find the KDD Cup 2009 dataset for analysis?"
    ],
    "domain_tags": [
      "telecommunications"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "benchmark_usage": [
      "Predict churn",
      "Assess customer appetency",
      "Evaluate up-selling opportunities"
    ],
    "model_score": 0.0001,
    "image_url": "/images/logos/kdd.png",
    "embedding_text": "The KDD Cup 2009 Customer Relationship Prediction dataset is a well-known resource in the field of customer relationship management (CRM) analytics. It contains data from Orange Telecom, featuring 50,000 customers and 230 anonymized variables that capture various aspects of customer behavior and demographics. The dataset is structured in a tabular format, with each row representing a unique customer and each column corresponding to a specific feature or variable. These features include customer demographics, service usage patterns, and historical interactions with the company, all of which are essential for understanding customer behavior and predicting future actions. The data was collected through Orange Telecom's customer relationship management systems, ensuring a rich and diverse representation of customer interactions. However, as with any dataset, there are limitations regarding data quality and completeness, which researchers should consider when conducting analyses. Common preprocessing steps may include handling missing values, normalizing data, and encoding categorical variables to prepare for machine learning models. The KDD dataset is particularly useful for addressing research questions related to customer retention, such as identifying factors that lead to churn or assessing the effectiveness of marketing strategies aimed at increasing customer appetency. Analysts can employ various techniques, including regression analysis, machine learning algorithms, and descriptive statistics, to extract insights from the data. Researchers typically use this dataset to benchmark their models against established performance metrics in the CRM domain, making it a valuable tool for both academic and industry applications.",
    "tfidf_keywords": [
      "customer-churn",
      "appetency",
      "up-selling",
      "CRM-analytics",
      "predictive-modeling",
      "anonymized-features",
      "telecommunications-data",
      "customer-behavior",
      "data-preprocessing",
      "machine-learning"
    ],
    "semantic_cluster": "customer-analytics-benchmarking",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "predictive-analytics",
      "customer-segmentation",
      "machine-learning",
      "data-mining",
      "behavioral-analysis"
    ],
    "canonical_topics": [
      "machine-learning",
      "consumer-behavior",
      "product-analytics"
    ]
  },
  {
    "name": "Lahman Baseball Database",
    "description": "Complete historical baseball statistics from 1871-2024 including batting, pitching, fielding, and salaries for every MLB player and team",
    "category": "Sports & Athletics",
    "url": "https://www.seanlahman.com/baseball-archive/statistics/",
    "docs_url": "https://www.seanlahman.com/baseball-archive/statistics/",
    "github_url": "https://github.com/chadwickbureau/baseballdatabank",
    "tags": [
      "baseball",
      "historical",
      "MLB",
      "player-statistics",
      "sabermetrics"
    ],
    "best_for": "Learning baseball analytics, historical trend analysis, and player valuation",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "sports",
      "statistics",
      "analytics"
    ],
    "summary": "The Lahman Baseball Database provides a comprehensive collection of historical baseball statistics from 1871 to 2024, covering various aspects such as batting, pitching, fielding, and player salaries. This dataset allows users to conduct in-depth analyses of player performance, team dynamics, and historical trends in Major League Baseball (MLB).",
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the historical batting statistics for MLB players?",
      "How can I analyze player salaries in baseball?",
      "What pitching statistics are available in the Lahman Baseball Database?",
      "Where can I find comprehensive baseball statistics from 1871 to 2024?",
      "How does player performance vary over different MLB seasons?",
      "What are the key metrics for evaluating baseball players?"
    ],
    "use_cases": [
      "Analyzing trends in player performance over time",
      "Comparing salaries of players across different teams and seasons",
      "Evaluating the impact of player statistics on team success",
      "Conducting sabermetric analyses to identify undervalued players"
    ],
    "domain_tags": [
      "sports"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1871-2024",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/datasets/lahman-baseball-database.png",
    "embedding_text": "The Lahman Baseball Database is a rich repository of historical baseball statistics, encompassing a wide array of data points from 1871 to 2024. This dataset is structured in a tabular format, featuring rows that represent individual players, teams, and seasons, while columns include variables such as batting averages, home runs, earned run averages, and player salaries. The data is meticulously compiled from various sources, including official MLB statistics, historical records, and player biographies, ensuring a high level of accuracy and comprehensiveness. Researchers and analysts can leverage this dataset to explore a multitude of research questions, such as the evolution of player performance metrics over time, the correlation between player statistics and team success, and the financial dynamics of player salaries in relation to performance. Common preprocessing steps may include data cleaning to handle missing values, normalization of statistics for comparative analysis, and the creation of derived metrics for deeper insights. The dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it a versatile tool for both academic research and practical applications in sports analytics. Typical uses of the Lahman Baseball Database in studies include performance evaluations, historical comparisons, and the application of sabermetric principles to uncover hidden patterns in player and team performance.",
    "tfidf_keywords": [
      "batting-average",
      "earned-run-average",
      "sabermetrics",
      "player-performance",
      "MLB-statistics",
      "historical-data",
      "team-dynamics",
      "player-salaries",
      "data-analysis",
      "baseball-analytics",
      "performance-metrics",
      "statistical-modeling",
      "data-visualization",
      "regression-analysis"
    ],
    "semantic_cluster": "sports-statistics-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "sports-analytics",
      "data-visualization",
      "statistical-analysis",
      "performance-evaluation",
      "historical-comparisons"
    ],
    "canonical_topics": [
      "statistics",
      "consumer-behavior",
      "machine-learning"
    ]
  },
  {
    "name": "Transitland GTFS Feeds",
    "description": "Aggregated GTFS data from 2,500+ transit agencies across 55+ countries. The largest open transit data aggregator with REST and GraphQL APIs.",
    "category": "Transportation Economics & Technology",
    "url": "https://www.transit.land/",
    "docs_url": "https://www.transit.land/documentation",
    "github_url": "https://github.com/transitland",
    "tags": [
      "transit",
      "GTFS",
      "schedules",
      "public-transportation",
      "global"
    ],
    "best_for": "Transit network analysis, accessibility research, and cross-city comparisons",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "transportation",
      "public-transportation",
      "data-aggregation"
    ],
    "summary": "The Transitland GTFS Feeds dataset provides aggregated General Transit Feed Specification (GTFS) data from over 2,500 transit agencies across more than 55 countries. This dataset serves as the largest open transit data aggregator, offering REST and GraphQL APIs for easy access and integration. Researchers and developers can utilize this dataset to analyze public transportation schedules, improve transit services, and enhance urban mobility planning.",
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Transitland GTFS Feeds dataset?",
      "How can I access GTFS data from multiple transit agencies?",
      "What are the benefits of using aggregated GTFS data?",
      "How does Transitland support public transportation research?",
      "What APIs are available for accessing Transitland data?",
      "In which countries is GTFS data available through Transitland?"
    ],
    "use_cases": [
      "Analyzing public transportation schedules",
      "Improving urban mobility planning",
      "Researching global transit patterns",
      "Developing applications for transit information"
    ],
    "domain_tags": [
      "transportation",
      "technology"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/datasets/transitland-gtfs-feeds.png",
    "embedding_text": "The Transitland GTFS Feeds dataset is a comprehensive collection of General Transit Feed Specification (GTFS) data sourced from over 2,500 transit agencies across more than 55 countries. This dataset is structured in a tabular format, typically comprising rows representing individual transit trips and columns detailing various attributes such as trip ID, route ID, stop ID, arrival and departure times, and service frequency. The data is collected through partnerships with transit agencies that provide their GTFS feeds, ensuring a wide-ranging and up-to-date dataset. While the dataset covers a vast geographic area, specific temporal and demographic coverage details are not explicitly mentioned. Key variables within the dataset include trip schedules, routes, and stops, which measure the operational aspects of public transportation systems. Users should be aware of potential data quality issues, such as incomplete feeds from some agencies or variations in data reporting standards. Common preprocessing steps may include data cleaning, normalization, and merging feeds from different agencies to create a unified dataset for analysis. Researchers can leverage this dataset to address various questions related to public transportation efficiency, accessibility, and service optimization. The dataset supports a range of analyses, including regression models to assess service performance, machine learning algorithms for predictive modeling, and descriptive statistics to summarize transit usage patterns. Typically, researchers utilize this dataset in studies aimed at enhancing urban mobility, evaluating transit service equity, and informing policy decisions regarding public transportation investment.",
    "tfidf_keywords": [
      "GTFS",
      "transit-agencies",
      "public-transportation",
      "data-aggregation",
      "urban-mobility",
      "API-access",
      "transit-schedules",
      "service-frequency",
      "trip-analysis",
      "transit-patterns"
    ],
    "semantic_cluster": "transit-data-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "urban-mobility",
      "public-transportation",
      "data-aggregation",
      "transit-analytics",
      "transportation-planning"
    ],
    "canonical_topics": [
      "transportation",
      "data-engineering",
      "consumer-behavior"
    ]
  },
  {
    "name": "NHANES (National Health and Nutrition Examination Survey)",
    "description": "Unique combination of interviews and physical examinations including blood/urine samples. Covers nutrition, chronic diseases, and environmental exposures. ~5,000 participants annually.",
    "category": "Healthcare Economics & Health-Tech",
    "url": "https://www.cdc.gov/nchs/nhanes/",
    "source": "CDC National Center for Health Statistics",
    "type": "Survey + Biomarkers",
    "access": "Free public use files",
    "format": "SAS/XPT",
    "tags": [
      "Healthcare",
      "Survey",
      "Biomarkers",
      "Nutrition",
      "Free"
    ],
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The NHANES dataset provides a unique combination of interviews and physical examinations, including blood and urine samples from approximately 5,000 participants annually. It covers various aspects of health, including nutrition, chronic diseases, and environmental exposures, making it a valuable resource for researchers in healthcare economics and health technology.",
    "use_cases": [
      "Analyzing the relationship between nutrition and chronic diseases.",
      "Studying environmental exposures and their impact on health outcomes.",
      "Evaluating public health interventions based on survey data.",
      "Investigating trends in health metrics over time."
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the NHANES dataset?",
      "How can I access the NHANES survey data?",
      "What types of health metrics are included in NHANES?",
      "How does NHANES measure environmental exposures?",
      "What are the key findings from NHANES data?",
      "How can NHANES data be used in healthcare research?",
      "What is the sample size of NHANES participants?",
      "What kind of analyses can be performed using NHANES data?"
    ],
    "update_frequency": "Biennial cycles",
    "geographic_coverage": "United States (national)",
    "domain_tags": [
      "healthcare"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/datasets/nhanes-national-health-and-nutrition-examination-survey.png",
    "embedding_text": "The National Health and Nutrition Examination Survey (NHANES) is a comprehensive dataset that combines interviews and physical examinations to assess the health and nutritional status of adults and children in the United States. The dataset includes a wide range of variables collected from approximately 5,000 participants each year, providing insights into nutrition, chronic diseases, and environmental exposures. The data structure typically consists of rows representing individual participants and columns capturing various health metrics, demographic information, and lifestyle factors. Key variables include dietary intake, biomarkers from blood and urine samples, and self-reported health conditions. Researchers often utilize NHANES data to explore complex relationships between diet and health outcomes, assess the prevalence of chronic diseases, and evaluate the effectiveness of public health policies. The collection methodology involves rigorous sampling techniques to ensure representativeness, and data quality is generally high, although limitations may arise from self-reported measures and the cross-sectional nature of the data. Common preprocessing steps include handling missing values, normalizing biomarker measurements, and categorizing dietary intake. NHANES supports a variety of analyses, including regression models, machine learning applications, and descriptive statistics, making it a versatile tool for health-related research. Researchers typically use NHANES data to address questions related to health disparities, the impact of nutrition on chronic disease prevalence, and the effects of environmental factors on public health.",
    "tfidf_keywords": [
      "biomarkers",
      "chronic diseases",
      "nutrition assessment",
      "environmental exposures",
      "health metrics",
      "public health",
      "dietary intake",
      "health disparities",
      "cross-sectional study",
      "sampling techniques"
    ],
    "semantic_cluster": "health-survey-data",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "public-health-research",
      "nutrition-science",
      "epidemiology",
      "health-policy",
      "biostatistics"
    ],
    "canonical_topics": [
      "healthcare",
      "policy-evaluation",
      "statistics",
      "consumer-behavior",
      "econometrics"
    ]
  },
  {
    "name": "JobHop (Flanders)",
    "description": "2.3M occupations and 391K resumes with real career trajectories mapped to ESCO codes. Labor mobility research",
    "category": "Labor Markets",
    "url": "https://huggingface.co/datasets/VDAB/jobhop",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "careers",
      "resumes",
      "occupations",
      "labor mobility",
      "ESCO"
    ],
    "best_for": "Learning labor markets analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The JobHop dataset contains 2.3 million occupations and 391,000 resumes, providing a comprehensive view of labor mobility and career trajectories mapped to ESCO codes. Researchers can utilize this dataset to analyze labor market dynamics, understand career progression, and study the factors influencing job transitions.",
    "use_cases": [
      "Analyzing the impact of labor mobility on career advancement.",
      "Studying the relationship between occupations and ESCO codes.",
      "Investigating trends in job transitions over time.",
      "Exploring demographic factors influencing labor mobility."
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the career trajectories associated with different occupations?",
      "How does labor mobility vary across different sectors?",
      "What insights can be drawn from analyzing resumes mapped to ESCO codes?",
      "What patterns exist in job transitions within the Flanders region?",
      "How can the JobHop dataset inform labor market research?",
      "What are the implications of labor mobility on career development?",
      "How do occupations relate to the ESCO classification?",
      "What trends can be identified in the data regarding job hopping?"
    ],
    "domain_tags": [
      "labor markets"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Flanders",
    "size_category": "massive",
    "model_score": 0.0001,
    "image_url": "/images/logos/huggingface.png",
    "embedding_text": "The JobHop dataset is a rich resource for labor market research, encompassing 2.3 million occupations and 391,000 resumes that are intricately mapped to ESCO codes. This dataset provides a structured representation of career trajectories, allowing researchers to delve into the dynamics of labor mobility. The data is organized in a tabular format, with rows representing individual resumes and occupations, while columns detail various attributes such as job titles, career paths, and ESCO classifications. The collection methodology involves aggregating real-world resumes and occupations, ensuring a diverse and comprehensive dataset that reflects actual career movements. The dataset covers a significant range of occupations within the Flanders region, making it particularly valuable for regional labor market studies. Key variables include job titles, career progression indicators, and ESCO codes, which measure the alignment of occupations with standardized classifications. While the dataset is extensive, researchers should be aware of potential limitations, such as the representativeness of the sample and the accuracy of self-reported data in resumes. Common preprocessing steps may include data cleaning, normalization of job titles, and mapping to standardized classifications. Researchers can address various research questions, such as the factors influencing job transitions, the impact of labor mobility on career advancement, and the relationship between occupations and ESCO codes. The dataset supports a range of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it a versatile tool for labor economics studies. Typically, researchers utilize the JobHop dataset to explore labor market trends, assess the implications of job hopping, and understand the complexities of career development in the context of changing economic conditions.",
    "tfidf_keywords": [
      "labor mobility",
      "career trajectories",
      "ESCO codes",
      "occupational analysis",
      "job transitions",
      "resume data",
      "Flanders labor market",
      "career progression",
      "data aggregation",
      "demographic factors",
      "job hopping",
      "career development",
      "occupational classification",
      "data preprocessing"
    ],
    "semantic_cluster": "labor-market-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "labor-economics",
      "career-development",
      "job-market-trends",
      "occupational-classification",
      "data-analysis"
    ],
    "canonical_topics": [
      "labor-economics",
      "consumer-behavior",
      "statistics"
    ]
  },
  {
    "name": "Amazon Fine Foods Reviews",
    "description": "500,000+ food product reviews from Amazon spanning 1999-2012. Includes user/product IDs, ratings, helpfulness votes, and full review text. Popular for sentiment analysis and review-based recommendations.",
    "category": "MarTech & Customer Analytics",
    "url": "https://snap.stanford.edu/data/web-FineFoods.html",
    "docs_url": "https://snap.stanford.edu/data/web-FineFoods.html",
    "github_url": null,
    "tags": [
      "reviews",
      "sentiment",
      "food",
      "NLP",
      "recommendations"
    ],
    "best_for": "Learning sentiment analysis, review mining, and hybrid recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "sentiment-analysis"
    ],
    "summary": "The Amazon Fine Foods Reviews dataset contains over 500,000 food product reviews from Amazon, covering the period from 1999 to 2012. This dataset is particularly useful for conducting sentiment analysis and developing review-based recommendation systems.",
    "use_cases": [
      "Sentiment analysis of food product reviews",
      "Building recommendation systems based on user reviews",
      "Analyzing trends in consumer preferences over time",
      "Evaluating the impact of product ratings on sales"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the most common sentiments expressed in Amazon food reviews?",
      "How can I use Amazon Fine Foods Reviews for sentiment analysis?",
      "What insights can be derived from food product reviews on Amazon?",
      "How do ratings correlate with helpfulness votes in Amazon reviews?",
      "What trends can be observed in food product reviews over time?",
      "How can I build a recommendation system using Amazon Fine Foods Reviews?"
    ],
    "domain_tags": [
      "retail",
      "food"
    ],
    "data_modality": "text",
    "size_category": "massive",
    "temporal_coverage": "1999-2012",
    "model_score": 0.0001,
    "embedding_text": "The Amazon Fine Foods Reviews dataset is a comprehensive collection of over 500,000 reviews of food products available on Amazon, spanning from 1999 to 2012. This dataset includes various variables such as user IDs, product IDs, ratings, helpfulness votes, and the full text of the reviews. The structure of the dataset is primarily tabular, with each row representing a unique review and columns containing the aforementioned variables. The collection methodology involved scraping reviews from the Amazon platform, ensuring a diverse range of opinions and experiences related to food products. The temporal coverage of this dataset allows researchers to analyze trends and shifts in consumer preferences over a significant period, while the geographic scope is inherently global, reflecting the diverse user base of Amazon. Key variables in the dataset include ratings, which measure the user's satisfaction with a product, and helpfulness votes, which indicate how many users found a review useful. Data quality is generally high, but researchers should be aware of potential biases in user-generated content, such as the tendency for more extreme reviews to be posted. Common preprocessing steps may include text normalization, tokenization, and sentiment scoring to prepare the data for analysis. This dataset supports various types of analyses, including regression, machine learning, and descriptive statistics, making it a versatile resource for researchers interested in consumer behavior, sentiment analysis, and recommendation systems. Researchers typically use this dataset to explore questions related to consumer preferences, the effectiveness of product ratings, and the overall sentiment towards food products on Amazon.",
    "benchmark_usage": [
      "Sentiment analysis",
      "Review-based recommendations"
    ],
    "tfidf_keywords": [
      "sentiment-analysis",
      "recommendation-systems",
      "consumer-preferences",
      "user-generated-content",
      "text-normalization",
      "tokenization",
      "ratings-correlation",
      "helpfulness-votes",
      "food-products",
      "e-commerce-reviews",
      "data-scraping",
      "temporal-analysis",
      "NLP",
      "machine-learning"
    ],
    "semantic_cluster": "nlp-for-economics",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "sentiment-analysis",
      "recommendation-systems",
      "consumer-behavior",
      "text-mining",
      "data-scraping"
    ],
    "canonical_topics": [
      "recommendation-systems",
      "natural-language-processing",
      "consumer-behavior"
    ]
  },
  {
    "name": "National Household Travel Survey (NHTS)",
    "description": "Comprehensive US travel behavior data since 1969 capturing daily non-commercial travel by all modes. The authoritative source on American travel patterns.",
    "category": "Transportation Economics & Technology",
    "url": "https://nhts.ornl.gov/",
    "docs_url": "https://nhts.ornl.gov/documentation",
    "github_url": null,
    "tags": [
      "travel-behavior",
      "survey",
      "mode-choice",
      "demographics",
      "commuting"
    ],
    "best_for": "Understanding travel demand patterns, mode choice analysis, and transportation planning",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The National Household Travel Survey (NHTS) provides comprehensive data on daily non-commercial travel behavior in the United States since 1969. Researchers can utilize this dataset to analyze travel patterns, mode choices, and demographic influences on commuting behaviors.",
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the travel behaviors captured in the NHTS?",
      "How has commuting changed in the US since 1969?",
      "What demographic factors influence mode choice in travel?",
      "How can the NHTS data be used to analyze transportation trends?",
      "What are the key variables in the National Household Travel Survey?",
      "How does the NHTS inform transportation policy?",
      "What methodologies are used to collect data in the NHTS?",
      "What limitations exist in the NHTS dataset?"
    ],
    "use_cases": [
      "Analyzing trends in commuting patterns over decades",
      "Examining the impact of demographics on travel behavior",
      "Evaluating the effectiveness of transportation policies",
      "Studying mode choice preferences among different population segments"
    ],
    "domain_tags": [
      "transportation",
      "economics"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1969-present",
    "geographic_scope": "United States",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/logos/ornl.png",
    "embedding_text": "The National Household Travel Survey (NHTS) is a pivotal dataset that has been collecting comprehensive information on daily non-commercial travel behavior in the United States since 1969. This dataset captures a wide array of travel behaviors, including the modes of transportation used, trip purposes, and the demographics of travelers. The data is structured in a tabular format, with rows representing individual trips and columns detailing various attributes such as trip distance, duration, mode of transport, and demographic information of the travelers. The collection methodology involves extensive surveys conducted across diverse households, ensuring a representative sample of the American population. The NHTS provides valuable insights into how travel behaviors have evolved over time, influenced by factors such as urbanization, economic conditions, and policy changes. Key variables in the dataset include trip purpose (e.g., work, leisure), mode of transportation (e.g., car, public transit, walking), and demographic details (e.g., age, income, household size). While the dataset is robust, researchers should be aware of potential limitations, such as underreporting of certain trip types or biases in self-reported data. Common preprocessing steps may include cleaning the data for missing values, normalizing trip distances, and categorizing modes of transport. The NHTS can address various research questions, such as the impact of socioeconomic factors on travel choices or the effectiveness of public transportation systems. It supports a range of analyses, from descriptive statistics to more complex regression models and machine learning approaches. Researchers often utilize this dataset to inform transportation policy, assess environmental impacts, and study the interplay between travel behavior and urban planning.",
    "tfidf_keywords": [
      "travel behavior",
      "mode choice",
      "demographics",
      "commuting patterns",
      "transportation policy",
      "data collection methodology",
      "trip purpose",
      "household travel",
      "non-commercial travel",
      "temporal analysis"
    ],
    "semantic_cluster": "transportation-behavior-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "transportation economics",
      "behavioral economics",
      "urban planning",
      "policy evaluation",
      "data analysis"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "econometrics",
      "policy-evaluation"
    ]
  },
  {
    "name": "FDA Adverse Event Reporting System (FAERS)",
    "description": "Database of 21+ million adverse event reports for drugs and therapeutic biologics. Free API access through openFDA. Supports pharmacovigilance and drug safety research.",
    "category": "Healthcare Economics & Health-Tech",
    "url": "https://open.fda.gov/data/faers/",
    "source": "FDA",
    "type": "Adverse Event Database",
    "access": "Free (API available)",
    "format": "JSON API/Downloads",
    "tags": [
      "Healthcare",
      "Drug Safety",
      "Pharmacovigilance",
      "Free",
      "API"
    ],
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "Healthcare",
      "Drug Safety",
      "Pharmacovigilance"
    ],
    "summary": "The FDA Adverse Event Reporting System (FAERS) is a comprehensive database containing over 21 million reports of adverse events associated with drugs and therapeutic biologics. Researchers can utilize this dataset for pharmacovigilance studies and drug safety research, leveraging the free API access provided through openFDA.",
    "use_cases": [
      "Analyzing trends in adverse drug reactions over time",
      "Identifying potential safety signals for newly approved drugs",
      "Conducting comparative studies on drug safety profiles",
      "Evaluating the impact of regulatory changes on drug safety reporting"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the FDA Adverse Event Reporting System?",
      "How can I access the FAERS database?",
      "What types of adverse events are reported in FAERS?",
      "How many reports are available in the FAERS dataset?",
      "What research can be conducted using FAERS data?",
      "What is pharmacovigilance and how does FAERS support it?",
      "What are the key features of the FAERS API?",
      "How can I analyze drug safety using FAERS?"
    ],
    "update_frequency": "Quarterly",
    "geographic_coverage": "United States (primarily)",
    "domain_tags": [
      "healthcare"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0001,
    "embedding_text": "The FDA Adverse Event Reporting System (FAERS) is a vital resource for researchers and healthcare professionals interested in drug safety and pharmacovigilance. This extensive database contains over 21 million adverse event reports related to drugs and therapeutic biologics, providing a rich source of information for analyzing the safety and efficacy of medications. FAERS is structured in a tabular format, with rows representing individual adverse event reports and columns capturing various attributes such as the drug involved, the nature of the adverse event, patient demographics, and outcomes. The data is collected through mandatory reporting by manufacturers and voluntary reporting by healthcare professionals and the public, ensuring a diverse range of input. However, it is important to note that the dataset may have limitations, including underreporting and potential biases in the data collection process. Researchers typically preprocess the data by cleaning and standardizing the entries, handling missing values, and possibly aggregating reports to focus on specific drugs or events. The FAERS dataset supports a variety of analytical approaches, including regression analysis, machine learning techniques, and descriptive statistics, allowing for comprehensive investigations into drug safety. Common research questions addressed using FAERS data include identifying patterns in adverse events, evaluating the safety of specific drugs, and assessing the impact of demographic factors on drug reactions. Overall, FAERS serves as a crucial tool for advancing our understanding of drug safety and improving patient outcomes.",
    "tfidf_keywords": [
      "adverse events",
      "pharmacovigilance",
      "drug safety",
      "openFDA",
      "reporting system",
      "therapeutic biologics",
      "data analysis",
      "safety signals",
      "regulatory changes",
      "healthcare research"
    ],
    "semantic_cluster": "drug-safety-research",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "pharmacovigilance",
      "adverse drug reactions",
      "data analysis",
      "healthcare research",
      "regulatory affairs"
    ],
    "canonical_topics": [
      "healthcare",
      "policy-evaluation",
      "statistics"
    ],
    "benchmark_usage": [
      "Pharmacovigilance studies",
      "Drug safety research"
    ]
  },
  {
    "name": "NIST National Vulnerability Database (NVD)",
    "description": "Comprehensive database of 500,000+ CVE vulnerability entries with CVSS severity scores and free API access",
    "category": "Cybersecurity",
    "url": "https://nvd.nist.gov/",
    "docs_url": "https://nvd.nist.gov/developers",
    "github_url": null,
    "tags": [
      "vulnerabilities",
      "CVE",
      "CVSS",
      "security"
    ],
    "best_for": "Vulnerability research and security investment prioritization",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The NIST National Vulnerability Database (NVD) is a comprehensive repository of over 500,000 CVE vulnerability entries that include CVSS severity scores. This dataset provides free API access, allowing users to query and analyze vulnerabilities in software and systems, facilitating security assessments and risk management.",
    "use_cases": [
      "Conducting security assessments for software applications.",
      "Analyzing trends in software vulnerabilities over time.",
      "Integrating vulnerability data into risk management frameworks."
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the latest CVE vulnerabilities listed in the NVD?",
      "How can I access the NVD API for vulnerability data?",
      "What CVSS scores are associated with specific CVE entries?",
      "How many vulnerabilities are recorded in the NVD?",
      "What types of vulnerabilities are most common in the NVD?",
      "How does the NVD categorize vulnerabilities?"
    ],
    "domain_tags": [
      "cybersecurity"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1999-present",
    "geographic_scope": "Global",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/logos/nist.png",
    "embedding_text": "The NIST National Vulnerability Database (NVD) serves as a critical resource for cybersecurity professionals, researchers, and organizations seeking to understand and mitigate vulnerabilities in software and systems. The dataset comprises over 500,000 entries of Common Vulnerabilities and Exposures (CVE), each associated with a Common Vulnerability Scoring System (CVSS) severity score that quantifies the potential impact of the vulnerabilities. The NVD is structured in a tabular format, where each row represents a unique CVE entry, and columns include variables such as CVE ID, description, CVSS score, and references to related resources. This structured approach allows for efficient querying and analysis, making it easier for users to extract relevant information for their specific needs.\n\nThe collection methodology of the NVD involves systematic aggregation of vulnerability data from various sources, including security advisories, vendor reports, and community contributions. This ensures that the database remains up-to-date with the latest vulnerabilities affecting software and systems across different platforms. However, users should be aware of potential limitations in data quality, such as the lag in reporting new vulnerabilities or discrepancies in CVSS scoring due to varying interpretations of severity.\n\nCommon preprocessing steps for utilizing the NVD data may include filtering entries based on CVSS scores, categorizing vulnerabilities by type (e.g., software, hardware), and merging with other datasets for comprehensive security analysis. Researchers and practitioners can leverage the NVD to address various research questions, such as identifying the most prevalent vulnerabilities in a specific software category, assessing the effectiveness of security patches over time, or evaluating the correlation between CVSS scores and real-world exploitation incidents.\n\nThe types of analyses supported by the NVD data range from descriptive statistics to more complex machine learning models aimed at predicting vulnerability trends or assessing risk levels. Cybersecurity professionals typically use this dataset to inform their security strategies, prioritize remediation efforts, and enhance their overall understanding of the evolving threat landscape in the field of cybersecurity.",
    "tfidf_keywords": [
      "CVE",
      "NVD",
      "CVSS",
      "vulnerability",
      "security",
      "risk management",
      "software vulnerabilities",
      "data aggregation",
      "impact assessment",
      "security assessments"
    ],
    "semantic_cluster": "cybersecurity-vulnerability-database",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "vulnerability assessment",
      "risk analysis",
      "cyber threat intelligence",
      "security frameworks",
      "software security"
    ],
    "canonical_topics": [
      "machine-learning",
      "statistics",
      "data-engineering",
      "policy-evaluation"
    ]
  },
  {
    "name": "Retail Rocket Recommender System Dataset",
    "description": "4.5 months of behavior data from a real e-commerce site: 2.7M sessions, item properties, and purchase events. Designed for session-based recommendation research.",
    "category": "MarTech & Customer Analytics",
    "url": "https://www.kaggle.com/datasets/retailrocket/ecommerce-dataset",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "session-based",
      "e-commerce",
      "recommendations",
      "behavior"
    ],
    "best_for": "Learning session-based recommendations and real-time personalization",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "recommendations"
    ],
    "summary": "The Retail Rocket Recommender System Dataset consists of 4.5 months of behavioral data from a real e-commerce site, featuring 2.7 million sessions, item properties, and purchase events. This dataset is designed for session-based recommendation research, allowing researchers to analyze consumer behavior and improve recommendation algorithms.",
    "use_cases": [
      "Analyzing consumer behavior patterns in e-commerce",
      "Developing and testing recommendation algorithms",
      "Evaluating the effectiveness of marketing strategies",
      "Studying session-based interactions and their impact on purchases"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the Retail Rocket Recommender System Dataset?",
      "How can I use session-based data for recommendations?",
      "What insights can be gained from e-commerce behavioral data?",
      "What are the key variables in the Retail Rocket dataset?",
      "How does consumer behavior influence purchase decisions?",
      "What preprocessing steps are needed for this dataset?",
      "What types of analyses can be performed on this dataset?",
      "Where can I find datasets for session-based recommendation systems?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/datasets/retail-rocket-recommender-system-dataset.png",
    "embedding_text": "The Retail Rocket Recommender System Dataset is a comprehensive collection of behavioral data from a real e-commerce platform, encompassing a total of 4.5 months of user interactions. This dataset includes approximately 2.7 million sessions, which are structured to facilitate session-based recommendation research. The data encompasses various item properties and purchase events, making it a rich resource for understanding consumer behavior in an online retail environment. The dataset is organized in a tabular format, with rows representing individual sessions and columns detailing variables such as session ID, user ID, item ID, timestamps, and purchase indicators. Each session captures the sequence of user interactions, providing insights into browsing patterns and purchase decisions. The data collection methodology involved tracking user interactions on the e-commerce site, ensuring that the dataset reflects real-world behavior and preferences. However, researchers should be aware of potential limitations, such as data sparsity and the need for preprocessing to handle missing values or outliers. Common preprocessing steps may include normalization of item properties, encoding categorical variables, and segmenting sessions based on user behavior. This dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics, allowing researchers to explore questions related to consumer behavior, recommendation effectiveness, and marketing strategies. Typical research questions might involve understanding how session length impacts purchase likelihood, identifying key factors that influence user engagement, and evaluating the performance of different recommendation algorithms. Overall, the Retail Rocket dataset serves as a valuable tool for researchers and practitioners aiming to enhance their understanding of e-commerce dynamics and improve recommendation systems.",
    "tfidf_keywords": [
      "session-based",
      "recommendation-systems",
      "consumer-behavior",
      "purchase-events",
      "e-commerce",
      "data-preprocessing",
      "user-interactions",
      "behavioral-data",
      "item-properties",
      "marketing-strategies",
      "data-analytics"
    ],
    "semantic_cluster": "session-based-recommendations",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "recommendation-systems",
      "consumer-behavior",
      "data-preprocessing",
      "machine-learning",
      "e-commerce"
    ],
    "canonical_topics": [
      "recommendation-systems",
      "consumer-behavior",
      "machine-learning",
      "data-engineering"
    ],
    "benchmark_usage": [
      "Session-based recommendation research"
    ]
  },
  {
    "name": "Ember Global Electricity Data",
    "description": "Monthly electricity generation, capacity, and emissions data for 200+ countries",
    "category": "Energy",
    "url": "https://ember-climate.org/data/",
    "docs_url": "https://ember-climate.org/data-catalogue/",
    "github_url": "https://github.com/ember-climate/data-guidelines",
    "tags": [
      "global",
      "electricity",
      "emissions",
      "monthly",
      "countries"
    ],
    "best_for": "Cross-country energy transition analysis and global electricity trends",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Ember Global Electricity Data provides comprehensive monthly data on electricity generation, capacity, and emissions across over 200 countries. This dataset can be utilized for analyzing trends in energy production, understanding emissions impacts, and comparing energy policies globally.",
    "use_cases": [
      "Analyzing trends in electricity generation and capacity over time.",
      "Comparing emissions data across different countries.",
      "Assessing the impact of renewable energy policies on electricity generation.",
      "Investigating the relationship between electricity generation and economic growth."
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the monthly electricity generation data for different countries?",
      "How does electricity capacity vary across 200+ countries?",
      "What are the emissions associated with electricity generation globally?",
      "How can I analyze trends in electricity generation over time?",
      "What countries have the highest electricity generation capacity?",
      "How does electricity generation impact emissions in various countries?"
    ],
    "domain_tags": [
      "energy"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2000-present",
    "geographic_scope": "global",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/datasets/ember-global-electricity-data.png",
    "embedding_text": "The Ember Global Electricity Data is a rich dataset that captures monthly electricity generation, capacity, and emissions data for over 200 countries. This dataset is structured in a tabular format, with rows representing individual data entries for each country and each month, and columns detailing key variables such as total electricity generation, generation capacity, and associated emissions. The collection methodology involves aggregating data from various national and international energy agencies, ensuring a comprehensive view of global electricity trends. Key variables include total electricity generation measured in megawatt-hours (MWh), capacity measured in megawatts (MW), and emissions measured in metric tons of CO2 equivalent. While the dataset provides valuable insights, users should be aware of potential limitations such as variations in data reporting standards across countries and the availability of data for certain regions. Common preprocessing steps may include handling missing values, normalizing data for comparative analysis, and aggregating data to specific time frames for trend analysis. Researchers can use this dataset to address critical research questions related to energy policy, sustainability, and economic impacts of electricity generation. The dataset supports various types of analyses, including regression analysis to explore relationships between variables, machine learning for predictive modeling, and descriptive statistics to summarize data trends. Typically, researchers leverage this dataset to understand the dynamics of electricity generation and its effects on emissions, enabling informed decision-making in energy policy and sustainability initiatives.",
    "tfidf_keywords": [
      "electricity-generation",
      "emissions-data",
      "renewable-energy",
      "energy-capacity",
      "global-energy-trends",
      "sustainability-analysis",
      "policy-impact",
      "data-aggregation",
      "comparative-analysis",
      "energy-reporting-standards"
    ],
    "semantic_cluster": "energy-data-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "sustainability",
      "energy-policy",
      "environmental-impact",
      "economic-analysis",
      "data-visualization"
    ],
    "canonical_topics": [
      "forecasting",
      "statistics",
      "policy-evaluation"
    ]
  },
  {
    "name": "Space-Track.org",
    "description": "Official U.S. Space Force data on tracked orbital objects with 138+ million historical element sets",
    "category": "Space",
    "url": "https://www.space-track.org/",
    "docs_url": "https://www.space-track.org/documentation",
    "github_url": null,
    "tags": [
      "satellites",
      "orbital debris",
      "TLE",
      "space situational awareness"
    ],
    "best_for": "Tracking satellites and debris for space economics research",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "space",
      "satellites",
      "orbital debris",
      "TLE",
      "space situational awareness"
    ],
    "summary": "Space-Track.org provides official data from the U.S. Space Force on tracked orbital objects, encompassing over 138 million historical element sets. This dataset can be utilized for various analyses related to space situational awareness, satellite tracking, and understanding orbital debris dynamics.",
    "use_cases": [
      "Analyzing the trajectory of satellites over time.",
      "Studying the impact of orbital debris on satellite operations.",
      "Developing models for predicting satellite collisions.",
      "Assessing the effectiveness of space situational awareness strategies."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is Space-Track.org?",
      "How can I access U.S. Space Force data on orbital objects?",
      "What types of data are available on tracked satellites?",
      "How does Space-Track.org contribute to space situational awareness?",
      "What are TLEs and how are they used in satellite tracking?",
      "What historical data does Space-Track.org provide?",
      "How can I analyze orbital debris using Space-Track.org data?",
      "What are the applications of satellite tracking data?"
    ],
    "domain_tags": [
      "space",
      "defense"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1957-present",
    "geographic_scope": "Global (orbital)",
    "size_category": "massive",
    "model_score": 0.0001,
    "image_url": "/images/logos/space-track.png",
    "embedding_text": "Space-Track.org is a comprehensive dataset that provides official data from the U.S. Space Force on tracked orbital objects, featuring over 138 million historical element sets. The data structure typically includes rows representing individual orbital objects and columns detailing various attributes such as satellite identifiers, orbital elements, and timestamps. This dataset is crucial for researchers and analysts interested in space situational awareness, satellite tracking, and the dynamics of orbital debris. The collection methodology involves systematic tracking of satellites and debris using radar and other tracking technologies, ensuring high data quality and accuracy. However, users should be aware of potential limitations, such as gaps in historical data or variations in tracking accuracy based on satellite altitude and size. Common preprocessing steps may include filtering for specific satellite types, normalizing orbital elements, and handling missing data. Researchers can address a variety of questions using this dataset, such as the frequency of satellite launches, the distribution of orbital debris, and the potential for collisions in specific orbits. The dataset supports various types of analyses, including regression modeling, machine learning applications for predictive analytics, and descriptive statistics to summarize orbital trends. Typically, researchers use this data to enhance understanding of orbital mechanics, improve satellite mission planning, and develop strategies for mitigating the risks posed by space debris.",
    "tfidf_keywords": [
      "orbital mechanics",
      "satellite tracking",
      "TLE data",
      "orbital debris",
      "collision prediction",
      "space situational awareness",
      "tracking technologies",
      "historical element sets",
      "data quality",
      "preprocessing",
      "radar tracking",
      "space operations",
      "satellite dynamics",
      "data analysis"
    ],
    "semantic_cluster": "space-data-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "orbital dynamics",
      "satellite technology",
      "space policy",
      "data analytics",
      "collision risk assessment"
    ],
    "canonical_topics": [
      "machine-learning",
      "statistics",
      "data-engineering",
      "forecasting",
      "policy-evaluation"
    ]
  },
  {
    "name": "Data.gov",
    "description": "370,000+ datasets from US federal, state, and local agencies",
    "category": "Dataset Aggregators",
    "url": "https://data.gov",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "government",
      "US",
      "open data",
      "CKAN"
    ],
    "best_for": "Central clearinghouse for BLS, Census, BEA, and trade data",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Data.gov is a comprehensive repository of over 370,000 datasets sourced from various US federal, state, and local agencies. This platform provides access to a wealth of open data that can be utilized for research, policy-making, and public information purposes.",
    "use_cases": [
      "Analyzing trends in government spending",
      "Researching public health data",
      "Evaluating environmental data for policy-making"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What datasets are available on Data.gov?",
      "How can I access government datasets from Data.gov?",
      "What types of data does Data.gov provide?",
      "Where can I find open data related to US government agencies?",
      "How many datasets are available on Data.gov?",
      "What categories of datasets can I explore on Data.gov?"
    ],
    "domain_tags": [
      "government"
    ],
    "data_modality": "tabular",
    "size_category": "massive",
    "geographic_scope": "US",
    "model_score": 0.0001,
    "embedding_text": "Data.gov serves as a pivotal resource for accessing a vast array of datasets compiled from various levels of government in the United States. The platform aggregates over 370,000 datasets, making it one of the largest repositories of open data available to the public. The datasets cover a wide range of topics, including health, education, transportation, and environmental data, among others. Each dataset typically includes a structured format with rows and columns, where rows represent individual records or observations, and columns correspond to variables or attributes associated with those records. The collection methodology involves contributions from federal, state, and local agencies, ensuring a diverse and comprehensive data landscape. However, users should be aware of potential limitations in data quality, such as inconsistencies in data reporting or variations in data collection methods across different agencies. Common preprocessing steps may include data cleaning, normalization, and transformation to prepare the datasets for analysis. Researchers and analysts can leverage this extensive data for various purposes, including descriptive analysis, regression modeling, and machine learning applications. The open nature of the data allows for innovative research questions to be explored, such as the impact of government policies on public health outcomes or the analysis of economic trends across different regions. Overall, Data.gov is a valuable tool for researchers, policymakers, and the general public seeking to harness the power of open data for informed decision-making.",
    "tfidf_keywords": [
      "open data",
      "government datasets",
      "data aggregation",
      "public health data",
      "environmental data",
      "data quality",
      "data preprocessing",
      "policy analysis",
      "economic trends",
      "data collection methodology"
    ],
    "semantic_cluster": "open-data-repositories",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "data governance",
      "public policy",
      "data transparency",
      "statistical analysis",
      "data visualization"
    ],
    "canonical_topics": [
      "data-engineering",
      "policy-evaluation",
      "statistics"
    ]
  },
  {
    "name": "World Bank Light Every Night",
    "description": "30 years of nighttime satellite imagery (250 terabytes) from DMSP and VIIRS sensors. Foundational dataset for using lights as GDP proxy.",
    "category": "Geospatial",
    "url": "https://registry.opendata.aws/wb-light-every-night/",
    "docs_url": "https://worldbank.github.io/OpenNightLights/welcome.html",
    "github_url": "https://github.com/worldbank/OpenNightLights",
    "tags": [
      "satellite",
      "nighttime-lights",
      "GDP",
      "development",
      "World Bank"
    ],
    "best_for": "Economic measurement using satellite imagery, development economics research",
    "model_score": 0.0001,
    "image_url": "/images/datasets/world-bank-light-every-night.png",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The World Bank Light Every Night dataset comprises 30 years of nighttime satellite imagery collected from DMSP and VIIRS sensors, totaling 250 terabytes of data. This dataset serves as a foundational resource for researchers and analysts interested in using nighttime lights as a proxy for GDP and development metrics across various regions.",
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the World Bank Light Every Night dataset?",
      "How can nighttime satellite imagery be used as a GDP proxy?",
      "What are the sources of the Light Every Night dataset?",
      "What insights can be gained from analyzing nighttime lights data?",
      "How does the World Bank utilize nighttime satellite imagery?",
      "What are the applications of nighttime lights in economic research?",
      "What sensors are used to collect nighttime satellite imagery?",
      "What is the significance of the 30-year coverage in this dataset?"
    ],
    "use_cases": [
      "Analyzing economic development trends using nighttime lights as a proxy for GDP.",
      "Studying the impact of urbanization on nighttime light intensity.",
      "Evaluating the effectiveness of policy interventions in developing regions.",
      "Exploring correlations between nighttime lights and demographic changes."
    ],
    "embedding_text": "The World Bank Light Every Night dataset is a comprehensive collection of nighttime satellite imagery that spans over three decades, sourced from the Defense Meteorological Satellite Program (DMSP) and the Visible Infrared Imaging Radiometer Suite (VIIRS). This dataset, totaling approximately 250 terabytes, is pivotal for researchers and policymakers aiming to leverage nighttime lights as a proxy for economic indicators such as Gross Domestic Product (GDP) and development levels. The imagery captures the illumination of urban and rural areas, providing insights into economic activity, infrastructure development, and population density. The data structure typically consists of rows representing individual satellite images, with columns detailing variables such as light intensity, geographic coordinates, and time stamps. The collection methodology involves satellite sensors that capture light emissions from the Earth's surface, which are then processed and made available for analysis. Key variables in this dataset include light intensity, which measures the brightness of specific geographic areas, and temporal markers that indicate when the images were captured. Researchers often face challenges related to data quality, such as variations in sensor calibration and atmospheric conditions that can affect light readings. Common preprocessing steps include normalizing light intensity values and aligning images to a consistent geographic framework. This dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics. It enables researchers to address critical research questions related to economic growth, urbanization patterns, and the socio-economic impacts of infrastructure development. By utilizing this dataset, analysts can derive meaningful insights that inform policy decisions and contribute to the understanding of global development trends.",
    "domain_tags": [
      "development",
      "economics",
      "geospatial"
    ],
    "data_modality": "image",
    "temporal_coverage": "30 years",
    "size_category": "massive",
    "benchmark_usage": [
      "Using lights as GDP proxy"
    ],
    "tfidf_keywords": [
      "nighttime satellite imagery",
      "DMSP",
      "VIIRS",
      "GDP proxy",
      "economic development",
      "urbanization",
      "light intensity",
      "policy evaluation",
      "data quality",
      "preprocessing",
      "geospatial analysis",
      "infrastructure development",
      "temporal coverage",
      "remote sensing"
    ],
    "semantic_cluster": "geospatial-economic-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "remote-sensing",
      "economic-indicators",
      "urban-studies",
      "data-visualization",
      "spatial-analysis"
    ],
    "canonical_topics": [
      "econometrics",
      "forecasting",
      "geospatial",
      "policy-evaluation"
    ]
  },
  {
    "name": "US 2020 Election Study",
    "description": "Facebook/Instagram impact on political attitudes. Published in Science/Nature 2023. SOMAR Michigan access",
    "category": "Social & Web",
    "url": "https://www.icpsr.umich.edu/web/ICPSR/series/2045",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Facebook",
      "Instagram",
      "elections",
      "politics",
      "social media"
    ],
    "best_for": "Learning social & web analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "social-media",
      "politics",
      "elections"
    ],
    "summary": "The US 2020 Election Study dataset examines the impact of Facebook and Instagram on political attitudes during the 2020 election cycle. Researchers can utilize this dataset to analyze social media's influence on voter behavior and political engagement.",
    "use_cases": [
      "Analyzing the correlation between social media usage and voter turnout",
      "Investigating the impact of targeted ads on political opinions",
      "Studying demographic differences in social media influence on elections"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the impact of Facebook on political attitudes?",
      "How does Instagram influence voter behavior?",
      "What are the effects of social media on elections?",
      "What data is available from the US 2020 Election Study?",
      "How can I analyze political engagement through social media?",
      "What variables are included in the US 2020 Election Study dataset?"
    ],
    "domain_tags": [
      "politics",
      "social-media"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/logos/umich.png",
    "embedding_text": "The US 2020 Election Study dataset provides a comprehensive examination of the effects of social media platforms, specifically Facebook and Instagram, on political attitudes during the 2020 election cycle. This dataset is structured in a tabular format, containing rows that represent individual respondents and columns that capture various demographic and behavioral variables. Key variables include social media usage frequency, political affiliation, voter turnout intentions, and responses to political advertisements. The data was collected through surveys administered to a diverse sample of participants, ensuring a broad representation of the electorate. Researchers can expect to encounter some limitations regarding data quality, such as potential biases in self-reported measures and the challenges of capturing nuanced political attitudes. Common preprocessing steps may involve cleaning the data, handling missing values, and normalizing responses for analysis. This dataset supports a range of analyses, including regression modeling, machine learning applications, and descriptive statistics, allowing researchers to explore questions related to the influence of social media on voter behavior and engagement. Typical research questions might include assessing how social media exposure correlates with changes in political attitudes or examining the effectiveness of targeted political advertising on different demographic groups. Overall, the US 2020 Election Study dataset serves as a valuable resource for understanding the intersection of social media and political behavior during a pivotal election period.",
    "tfidf_keywords": [
      "social-media-influence",
      "political-attitudes",
      "voter-behavior",
      "Facebook",
      "Instagram",
      "political-engagement",
      "targeted-advertising",
      "demographic-analysis",
      "election-study",
      "survey-data"
    ],
    "semantic_cluster": "social-media-politics",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "causal-inference",
      "behavioral-economics",
      "consumer-behavior",
      "political-economy",
      "data-collection"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "policy-evaluation",
      "experimentation"
    ]
  },
  {
    "name": "Medicare Claims (ResDAC)",
    "description": "Comprehensive claims data covering 98%+ of adults 65+ in the United States. Includes inpatient, outpatient, physician, and prescription drug claims. Research Identifiable Files require 6-12 month DUA approval.",
    "category": "Healthcare Economics & Health-Tech",
    "url": "https://resdac.org/",
    "source": "CMS via Research Data Assistance Center",
    "type": "Claims Database",
    "access": "Fee-based (DUA required)",
    "format": "SAS/CSV",
    "tags": [
      "Healthcare",
      "Claims",
      "Medicare",
      "Administrative"
    ],
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "healthcare",
      "claims",
      "medicare"
    ],
    "summary": "The Medicare Claims dataset provides comprehensive claims data for over 98% of adults aged 65 and older in the United States, covering various types of healthcare services including inpatient, outpatient, physician, and prescription drug claims. Researchers can utilize this dataset to analyze healthcare utilization, costs, and outcomes among the elderly population.",
    "use_cases": [
      "Analyzing trends in healthcare utilization among older adults",
      "Evaluating the impact of policy changes on Medicare spending",
      "Investigating prescription drug usage patterns in the elderly",
      "Assessing the quality of care received by Medicare beneficiaries"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Medicare Claims dataset?",
      "How can I access Medicare claims data?",
      "What types of claims are included in the Medicare dataset?",
      "What demographic information is available in the Medicare Claims dataset?",
      "How can I analyze healthcare costs using Medicare claims?",
      "What research questions can be addressed with Medicare claims data?",
      "What are the limitations of the Medicare Claims dataset?",
      "How does Medicare claims data support healthcare research?"
    ],
    "update_frequency": "Annual",
    "geographic_coverage": "United States (national)",
    "domain_tags": [
      "healthcare"
    ],
    "data_modality": "tabular",
    "geographic_scope": "United States",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/logos/resdac.png",
    "embedding_text": "The Medicare Claims dataset, provided by ResDAC, is a rich source of healthcare claims data that encompasses over 98% of adults aged 65 and older in the United States. This dataset includes various types of claims such as inpatient, outpatient, physician, and prescription drug claims, making it a comprehensive resource for researchers interested in the healthcare experiences of the elderly population. The data structure typically consists of rows representing individual claims and columns detailing various attributes such as claim type, service dates, provider information, and patient demographics. Researchers often utilize this dataset to explore a wide range of research questions related to healthcare utilization, costs, and outcomes. The collection methodology involves gathering claims data from Medicare administrative records, ensuring a high level of accuracy and reliability. However, researchers should be aware of potential limitations, including data quality issues and the need for appropriate data use agreements (DUAs) for accessing research identifiable files. Common preprocessing steps may include data cleaning, normalization, and merging with other datasets to enrich the analysis. The dataset supports various types of analyses, including regression, machine learning, and descriptive statistics, allowing for in-depth exploration of healthcare trends and impacts. Researchers typically use this dataset to inform policy evaluations, assess healthcare interventions, and study the effects of demographic factors on healthcare access and outcomes.",
    "tfidf_keywords": [
      "claims-data",
      "healthcare-utilization",
      "elderly-population",
      "Medicare",
      "inpatient-claims",
      "outpatient-claims",
      "prescription-drug-claims",
      "data-quality",
      "policy-evaluation",
      "healthcare-costs",
      "demographic-analysis",
      "data-use-agreement",
      "administrative-records"
    ],
    "semantic_cluster": "healthcare-claims-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "healthcare-economics",
      "policy-evaluation",
      "cost-analysis",
      "demographic-studies",
      "healthcare-outcomes"
    ],
    "canonical_topics": [
      "healthcare",
      "econometrics",
      "policy-evaluation"
    ]
  },
  {
    "name": "Citi Bike System Data",
    "description": "Trip-level records for NYC's bike-share system since 2013. ~2 million trips monthly in peak season with station-level origin-destination and duration data.",
    "category": "Transportation Economics & Technology",
    "url": "https://citibikenyc.com/system-data",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "bike-share",
      "NYC",
      "micromobility",
      "trip-data",
      "cycling"
    ],
    "best_for": "Bike-share demand analysis, first/last mile research, and micromobility studies",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "transportation",
      "urban-planning",
      "sustainability"
    ],
    "summary": "The Citi Bike System Data provides trip-level records for New York City's bike-share system, offering insights into cycling patterns, station usage, and trip durations. Researchers can leverage this dataset to analyze urban mobility trends, evaluate the impact of bike-sharing on transportation, and inform policy decisions regarding micromobility.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the trip patterns in NYC's Citi Bike system?",
      "How does weather affect bike-share usage in NYC?",
      "What are the peak usage times for Citi Bike?",
      "How do trip durations vary by station in NYC?",
      "What demographic factors influence bike-share usage?",
      "How has the Citi Bike system evolved since 2013?",
      "What is the impact of bike-sharing on urban traffic congestion?",
      "How can data from Citi Bike inform sustainable transportation policies?"
    ],
    "use_cases": [
      "Analyzing seasonal trends in bike-share usage",
      "Evaluating the effectiveness of bike-share programs in reducing traffic",
      "Studying the relationship between bike-share usage and urban infrastructure",
      "Assessing the impact of special events on bike-share demand"
    ],
    "domain_tags": [
      "transportation",
      "urban-planning"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2013-present",
    "geographic_scope": "New York City",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/logos/citibikenyc.png",
    "embedding_text": "The Citi Bike System Data is a comprehensive dataset that contains trip-level records from New York City's bike-share program, which has been operational since 2013. This dataset includes a wealth of information on approximately 2 million trips taken each month during peak seasons, providing valuable insights into urban mobility and transportation economics. The data structure typically consists of rows representing individual trips, with columns detailing variables such as trip duration, start and end station locations, and timestamps. Researchers can utilize this dataset to explore various aspects of bike-sharing, including usage patterns, demographic influences, and the impact of external factors like weather and events on cycling behavior. The collection methodology involves aggregating trip data from the Citi Bike system, which captures real-time usage statistics, ensuring a robust and dynamic dataset. Coverage is extensive, both temporally and geographically, focusing on New York City and spanning nearly a decade of bike-share operations. Key variables in the dataset include trip duration, start and end station IDs, and timestamps, which collectively measure the frequency and duration of bike usage across different locations and times. While the dataset is rich in detail, researchers should be aware of potential limitations, such as data quality issues related to incomplete records or variations in user behavior. Common preprocessing steps may include cleaning the data for missing values, normalizing trip durations, and aggregating data by time intervals for analysis. This dataset supports a variety of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it a versatile resource for researchers interested in transportation economics and urban planning. Typical research questions addressed using this dataset include understanding the factors that drive bike-share usage, evaluating the effectiveness of bike-sharing in reducing urban congestion, and analyzing the spatial distribution of bike trips across the city. By leveraging the Citi Bike System Data, researchers can contribute to the growing body of knowledge surrounding sustainable transportation solutions and urban mobility.",
    "tfidf_keywords": [
      "bike-share",
      "urban mobility",
      "trip duration",
      "station usage",
      "micromobility",
      "data analysis",
      "transportation economics",
      "NYC cycling",
      "spatial distribution",
      "seasonal trends",
      "user behavior",
      "policy evaluation",
      "data preprocessing",
      "regression modeling",
      "machine learning"
    ],
    "semantic_cluster": "urban-mobility-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "urban-planning",
      "sustainable-transportation",
      "data-visualization",
      "public-policy",
      "mobility-patterns"
    ],
    "canonical_topics": [
      "transportation",
      "consumer-behavior",
      "policy-evaluation",
      "econometrics"
    ]
  },
  {
    "name": "NBA Stats API",
    "description": "Official NBA statistics including shot charts, play-by-play, player tracking data, and historical records",
    "category": "Sports & Athletics",
    "url": "https://www.nba.com/stats/",
    "docs_url": "https://github.com/swar/nba_api/blob/master/docs/table_of_contents.md",
    "github_url": null,
    "tags": [
      "basketball",
      "NBA",
      "shot-charts",
      "tracking-data",
      "play-by-play"
    ],
    "best_for": "Basketball analytics, shot analysis, and player performance evaluation",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "sports",
      "data-analysis",
      "statistics"
    ],
    "summary": "The NBA Stats API provides comprehensive official statistics from the National Basketball Association, including detailed shot charts, play-by-play data, player tracking metrics, and historical records. This dataset allows users to perform in-depth analyses of player performance, game strategies, and historical trends in basketball.",
    "audience": [
      "Curious-browser",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What are the official NBA statistics available through the NBA Stats API?",
      "How can I access shot charts from the NBA Stats API?",
      "What types of player tracking data does the NBA Stats API provide?",
      "Where can I find historical NBA records using the NBA Stats API?",
      "How do I analyze play-by-play data from the NBA Stats API?",
      "What insights can I gain from the NBA Stats API regarding basketball performance?"
    ],
    "use_cases": [
      "Analyzing player shooting efficiency through shot charts.",
      "Evaluating game strategies using play-by-play data.",
      "Tracking player movements and performance metrics over a season.",
      "Comparing historical player statistics to current performance."
    ],
    "domain_tags": [
      "sports",
      "entertainment"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/datasets/nba-stats-api.png",
    "embedding_text": "The NBA Stats API is a robust resource for accessing official statistics from the National Basketball Association. This dataset encompasses a variety of data structures, including shot charts that visualize player shooting patterns, play-by-play data that details every action during a game, and player tracking data that captures the movements and performance metrics of players throughout the season. The data is structured in a tabular format, with rows representing individual game events or player actions, and columns containing variables such as player names, shot locations, time stamps, and game scores. The collection methodology involves aggregating data from official NBA games, ensuring high accuracy and reliability. While the dataset provides a wealth of information, users should be aware of potential limitations, such as missing data for certain games or players and the need for preprocessing steps like data cleaning and normalization to prepare the data for analysis. Researchers and analysts can leverage this dataset to address various research questions, such as evaluating player efficiency, analyzing team strategies, and understanding trends in player performance over time. The types of analyses supported by this dataset include regression analysis, machine learning applications, and descriptive statistics, making it a versatile tool for sports analysts, data scientists, and basketball enthusiasts alike.",
    "tfidf_keywords": [
      "shot-charts",
      "play-by-play",
      "player-tracking",
      "NBA",
      "basketball-statistics",
      "game-strategy",
      "performance-analysis",
      "data-visualization",
      "sports-analytics",
      "historical-records",
      "efficiency-metrics",
      "data-collection",
      "event-logging",
      "statistical-modeling"
    ],
    "semantic_cluster": "sports-data-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "sports-analytics",
      "data-visualization",
      "performance-metrics",
      "statistical-analysis",
      "machine-learning"
    ],
    "canonical_topics": [
      "statistics",
      "machine-learning",
      "data-engineering"
    ]
  },
  {
    "name": "Call Centre Queue Simulation",
    "description": "One year of simulated call center data with arrival times, handle times, and outcomes. Ideal for estimating arrival rates and validating Erlang-C staffing models.",
    "category": "Operations & Service",
    "url": "https://www.kaggle.com/datasets/donovanbangs/call-centre-queue-simulation",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "queueing",
      "call-center",
      "simulation",
      "Erlang-C",
      "staffing",
      "Kaggle"
    ],
    "best_for": "Queueing theory - estimate arrival rates \u03bb(t), fit service time distributions, validate Erlang-C formulas",
    "image_url": "/images/datasets/call-centre-queue-simulation.png",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Call Centre Queue Simulation dataset contains one year of simulated call center data, including arrival times, handle times, and outcomes. This dataset is particularly useful for estimating arrival rates and validating Erlang-C staffing models, making it valuable for operations and service analysis.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Call Centre Queue Simulation dataset?",
      "How can I use simulated call center data for staffing models?",
      "What are the key variables in the Call Centre Queue Simulation dataset?",
      "Where can I find datasets for call center operations analysis?",
      "How does Erlang-C apply to call center simulations?",
      "What types of analyses can be performed with call center data?",
      "What are the outcomes measured in call center simulations?",
      "How can I estimate arrival rates using this dataset?"
    ],
    "use_cases": [
      "Estimating staffing requirements for call centers",
      "Validating Erlang-C models for service operations",
      "Analyzing call arrival patterns and handle times",
      "Simulating different call center scenarios to improve efficiency"
    ],
    "embedding_text": "The Call Centre Queue Simulation dataset is a comprehensive resource for researchers and practitioners interested in the dynamics of call center operations. This dataset consists of simulated call center data over a one-year period, capturing critical metrics such as arrival times, handle times, and outcomes. The data is structured in a tabular format, with rows representing individual calls and columns detailing various attributes related to each call, including timestamps for arrivals and completions, as well as the outcomes of each interaction. This structured approach allows for easy manipulation and analysis using common data analysis tools and programming languages, such as Python with pandas or R. The collection methodology involves simulating call center operations, which provides a controlled environment to study the effects of different variables on performance metrics. As such, the dataset is particularly useful for estimating arrival rates and validating staffing models based on the Erlang-C formula, a widely used mathematical model in telecommunications and service operations. Key variables in the dataset include arrival times, which indicate when calls enter the queue, handle times that reflect the duration of each call, and outcomes that categorize the result of each interaction (e.g., resolved, abandoned). While the dataset offers valuable insights, it is important to note that being simulated data, it may not capture all the complexities and variabilities of real-world call center operations, which could affect the generalizability of findings. Common preprocessing steps may include cleaning the data for any inconsistencies, transforming timestamps into appropriate formats, and aggregating data for analysis. Researchers can utilize this dataset to address various research questions, such as determining optimal staffing levels during peak hours, analyzing the impact of call volume on service quality, or evaluating the effectiveness of different call handling strategies. The dataset supports a range of analyses, including regression analyses to identify relationships between variables, machine learning models for predictive analytics, and descriptive statistics to summarize call center performance. Overall, the Call Centre Queue Simulation dataset serves as a valuable tool for those looking to enhance their understanding of call center dynamics and improve operational efficiency.",
    "domain_tags": [
      "operations",
      "service"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "null",
    "geographic_scope": "null",
    "size_category": "medium",
    "benchmark_usage": [
      "Estimating arrival rates",
      "Validating Erlang-C staffing models"
    ],
    "tfidf_keywords": [
      "call center",
      "queue simulation",
      "Erlang-C",
      "arrival times",
      "handle times",
      "staffing models",
      "service operations",
      "performance metrics",
      "data analysis",
      "predictive analytics",
      "operational efficiency",
      "simulated data",
      "service quality",
      "call volume",
      "data preprocessing"
    ],
    "semantic_cluster": "call-center-operations",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "queueing theory",
      "operations management",
      "service optimization",
      "simulation modeling",
      "performance analysis"
    ],
    "canonical_topics": [
      "operations",
      "statistics",
      "machine-learning"
    ],
    "model_score": 0.0001
  },
  {
    "name": "Call Center Data",
    "description": "Daily call center performance metrics including call volumes, handle times, and agent performance.",
    "category": "Operations & Service",
    "url": "https://www.kaggle.com/datasets/satvicoder/call-center-data",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "queueing",
      "call-center",
      "performance-metrics",
      "operations",
      "Kaggle"
    ],
    "best_for": "Queueing theory - analyze daily patterns, agent utilization, service level optimization",
    "image_url": "/images/datasets/call-center-data.jpg",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Call Center Data provides daily performance metrics for call centers, including key indicators such as call volumes, handle times, and agent performance. This dataset can be used to analyze operational efficiency, identify trends in call handling, and improve service delivery.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the daily call volumes in the call center dataset?",
      "How can I analyze agent performance using call center metrics?",
      "What is the average handle time for calls in the dataset?",
      "How do call volumes vary over different times of the day?",
      "What performance metrics are included in the call center data?",
      "How can I visualize call center performance metrics?",
      "What trends can be identified from daily call center data?",
      "How does agent performance impact overall call center efficiency?"
    ],
    "use_cases": [
      "Analyzing trends in call volumes over time.",
      "Evaluating agent performance based on handle times and call outcomes.",
      "Identifying peak call times to optimize staffing.",
      "Assessing the impact of operational changes on service metrics."
    ],
    "embedding_text": "The Call Center Data is structured as a tabular dataset containing daily performance metrics from call centers. Each row represents a day of operations, while columns include variables such as call volumes, average handle times, and individual agent performance metrics. This dataset is crucial for understanding the operational dynamics of call centers, allowing analysts to derive insights into efficiency and service quality. The data is typically collected through call center management systems that log each interaction, capturing essential details that reflect both the volume of calls and the performance of agents handling those calls. Researchers can utilize this dataset to address various research questions, such as identifying factors that influence call handling efficiency or examining the relationship between call volumes and customer satisfaction. Common preprocessing steps may include cleaning the data to handle missing values, normalizing metrics for comparative analysis, and aggregating data to derive insights over specific time frames. The dataset supports a range of analyses, including regression analysis to predict call handling times based on volume, machine learning models to classify agent performance, and descriptive statistics to summarize overall performance trends. Researchers typically use this dataset to inform operational decisions, enhance training programs for agents, and improve customer service strategies.",
    "domain_tags": [
      "operations",
      "service"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "tfidf_keywords": [
      "call-volume",
      "handle-time",
      "agent-performance",
      "operational-efficiency",
      "service-delivery",
      "performance-metrics",
      "call-center-management",
      "data-analysis",
      "customer-satisfaction",
      "trend-analysis",
      "staff-optimization",
      "data-visualization",
      "efficiency-improvement"
    ],
    "semantic_cluster": "call-center-analytics",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "performance-evaluation",
      "operational-analysis",
      "customer-service",
      "data-visualization",
      "statistical-analysis"
    ],
    "canonical_topics": [
      "statistics",
      "data-engineering",
      "consumer-behavior",
      "operations",
      "machine-learning"
    ],
    "model_score": 0.0001
  },
  {
    "name": "Queue Waiting Time Prediction",
    "description": "Call center data with arrival time, service start/end times, waiting time, and queue length. Perfect for validating queueing theory formulas.",
    "category": "Operations & Service",
    "url": "https://www.kaggle.com/datasets/sanjeebtiwary/queue-waiting-time-prediction",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "queueing",
      "waiting-time",
      "service-times",
      "queue-length",
      "Erlang-C",
      "Kaggle"
    ],
    "best_for": "Queueing theory - direct Erlang-C validation, compare predicted vs actual wait times",
    "image_url": "/images/datasets/queue-waiting-time-prediction.jpg",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [],
    "summary": "The Queue Waiting Time Prediction dataset contains call center data that includes variables such as arrival time, service start and end times, waiting time, and queue length. This dataset is ideal for validating queueing theory formulas and can be utilized for various analyses related to service efficiency and customer experience.",
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the average waiting time in the dataset?",
      "How does queue length affect service times?",
      "Can we predict waiting times based on arrival patterns?",
      "What are the peak hours for call center traffic?",
      "How does the data validate Erlang-C formulas?",
      "What is the distribution of service times in this dataset?",
      "How can we visualize waiting times over different time periods?",
      "What factors contribute to longer waiting times?"
    ],
    "use_cases": [
      "Analyzing peak waiting times during specific hours",
      "Validating queueing theory models",
      "Predicting service efficiency based on historical data"
    ],
    "embedding_text": "The Queue Waiting Time Prediction dataset is a structured collection of call center data that provides insights into customer service dynamics. The dataset includes various key variables such as arrival time, service start and end times, waiting time, and queue length, which are essential for understanding and analyzing the performance of call centers. Researchers and analysts can utilize this dataset to explore the relationships between different variables, such as how arrival patterns influence waiting times and service efficiency. The data is typically organized in a tabular format, with each row representing a unique call instance and columns corresponding to the various attributes of that call. This structure allows for straightforward data manipulation and analysis using tools like pandas in Python.\n\nThe collection methodology for this dataset likely involves systematic logging of call center operations, capturing relevant timestamps and metrics as calls are processed. While the specific data sources are not mentioned, it can be inferred that the data is collected from operational call center software or systems that track customer interactions.\n\nIn terms of coverage, the dataset does not specify any temporal or geographic limitations, making it potentially applicable to various call center environments. However, researchers should be aware of any limitations in data quality, such as missing values or outliers, which may affect the analysis. Common preprocessing steps might include handling missing data, normalizing time formats, and aggregating data to analyze trends over specific periods.\n\nThe dataset supports a range of analyses, including regression modeling to predict waiting times based on arrival patterns and service times, as well as descriptive statistics to summarize the overall performance of the call center. Researchers can also explore machine learning techniques to develop predictive models that enhance service efficiency. By leveraging this dataset, analysts can address critical research questions related to customer experience, operational efficiency, and the validation of queueing theory formulas, particularly the Erlang-C model, which is widely used in telecommunications and service operations. Overall, the Queue Waiting Time Prediction dataset serves as a valuable resource for those studying operations management and service optimization.",
    "domain_tags": [
      "operations",
      "service"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "tfidf_keywords": [
      "queueing-theory",
      "Erlang-C",
      "service-efficiency",
      "waiting-time",
      "call-center-operations",
      "predictive-modeling",
      "customer-experience",
      "data-preprocessing",
      "regression-analysis",
      "time-series-analysis"
    ],
    "semantic_cluster": "queueing-theory-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "service-operations",
      "predictive-analytics",
      "customer-service",
      "data-visualization",
      "performance-metrics"
    ],
    "canonical_topics": [
      "operations",
      "statistics",
      "machine-learning"
    ],
    "model_score": 0.0001
  },
  {
    "name": "Server Logs",
    "description": "Apache server logs with timestamps and response times for modeling web server queues.",
    "category": "Technology & Infrastructure",
    "url": "https://www.kaggle.com/datasets/vishnu0399/server-logs",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "queueing",
      "server-logs",
      "response-time",
      "web",
      "Apache",
      "Kaggle"
    ],
    "best_for": "Queueing theory - estimate request arrival rates, M/M/c or M/G/1 modeling, SLA planning",
    "image_url": "/images/datasets/server-logs.png",
    "difficulty": "beginner",
    "prerequisites": [
      "pandas-dataframes",
      "basic-analytics"
    ],
    "topic_tags": [],
    "summary": "The Server Logs dataset contains Apache server logs that include timestamps and response times, which are essential for modeling web server queues. This dataset can be utilized to analyze server performance, optimize response times, and understand user interactions with web applications.",
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are Apache server logs?",
      "How can I analyze response times from server logs?",
      "What insights can be gained from web server queue modeling?",
      "How do timestamps in server logs affect performance analysis?",
      "What tools can be used to process Apache server logs?",
      "How to visualize response times from server logs?"
    ],
    "use_cases": [
      "Analyzing server performance",
      "Optimizing web application response times",
      "Modeling user interactions",
      "Identifying traffic patterns"
    ],
    "embedding_text": "The Server Logs dataset consists of Apache server logs that capture critical information about web server performance, including timestamps and response times. Each log entry typically contains various fields such as the date and time of the request, the IP address of the client, the requested resource, the response status code, and the time taken to serve the request. This structured data is invaluable for modeling web server queues, allowing researchers and practitioners to analyze how servers handle incoming requests and to identify potential bottlenecks in performance. The collection methodology for these logs involves the automatic logging of requests by the Apache web server software, which records each interaction with the server in real-time. This dataset does not specify temporal or geographic coverage, but it is generally applicable to any web application utilizing Apache as its server. Key variables in the dataset include timestamps, which measure when each request was made, and response times, which quantify how long the server took to respond to each request. These variables are crucial for understanding server load and performance metrics. However, known limitations may include the potential for incomplete logs if the server experiences outages or if logging is misconfigured. Common preprocessing steps for this dataset might involve cleaning the log entries to remove any irrelevant data, parsing the timestamps into a usable format, and aggregating response times to analyze trends over specific periods. Researchers can address various questions using this dataset, such as: How does server load impact response times? What are the peak usage times for a web application? How can response times be improved through optimization techniques? The types of analyses supported by this dataset include regression analysis to predict response times based on server load, machine learning models to classify request types, and descriptive statistics to summarize server performance over time. Typically, researchers use server logs in studies to assess the efficiency of web applications, to conduct performance benchmarking, and to inform infrastructure improvements based on user behavior and server interactions.",
    "domain_tags": [
      "technology",
      "infrastructure"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "tfidf_keywords": [
      "Apache",
      "server-logs",
      "response-times",
      "web-application",
      "performance-analysis",
      "queue-modeling",
      "timestamp",
      "request",
      "bottlenecks",
      "log-entry"
    ],
    "semantic_cluster": "web-performance-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "web-analytics",
      "server-optimization",
      "performance-monitoring",
      "data-logging",
      "traffic-analysis"
    ],
    "canonical_topics": [
      "machine-learning",
      "data-engineering",
      "statistics"
    ],
    "model_score": 0.0001
  },
  {
    "name": "Synthetic Distributed System Logs",
    "description": "Log data from distributed systems showing request patterns and processing times across multiple nodes.",
    "category": "Technology & Infrastructure",
    "url": "https://www.kaggle.com/datasets/shubhampatil1999/synthetic-log-data-of-distributed-system",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "queueing",
      "distributed-systems",
      "logs",
      "synthetic",
      "microservices",
      "Kaggle"
    ],
    "best_for": "Queueing theory - network of queues, load balancing analysis, service time estimation",
    "image_url": "/images/datasets/synthetic-distributed-system-logs.png",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [],
    "summary": "The Synthetic Distributed System Logs dataset contains log data from distributed systems, capturing request patterns and processing times across multiple nodes. Researchers can utilize this dataset to analyze system performance, identify bottlenecks, and optimize resource allocation in distributed architectures.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the request patterns in distributed systems?",
      "How do processing times vary across nodes?",
      "What insights can be drawn from synthetic log data?",
      "How can this dataset help optimize microservices?",
      "What are common bottlenecks in distributed systems?",
      "How does queueing affect system performance?"
    ],
    "use_cases": [
      "Analyzing request patterns in distributed systems",
      "Identifying performance bottlenecks in microservices",
      "Optimizing resource allocation across nodes"
    ],
    "embedding_text": "The Synthetic Distributed System Logs dataset is a comprehensive collection of log data generated from distributed systems, specifically designed to reflect realistic request patterns and processing times across multiple nodes. This dataset is structured in a tabular format, with each row representing a unique log entry that captures various attributes such as timestamp, request type, processing time, and node identifier. The columns are meticulously designed to facilitate analysis, allowing researchers to delve into the intricacies of distributed system behavior. The collection methodology involves simulating a distributed environment where synthetic requests are generated and logged, ensuring a diverse range of scenarios are covered. While the dataset does not explicitly mention temporal or geographic coverage, it is essential to note that the synthetic nature of the data allows for flexibility in application across various contexts. Key variables within the dataset include request type, which categorizes the nature of the requests made, processing time, which measures the efficiency of the system in handling requests, and node identifier, which distinguishes between different nodes in the distributed architecture. Researchers should be aware of potential limitations regarding data quality, as synthetic data may not capture all real-world complexities. Common preprocessing steps may include data cleaning, normalization of processing times, and transformation of categorical variables for analysis. The dataset supports a variety of research questions, such as examining the impact of request patterns on system performance and identifying optimal configurations for resource allocation. It is particularly useful for analyses involving regression techniques, machine learning models, and descriptive statistics. Researchers typically leverage this dataset to explore system performance metrics, optimize microservices, and enhance the overall efficiency of distributed systems.",
    "domain_tags": [
      "technology",
      "infrastructure"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "tfidf_keywords": [
      "distributed-systems",
      "synthetic-logs",
      "request-patterns",
      "processing-times",
      "microservices",
      "queueing",
      "performance-optimization",
      "resource-allocation",
      "bottlenecks",
      "log-analysis"
    ],
    "semantic_cluster": "distributed-systems-optimization",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "system-performance",
      "resource-management",
      "log-analysis",
      "queueing-theory",
      "microservices-architecture"
    ],
    "canonical_topics": [
      "machine-learning",
      "data-engineering",
      "statistics"
    ],
    "model_score": 0.0001
  },
  {
    "name": "TfL Unified API",
    "description": "Transport for London real-time and historical transit data including arrivals, departures, and service disruptions across all modes.",
    "category": "Transportation & Mobility",
    "url": "https://api.tfl.gov.uk/",
    "docs_url": "https://api-portal.tfl.gov.uk/docs",
    "github_url": null,
    "tags": [
      "transit",
      "London",
      "API",
      "real-time",
      "public-transport"
    ],
    "best_for": "Platform crowding analysis, headway modeling, service reliability",
    "image_url": "/images/logos/tfl.gov.png",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The TfL Unified API provides real-time and historical transit data for Transport for London, covering arrivals, departures, and service disruptions across various modes of transport. This dataset can be utilized for analyzing public transportation patterns, optimizing transit operations, and enhancing user experience in urban mobility.",
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the TfL Unified API?",
      "How can I access real-time transit data for London?",
      "What types of data does the TfL Unified API provide?",
      "How can I analyze service disruptions in London transit?",
      "What are the historical transit patterns in London?",
      "How does the TfL Unified API support public transport analysis?"
    ],
    "use_cases": [
      "Analyzing transit delays and their impact on commuter behavior",
      "Optimizing bus and train schedules based on historical data",
      "Studying the effects of service disruptions on passenger traffic",
      "Enhancing mobile applications for real-time transit updates"
    ],
    "embedding_text": "The TfL Unified API is a comprehensive resource for accessing real-time and historical transit data from Transport for London (TfL). This dataset encompasses a wide range of information, including arrivals, departures, and service disruptions across all modes of public transport in London. The data is structured in a tabular format, with rows representing individual transit events and columns detailing various attributes such as time, mode of transport, and service status. The collection methodology involves aggregating data from TfL's operational systems, ensuring that users receive accurate and timely information. Researchers and developers can leverage this dataset to address a variety of research questions, such as analyzing the impact of service disruptions on commuter behavior, optimizing transit schedules based on historical patterns, and studying the overall efficiency of public transport systems. Common preprocessing steps may include cleaning the data for missing values, normalizing time formats, and aggregating data over specific time intervals for analysis. The dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics. It is particularly valuable for urban planners, transportation analysts, and developers creating applications that enhance user experience in navigating public transport. However, users should be aware of potential limitations, such as data quality issues related to real-time updates and the need for careful interpretation of historical trends. Overall, the TfL Unified API serves as a vital tool for understanding and improving public transportation in one of the world's busiest cities.",
    "tfidf_keywords": [
      "real-time transit data",
      "service disruptions",
      "public transport analysis",
      "historical transit patterns",
      "urban mobility",
      "transit optimization",
      "commuter behavior",
      "TfL Unified API",
      "London transport",
      "data aggregation",
      "transit schedules",
      "data quality",
      "time-series analysis",
      "public transportation systems",
      "operational data"
    ],
    "semantic_cluster": "transportation-data-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "urban mobility",
      "transportation analytics",
      "public transport systems",
      "data visualization",
      "service optimization"
    ],
    "canonical_topics": [
      "transportation",
      "machine-learning",
      "statistics"
    ],
    "domain_tags": [
      "transportation",
      "mobility"
    ],
    "data_modality": "time-series",
    "geographic_scope": "London",
    "size_category": "medium",
    "model_score": 0.0001
  },
  {
    "name": "MTA Open Data",
    "description": "NYC subway and bus real-time feeds and historical performance data including ridership, delays, and service metrics.",
    "category": "Transportation & Mobility",
    "url": "https://new.mta.info/open-data",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "transit",
      "NYC",
      "subway",
      "bus",
      "GTFS",
      "real-time"
    ],
    "best_for": "Train frequency modeling, station crowding, transit network analysis",
    "image_url": "/images/datasets/mta-open-data.jpg",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "transportation",
      "mobility",
      "urban-planning"
    ],
    "summary": "The MTA Open Data provides real-time feeds and historical performance data for NYC's subway and bus systems. Researchers and analysts can utilize this dataset to explore ridership patterns, service delays, and overall transit efficiency, enabling informed decision-making for urban mobility improvements.",
    "audience": [
      "Curious-browser",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What are the real-time subway and bus feeds in NYC?",
      "How can I analyze historical performance data of NYC transit?",
      "What metrics are available for NYC subway ridership?",
      "Where can I find data on NYC transit delays?",
      "What service metrics does the MTA Open Data provide?",
      "How does NYC transit performance vary over time?"
    ],
    "use_cases": [
      "Analyzing ridership trends during peak hours",
      "Evaluating the impact of service delays on commuter satisfaction",
      "Modeling transit demand based on historical performance",
      "Assessing the effectiveness of transit service changes"
    ],
    "embedding_text": "The MTA Open Data is a comprehensive dataset that encompasses real-time feeds and historical performance metrics for the New York City subway and bus systems. This dataset is structured in a tabular format, containing rows that represent individual transit events or records, and columns that capture various attributes such as ridership numbers, delay times, service metrics, and timestamps. The data is collected through the Metropolitan Transportation Authority's (MTA) operational systems, which continuously monitor transit services and gather information on performance metrics. The coverage of this dataset is geographically limited to New York City, providing a rich source of information for urban mobility studies. Key variables within the dataset include ridership counts, which measure the number of passengers using the subway or bus services, delay times that quantify the punctuality of transit services, and service metrics that evaluate the overall performance of the transit system. While the dataset is robust, it may have limitations related to data quality, such as missing values or discrepancies in reporting during peak disruptions. Common preprocessing steps may involve cleaning the data to handle missing entries, normalizing time formats, and aggregating data for specific analysis periods. Researchers can leverage this dataset to address various research questions, such as understanding the factors influencing ridership fluctuations, evaluating the effectiveness of transit policies, or modeling the impact of service delays on commuter behavior. The types of analyses supported by this dataset include regression analyses to identify trends, machine learning models for predictive analytics, and descriptive statistics to summarize transit performance. Typically, researchers utilize the MTA Open Data in studies focused on urban planning, transportation efficiency, and public policy evaluation, making it a valuable resource for those interested in the dynamics of urban mobility.",
    "tfidf_keywords": [
      "real-time feeds",
      "historical performance",
      "ridership metrics",
      "service delays",
      "transit efficiency",
      "urban mobility",
      "data collection",
      "performance metrics",
      "transit demand",
      "NYC transportation",
      "operational systems",
      "data preprocessing",
      "commuter behavior",
      "public policy evaluation"
    ],
    "semantic_cluster": "urban-mobility-data",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "urban-planning",
      "transportation-analysis",
      "public-transportation",
      "data-visualization",
      "policy-evaluation"
    ],
    "canonical_topics": [
      "transportation",
      "policy-evaluation",
      "urban-planning"
    ],
    "domain_tags": [
      "transportation",
      "urban-planning"
    ],
    "data_modality": "time-series",
    "geographic_scope": "NYC",
    "size_category": "medium",
    "model_score": 0.0001
  },
  {
    "name": "Bank of Israel Call Center (Technion)",
    "description": "250K+ calls with second-level timestamps from an Israeli bank. The most extensively validated call center dataset in OR literature with ~20% abandonment.",
    "category": "Operations & Service",
    "url": "https://iew.technion.ac.il/serveng/callcenterdata/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "call-center",
      "abandonment",
      "Erlang",
      "service-times",
      "operations-research"
    ],
    "best_for": "M/M/c+G modeling, Erlang-A validation, patience distribution estimation",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [],
    "summary": "The Bank of Israel Call Center dataset comprises over 250,000 calls with detailed second-level timestamps from an Israeli bank. This extensively validated dataset is particularly useful for analyzing call center operations, including metrics such as abandonment rates and service times.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the Bank of Israel Call Center dataset?",
      "How can I analyze call abandonment using this dataset?",
      "What insights can be gained from the service times in this dataset?",
      "What are the key metrics available in the Bank of Israel Call Center dataset?",
      "How does the dataset support operations research in call centers?",
      "What methodologies can be applied to analyze the call center data?",
      "What are the common challenges in working with call center datasets?",
      "How can I visualize call patterns from this dataset?"
    ],
    "use_cases": [
      "Analyzing call abandonment rates and their impact on customer satisfaction.",
      "Evaluating service times to optimize call center operations.",
      "Conducting regression analysis to identify factors affecting call durations.",
      "Developing predictive models for call volume forecasting."
    ],
    "embedding_text": "The Bank of Israel Call Center dataset is a comprehensive collection of over 250,000 calls recorded from an Israeli bank's call center, featuring second-level timestamps that provide detailed insights into call dynamics. This dataset is recognized as one of the most validated resources in operations research literature, particularly in the study of call center performance and customer service metrics. The structure of the dataset includes various columns representing key variables such as call duration, wait times, abandonment rates, and timestamps, which are critical for understanding operational efficiency. Researchers typically utilize this dataset to explore various research questions related to call center management, including the analysis of abandonment rates, service times, and overall customer experience. The data collection methodology involved systematic logging of call interactions, ensuring high data quality and reliability for subsequent analyses. However, researchers should be aware of potential limitations, such as the specific context of the Israeli banking sector, which may affect the generalizability of findings to other industries or regions. Common preprocessing steps may include handling missing values, normalizing call durations, and segmenting calls based on various attributes for more granular analysis. The dataset supports a range of analytical approaches, from descriptive statistics to more complex regression and machine learning models, allowing researchers to derive actionable insights that can inform operational improvements. By leveraging this dataset, analysts can address critical questions about customer behavior, service efficiency, and the impact of operational changes on call center performance.",
    "tfidf_keywords": [
      "call-center",
      "abandonment",
      "service-times",
      "Erlang",
      "operations-research",
      "customer-satisfaction",
      "call-duration",
      "wait-times",
      "predictive-modeling",
      "data-quality",
      "regression-analysis",
      "call-volume-forecasting",
      "operational-efficiency",
      "customer-experience"
    ],
    "semantic_cluster": "call-center-analytics",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "operations-research",
      "customer-service",
      "data-analysis",
      "predictive-analytics",
      "performance-metrics"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "operations-research",
      "statistics",
      "data-engineering"
    ],
    "domain_tags": [
      "fintech"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Israel",
    "size_category": "medium",
    "model_score": 0.0001
  },
  {
    "name": "CAIDA Anonymized Internet Traces",
    "description": "Backbone packet captures from 10-100 Gbps commercial links with nanosecond precision. 80-200GB per hourly trace.",
    "category": "Technology & Infrastructure",
    "url": "https://www.caida.org/catalog/datasets/passive_dataset/",
    "docs_url": "https://www.caida.org/catalog/datasets/passive_dataset_download/",
    "github_url": null,
    "tags": [
      "network",
      "packets",
      "backbone",
      "high-frequency",
      "internet"
    ],
    "best_for": "Heavy traffic analysis, self-similar traffic validation, M/G/1 heavy-tailed service",
    "image_url": "/images/datasets/caida-anonymized-internet-traces.png",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The CAIDA Anonymized Internet Traces dataset consists of backbone packet captures from high-speed commercial links, offering detailed insights into internet traffic patterns. Researchers can utilize this dataset for various analyses related to network performance, traffic engineering, and internet usage trends.",
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the characteristics of CAIDA Anonymized Internet Traces?",
      "How can I analyze backbone packet captures from CAIDA?",
      "What insights can be derived from high-frequency internet traffic data?",
      "What is the size of the CAIDA Anonymized Internet Traces dataset?",
      "How does CAIDA collect its internet traffic data?",
      "What types of analyses can be performed on CAIDA's packet captures?"
    ],
    "use_cases": [
      "Analyzing network congestion and performance metrics.",
      "Studying internet traffic patterns over time.",
      "Investigating the impact of network changes on traffic flows."
    ],
    "embedding_text": "The CAIDA Anonymized Internet Traces dataset provides a rich source of backbone packet capture data collected from high-speed commercial internet links, specifically ranging from 10 to 100 Gbps. This dataset is characterized by its high temporal resolution, capturing packets with nanosecond precision, which allows for detailed analysis of internet traffic dynamics. Each hourly trace can range from 80 to 200 GB, making it a significant resource for researchers and practitioners interested in network performance and internet infrastructure. The dataset is structured in a time-series format, with rows representing individual packets and columns capturing various attributes such as timestamps, source and destination IP addresses, protocol types, and packet sizes. The collection methodology employed by CAIDA involves monitoring traffic on commercial backbone links, ensuring that the data reflects real-world internet usage patterns. However, it is important to note that the dataset is anonymized to protect user privacy, which may limit certain types of analyses. Key variables in the dataset include packet timestamps, which measure the precise time each packet was captured, and packet sizes, which provide insights into bandwidth usage. Researchers typically preprocess the data to filter out noise, handle missing values, and aggregate packets into meaningful time intervals for analysis. Common research questions addressed using this dataset include understanding traffic patterns during peak usage times, analyzing the effects of network changes on traffic flows, and studying the behavior of specific protocols under different conditions. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile tool for network researchers. Overall, the CAIDA Anonymized Internet Traces dataset serves as a foundational resource for studies aimed at enhancing our understanding of internet traffic and improving network performance.",
    "tfidf_keywords": [
      "packet-capture",
      "backbone-network",
      "internet-traffic",
      "nanosecond-precision",
      "high-speed-links",
      "traffic-patterns",
      "network-performance",
      "data-anonymization",
      "bandwidth-usage",
      "protocol-analysis",
      "time-series-data",
      "network-engineering",
      "internet-infrastructure",
      "traffic-engineering",
      "data-collection-methodology"
    ],
    "semantic_cluster": "internet-traffic-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "network-analysis",
      "traffic-engineering",
      "data-anonymization",
      "packet-switching",
      "internet-performance",
      "bandwidth-optimization",
      "protocol-analysis",
      "time-series-analysis"
    ],
    "canonical_topics": [
      "data-engineering",
      "machine-learning",
      "statistics"
    ],
    "domain_tags": [
      "technology",
      "infrastructure"
    ],
    "data_modality": "time-series",
    "size_category": "medium",
    "model_score": 0.0001
  },
  {
    "name": "Google Cluster Workload Traces 2019",
    "description": "2.4 TiB of job scheduling data from 8 Borg clusters (~12,000 machines each) with microsecond timestamps for submissions and completions.",
    "category": "Technology & Infrastructure",
    "url": "https://github.com/google/cluster-data",
    "docs_url": "https://github.com/google/cluster-data/blob/master/ClusterData2019.md",
    "github_url": "https://github.com/google/cluster-data",
    "tags": [
      "cluster",
      "scheduling",
      "jobs",
      "Google",
      "Borg",
      "cloud"
    ],
    "best_for": "G/G/c multi-server queues, priority scheduling, resource contention modeling",
    "image_url": "/images/datasets/google-cluster-workload-traces-2019.png",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "time-series-analysis"
    ],
    "topic_tags": [],
    "summary": "The Google Cluster Workload Traces 2019 dataset comprises 2.4 TiB of detailed job scheduling data from eight Borg clusters, each containing approximately 12,000 machines. This dataset features microsecond timestamps for job submissions and completions, enabling researchers to analyze scheduling efficiency, resource utilization, and job performance in cloud environments.",
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the job scheduling patterns in Google Cluster Workload Traces 2019?",
      "How can I analyze resource utilization in Borg clusters?",
      "What insights can be gained from Google Cluster Workload data?",
      "How does job completion time vary across different clusters?",
      "What are the characteristics of job submissions in Google clusters?",
      "How can microsecond timestamp data improve scheduling algorithms?"
    ],
    "use_cases": [
      "Analyzing job scheduling efficiency in cloud computing environments.",
      "Investigating resource allocation and utilization patterns in large-scale clusters.",
      "Developing and testing scheduling algorithms based on real-world data.",
      "Studying the impact of job characteristics on completion times."
    ],
    "embedding_text": "The Google Cluster Workload Traces 2019 dataset offers a rich source of information for researchers interested in job scheduling and resource management within cloud computing environments. Comprising 2.4 TiB of data collected from eight Borg clusters, each with approximately 12,000 machines, this dataset provides microsecond-level timestamps for job submissions and completions. The data structure includes various columns that capture key variables such as job IDs, submission times, completion times, resource requests, and machine identifiers. This detailed schema allows for in-depth analysis of job scheduling patterns, resource utilization, and performance metrics across different clusters. The collection methodology involved monitoring the scheduling and execution of jobs within the Borg system, ensuring high fidelity and accuracy in the recorded timestamps. However, researchers should be aware of potential limitations, such as the specific operational context of Google's Borg system, which may not generalize to other environments. Common preprocessing steps may include handling missing values, normalizing timestamps, and aggregating data for specific analyses. Researchers can leverage this dataset to address various research questions, such as identifying factors that influence job completion times, evaluating the efficiency of different scheduling strategies, and exploring the relationship between resource allocation and job performance. The dataset supports a range of analyses, including regression modeling, machine learning applications, and descriptive statistics. Overall, the Google Cluster Workload Traces 2019 dataset serves as a valuable resource for advancing our understanding of cloud computing infrastructure and job scheduling dynamics.",
    "tfidf_keywords": [
      "job-scheduling",
      "Borg",
      "cloud-computing",
      "resource-utilization",
      "microsecond-timestamps",
      "job-performance",
      "scheduling-efficiency",
      "cluster-management",
      "data-analysis",
      "workload-traces",
      "performance-metrics",
      "resource-allocation",
      "job-characteristics"
    ],
    "semantic_cluster": "cloud-computing-scheduling",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "resource-allocation",
      "cloud-infrastructure",
      "job-scheduling-algorithms",
      "performance-evaluation",
      "data-analysis-methods"
    ],
    "canonical_topics": [
      "machine-learning",
      "data-engineering",
      "optimization",
      "statistics"
    ],
    "domain_tags": [
      "cloud",
      "technology"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0001
  },
  {
    "name": "MAWI Traffic Archive",
    "description": "Daily 15-minute packet traces from Japan's WIDE backbone with microsecond precision. Free download without registration.",
    "category": "Technology & Infrastructure",
    "url": "https://mawi.wide.ad.jp/mawi/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "network",
      "packets",
      "Japan",
      "backbone",
      "free"
    ],
    "best_for": "Daily traffic patterns, long-term trend analysis, accessible network queueing data",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The MAWI Traffic Archive consists of daily 15-minute packet traces collected from Japan's WIDE backbone, offering microsecond precision. Researchers can utilize this dataset for network analysis, performance evaluation, and traffic modeling.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the MAWI Traffic Archive?",
      "How can I download MAWI Traffic data?",
      "What type of data does the MAWI Traffic Archive provide?",
      "What are the applications of packet traces from the MAWI Traffic Archive?",
      "What is the significance of microsecond precision in network data?",
      "How does the MAWI Traffic Archive contribute to network research?"
    ],
    "use_cases": [
      "Network performance analysis",
      "Traffic modeling",
      "Network security assessments"
    ],
    "embedding_text": "The MAWI Traffic Archive provides a rich source of daily 15-minute packet traces from Japan's WIDE backbone, characterized by microsecond precision. This dataset is invaluable for researchers and practitioners in the field of network analysis, enabling them to study various aspects of data traffic flow, network performance, and related phenomena. The structure of the dataset typically includes rows representing individual packet traces, with columns detailing variables such as timestamps, packet sizes, and types of protocols used. The collection methodology involves capturing real-time traffic data from the backbone network, ensuring that the dataset reflects actual usage patterns and behaviors. While the archive is freely available for download without registration, users should be aware of potential limitations in data quality, such as missing packets or variations in traffic due to external factors. Common preprocessing steps may include filtering irrelevant data, normalizing packet sizes, and aggregating data over specific time intervals for analysis. Researchers can leverage this dataset to address a variety of research questions, such as examining traffic patterns during peak usage times, analyzing the impact of specific events on network performance, or developing models to predict future traffic trends. The types of analyses supported by this dataset range from descriptive statistics to more complex regression and machine learning techniques, making it a versatile tool for both academic and practical applications in network research. Overall, the MAWI Traffic Archive stands as a significant resource for understanding the intricacies of network traffic and its implications for technology and infrastructure development.",
    "tfidf_keywords": [
      "packet traces",
      "network analysis",
      "WIDE backbone",
      "microsecond precision",
      "traffic modeling",
      "data traffic flow",
      "network performance",
      "real-time traffic",
      "protocol analysis",
      "data normalization"
    ],
    "semantic_cluster": "network-traffic-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "network-performance",
      "traffic-analysis",
      "data-collection-methods",
      "packet-analysis",
      "real-time-data"
    ],
    "canonical_topics": [
      "data-engineering",
      "statistics",
      "machine-learning"
    ],
    "domain_tags": [
      "technology",
      "infrastructure"
    ],
    "data_modality": "time-series",
    "geographic_scope": "Japan",
    "size_category": "medium",
    "model_score": 0.0001
  },
  {
    "name": "Alibaba Cluster Trace v2018",
    "description": "280 GB of container scheduling data from Alibaba production clusters with job DAGs and resource utilization.",
    "category": "Technology & Infrastructure",
    "url": "https://github.com/alibaba/clusterdata",
    "docs_url": null,
    "github_url": "https://github.com/alibaba/clusterdata",
    "tags": [
      "cluster",
      "containers",
      "scheduling",
      "Alibaba",
      "cloud"
    ],
    "best_for": "Container orchestration queues, DAG scheduling, cloud workload characterization",
    "image_url": "/images/datasets/alibaba-cluster-trace-v2018.png",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Alibaba Cluster Trace v2018 dataset contains 280 GB of detailed container scheduling data from Alibaba's production clusters, including job Directed Acyclic Graphs (DAGs) and resource utilization metrics. This dataset can be utilized for analyzing cloud resource management, optimizing scheduling algorithms, and understanding container orchestration in large-scale environments.",
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the Alibaba Cluster Trace v2018 dataset?",
      "How can I analyze container scheduling data from Alibaba?",
      "What insights can be gained from Alibaba's production cluster data?",
      "What are the resource utilization metrics in the Alibaba Cluster Trace?",
      "How does job DAGs impact scheduling in cloud environments?",
      "What are the applications of container scheduling data in research?"
    ],
    "use_cases": [
      "Analyzing resource utilization patterns in cloud environments.",
      "Optimizing scheduling algorithms for container orchestration.",
      "Investigating the performance of different job DAGs in production settings."
    ],
    "embedding_text": "The Alibaba Cluster Trace v2018 dataset is a comprehensive collection of container scheduling data sourced from Alibaba's production clusters. This dataset encompasses a substantial volume of 280 GB, making it a rich resource for researchers and practitioners interested in cloud computing and container orchestration. The data structure typically includes rows representing individual scheduling events or resource utilization metrics, while columns may include variables such as job identifiers, timestamps, resource types, and utilization rates. The collection methodology involves capturing real-time scheduling data from Alibaba's operational environment, ensuring that the dataset reflects actual usage patterns and performance metrics. While the dataset does not specify temporal or geographic coverage, it is inherently tied to the operational practices of Alibaba, a leading cloud service provider. Key variables in this dataset measure aspects of resource allocation, job execution times, and the efficiency of scheduling algorithms. However, potential limitations include the specificity of the data to Alibaba's infrastructure, which may not generalize to other cloud environments. Common preprocessing steps may involve cleaning the data for missing values, normalizing resource utilization metrics, and transforming job DAGs into a format suitable for analysis. Researchers can leverage this dataset to address various research questions, such as the impact of scheduling strategies on resource efficiency or the performance implications of different job configurations. The dataset supports a range of analyses, including regression modeling, machine learning applications, and descriptive statistics. Typically, researchers utilize this dataset to enhance their understanding of cloud resource management and to develop more effective scheduling algorithms that can be applied in diverse cloud environments.",
    "tfidf_keywords": [
      "container-scheduling",
      "resource-utilization",
      "job-DAGs",
      "cloud-computing",
      "orchestration",
      "Alibaba",
      "performance-analysis",
      "scheduling-algorithms",
      "data-collection-methodology",
      "cloud-infrastructure"
    ],
    "semantic_cluster": "cloud-resource-management",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "cloud-computing",
      "containerization",
      "scheduling-theory",
      "resource-allocation",
      "performance-optimization"
    ],
    "canonical_topics": [
      "machine-learning",
      "data-engineering",
      "optimization",
      "statistics"
    ],
    "domain_tags": [
      "cloud",
      "technology"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0001
  },
  {
    "name": "Port of Los Angeles Port Optimizer",
    "description": "Container dwell times and truck turn times from the largest US port since 2021. Real operational metrics.",
    "category": "Logistics & Supply Chain",
    "url": "https://www.portoflosangeles.org/business/supply-chain/port-optimizer",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "port",
      "containers",
      "trucks",
      "dwell-time",
      "Los-Angeles"
    ],
    "best_for": "Container terminal queueing, truck appointment systems, port efficiency",
    "image_url": "/images/logos/portoflosangeles.png",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Port of Los Angeles Port Optimizer dataset provides insights into container dwell times and truck turn times at the largest US port, offering real operational metrics since 2021. Researchers and analysts can utilize this data to optimize logistics, improve supply chain efficiency, and understand operational bottlenecks.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the container dwell times at the Port of Los Angeles?",
      "How do truck turn times vary at the Port of Los Angeles?",
      "What operational metrics are available for the Port of Los Angeles?",
      "How has container dwell time changed since 2021?",
      "What factors influence truck turn times at the Port of Los Angeles?",
      "How can the Port Optimizer dataset improve logistics operations?"
    ],
    "use_cases": [
      "Analyzing trends in container dwell times over time.",
      "Evaluating the impact of truck turn times on supply chain efficiency.",
      "Identifying operational bottlenecks at the port.",
      "Developing predictive models for logistics optimization."
    ],
    "embedding_text": "The Port of Los Angeles Port Optimizer dataset is a comprehensive collection of operational metrics that detail container dwell times and truck turn times at the largest port in the United States. This dataset has been meticulously compiled since 2021, providing a rich source of information for researchers and industry professionals interested in logistics and supply chain management. The data structure is primarily tabular, consisting of rows that represent individual operational metrics recorded at the port, with columns detailing various attributes such as time stamps, container types, truck identifiers, and specific dwell or turn times. The collection methodology involves real-time tracking of container movements and truck operations, ensuring that the data reflects actual conditions and practices at the port. As such, it serves as a valuable resource for understanding the dynamics of port operations and their implications for broader supply chain processes.\n\nKey variables in this dataset include container dwell time, which measures the duration containers remain at the port before being moved, and truck turn time, which indicates the time taken for trucks to load or unload containers. These metrics are crucial for identifying inefficiencies and optimizing logistics operations. However, users should be aware of potential data quality issues, such as missing values or discrepancies in reporting, which may arise from the complexities of real-time data collection in a busy port environment.\n\nCommon preprocessing steps may include cleaning the data to handle missing or erroneous entries, normalizing time formats, and aggregating metrics for analysis. Researchers can leverage this dataset to address various research questions, such as how dwell times impact overall supply chain efficiency, or the relationship between truck turn times and operational delays. The dataset supports a range of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it a versatile tool for both academic research and practical applications in the logistics sector. Typically, researchers utilize this dataset to conduct studies aimed at improving operational efficiency, reducing costs, and enhancing the overall performance of supply chain systems.",
    "tfidf_keywords": [
      "container-dwell-time",
      "truck-turn-time",
      "logistics-optimization",
      "operational-metrics",
      "supply-chain-efficiency",
      "real-time-tracking",
      "data-quality",
      "preprocessing",
      "predictive-modeling",
      "bottleneck-analysis",
      "Los-Angeles-port",
      "logistics-data",
      "time-series-analysis",
      "data-collection-methodology"
    ],
    "semantic_cluster": "logistics-optimization",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "supply-chain-management",
      "operational-efficiency",
      "logistics-analytics",
      "data-driven-decision-making",
      "transportation-optimization"
    ],
    "canonical_topics": [
      "optimization",
      "statistics",
      "data-engineering",
      "consumer-behavior"
    ],
    "domain_tags": [
      "logistics",
      "supply-chain"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2021-present",
    "geographic_scope": "Los Angeles, USA",
    "size_category": "medium",
    "model_score": 0.0001
  },
  {
    "name": "All of Us Research Program",
    "description": "NIH precision medicine initiative enrolling 1+ million diverse U.S. participants. Includes EHR, surveys, wearables, and genomics. Cloud-based Researcher Workbench provides secure access.",
    "category": "Healthcare Economics & Health-Tech",
    "url": "https://allofus.nih.gov/",
    "source": "NIH",
    "type": "Cohort + Multi-modal",
    "access": "Free (registration required)",
    "format": "Cloud-based (BigQuery)",
    "tags": [
      "Healthcare",
      "Precision Medicine",
      "Diverse",
      "Wearables",
      "Free"
    ],
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The All of Us Research Program is a comprehensive NIH initiative aimed at advancing precision medicine through the enrollment of over one million diverse participants across the United States. The dataset includes a rich array of data types such as electronic health records (EHR), surveys, wearable device data, and genomic information, enabling researchers to explore various health-related questions and trends.",
    "use_cases": [
      "Analyzing health outcomes based on genomic data",
      "Studying the impact of wearable devices on chronic disease management",
      "Exploring health disparities among diverse populations"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the All of Us Research Program?",
      "How can I access the All of Us dataset?",
      "What types of data are included in the All of Us Research Program?",
      "What research questions can be addressed using the All of Us dataset?",
      "What is precision medicine and how is it related to the All of Us initiative?",
      "How does the All of Us Research Program ensure participant diversity?"
    ],
    "update_frequency": "Ongoing enrollment",
    "geographic_coverage": "United States (national)",
    "domain_tags": [
      "healthcare"
    ],
    "data_modality": "mixed",
    "geographic_scope": "United States",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/logos/nih.png",
    "embedding_text": "The All of Us Research Program is a landmark initiative by the National Institutes of Health (NIH) aiming to revolutionize the field of precision medicine. By enrolling over one million diverse participants from across the United States, this program seeks to gather a comprehensive dataset that includes electronic health records (EHR), surveys, wearable device data, and genomic information. The data structure is designed to support a wide range of research questions, with variables capturing health metrics, lifestyle factors, and genetic information. Researchers can access this rich dataset through a secure, cloud-based Researcher Workbench, which facilitates data analysis while ensuring participant privacy and data security. The collection methodology involves a multi-faceted approach, utilizing both passive data collection through wearables and active data collection via surveys and EHRs. This diverse data source allows for a nuanced understanding of health trends and outcomes across different demographics. Key variables in the dataset include health indicators, demographic information, and behavioral data, which together provide a holistic view of participant health. While the dataset is robust, researchers should be aware of potential limitations such as data completeness and participant dropout rates. Common preprocessing steps may include data cleaning, normalization, and handling missing values to prepare the dataset for analysis. The All of Us dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it a versatile resource for researchers. Typical research questions may involve examining the relationship between genetic predispositions and health outcomes, assessing the effectiveness of wearable devices in managing chronic conditions, or exploring health disparities among different population groups. Overall, the All of Us Research Program represents a significant step forward in personalized healthcare, providing invaluable insights that can inform future medical practices and policies.",
    "tfidf_keywords": [
      "precision medicine",
      "electronic health records",
      "wearable devices",
      "genomics",
      "health disparities",
      "participant diversity",
      "health outcomes",
      "data security",
      "research methodology",
      "health metrics"
    ],
    "semantic_cluster": "healthcare-data-initiatives",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "precision medicine",
      "healthcare analytics",
      "biostatistics",
      "epidemiology",
      "public health"
    ],
    "canonical_topics": [
      "healthcare",
      "machine-learning",
      "statistics"
    ]
  },
  {
    "name": "Human Mortality Database",
    "description": "Detailed mortality and population data for 40+ countries with life tables and exposure-to-risk calculations",
    "category": "Insurance & Actuarial",
    "url": "https://www.mortality.org/",
    "docs_url": "https://www.mortality.org/Data/DataAvailability",
    "github_url": null,
    "tags": [
      "mortality",
      "life-insurance",
      "demographics",
      "life-tables",
      "longevity"
    ],
    "best_for": "Life insurance pricing, longevity research, and demographic modeling",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Human Mortality Database provides detailed mortality and population data for over 40 countries, including life tables and exposure-to-risk calculations. This dataset is essential for researchers and practitioners in the fields of insurance and actuarial science, allowing for in-depth analysis of demographic trends and longevity.",
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the key mortality rates in different countries?",
      "How does longevity vary across demographics?",
      "What insights can be drawn from life tables in the Human Mortality Database?",
      "How can exposure-to-risk calculations inform life insurance policies?",
      "What trends in population data can be observed over the last decades?",
      "How does the Human Mortality Database support actuarial science?"
    ],
    "use_cases": [
      "Analyzing trends in mortality rates across different countries",
      "Evaluating the impact of demographic factors on life expectancy",
      "Conducting actuarial assessments for life insurance products",
      "Investigating the relationship between longevity and socioeconomic status"
    ],
    "domain_tags": [
      "insurance",
      "actuarial"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/logos/mortality.png",
    "embedding_text": "The Human Mortality Database is a comprehensive resource that offers detailed mortality and population data for over 40 countries, making it an invaluable tool for researchers in the fields of insurance and actuarial science. The dataset includes life tables, which provide essential information about the mortality rates and life expectancy of different populations, as well as exposure-to-risk calculations that are crucial for understanding the dynamics of mortality over time. The data is structured in a tabular format, with rows representing different demographic groups and columns containing variables such as age, sex, and mortality rates. This structure allows for easy manipulation and analysis using statistical software. The collection methodology involves aggregating data from national vital statistics and census data, ensuring a high level of accuracy and reliability. However, researchers should be aware of potential limitations, such as variations in data collection practices across countries and the availability of data for certain demographic groups. Common preprocessing steps may include handling missing data, normalizing variables, and transforming data for specific analyses. The dataset supports a wide range of research questions, including those related to the effects of public health interventions, socioeconomic factors influencing longevity, and the evaluation of life insurance products. Analysts can employ various methods, including regression analysis, machine learning techniques, and descriptive statistics, to derive insights from the data. Researchers typically use the Human Mortality Database to inform policy decisions, improve actuarial models, and enhance our understanding of demographic trends that impact public health and insurance.",
    "tfidf_keywords": [
      "mortality rates",
      "life tables",
      "exposure-to-risk",
      "demographics",
      "longevity",
      "actuarial science",
      "population data",
      "socioeconomic status",
      "public health",
      "insurance products"
    ],
    "semantic_cluster": "demographic-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "demographic-analysis",
      "actuarial-modeling",
      "public-health-research",
      "socioeconomic-factors",
      "life-insurance"
    ],
    "canonical_topics": [
      "healthcare",
      "econometrics",
      "statistics"
    ]
  },
  {
    "name": "NYC Property Sales",
    "description": "NYC property sales transactions across all boroughs",
    "category": "Real Estate",
    "url": "https://data.cityofnewyork.us/Housing-Development/NYC-Calendar-Sales-Archive-/uzf5-f8n2/about_data",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "real estate",
      "NYC",
      "transactions",
      "government"
    ],
    "best_for": "Learning real estate analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The NYC Property Sales dataset contains detailed records of property sales transactions across all boroughs of New York City. This dataset can be used to analyze market trends, property values, and the impact of government policies on real estate transactions.",
    "use_cases": [
      "Analyzing property price trends over time",
      "Comparing property sales across different boroughs",
      "Evaluating the impact of economic conditions on real estate transactions"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the recent property sales in NYC?",
      "How do property prices vary across different boroughs in NYC?",
      "What trends can be observed in NYC property transactions over the years?",
      "How do government policies affect real estate transactions in NYC?",
      "What is the average property sale price in NYC?",
      "What types of properties are most frequently sold in NYC?"
    ],
    "domain_tags": [
      "real estate"
    ],
    "data_modality": "tabular",
    "geographic_scope": "New York City",
    "size_category": "medium",
    "model_score": 0.0,
    "embedding_text": "The NYC Property Sales dataset is a comprehensive collection of property sales transactions that occur across all five boroughs of New York City. This dataset typically includes a variety of variables such as transaction dates, sale prices, property types, and locations, which are essential for conducting thorough analyses of the real estate market. The data is structured in a tabular format, with rows representing individual property sales and columns detailing various attributes of each sale. Key variables may include sale price, property type (e.g., residential, commercial), square footage, and the borough in which the property is located. Researchers and analysts often utilize this dataset to explore market trends, assess property values, and understand the dynamics of real estate transactions in an urban environment. The dataset is collected from government sources, ensuring a level of reliability and accuracy, though users should be aware of potential limitations such as missing data or inconsistencies in reporting. Common preprocessing steps may involve cleaning the data, handling missing values, and normalizing sale prices for inflation. This dataset supports a variety of analyses, including regression modeling to predict property prices, machine learning algorithms for classification of property types, and descriptive statistics to summarize sales trends. Researchers typically use this dataset to address questions related to market dynamics, the effects of policy changes on property sales, and demographic influences on real estate transactions. Overall, the NYC Property Sales dataset serves as a vital resource for anyone interested in the intersection of real estate and economic analysis.",
    "tfidf_keywords": [
      "property sales",
      "real estate transactions",
      "NYC boroughs",
      "market trends",
      "sale prices",
      "property types",
      "urban economics",
      "transaction analysis",
      "government data",
      "real estate market"
    ],
    "semantic_cluster": "real-estate-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "urban economics",
      "market analysis",
      "property valuation",
      "data visualization",
      "economic policy"
    ],
    "canonical_topics": [
      "econometrics",
      "consumer-behavior",
      "marketplaces"
    ]
  },
  {
    "name": "Jeff Sackmann Tennis Data",
    "description": "Comprehensive tennis match results and point-by-point data from 1973-present for ATP, WTA, and Grand Slam tournaments",
    "category": "Sports & Athletics",
    "url": "https://github.com/JeffSackmann/tennis_atp",
    "docs_url": null,
    "github_url": "https://github.com/JeffSackmann/tennis_atp",
    "tags": [
      "tennis",
      "ATP",
      "WTA",
      "match-results",
      "point-by-point"
    ],
    "best_for": "Tennis analytics, player performance analysis, and tournament prediction",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Jeff Sackmann Tennis Data provides a comprehensive collection of tennis match results and point-by-point data from 1973 to the present, covering ATP, WTA, and Grand Slam tournaments. Researchers and analysts can utilize this dataset to explore player performance, match outcomes, and trends in tennis over time.",
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the historical match results in tennis?",
      "How can I analyze point-by-point data for ATP matches?",
      "What trends can be identified in WTA tournament results?",
      "Where can I find comprehensive tennis statistics from 1973 to present?",
      "How do player performances compare across different tournaments?",
      "What data is available for Grand Slam tennis matches?"
    ],
    "use_cases": [
      "Analyzing player performance trends over time",
      "Comparing match outcomes between ATP and WTA tournaments",
      "Investigating the impact of surface type on match results",
      "Studying point-by-point data to understand game strategies"
    ],
    "domain_tags": [
      "sports",
      "athletics"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1973-present",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/jeff-sackmann-tennis-data.png",
    "embedding_text": "The Jeff Sackmann Tennis Data is a rich dataset that encompasses a wide range of tennis match results and point-by-point data from 1973 to the present, specifically for ATP, WTA, and Grand Slam tournaments. This dataset is structured in a tabular format, where each row represents a unique match, and columns include variables such as player names, match scores, tournament type, surface type, and detailed point-by-point breakdowns. The collection methodology involves aggregating data from official tournament records, match reports, and statistical databases, ensuring a comprehensive coverage of tennis matches over several decades. Key variables in the dataset measure aspects such as player performance, match outcomes, and the influence of different playing surfaces on results. Researchers should be aware of potential limitations in data quality, including missing data for certain matches or inconsistencies in reporting across different tournaments. Common preprocessing steps might include cleaning the data for missing values, normalizing player names, and categorizing matches by tournament type. This dataset supports a variety of research questions, such as analyzing the evolution of player performance over time, comparing match outcomes based on surface types, and exploring the dynamics of match play through point-by-point analysis. The types of analyses it supports include regression analysis, machine learning applications, and descriptive statistics, making it a versatile resource for sports analysts, researchers, and enthusiasts interested in the intricacies of tennis.",
    "tfidf_keywords": [
      "tennis",
      "match-results",
      "point-by-point",
      "ATP",
      "WTA",
      "Grand Slam",
      "player-performance",
      "tournament-analysis",
      "surface-type",
      "game-strategies",
      "historical-data",
      "statistical-analysis",
      "sports-data",
      "performance-trends"
    ],
    "semantic_cluster": "sports-data-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "sports-statistics",
      "performance-analysis",
      "data-visualization",
      "time-series-analysis",
      "match-strategy"
    ],
    "canonical_topics": [
      "statistics",
      "consumer-behavior"
    ]
  },
  {
    "name": "StatsBomb Open Data",
    "description": "Free soccer event data with 3,400+ events per match including xG, freeze-frame data, and 360 player positioning. Includes Messi's complete La Liga career and multiple World Cups",
    "category": "Sports & Athletics",
    "url": "https://github.com/statsbomb/open-data",
    "docs_url": "https://github.com/statsbomb/open-data/tree/master/doc",
    "github_url": "https://github.com/statsbomb/open-data",
    "tags": [
      "soccer",
      "football",
      "event-data",
      "xG",
      "tracking-data"
    ],
    "best_for": "Soccer analytics, expected goals modeling, and tactical analysis",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "sports",
      "analytics",
      "data-visualization"
    ],
    "summary": "The StatsBomb Open Data provides comprehensive soccer event data, capturing over 3,400 events per match, including advanced metrics like expected goals (xG) and detailed player positioning. This dataset can be utilized for various analyses, including performance evaluation, tactical analysis, and player scouting.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the StatsBomb Open Data?",
      "How can I access soccer event data?",
      "What metrics are included in the StatsBomb dataset?",
      "How does xG work in soccer analytics?",
      "What are the applications of soccer event data?",
      "Where can I find Messi's La Liga career data?",
      "What types of analyses can be performed with soccer tracking data?",
      "How many events are recorded per match in the StatsBomb dataset?"
    ],
    "use_cases": [
      "Analyzing player performance using xG metrics",
      "Evaluating team strategies through event data",
      "Conducting tactical analysis for match preparation",
      "Scouting players based on detailed event tracking"
    ],
    "domain_tags": [
      "sports",
      "analytics"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/statsbomb-open-data.png",
    "embedding_text": "The StatsBomb Open Data is a rich dataset that provides extensive soccer event data, featuring over 3,400 recorded events per match. This dataset includes advanced metrics such as expected goals (xG), freeze-frame data, and detailed 360-degree player positioning, making it an invaluable resource for sports analysts and enthusiasts alike. The data is structured in a tabular format, with rows representing individual events and columns detailing various attributes of each event, such as the type of event, involved players, time of occurrence, and spatial coordinates on the field. The collection methodology involves aggregating data from live match events, ensuring a high level of accuracy and detail. While the dataset covers significant events in soccer history, including Messi's complete La Liga career and multiple World Cups, it is essential to note that the dataset may have limitations in terms of completeness for certain matches or events. Common preprocessing steps include cleaning the data for missing values, normalizing player positions, and transforming event types into categorical variables for analysis. Researchers can leverage this dataset to address various research questions, such as evaluating player efficiency, understanding team dynamics, and predicting match outcomes based on historical performance. The dataset supports a range of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile tool for both academic research and practical applications in sports analytics. Typically, researchers use this dataset to conduct in-depth analyses of player and team performance, develop predictive models, and enhance the understanding of tactical approaches in soccer.",
    "tfidf_keywords": [
      "expected-goals",
      "event-data",
      "player-tracking",
      "soccer-analytics",
      "performance-evaluation",
      "tactical-analysis",
      "data-visualization",
      "match-events",
      "freeze-frame-data",
      "360-degree-positioning",
      "Messi-La-Liga",
      "World-Cup-data",
      "soccer-metrics",
      "data-collection-methodology",
      "sports-data"
    ],
    "semantic_cluster": "soccer-data-analytics",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "sports-analytics",
      "performance-metrics",
      "data-visualization",
      "tactical-analysis",
      "player-scouting"
    ],
    "canonical_topics": [
      "statistics",
      "machine-learning",
      "consumer-behavior"
    ]
  },
  {
    "name": "Amazon ShopBench (KDD Cup 2024)",
    "description": "57 tasks, 20K questions derived from real Amazon shopping data for LLM shopping assistants",
    "category": "Data Portals",
    "url": "https://www.aicrowd.com/challenges/amazon-kdd-cup-2024-multi-task-online-shopping-challenge-for-llms",
    "docs_url": null,
    "github_url": "https://github.com/fzp0424/ec-guide-kddup-2024",
    "tags": [
      "KDD",
      "Amazon",
      "e-commerce",
      "LLM",
      "2024",
      "multi-task"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Amazon ShopBench dataset consists of 57 tasks and 20,000 questions derived from actual Amazon shopping data, specifically designed for training large language models (LLMs) in the context of shopping assistance. Researchers can utilize this dataset to enhance LLM capabilities in understanding consumer behavior and improving e-commerce interactions.",
    "use_cases": [
      "Training LLMs for e-commerce applications",
      "Analyzing consumer behavior patterns",
      "Improving recommendation systems",
      "Evaluating multi-task learning approaches"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the tasks in the Amazon ShopBench dataset?",
      "How can I access the Amazon ShopBench dataset?",
      "What types of questions are included in the Amazon ShopBench dataset?",
      "What is the purpose of the Amazon ShopBench dataset?",
      "How is the Amazon ShopBench dataset structured?",
      "What insights can be gained from analyzing the Amazon ShopBench dataset?",
      "What are the applications of the Amazon ShopBench dataset for LLMs?",
      "What challenges are associated with using the Amazon ShopBench dataset?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/amazon-shopbench-kdd-cup-2024.jpg",
    "embedding_text": "The Amazon ShopBench dataset is a comprehensive collection designed for the KDD Cup 2024, featuring 57 distinct tasks and a total of 20,000 questions derived from real Amazon shopping data. This dataset is particularly valuable for researchers and practitioners focused on developing large language models (LLMs) tailored for shopping assistance. The data structure includes a variety of rows and columns, each representing different tasks and questions that encapsulate the complexities of consumer interactions in an e-commerce environment. The collection methodology involved extracting and curating data from actual shopping experiences on Amazon, ensuring that the dataset reflects genuine consumer behavior and preferences. While the dataset does not specify temporal or geographic coverage, it is designed to be applicable across various contexts within the e-commerce sector. Key variables within the dataset measure aspects such as product categories, consumer queries, and task-specific requirements, providing a rich foundation for analysis. However, users should be aware of potential limitations in data quality, including biases inherent in user-generated content and the need for careful preprocessing to ensure effective use. Common preprocessing steps may include data cleaning, normalization, and the transformation of categorical variables into numerical formats suitable for machine learning algorithms. Researchers can leverage the Amazon ShopBench dataset to address a range of research questions, such as understanding consumer preferences, evaluating the effectiveness of LLMs in providing shopping assistance, and exploring the dynamics of e-commerce interactions. The dataset supports various types of analyses, including regression, machine learning, and descriptive statistics, making it a versatile tool for both academic and practical applications. Researchers typically use this dataset to enhance LLM training, improve recommendation systems, and analyze consumer behavior patterns, contributing to advancements in e-commerce technologies.",
    "tfidf_keywords": [
      "large-language-models",
      "consumer-interaction",
      "e-commerce-dataset",
      "task-specific-learning",
      "shopping-assistance",
      "data-curation",
      "consumer-preferences",
      "recommendation-systems",
      "data-preprocessing",
      "machine-learning-applications"
    ],
    "semantic_cluster": "nlp-for-e-commerce",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "machine-learning",
      "consumer-behavior",
      "recommendation-systems",
      "data-preprocessing",
      "natural-language-processing"
    ],
    "canonical_topics": [
      "machine-learning",
      "consumer-behavior",
      "recommendation-systems",
      "natural-language-processing",
      "experimentation"
    ]
  },
  {
    "name": "Alibaba User Behavior 2018",
    "description": "649M user interactions (clicks, carts, buys) on 25M items",
    "category": "E-Commerce",
    "url": "https://tianchi.aliyun.com/dataset/649",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "user behavior",
      "large-scale",
      "interactions"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "user-interactions"
    ],
    "summary": "The Alibaba User Behavior 2018 dataset contains 649 million user interactions, including clicks, carts, and purchases across 25 million items. This dataset can be used to analyze consumer behavior, optimize product recommendations, and study the dynamics of online shopping.",
    "use_cases": [
      "Analyzing trends in user interactions over time.",
      "Optimizing product recommendation algorithms based on user behavior.",
      "Studying the impact of promotions on user purchasing decisions."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the user interactions in the Alibaba User Behavior 2018 dataset?",
      "How can I analyze consumer behavior using the Alibaba dataset?",
      "What insights can be gained from 649M user interactions on Alibaba?",
      "What types of analyses can be performed with Alibaba's user behavior data?",
      "How does user behavior vary across different product categories in Alibaba?",
      "What are the implications of user interactions for e-commerce strategies?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2018",
    "size_category": "large",
    "model_score": 0.0,
    "embedding_text": "The Alibaba User Behavior 2018 dataset is a comprehensive collection of user interactions on the Alibaba platform, encompassing 649 million actions such as clicks, items added to carts, and purchases across 25 million distinct items. This dataset is structured in a tabular format, with rows representing individual user interactions and columns detailing various attributes such as user ID, item ID, interaction type (click, cart, buy), and timestamps. The collection methodology involves tracking user actions on the Alibaba platform, providing a rich source of data for understanding consumer behavior in an online retail context. Researchers can leverage this dataset to address a variety of research questions, such as identifying patterns in user engagement, analyzing the effectiveness of marketing strategies, and developing predictive models for future purchases. Common preprocessing steps may include cleaning the data to remove duplicates, handling missing values, and transforming categorical variables into numerical formats for analysis. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile resource for both academic and commercial research. However, researchers should be aware of potential limitations, such as biases in user behavior or the influence of external factors like seasonality on purchasing patterns. Overall, this dataset serves as a valuable tool for understanding the complexities of online shopping behavior and can inform strategies for enhancing user experience and increasing sales in e-commerce.",
    "tfidf_keywords": [
      "user-interactions",
      "e-commerce",
      "consumer-behavior",
      "clickstream",
      "purchase-patterns",
      "data-mining",
      "recommendation-systems",
      "user-engagement",
      "big-data",
      "behavioral-analysis"
    ],
    "semantic_cluster": "e-commerce-user-behavior",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "consumer-behavior",
      "data-mining",
      "recommendation-systems",
      "predictive-analytics",
      "user-engagement"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "machine-learning",
      "data-engineering",
      "product-analytics",
      "recommendation-systems"
    ]
  },
  {
    "name": "Open e-commerce 1.0",
    "description": "1.85M Amazon purchases from 5,027 US consumers (2018-2022) linked to demographics (age, gender, income, education)",
    "category": "E-Commerce",
    "url": "https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/YGLYDY",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Amazon",
      "demographics",
      "purchases",
      "linked data"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "demographics"
    ],
    "summary": "The Open e-commerce 1.0 dataset contains 1.85 million Amazon purchases made by 5,027 US consumers from 2018 to 2022, linked to various demographic attributes such as age, gender, income, and education. This dataset allows researchers to analyze consumer purchasing behavior in relation to demographic factors, providing insights into trends and patterns in e-commerce.",
    "use_cases": [
      "Analyzing the impact of demographic variables on purchasing decisions.",
      "Identifying trends in consumer behavior over time.",
      "Exploring correlations between income levels and spending patterns.",
      "Segmenting consumers based on demographic attributes for targeted marketing."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What demographic factors influence Amazon purchase behavior?",
      "How do age and income affect consumer spending on Amazon?",
      "What trends can be observed in Amazon purchases over the years 2018-2022?",
      "How does gender impact purchasing decisions on e-commerce platforms?",
      "What is the relationship between education level and online shopping habits?",
      "How can demographic data enhance understanding of consumer behavior in e-commerce?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2018-2022",
    "geographic_scope": "US",
    "size_category": "medium",
    "model_score": 0.0,
    "embedding_text": "The Open e-commerce 1.0 dataset is a comprehensive collection of 1.85 million Amazon purchases made by 5,027 consumers in the United States between 2018 and 2022. This dataset is structured in a tabular format, comprising rows that represent individual purchases and columns that capture various attributes, including demographic information such as age, gender, income, and education level. The collection methodology involved aggregating purchase data from Amazon, ensuring a diverse representation of consumers across different demographics. The temporal coverage of the dataset spans four years, allowing for the analysis of trends and changes in consumer behavior over time. The key variables in this dataset measure demographic characteristics and purchasing behavior, providing a rich foundation for analysis. However, researchers should be aware of potential limitations regarding data quality, including possible biases in self-reported demographic information and the representativeness of the sample. Common preprocessing steps may include cleaning the data, handling missing values, and normalizing variables for analysis. Researchers can utilize this dataset to address various research questions, such as examining the influence of demographic factors on purchasing decisions or identifying trends in consumer spending. The dataset supports a range of analyses, including regression analysis, machine learning applications, and descriptive statistics. Typically, researchers use this dataset to gain insights into consumer behavior, develop marketing strategies, and inform policy decisions in the e-commerce sector.",
    "tfidf_keywords": [
      "Amazon",
      "demographics",
      "consumer behavior",
      "purchases",
      "income",
      "education",
      "gender",
      "e-commerce",
      "data analysis",
      "trend analysis"
    ],
    "semantic_cluster": "consumer-behavior-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "demographic-analysis",
      "market-research",
      "e-commerce-strategies",
      "data-analytics"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "marketplaces",
      "econometrics"
    ]
  },
  {
    "name": "ASOS Digital Experiments",
    "description": "99 real A/B experiments with 24,153 time-granular snapshots for adaptive stopping research",
    "category": "E-Commerce",
    "url": "https://osf.io/64jsb/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "A/B testing",
      "experiments",
      "adaptive",
      "fashion"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The ASOS Digital Experiments dataset comprises 99 real A/B experiments with 24,153 time-granular snapshots, specifically designed for adaptive stopping research. Researchers can utilize this dataset to analyze consumer behavior in the fashion industry and evaluate the effectiveness of various pricing strategies through A/B testing methodologies.",
    "use_cases": [
      "Evaluating the impact of pricing changes on consumer purchasing behavior",
      "Analyzing the effectiveness of different marketing strategies",
      "Conducting regression analysis to identify key factors influencing sales",
      "Implementing machine learning models to predict consumer responses to A/B tests"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the insights from ASOS A/B experiments?",
      "How can adaptive stopping be applied in e-commerce?",
      "What consumer behaviors can be analyzed using ASOS data?",
      "What pricing strategies were tested in ASOS experiments?",
      "How many snapshots are available in the ASOS dataset?",
      "What types of A/B tests were conducted by ASOS?",
      "How does the ASOS dataset contribute to adaptive research?",
      "What is the significance of time-granular snapshots in A/B testing?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/logos/osf.png",
    "embedding_text": "The ASOS Digital Experiments dataset presents a comprehensive collection of 99 real A/B experiments, which are critical for understanding consumer behavior in the e-commerce sector, particularly within the fashion industry. This dataset contains 24,153 time-granular snapshots that facilitate adaptive stopping research, allowing researchers to explore the dynamics of consumer responses over time. The data structure is primarily tabular, with rows representing individual snapshots of experiments and columns detailing various variables such as treatment conditions, consumer responses, and time intervals. This rich dataset is ideal for researchers interested in evaluating the effectiveness of different marketing strategies and pricing models through rigorous statistical analysis. The collection methodology involved systematic experimentation by ASOS, a leading online fashion retailer, ensuring that the data is both relevant and applicable to real-world scenarios. Key variables within the dataset measure consumer engagement, conversion rates, and other performance metrics, providing a robust foundation for analysis. However, researchers should be aware of potential limitations, such as the specific context of the fashion industry, which may not generalize to other sectors. Common preprocessing steps may include data cleaning, normalization, and the handling of missing values to prepare the dataset for analysis. Researchers can address various research questions, such as the impact of pricing changes on consumer behavior or the effectiveness of different marketing strategies. The dataset supports a range of analyses, including regression, machine learning, and descriptive statistics, making it a versatile resource for data scientists and economists alike. Typically, researchers utilize this dataset to conduct experiments that inform business decisions, optimize marketing strategies, and enhance consumer engagement through data-driven insights.",
    "tfidf_keywords": [
      "A/B testing",
      "adaptive stopping",
      "consumer behavior",
      "pricing strategies",
      "fashion industry",
      "experimental design",
      "marketing effectiveness",
      "data analysis",
      "time-granular snapshots",
      "retail analytics"
    ],
    "semantic_cluster": "ab-testing-experiments",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "causal-inference",
      "treatment-effects",
      "experimental-design",
      "consumer-analytics",
      "marketing-strategies"
    ],
    "canonical_topics": [
      "experimentation",
      "consumer-behavior",
      "pricing",
      "statistics",
      "data-engineering"
    ],
    "benchmark_usage": [
      "A/B testing",
      "adaptive stopping research"
    ]
  },
  {
    "name": "Alibaba Mobile 2021",
    "description": "Mobile user behavior data from Alibaba (2021)",
    "category": "E-Commerce",
    "url": "https://tianchi.aliyun.com/dataset/109858",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "mobile",
      "user behavior",
      "Alibaba"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Alibaba Mobile 2021 dataset provides insights into mobile user behavior specifically from Alibaba's platform in 2021. Researchers can utilize this dataset to analyze consumer patterns, preferences, and engagement metrics within the e-commerce space.",
    "use_cases": [
      "Analyzing consumer purchasing patterns",
      "Evaluating the effectiveness of mobile marketing strategies",
      "Understanding user engagement metrics",
      "Segmenting users based on behavior"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the mobile user behavior trends on Alibaba in 2021?",
      "How does user engagement vary across different mobile platforms?",
      "What factors influence purchasing decisions among Alibaba mobile users?",
      "How can mobile user behavior data inform marketing strategies?",
      "What are the demographics of Alibaba mobile users in 2021?",
      "How does Alibaba's mobile user behavior compare to other e-commerce platforms?"
    ],
    "domain_tags": [
      "e-commerce"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2021",
    "size_category": "medium",
    "model_score": 0.0,
    "embedding_text": "The Alibaba Mobile 2021 dataset is a comprehensive collection of mobile user behavior data sourced from Alibaba's platform, reflecting user interactions and engagement throughout the year 2021. This dataset is structured in a tabular format, comprising various rows and columns that capture key variables related to user behavior. The data includes metrics such as user demographics, purchase history, session duration, and interaction rates, providing a rich foundation for analysis. The collection methodology involves aggregating user activity logs and transaction records, ensuring a robust representation of mobile user behavior. Researchers can leverage this dataset to address a multitude of research questions, such as identifying trends in consumer behavior, understanding the impact of mobile user engagement on sales, and evaluating the effectiveness of marketing campaigns. Common preprocessing steps may include data cleaning, normalization, and feature engineering to prepare the data for analysis. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile resource for both academic and commercial research. However, users should be aware of potential limitations, such as data quality issues and the need for contextual understanding of the e-commerce landscape. Overall, the Alibaba Mobile 2021 dataset serves as a valuable tool for researchers and practitioners aiming to gain insights into mobile consumer behavior within the e-commerce sector.",
    "tfidf_keywords": [
      "user engagement",
      "mobile behavior",
      "e-commerce analytics",
      "consumer insights",
      "purchase patterns",
      "session duration",
      "demographic analysis",
      "marketing effectiveness",
      "behavioral segmentation",
      "Alibaba"
    ],
    "semantic_cluster": "e-commerce-consumer-behavior",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "data-analysis",
      "mobile-marketing",
      "user-experience",
      "e-commerce-strategies"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "e-commerce",
      "machine-learning"
    ]
  },
  {
    "name": "PaySim Synthetic Transactions",
    "description": "6M+ mobile money transactions simulating real fraud patterns. Agent-based model calibrated on real African mobile money logs",
    "category": "Financial Services",
    "url": "https://www.kaggle.com/datasets/ealaxi/paysim1",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "fraud detection",
      "mobile payments",
      "transactions",
      "large-scale",
      "synthetic"
    ],
    "best_for": "Learning financial services analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "fraud detection",
      "mobile payments",
      "transactions"
    ],
    "summary": "The PaySim Synthetic Transactions dataset consists of over 6 million mobile money transactions that simulate real-world fraud patterns. This dataset is particularly useful for researchers and practitioners interested in analyzing fraud detection mechanisms within mobile payment systems.",
    "use_cases": [
      "Analyzing fraud detection algorithms",
      "Evaluating mobile payment security measures",
      "Simulating transaction behaviors under different conditions"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "synthetic mobile money transactions dataset",
      "fraud detection in mobile payments",
      "PaySim dataset for fraud analysis",
      "large-scale synthetic transactions",
      "agent-based model for mobile payments",
      "real fraud patterns in mobile transactions",
      "African mobile money transaction simulation"
    ],
    "domain_tags": [
      "fintech"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Africa",
    "size_category": "massive",
    "model_score": 0.0,
    "image_url": "/images/datasets/paysim-synthetic-transactions.jpg",
    "embedding_text": "The PaySim Synthetic Transactions dataset is a comprehensive collection of over 6 million synthetic mobile money transactions designed to simulate real-world fraud patterns. This dataset is structured in a tabular format, where each row represents a transaction and includes various columns that capture key variables such as transaction amount, transaction type, and user behavior. The data is generated using an agent-based modeling approach, calibrated on actual mobile money logs from Africa, ensuring that the synthetic transactions reflect realistic patterns of behavior and fraud. Researchers can utilize this dataset to explore various aspects of mobile payment systems, including the effectiveness of different fraud detection algorithms and the overall security of mobile transactions. The dataset's large size allows for robust statistical analyses and machine learning applications, making it suitable for both descriptive and predictive modeling tasks. Common preprocessing steps may include data cleaning, normalization of transaction amounts, and feature engineering to extract relevant patterns for analysis. While the dataset provides a rich resource for understanding mobile money transactions, it is important to acknowledge potential limitations, such as the absence of real-world noise and the synthetic nature of the data, which may not capture all nuances of actual transactions. Researchers typically use this dataset to address questions related to fraud detection, user behavior analysis, and the impact of various transaction characteristics on fraud risk. Overall, the PaySim Synthetic Transactions dataset serves as a valuable tool for advancing research in the field of mobile payments and fraud detection, providing insights that can inform the development of more secure financial technologies.",
    "tfidf_keywords": [
      "synthetic transactions",
      "fraud detection",
      "mobile money",
      "agent-based model",
      "transaction simulation",
      "behavioral patterns",
      "financial technology",
      "data security",
      "machine learning",
      "predictive modeling"
    ],
    "semantic_cluster": "fraud-detection-modeling",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "fraud detection",
      "mobile payments",
      "agent-based modeling",
      "machine learning",
      "data security"
    ],
    "canonical_topics": [
      "finance",
      "machine-learning",
      "consumer-behavior"
    ],
    "benchmark_usage": [
      "Fraud detection research",
      "Mobile payment system evaluation"
    ]
  },
  {
    "name": "European Car Market",
    "description": "Car information including prices and attributes (1970-1999)",
    "category": "Automotive",
    "url": "https://sites.google.com/site/frankverbo/data-and-software/data-set-on-the-european-car-market",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "cars",
      "Europe",
      "prices",
      "IO"
    ],
    "best_for": "Learning automotive analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The European Car Market dataset contains comprehensive information on car prices and attributes from 1970 to 1999 across Europe. Researchers can utilize this dataset to analyze historical trends in car pricing, consumer preferences, and market dynamics within the automotive sector.",
    "use_cases": [
      "Analyzing price trends of cars over time",
      "Investigating consumer preferences for car attributes",
      "Comparing car prices across different European countries",
      "Studying the impact of economic factors on car pricing"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the historical car prices in Europe from 1970 to 1999?",
      "How do car attributes correlate with pricing in the European market?",
      "What trends can be observed in the European car market over the decades?",
      "Which car attributes are most valued by consumers in Europe?",
      "How did the automotive market evolve in Europe between 1970 and 1999?",
      "What factors influenced car prices in Europe during the late 20th century?"
    ],
    "domain_tags": [
      "automotive"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1970-1999",
    "geographic_scope": "Europe",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/logos/google.png",
    "embedding_text": "The European Car Market dataset is a rich source of information that encompasses various attributes and prices of cars sold in Europe from 1970 to 1999. This dataset is structured in a tabular format, with rows representing individual car entries and columns detailing various attributes such as make, model, year, price, and additional specifications. The collection methodology likely involved aggregating data from automotive sales records, manufacturer reports, and market surveys, ensuring a comprehensive representation of the car market during this period. The temporal coverage of the dataset spans three decades, providing a longitudinal view of how car prices and attributes evolved in response to changing consumer preferences and economic conditions in Europe. Key variables within the dataset include car prices, which measure the market value of vehicles, and attributes such as engine size, fuel type, and body style, which help in understanding consumer choices. However, researchers should be aware of potential limitations in data quality, such as inconsistencies in reporting across different countries and the absence of certain models or brands in specific years. Common preprocessing steps may involve cleaning the data to handle missing values, normalizing prices for inflation, and categorizing attributes for easier analysis. This dataset supports various types of analyses, including regression analysis to explore the relationship between car attributes and pricing, machine learning models for predictive analytics, and descriptive statistics to summarize trends over time. Researchers typically use this dataset to address questions related to market dynamics, consumer behavior, and the impact of economic factors on car pricing, making it a valuable resource for studies in automotive economics and consumer research.",
    "tfidf_keywords": [
      "car-prices",
      "consumer-preferences",
      "automotive-market",
      "historical-data",
      "price-trends",
      "market-dynamics",
      "car-attributes",
      "economic-factors",
      "data-collection",
      "longitudinal-study"
    ],
    "semantic_cluster": "automotive-market-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "pricing",
      "market-dynamics",
      "historical-analysis",
      "data-collection"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "pricing",
      "econometrics"
    ]
  },
  {
    "name": "Mendeley Food Delivery Reviews",
    "description": "1.69M reviews from DoorDash, Grubhub, Uber Eats. Ratings, text reviews, restaurant metadata. Gig economy platform research",
    "category": "Food & Delivery",
    "url": "https://data.mendeley.com/datasets/m5jk7wzyg7/1",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "food delivery",
      "reviews",
      "DoorDash",
      "Grubhub",
      "Uber Eats",
      "gig economy"
    ],
    "best_for": "Learning food & delivery analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "gig economy"
    ],
    "summary": "The Mendeley Food Delivery Reviews dataset comprises 1.69 million reviews from popular food delivery platforms such as DoorDash, Grubhub, and Uber Eats. This dataset includes ratings, text reviews, and restaurant metadata, making it a valuable resource for researchers studying consumer behavior in the gig economy and food delivery services.",
    "use_cases": [
      "Analyzing customer satisfaction trends across different food delivery platforms.",
      "Investigating the impact of restaurant metadata on consumer ratings.",
      "Exploring sentiment analysis of text reviews to understand consumer preferences."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the most common ratings given in food delivery reviews?",
      "How do customer reviews vary between DoorDash, Grubhub, and Uber Eats?",
      "What factors influence customer satisfaction in food delivery services?",
      "How does the gig economy impact consumer behavior in food delivery?",
      "What trends can be identified in food delivery reviews over time?",
      "How do restaurant characteristics affect customer ratings?"
    ],
    "domain_tags": [
      "retail",
      "food-service"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/logos/mendeley.png",
    "embedding_text": "The Mendeley Food Delivery Reviews dataset is a comprehensive collection of 1.69 million reviews sourced from major food delivery platforms including DoorDash, Grubhub, and Uber Eats. This dataset is structured in a tabular format, with each row representing an individual review and associated metadata. Key columns in the dataset include user ratings, text reviews, and various restaurant attributes such as cuisine type, location, and delivery times. The reviews provide insights into customer experiences, allowing researchers to explore patterns in consumer behavior and preferences within the gig economy. The data collection methodology involves aggregating user-generated content from the platforms, ensuring a diverse range of opinions and experiences are captured. Researchers can utilize this dataset to address various research questions, such as identifying factors that influence customer satisfaction, analyzing trends in food delivery service ratings, and conducting sentiment analysis on text reviews. Common preprocessing steps may include cleaning text data, normalizing ratings, and categorizing restaurant metadata for more nuanced analysis. The dataset supports a variety of analytical approaches, including regression analysis, machine learning techniques, and descriptive statistics. Researchers typically leverage this dataset to gain insights into consumer behavior, evaluate service quality, and inform business strategies within the food delivery sector. However, users should be aware of potential limitations, such as biases in user reviews and the representativeness of the data across different demographics. Overall, the Mendeley Food Delivery Reviews dataset serves as a valuable resource for understanding the dynamics of the food delivery market and the gig economy.",
    "tfidf_keywords": [
      "food delivery",
      "consumer satisfaction",
      "gig economy",
      "text reviews",
      "restaurant metadata",
      "sentiment analysis",
      "user ratings",
      "platform comparison",
      "customer preferences",
      "data aggregation"
    ],
    "semantic_cluster": "consumer-behavior-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "sentiment-analysis",
      "marketplace-dynamics",
      "data-mining",
      "user-experience"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "marketplaces",
      "natural-language-processing",
      "data-engineering"
    ]
  },
  {
    "name": "Alibaba Brick and Mortar (IJCAI-16)",
    "description": "Online and offline check-ins/purchases from 1,000+ stores",
    "category": "E-Commerce",
    "url": "https://tianchi.aliyun.com/dataset/53",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "O2O",
      "offline",
      "retail",
      "IJCAI"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "retail"
    ],
    "summary": "The Alibaba Brick and Mortar dataset contains information on online and offline check-ins and purchases from over 1,000 stores. This dataset can be used to analyze consumer behavior, evaluate the effectiveness of marketing strategies, and study the interaction between online and offline retail channels.",
    "use_cases": [
      "Analyzing consumer purchasing behavior across different retail channels",
      "Evaluating the impact of promotional campaigns on offline sales",
      "Studying the relationship between online engagement and in-store purchases",
      "Identifying trends in retail performance over time"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the online and offline purchasing patterns in the Alibaba dataset?",
      "How does consumer behavior differ between online and offline shopping?",
      "What insights can be drawn from the check-in data of 1,000+ stores?",
      "How can the Alibaba Brick and Mortar dataset inform retail marketing strategies?",
      "What trends can be identified in the retail sector using this dataset?",
      "How does the dataset support analysis of O2O (online-to-offline) commerce?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "embedding_text": "The Alibaba Brick and Mortar dataset is a comprehensive collection of online and offline check-ins and purchases from over 1,000 retail stores, providing a rich resource for researchers and analysts interested in the dynamics of consumer behavior in the e-commerce landscape. This dataset is structured in a tabular format, with rows representing individual transactions and check-ins, and columns capturing various attributes such as store ID, transaction amount, timestamp, and customer demographics. The collection methodology involves aggregating data from Alibaba's extensive retail network, ensuring a diverse representation of consumer interactions across different retail environments. While the dataset does not specify temporal or geographic coverage, it offers a wealth of information that can be leveraged to explore various research questions, such as the effectiveness of online marketing strategies in driving offline sales or the factors influencing consumer preferences in a multi-channel shopping environment. Key variables within the dataset include transaction amounts, store locations, and customer demographics, which can be used to measure purchasing trends and consumer engagement levels. However, researchers should be aware of potential limitations regarding data quality, such as missing values or inconsistencies in transaction records. Common preprocessing steps may involve cleaning the data, handling missing values, and transforming categorical variables into numerical formats suitable for analysis. The dataset supports a range of analytical approaches, including regression analysis, machine learning techniques, and descriptive statistics, making it a versatile tool for understanding the complexities of retail consumer behavior. Researchers typically utilize this dataset to conduct studies that inform marketing strategies, optimize inventory management, and enhance customer experience in both online and offline settings.",
    "tfidf_keywords": [
      "check-ins",
      "purchases",
      "O2O",
      "consumer behavior",
      "retail analytics",
      "transaction data",
      "marketing strategies",
      "multi-channel",
      "data aggregation",
      "e-commerce"
    ],
    "semantic_cluster": "consumer-behavior-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "consumer-behavior",
      "retail-analytics",
      "marketing-strategies",
      "multi-channel-retailing",
      "data-analysis"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "marketplaces",
      "data-engineering",
      "econometrics"
    ]
  },
  {
    "name": "ICPSR Auction Studies",
    "description": "Search results for auction studies from ICPSR",
    "category": "Advertising",
    "url": "https://www.openicpsr.org/openicpsr/search/studies",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "auctions",
      "research",
      "social science"
    ],
    "best_for": "Learning advertising analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The ICPSR Auction Studies dataset provides a comprehensive collection of auction-related research studies. Researchers can utilize this dataset to explore various aspects of auction mechanisms, bidder behavior, and market dynamics in the context of advertising and social sciences.",
    "use_cases": [
      "Analyzing bidder behavior in different auction formats",
      "Exploring the impact of reserve prices on auction outcomes",
      "Investigating the effects of advertising on auction participation"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the key findings from the ICPSR Auction Studies?",
      "How do auction formats impact bidder behavior?",
      "What research methodologies are used in auction studies?",
      "What are common themes in auction research?",
      "How can auction studies inform advertising strategies?",
      "What variables are typically analyzed in auction studies?"
    ],
    "domain_tags": [
      "advertising"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "embedding_text": "The ICPSR Auction Studies dataset is a rich repository of research findings focused on auction mechanisms and their implications in various fields, particularly advertising and social sciences. The dataset consists of a structured collection of studies, each with its own set of variables that capture essential aspects of auction dynamics, including bidder behavior, auction formats, and market responses. Researchers can expect to find data organized in a tabular format, with rows representing individual studies and columns detailing key variables such as auction type, participant demographics, and outcomes. The collection methodology involves rigorous data curation from reputable sources, ensuring high data quality and reliability. However, researchers should be aware of potential limitations, such as variations in study design and sample sizes, which may affect the generalizability of findings. Common preprocessing steps may include standardizing variable formats and handling missing data. This dataset supports a wide range of analyses, from regression models to machine learning techniques, enabling researchers to address critical questions about auction efficiency, bidder strategies, and the influence of external factors like advertising on auction participation. Overall, the ICPSR Auction Studies dataset serves as a valuable resource for scholars and practitioners interested in understanding the complexities of auction markets and their broader implications.",
    "tfidf_keywords": [
      "auction-mechanisms",
      "bidder-behavior",
      "reserve-prices",
      "auction-formats",
      "market-dynamics",
      "advertising-strategies",
      "research-methodologies",
      "data-curation",
      "study-design",
      "sample-sizes"
    ],
    "semantic_cluster": "auction-research-methods",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "marketplace-dynamics",
      "behavioral-economics",
      "experimental-design",
      "consumer-behavior",
      "economic-theory"
    ],
    "canonical_topics": [
      "causal-inference",
      "experimentation",
      "econometrics",
      "consumer-behavior",
      "pricing"
    ]
  },
  {
    "name": "Online Shopping Intention",
    "description": "12,330 user sessions with numerical and categorical features for purchase prediction",
    "category": "E-Commerce",
    "url": "https://www.kaggle.com/datasets/henrysue/online-shoppers-intention",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "purchase prediction",
      "sessions",
      "Kaggle"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "purchase-prediction"
    ],
    "summary": "The Online Shopping Intention dataset consists of 12,330 user sessions, capturing both numerical and categorical features that can be utilized for predicting purchase behavior. Researchers and analysts can leverage this dataset to understand consumer intentions and enhance e-commerce strategies.",
    "use_cases": [
      "Predicting purchase behavior based on user sessions",
      "Analyzing factors influencing online shopping decisions",
      "Segmenting users based on shopping intentions"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What features are available in the Online Shopping Intention dataset?",
      "How can I use this dataset for purchase prediction?",
      "What insights can be drawn from user sessions in e-commerce?",
      "Is there a way to analyze consumer behavior using this dataset?",
      "What machine learning techniques can be applied to this dataset?",
      "How does this dataset help in understanding online shopping intentions?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/online-shopping-intention.jpg",
    "embedding_text": "The Online Shopping Intention dataset is a rich resource comprising 12,330 user sessions, each characterized by various numerical and categorical features that are instrumental in predicting purchase behavior. The dataset is structured in a tabular format, where each row represents a unique user session, and the columns include diverse variables that capture user interactions, preferences, and outcomes during their shopping journey. The collection methodology for this dataset typically involves tracking user sessions on e-commerce platforms, where data is gathered through cookies and session identifiers, ensuring that a comprehensive view of user behavior is captured. While the dataset does not specify temporal or geographic coverage, it is designed to reflect a broad spectrum of consumer interactions in an online shopping context. Key variables within the dataset may include session duration, product categories viewed, items added to cart, and final purchase decisions, each measuring distinct aspects of user engagement and intent. However, researchers should be aware of potential limitations, such as data sparsity for certain user segments or the influence of external factors like promotions that may not be captured. Common preprocessing steps for this dataset may involve handling missing values, normalizing numerical features, and encoding categorical variables to prepare the data for analysis. Researchers can utilize this dataset to address various research questions, such as identifying key predictors of purchase intention, understanding the impact of user behavior on sales conversions, and exploring the relationship between session characteristics and final purchase outcomes. The dataset supports a variety of analytical approaches, including regression analysis, machine learning algorithms, and descriptive statistics, making it a versatile tool for both exploratory and predictive analyses. In studies, researchers typically use this dataset to gain insights into consumer behavior patterns, optimize marketing strategies, and enhance user experience on e-commerce platforms.",
    "tfidf_keywords": [
      "purchase-intention",
      "user-sessions",
      "e-commerce",
      "consumer-behavior",
      "predictive-modeling",
      "session-analysis",
      "feature-engineering",
      "data-preprocessing",
      "machine-learning",
      "regression-analysis"
    ],
    "semantic_cluster": "e-commerce-purchase-prediction",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "predictive-modeling",
      "data-preprocessing",
      "feature-engineering",
      "machine-learning"
    ],
    "canonical_topics": [
      "machine-learning",
      "consumer-behavior",
      "product-analytics"
    ]
  },
  {
    "name": "Criteo Attribution Dataset",
    "description": "30 days of advertising traffic with conversion attribution data for multi-touch attribution research",
    "category": "Advertising",
    "url": "https://ailab.criteo.com/criteo-attribution-modeling-bidding-dataset/",
    "docs_url": "https://ailab.criteo.com/criteo-attribution-modeling-bidding-dataset/",
    "github_url": null,
    "tags": [
      "attribution",
      "conversions",
      "touchpoints",
      "Criteo"
    ],
    "best_for": "Multi-touch attribution modeling and conversion path analysis",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Criteo Attribution Dataset provides 30 days of advertising traffic data, including conversion attribution information, which is essential for multi-touch attribution research. Researchers can utilize this dataset to analyze the effectiveness of various advertising touchpoints and their impact on conversion rates.",
    "use_cases": [
      "Analyzing the effectiveness of different advertising channels",
      "Evaluating the impact of touchpoints on conversion rates",
      "Conducting regression analysis to identify key factors influencing conversions",
      "Developing machine learning models for predicting conversion outcomes"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the Criteo Attribution Dataset?",
      "How can I analyze multi-touch attribution using the Criteo dataset?",
      "What variables are included in the Criteo Attribution Dataset?",
      "How does advertising traffic affect conversion rates?",
      "What methodologies can be applied to the Criteo Attribution Dataset?",
      "What insights can be gained from analyzing touchpoints in advertising?",
      "How does the Criteo Attribution Dataset support regression analysis?",
      "What are the limitations of the Criteo Attribution Dataset?"
    ],
    "domain_tags": [
      "advertising"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "30 days",
    "geographic_scope": "Global",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/criteo-attribution-dataset.jpg",
    "embedding_text": "The Criteo Attribution Dataset is a comprehensive collection of advertising traffic data spanning a period of 30 days, specifically designed for research in multi-touch attribution. This dataset includes various key variables that measure advertising interactions and conversions, allowing researchers to explore the intricate relationships between different advertising touchpoints and their effectiveness in driving conversions. The data is structured in a tabular format, with rows representing individual advertising interactions and columns capturing essential metrics such as click-through rates, conversion events, and attribution details. Researchers can leverage this dataset to conduct a variety of analyses, including regression analysis to identify significant predictors of conversion, as well as machine learning applications to develop predictive models that enhance advertising strategies.\n\nThe collection methodology for the Criteo Attribution Dataset involves aggregating data from real advertising campaigns, ensuring a rich source of insights into consumer behavior and advertising effectiveness. However, researchers should be aware of potential limitations regarding data quality, such as missing values or biases inherent in the data collection process. Common preprocessing steps may include data cleaning, normalization, and feature engineering to prepare the dataset for analysis.\n\nThis dataset is particularly valuable for addressing research questions related to the effectiveness of advertising strategies, the role of different touchpoints in the consumer journey, and the overall impact of advertising on conversion rates. By utilizing the Criteo Attribution Dataset, researchers can gain a deeper understanding of consumer behavior in the context of advertising, ultimately leading to more effective marketing strategies and improved ROI for advertising campaigns.",
    "tfidf_keywords": [
      "multi-touch attribution",
      "advertising traffic",
      "conversion rates",
      "click-through rates",
      "attribution details",
      "consumer behavior",
      "predictive modeling",
      "data preprocessing",
      "regression analysis",
      "advertising strategies",
      "feature engineering",
      "data quality",
      "advertising effectiveness",
      "touchpoint analysis",
      "data collection methodology"
    ],
    "semantic_cluster": "advertising-attribution-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "causal-inference",
      "machine-learning",
      "consumer-behavior",
      "advertising-strategies",
      "data-preprocessing"
    ],
    "canonical_topics": [
      "causal-inference",
      "machine-learning",
      "consumer-behavior",
      "advertising",
      "econometrics"
    ]
  },
  {
    "name": "BTS Airline On-Time Performance",
    "description": "All US flights since 1987. Delays, cancellations, fares, capacity. Revenue management research goldmine",
    "category": "Transportation & Mobility",
    "url": "https://www.transtats.bts.gov/ontime/",
    "docs_url": "https://www.bts.gov/topics/airline-time-tables",
    "github_url": null,
    "tags": [
      "airline",
      "flights",
      "delays",
      "pricing",
      "large-scale"
    ],
    "best_for": "Learning transportation & mobility analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The BTS Airline On-Time Performance dataset includes comprehensive data on all US flights since 1987, detailing delays, cancellations, fares, and capacity. Researchers can utilize this dataset to conduct revenue management analysis, explore factors affecting flight performance, and assess pricing strategies in the airline industry.",
    "use_cases": [
      "Analyzing trends in flight delays over time",
      "Evaluating the impact of pricing strategies on flight occupancy",
      "Assessing the relationship between capacity and cancellations",
      "Conducting regression analysis to predict flight delays"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the BTS Airline On-Time Performance dataset?",
      "How can I analyze flight delays using the BTS dataset?",
      "What variables are included in the BTS Airline dataset?",
      "How has airline pricing changed over the years?",
      "What factors contribute to flight cancellations?",
      "How can I visualize trends in airline performance?",
      "What is the significance of the BTS dataset for revenue management research?"
    ],
    "domain_tags": [
      "transportation",
      "aviation"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1987-present",
    "geographic_scope": "United States",
    "size_category": "large",
    "model_score": 0.0,
    "embedding_text": "The BTS Airline On-Time Performance dataset is a rich resource for analyzing the performance of US flights since 1987. It contains a variety of data points including delays, cancellations, fares, and capacity, making it an invaluable asset for researchers in the fields of transportation and economics. The dataset is structured in a tabular format, with rows representing individual flights and columns capturing key variables such as flight number, departure and arrival times, delays, cancellations, and fare information. This comprehensive structure allows for detailed analysis of flight performance and pricing strategies. The data is collected from various sources, including airlines and government reports, ensuring a high level of accuracy and reliability. However, researchers should be aware of potential limitations such as missing data for certain flights or inconsistencies in reporting across different airlines. Common preprocessing steps may include handling missing values, normalizing fare data, and aggregating information to analyze trends over time. The dataset supports a wide range of analyses, from descriptive statistics to advanced regression models and machine learning techniques. Researchers typically use this dataset to address questions related to the factors influencing flight delays, the effectiveness of pricing strategies, and the overall performance of the airline industry. By leveraging this dataset, analysts can uncover insights that inform policy decisions, optimize revenue management practices, and enhance the understanding of consumer behavior in the aviation sector.",
    "tfidf_keywords": [
      "flight-performance",
      "delay-analysis",
      "cancellation-rates",
      "fare-optimization",
      "capacity-utilization",
      "revenue-management",
      "airline-industry",
      "time-series-analysis",
      "predictive-modeling",
      "data-visualization",
      "operational-efficiency",
      "consumer-demand",
      "market-trends"
    ],
    "semantic_cluster": "airline-performance-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "time-series-analysis",
      "regression-modeling",
      "predictive-analytics",
      "consumer-behavior",
      "market-analysis"
    ],
    "canonical_topics": [
      "econometrics",
      "pricing",
      "consumer-behavior",
      "forecasting",
      "transportation"
    ],
    "benchmark_usage": [
      "Revenue management research",
      "Flight performance analysis"
    ]
  },
  {
    "name": "ORBITAAL Bitcoin Transactions",
    "description": "13 years of Bitcoin transaction graphs (2009-2022). Complete blockchain with labeled entities. network analysis at scale",
    "category": "Financial Services",
    "url": "https://zenodo.org/records/7958648",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Bitcoin",
      "blockchain",
      "transactions",
      "network analysis",
      "cryptocurrency"
    ],
    "best_for": "Learning financial services analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "cryptocurrency",
      "blockchain",
      "network analysis"
    ],
    "summary": "The ORBITAAL Bitcoin Transactions dataset provides a comprehensive view of Bitcoin transactions over a 13-year period, from 2009 to 2022. It includes complete blockchain data with labeled entities, enabling researchers to perform extensive network analysis and gain insights into cryptocurrency transactions.",
    "use_cases": [
      "Analyzing transaction patterns over time to identify trends in Bitcoin usage.",
      "Investigating the relationships between different entities in the Bitcoin network.",
      "Conducting network analysis to understand the flow of transactions.",
      "Studying the impact of major events on Bitcoin transaction volumes."
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the key features of Bitcoin transactions in the ORBITAAL dataset?",
      "How can I analyze Bitcoin transaction networks using this dataset?",
      "What insights can be derived from 13 years of Bitcoin transaction data?",
      "How does the ORBITAAL dataset facilitate blockchain analysis?",
      "What entities are labeled in the ORBITAAL Bitcoin Transactions dataset?",
      "What types of network analysis can be performed with Bitcoin transaction graphs?",
      "How has Bitcoin transaction behavior evolved from 2009 to 2022?",
      "What are the implications of the ORBITAAL dataset for cryptocurrency research?"
    ],
    "domain_tags": [
      "fintech"
    ],
    "data_modality": "graph",
    "temporal_coverage": "2009-2022",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/logos/zenodo.png",
    "embedding_text": "The ORBITAAL Bitcoin Transactions dataset is a rich resource for researchers and analysts interested in the dynamics of Bitcoin transactions over an extensive period, spanning from 2009 to 2022. This dataset encompasses a complete blockchain with detailed transaction graphs, allowing for in-depth exploration of the cryptocurrency landscape. The data structure consists of various rows and columns representing individual transactions, timestamps, transaction amounts, and labeled entities involved in each transaction. The collection methodology involves aggregating data from the Bitcoin blockchain, ensuring comprehensive coverage of all transactions during the specified timeframe. Key variables within the dataset include transaction IDs, sender and receiver addresses, transaction amounts, and timestamps, which collectively measure the flow and volume of Bitcoin transactions. While the dataset is extensive, researchers should be aware of potential limitations related to data quality, such as the accuracy of labeled entities and the completeness of transaction records. Common preprocessing steps may include data cleaning, normalization, and entity resolution to prepare the data for analysis. This dataset supports a variety of research questions, including how transaction behaviors change over time, the identification of influential entities within the Bitcoin network, and the analysis of transaction patterns in response to market events. Researchers typically employ methods such as regression analysis, machine learning, and descriptive statistics to extract meaningful insights from the data. Overall, the ORBITAAL Bitcoin Transactions dataset serves as a foundational tool for understanding the complexities of Bitcoin transactions and their implications for the broader financial ecosystem.",
    "tfidf_keywords": [
      "Bitcoin",
      "blockchain",
      "transactions",
      "network analysis",
      "cryptocurrency",
      "entity labeling",
      "transaction graphs",
      "temporal analysis",
      "flow of transactions",
      "transaction patterns",
      "data aggregation",
      "financial ecosystem",
      "data quality",
      "entity resolution",
      "market events"
    ],
    "semantic_cluster": "bitcoin-network-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "network-theory",
      "cryptoeconomics",
      "data-visualization",
      "transaction-analysis",
      "financial-technology"
    ],
    "canonical_topics": [
      "finance",
      "machine-learning",
      "data-engineering",
      "consumer-behavior"
    ]
  },
  {
    "name": "Criteo 1TB Click Logs",
    "description": "World's largest public ML advertising dataset with 4+ billion events, 13 integer and 26 categorical features across 24 days",
    "category": "Advertising",
    "url": "https://ailab.criteo.com/download-criteo-1tb-click-logs-dataset/",
    "docs_url": "https://ailab.criteo.com/download-criteo-1tb-click-logs-dataset/",
    "github_url": null,
    "tags": [
      "CTR prediction",
      "click logs",
      "large-scale",
      "Criteo"
    ],
    "best_for": "Training and benchmarking large-scale CTR prediction models",
    "difficulty": "beginner",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "advertising"
    ],
    "summary": "The Criteo 1TB Click Logs dataset is the world's largest public machine learning advertising dataset, featuring over 4 billion events collected over 24 days. It includes a rich set of 13 integer and 26 categorical features, making it ideal for various analyses, particularly in click-through rate (CTR) prediction and advertising effectiveness.",
    "use_cases": [
      "CTR prediction modeling",
      "User behavior analysis",
      "Ad performance evaluation"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Criteo 1TB Click Logs dataset?",
      "How can I use the Criteo dataset for CTR prediction?",
      "What features are included in the Criteo click logs?",
      "Where can I find large-scale advertising datasets?",
      "What are the applications of click log data?",
      "How does the Criteo dataset support machine learning research?"
    ],
    "domain_tags": [
      "advertising"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "24 days",
    "geographic_scope": "Global",
    "size_category": "massive",
    "model_score": 0.0,
    "embedding_text": "The Criteo 1TB Click Logs dataset represents a significant advancement in the availability of large-scale advertising data for machine learning applications. Comprising over 4 billion click events, this dataset spans a period of 24 days and includes a comprehensive schema with 13 integer features and 26 categorical features. The data structure is designed to facilitate various types of analyses, particularly in the realm of click-through rate (CTR) prediction, which is essential for optimizing advertising strategies. Researchers and data scientists can leverage this dataset to explore user behavior, ad performance, and the effectiveness of different advertising strategies. The collection methodology involves tracking user interactions with ads across a wide array of platforms, ensuring a diverse and representative sample of user behavior. Key variables in the dataset include user identifiers, ad identifiers, and various contextual features that provide insights into the conditions under which clicks occur. However, users should be aware of potential data quality issues, such as missing values or biases inherent in user behavior tracking. Common preprocessing steps may include handling missing data, encoding categorical variables, and normalizing numerical features to prepare the dataset for analysis. The dataset supports a range of analytical techniques, including regression analysis, machine learning model training, and descriptive statistics. Researchers typically use the Criteo dataset to address questions related to advertising effectiveness, user engagement, and the optimization of marketing strategies. Overall, the Criteo 1TB Click Logs dataset is a valuable resource for those looking to delve into the complexities of online advertising and consumer behavior.",
    "benchmark_usage": [
      "CTR prediction",
      "large-scale advertising analysis"
    ],
    "tfidf_keywords": [
      "click-through-rate",
      "advertising-performance",
      "user-behavior",
      "machine-learning",
      "feature-engineering",
      "data-preprocessing",
      "large-scale-data",
      "CTR-prediction",
      "ad-optimization",
      "event-logs"
    ],
    "semantic_cluster": "advertising-analytics",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "machine-learning",
      "consumer-behavior",
      "advertising-strategies",
      "data-preprocessing",
      "feature-engineering"
    ],
    "canonical_topics": [
      "machine-learning",
      "advertising",
      "consumer-behavior"
    ]
  },
  {
    "name": "JD.com Open Datasets",
    "description": "Open dataset portal for e-commerce and logistics from JD.com",
    "category": "Data Portals",
    "url": "https://datascience.jd.com/page/opendataset.html",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "JD.com",
      "e-commerce",
      "logistics",
      "China"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "logistics",
      "data analysis"
    ],
    "summary": "The JD.com Open Datasets provide a comprehensive collection of data related to e-commerce and logistics in China. Researchers and analysts can utilize this dataset to explore consumer behavior, pricing strategies, and operational efficiencies within the e-commerce sector.",
    "use_cases": [
      "Analyzing consumer purchasing patterns",
      "Evaluating logistics efficiency",
      "Studying pricing strategies in e-commerce"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What datasets are available on JD.com for e-commerce analysis?",
      "How can I access logistics data from JD.com?",
      "What insights can be derived from JD.com open datasets?",
      "Are there datasets related to consumer behavior on JD.com?",
      "What e-commerce trends can be analyzed using JD.com data?",
      "How does JD.com data support logistics research?"
    ],
    "domain_tags": [
      "retail",
      "e-commerce"
    ],
    "data_modality": "tabular",
    "geographic_scope": "China",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/logos/jd.png",
    "embedding_text": "The JD.com Open Datasets serve as a vital resource for researchers and practitioners interested in the fields of e-commerce and logistics. This dataset encompasses a variety of data structures, primarily in tabular format, which includes rows representing individual transactions or logistics events and columns detailing various attributes such as product categories, pricing, customer demographics, and shipping times. The collection methodology involves aggregating data from JD.com's extensive operational framework, ensuring a rich dataset that reflects real-world e-commerce dynamics in China. Key variables within the dataset measure aspects such as sales volume, customer engagement metrics, and logistical performance indicators. While the data quality is generally high, users should be aware of potential limitations such as missing values or biases inherent in consumer reporting. Common preprocessing steps may include data cleaning, normalization, and feature engineering to prepare the dataset for analysis. Researchers can leverage this dataset to address a range of research questions, including the impact of pricing changes on sales, the effectiveness of promotional strategies, and the optimization of supply chain logistics. The dataset supports various analytical approaches, including regression analysis, machine learning models, and descriptive statistics, making it a versatile tool for both academic research and practical applications in the industry. Typically, researchers utilize the JD.com Open Datasets to conduct studies that inform business strategies, enhance operational efficiencies, and contribute to the broader understanding of consumer behavior in the rapidly evolving e-commerce landscape.",
    "tfidf_keywords": [
      "e-commerce",
      "logistics",
      "consumer behavior",
      "pricing strategies",
      "data analysis",
      "sales volume",
      "customer engagement",
      "supply chain",
      "promotional strategies",
      "data cleaning"
    ],
    "semantic_cluster": "ecommerce-logistics-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "pricing",
      "logistics",
      "data-analysis",
      "marketplaces"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "pricing",
      "marketplaces",
      "data-engineering",
      "econometrics"
    ]
  },
  {
    "name": "RecSys Challenge 2025 (Synerise)",
    "description": "1M users, 6 months of real e-commerce behavior logs with 5 event types for universal behavioral modeling",
    "category": "Data Portals",
    "url": "https://recsys.synerise.com/data-set",
    "docs_url": "https://recsys.synerise.com/",
    "github_url": "https://github.com/Synerise/recsys2025",
    "tags": [
      "RecSys",
      "e-commerce",
      "large-scale",
      "real-world",
      "2025",
      "user behavior"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "user behavior"
    ],
    "summary": "The RecSys Challenge 2025 dataset comprises 1 million users' real e-commerce behavior logs collected over six months, featuring five distinct event types. This dataset enables researchers to model user behavior in a comprehensive manner, facilitating various analyses in the realm of recommendation systems and consumer behavior.",
    "use_cases": [
      "Modeling user behavior in e-commerce",
      "Analyzing event types for recommendation systems",
      "Evaluating consumer engagement over time"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the event types in the RecSys Challenge 2025 dataset?",
      "How can I analyze user behavior using this dataset?",
      "What insights can be derived from 1M users' e-commerce logs?",
      "What is the temporal coverage of the RecSys Challenge 2025 dataset?",
      "How does this dataset support recommendation system research?",
      "What preprocessing steps are needed for the RecSys Challenge 2025 data?",
      "What are the key variables in the RecSys Challenge 2025 dataset?",
      "How can I model user behavior with this dataset?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "large",
    "model_score": 0.0,
    "image_url": "/images/datasets/recsys-challenge-2025-synerise.png",
    "embedding_text": "The RecSys Challenge 2025 dataset represents a significant advancement in the field of recommendation systems and consumer behavior analysis. Comprising 1 million users' real e-commerce behavior logs over a period of six months, this dataset includes five distinct event types, providing a rich source of information for researchers and practitioners alike. The data structure is organized in a tabular format, featuring rows that correspond to individual user events and columns that capture various attributes such as user ID, event type, timestamp, and potentially other behavioral metrics. The collection methodology involves tracking user interactions within an e-commerce platform, ensuring that the data reflects genuine consumer behavior in a real-world setting. This dataset's temporal coverage of six months allows for the exploration of trends and patterns in user behavior over time, making it particularly valuable for longitudinal studies. However, researchers should be aware of potential limitations in data quality, such as missing values or biases in user engagement that may affect the analysis. Common preprocessing steps may include data cleaning, normalization, and transformation to prepare the dataset for analysis. Key variables in the dataset measure user engagement, event frequency, and interaction types, which can be leveraged to address various research questions related to consumer behavior, such as understanding purchasing patterns, the impact of different event types on user engagement, and the effectiveness of recommendation algorithms. The dataset supports a range of analytical methods, including regression analysis, machine learning techniques, and descriptive statistics, enabling researchers to derive actionable insights and contribute to the development of more effective recommendation systems. Typically, researchers utilize this dataset to benchmark algorithms, test hypotheses about user behavior, and develop models that predict future consumer actions based on historical data.",
    "temporal_coverage": "6 months",
    "tfidf_keywords": [
      "user behavior",
      "e-commerce logs",
      "event types",
      "recommendation systems",
      "consumer engagement",
      "behavioral modeling",
      "longitudinal analysis",
      "data preprocessing",
      "machine learning",
      "regression analysis"
    ],
    "semantic_cluster": "user-behavior-modeling",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "behavioral-economics",
      "machine-learning",
      "consumer-behavior",
      "recommendation-systems",
      "data-preprocessing"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "recommendation-systems",
      "machine-learning"
    ]
  },
  {
    "name": "IJCAI Competitions",
    "description": "International AI conference with competitions",
    "category": "Data Portals",
    "url": "https://www.ijcai.org/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "IJCAI",
      "AI",
      "competitions"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The IJCAI Competitions dataset encompasses various competitions held at the International Joint Conference on Artificial Intelligence, focusing on advancements in AI. Researchers and practitioners can utilize this dataset to explore competition results, methodologies, and innovations in AI technologies.",
    "use_cases": [],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the IJCAI competitions?",
      "How can I participate in IJCAI competitions?",
      "What AI technologies are showcased in IJCAI competitions?",
      "What are the results of past IJCAI competitions?",
      "How do IJCAI competitions contribute to AI research?",
      "Where can I find datasets from IJCAI competitions?"
    ],
    "domain_tags": [
      "AI"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/logos/ijcai.png",
    "embedding_text": "The IJCAI Competitions dataset is a rich resource that captures the essence of the International Joint Conference on Artificial Intelligence, which is renowned for its competitive events aimed at pushing the boundaries of AI research and application. This dataset includes various competitions that have taken place over the years, showcasing innovative approaches and solutions developed by participants from around the globe. The structure of the dataset is likely to include rows representing individual competition entries, with columns detailing key variables such as competition name, year, participating teams, methodologies employed, and results achieved. The collection methodology for this dataset typically involves gathering information directly from competition organizers and participants, ensuring a comprehensive overview of the competitive landscape in AI. While specific temporal and geographic coverage details may not be explicitly mentioned, the dataset reflects a broad spectrum of AI advancements over time and across different regions, highlighting the global nature of the IJCAI competitions. Key variables within the dataset measure aspects such as performance metrics, algorithmic approaches, and innovation levels, providing insights into the effectiveness of various AI techniques. Researchers utilizing this dataset can expect to engage in a variety of analyses, including regression analyses to identify trends in competition outcomes, machine learning techniques to model performance predictors, and descriptive analyses to summarize the competitive landscape. Common preprocessing steps may involve cleaning the data to ensure consistency in competition entries, normalizing performance metrics for comparative analysis, and categorizing methodologies for easier interpretation. The dataset supports research questions related to the evolution of AI methodologies, the impact of competition participation on innovation, and the effectiveness of different AI approaches in real-world applications. Researchers typically use this dataset to benchmark AI techniques, explore the dynamics of competition in AI, and derive insights that can inform future research and development in the field.",
    "tfidf_keywords": [
      "AI competitions",
      "IJCAI",
      "machine learning",
      "algorithm performance",
      "innovation metrics",
      "competition results",
      "data collection",
      "research methodologies",
      "performance benchmarking",
      "competitive landscape"
    ],
    "semantic_cluster": "ai-competitions",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "machine-learning",
      "competitions",
      "artificial-intelligence",
      "data-analysis",
      "performance-evaluation"
    ],
    "canonical_topics": [
      "machine-learning",
      "experimentation",
      "artificial-intelligence"
    ]
  },
  {
    "name": "OTTO Session Data",
    "description": "12M German e-commerce sessions with click \u2192 cart \u2192 order sequences. RecSys 2022 competition",
    "category": "E-Commerce",
    "url": "https://github.com/otto-de/recsys-dataset",
    "docs_url": null,
    "github_url": "https://github.com/otto-de/recsys-dataset",
    "tags": [
      "sessions",
      "recommendations",
      "Germany",
      "RecSys"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "recommendations"
    ],
    "summary": "The OTTO Session Data consists of 12 million e-commerce sessions from Germany, capturing the click, cart, and order sequences of users. This dataset is particularly useful for developing recommendation systems and understanding consumer behavior in online shopping environments.",
    "use_cases": [
      "Analyzing consumer behavior patterns",
      "Developing recommendation algorithms",
      "Evaluating the effectiveness of marketing strategies",
      "Understanding the conversion funnel in e-commerce"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the OTTO Session Data?",
      "How can I analyze e-commerce session data?",
      "What insights can be gained from click to cart sequences?",
      "How does consumer behavior manifest in online shopping?",
      "What are the key variables in the OTTO Session Data?",
      "How can this dataset be used for recommendation systems?",
      "What are the limitations of the OTTO Session Data?",
      "What preprocessing steps are needed for this dataset?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Germany",
    "size_category": "massive",
    "model_score": 0.0,
    "image_url": "/images/datasets/otto-session-data.png",
    "embedding_text": "The OTTO Session Data is a comprehensive dataset that captures 12 million e-commerce sessions from a leading German online retailer, OTTO. It includes detailed clickstream data, which tracks user interactions as they move from product views to adding items to their cart and ultimately completing purchases. The dataset is structured in a tabular format, with rows representing individual sessions and columns capturing various attributes such as session ID, user ID, timestamps, product IDs, and actions taken (clicks, adds to cart, orders). This rich dataset provides a valuable resource for researchers and practitioners interested in the fields of e-commerce, consumer behavior, and recommendation systems. The data collection methodology involves logging user interactions on the OTTO platform, ensuring a robust representation of real-world shopping behavior. Key variables in the dataset include session duration, number of clicks, conversion rates, and product categories, which can be analyzed to derive insights into user preferences and shopping patterns. However, researchers should be aware of potential limitations, such as data sparsity in certain product categories and the need for careful preprocessing to handle missing values and outliers. Common preprocessing steps may include normalization of session times, encoding categorical variables, and aggregating data to analyze user behavior at different levels (e.g., session-level or user-level). The dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics. Researchers typically use this dataset to address questions related to consumer decision-making processes, the effectiveness of recommendation algorithms, and the overall dynamics of online shopping behavior. By leveraging the insights gained from this dataset, businesses can enhance their marketing strategies, improve user experience, and ultimately drive sales growth.",
    "tfidf_keywords": [
      "clickstream",
      "e-commerce",
      "recommendation-systems",
      "consumer-behavior",
      "session-analysis",
      "conversion-funnel",
      "user-interaction",
      "data-preprocessing",
      "behavioral-analysis",
      "market-strategies"
    ],
    "semantic_cluster": "e-commerce-analytics",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "recommendation-systems",
      "consumer-behavior",
      "data-preprocessing",
      "clickstream-analysis",
      "e-commerce-strategies"
    ],
    "canonical_topics": [
      "recommendation-systems",
      "consumer-behavior",
      "machine-learning"
    ],
    "benchmark_usage": [
      "Recommendation system development",
      "Consumer behavior analysis"
    ]
  },
  {
    "name": "ORBITAAL Bitcoin Graph",
    "description": "13 years (2009-2021) of entity-level Bitcoin transaction networks with BTC/USD values",
    "category": "Financial Services",
    "url": "https://www.nature.com/articles/s41597-023-02416-6",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Bitcoin",
      "crypto",
      "graph",
      "transactions"
    ],
    "best_for": "Learning financial services analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "network-analysis"
    ],
    "topic_tags": [
      "financial-services",
      "cryptocurrency",
      "blockchain"
    ],
    "summary": "The ORBITAAL Bitcoin Graph dataset encompasses 13 years of entity-level Bitcoin transaction networks, providing insights into the dynamics of Bitcoin transactions alongside BTC/USD values. Researchers can utilize this dataset to analyze transaction patterns, study market behaviors, and explore the relationships between entities in the Bitcoin ecosystem.",
    "use_cases": [
      "Analyzing the evolution of Bitcoin transaction networks over time.",
      "Studying the impact of external market factors on Bitcoin transactions.",
      "Exploring the relationships between different entities in the Bitcoin ecosystem.",
      "Investigating the correlation between transaction volume and BTC/USD values."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the transaction patterns in Bitcoin over the years?",
      "How do BTC/USD values correlate with transaction networks?",
      "What insights can be drawn from entity-level Bitcoin transactions?",
      "How has the Bitcoin ecosystem evolved from 2009 to 2021?",
      "What are the key entities involved in Bitcoin transactions?",
      "How can network analysis be applied to Bitcoin transaction data?",
      "What trends can be identified in Bitcoin transactions during market fluctuations?",
      "How does the structure of Bitcoin transactions inform economic behavior?"
    ],
    "domain_tags": [
      "fintech"
    ],
    "data_modality": "graph",
    "temporal_coverage": "2009-2021",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/logos/nature.png",
    "embedding_text": "The ORBITAAL Bitcoin Graph dataset provides a comprehensive view of Bitcoin transactions over a significant temporal span, from 2009 to 2021. It captures entity-level transaction networks, allowing researchers to analyze the intricate relationships and interactions among various entities involved in Bitcoin transactions. The dataset is structured in a graph format, where nodes represent entities (such as wallets or exchanges) and edges represent transactions between them, enriched with BTC/USD values to provide context on market fluctuations. The collection methodology likely involves aggregating transaction data from blockchain records, ensuring a rich dataset that reflects real-world interactions in the Bitcoin ecosystem. Key variables in this dataset include transaction amounts, timestamps, and the identities of the entities involved, which can be utilized to measure transaction volume, frequency, and the overall network structure. Researchers can address a variety of research questions, such as the evolution of transaction patterns, the identification of key players in the Bitcoin market, and the impact of external economic factors on transaction behavior. Common preprocessing steps may include cleaning the data to remove anomalies, normalizing transaction values, and constructing the graph structure for analysis. The dataset supports various types of analyses, including regression analysis to explore relationships between variables, machine learning techniques for predictive modeling, and descriptive statistics to summarize transaction behaviors. Researchers typically use this dataset to gain insights into the dynamics of Bitcoin transactions, study market trends, and understand the underlying network structures that drive the cryptocurrency market.",
    "tfidf_keywords": [
      "entity-level transactions",
      "Bitcoin transaction networks",
      "BTC/USD values",
      "network analysis",
      "cryptocurrency dynamics",
      "transaction patterns",
      "market behavior",
      "blockchain data",
      "economic analysis",
      "financial services",
      "data visualization",
      "temporal analysis",
      "graph theory",
      "data preprocessing",
      "market fluctuations"
    ],
    "semantic_cluster": "bitcoin-transaction-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "network-analysis",
      "cryptocurrency-economics",
      "financial-network-analysis",
      "market-dynamics",
      "transaction-cost-economics"
    ],
    "canonical_topics": [
      "finance",
      "consumer-behavior",
      "econometrics"
    ]
  },
  {
    "name": "Adform Display",
    "description": "Display advertising dataset with impressions and clicks",
    "category": "Advertising",
    "url": "https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/TADBY7",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "display ads",
      "impressions",
      "Harvard Dataverse"
    ],
    "best_for": "Learning advertising analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Adform Display dataset provides insights into display advertising through recorded impressions and clicks. Researchers can utilize this data to analyze advertising effectiveness, consumer engagement, and overall campaign performance.",
    "use_cases": [
      "Analyzing the effectiveness of display ads in driving consumer engagement.",
      "Evaluating the relationship between impressions and clicks to optimize ad campaigns.",
      "Conducting A/B testing on different ad formats using the dataset."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Adform Display dataset?",
      "How can I analyze display advertising effectiveness?",
      "What insights can be gained from impressions and clicks data?",
      "Where can I find datasets on display ads?",
      "What are the key metrics in display advertising?",
      "How to use Adform Display data for consumer behavior analysis?"
    ],
    "domain_tags": [
      "advertising"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "embedding_text": "The Adform Display dataset is a comprehensive collection of data related to display advertising, specifically focusing on impressions and clicks. This dataset typically consists of rows representing individual ad impressions, with columns capturing various attributes such as timestamp, ad ID, user ID, and click status. The data is structured to facilitate analysis of advertising performance metrics, allowing researchers to derive insights into consumer behavior and the effectiveness of different advertising strategies. The collection methodology for this dataset involves tracking user interactions with display ads across various platforms, ensuring a rich source of information for analysis. However, it is important to note that the dataset may have limitations in terms of data quality, such as potential inaccuracies in user tracking or incomplete records. Common preprocessing steps may include data cleaning to remove duplicates, handling missing values, and transforming variables for analysis. Researchers can leverage this dataset to address a variety of research questions, such as understanding the impact of ad impressions on click-through rates, identifying trends in consumer engagement over time, and evaluating the effectiveness of different ad formats. The dataset supports various types of analyses, including regression analysis, machine learning models, and descriptive statistics, making it a valuable resource for both academic and industry research in the advertising domain. Typically, researchers use this dataset to inform marketing strategies, optimize ad placements, and enhance overall campaign performance.",
    "tfidf_keywords": [
      "click-through-rate",
      "impressions",
      "display-advertising",
      "consumer-engagement",
      "ad-performance",
      "A/B-testing",
      "advertising-strategy",
      "user-interaction",
      "campaign-optimization",
      "data-collection-methodology"
    ],
    "semantic_cluster": "advertising-analytics",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "advertising-strategy",
      "consumer-behavior",
      "data-analytics",
      "marketing-optimization",
      "digital-marketing"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "advertising",
      "statistics"
    ]
  },
  {
    "name": "Tmall Reviews",
    "description": "Product reviews from Tmall (Alibaba's B2C platform)",
    "category": "E-Commerce",
    "url": "https://tianchi.aliyun.com/dataset/140281",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "reviews",
      "Tmall",
      "China",
      "B2C"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Tmall Reviews dataset consists of product reviews sourced from Tmall, Alibaba's B2C platform in China. This dataset can be utilized for sentiment analysis, consumer behavior studies, and market trend analysis.",
    "use_cases": [
      "Sentiment analysis of consumer reviews",
      "Market trend analysis based on product feedback",
      "Comparative analysis of product features based on reviews"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the common sentiments expressed in Tmall reviews?",
      "How do product ratings correlate with review length?",
      "What trends can be identified in consumer preferences on Tmall?",
      "How does pricing affect consumer reviews on Tmall?",
      "What are the most frequently mentioned product features in Tmall reviews?",
      "How do seasonal trends impact Tmall reviews?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "text",
    "geographic_scope": "China",
    "size_category": "medium",
    "model_score": 0.0,
    "embedding_text": "The Tmall Reviews dataset is a rich collection of product reviews obtained from Tmall, which is Alibaba's prominent B2C platform in China. This dataset is structured in a tabular format, where each row represents an individual review, and the columns typically include variables such as review ID, product ID, user ID, review text, star rating, and timestamp. The data is collected directly from the Tmall platform, capturing a wide range of consumer opinions and experiences related to various products available on the site. The dataset provides valuable insights into consumer behavior, allowing researchers and analysts to explore patterns in product satisfaction, identify key drivers of consumer sentiment, and assess the impact of product features on overall ratings. Key variables in this dataset include the star rating, which quantifies consumer satisfaction, and the review text, which offers qualitative insights into consumer experiences. The quality of the data is generally high, as it is sourced from actual consumer interactions, although it may contain biases based on the demographics of Tmall users and the products reviewed. Common preprocessing steps include text cleaning, sentiment scoring, and feature extraction from the review text. Researchers can utilize this dataset to address various research questions, such as how product features influence consumer satisfaction, the relationship between review length and star ratings, and the identification of trends in consumer preferences over time. The dataset supports a range of analyses, including regression analysis, machine learning models for sentiment prediction, and descriptive statistics to summarize consumer feedback. Overall, the Tmall Reviews dataset serves as a vital resource for understanding consumer behavior in the e-commerce sector, enabling deeper insights into market dynamics and consumer preferences.",
    "tfidf_keywords": [
      "sentiment-analysis",
      "consumer-behavior",
      "product-feedback",
      "market-trends",
      "review-length",
      "feature-extraction",
      "text-mining",
      "e-commerce",
      "Tmall",
      "Alibaba"
    ],
    "semantic_cluster": "e-commerce-consumer-insights",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "sentiment-analysis",
      "text-mining",
      "market-research",
      "consumer-insights",
      "product-analytics"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "marketplaces",
      "natural-language-processing"
    ]
  },
  {
    "name": "Airline Delay",
    "description": "Airline flight delays and carrier information",
    "category": "Transportation & Mobility",
    "url": "https://www.kaggle.com/datasets/sriharshaeedala/airline-delay",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "airlines",
      "delays",
      "transportation"
    ],
    "best_for": "Learning transportation & mobility analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "transportation",
      "mobility"
    ],
    "summary": "The Airline Delay dataset contains information on airline flight delays and the corresponding carrier details. It can be utilized to analyze patterns in flight delays, assess the performance of different airlines, and explore factors contributing to delays.",
    "use_cases": [
      "Analyzing the impact of weather on flight delays",
      "Comparing delay patterns across different airlines",
      "Investigating the relationship between flight routes and delays",
      "Studying the effects of time of day on airline performance"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the average flight delays by airline?",
      "How do weather conditions affect airline delays?",
      "What is the impact of time of day on flight delays?",
      "Which airlines have the highest on-time performance?",
      "How do delays vary by airport?",
      "What trends can be observed in airline delays over the years?"
    ],
    "domain_tags": [
      "transportation"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/airline-delay.jpg",
    "embedding_text": "The Airline Delay dataset is structured in a tabular format, typically comprising rows representing individual flights and columns detailing various attributes such as flight number, departure and arrival times, delay duration, and airline carrier. This dataset is crucial for researchers and analysts interested in understanding the dynamics of airline performance and the factors contributing to flight delays. Data collection methodologies often involve aggregating information from airline operational reports, government aviation databases, and real-time flight tracking systems. The dataset may cover a range of temporal aspects, including daily, monthly, or yearly flight data, though specific temporal coverage is not explicitly mentioned. Geographic scope is also not specified, but it likely encompasses multiple airports and regions depending on the data source. Key variables in the dataset include flight delays, which measure the time a flight is late compared to its scheduled arrival, and carrier information, which identifies the airline responsible for the flight. Data quality is generally high, but known limitations may include missing values, discrepancies in reporting standards among airlines, and variations in data collection practices. Common preprocessing steps include handling missing data, normalizing time formats, and filtering out irrelevant flights. Researchers can address various research questions using this dataset, such as analyzing the impact of external factors like weather or airport congestion on flight delays, comparing performance metrics across different airlines, and identifying trends in delay patterns over time. The dataset supports a range of analytical techniques, including regression analysis, machine learning models, and descriptive statistics. Typically, researchers leverage this dataset to inform policy decisions, improve operational efficiency, and enhance customer satisfaction within the airline industry.",
    "tfidf_keywords": [
      "flight delays",
      "airline performance",
      "operational reports",
      "delay duration",
      "flight tracking",
      "weather impact",
      "airport congestion",
      "performance metrics",
      "data preprocessing",
      "temporal analysis"
    ],
    "semantic_cluster": "airline-performance-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "transportation-analysis",
      "time-series-analysis",
      "predictive-modeling",
      "data-visualization",
      "statistical-analysis"
    ],
    "canonical_topics": [
      "statistics",
      "consumer-behavior",
      "forecasting"
    ]
  },
  {
    "name": "Flipkart Products",
    "description": "Product information scraped from Flipkart e-commerce platform",
    "category": "E-Commerce",
    "url": "https://www.kaggle.com/datasets/PromptCloudHQ/flipkart-products",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "products",
      "India",
      "e-commerce"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Flipkart Products dataset contains detailed product information scraped from the Flipkart e-commerce platform. It allows researchers and analysts to explore various aspects of consumer behavior, pricing strategies, and product offerings in the Indian e-commerce market.",
    "use_cases": [
      "Analyzing pricing strategies of products",
      "Studying consumer behavior in online shopping",
      "Comparing product features across different brands",
      "Evaluating the impact of promotions on sales"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What product categories are available on Flipkart?",
      "How do prices vary across different brands on Flipkart?",
      "What are the most popular products in India on Flipkart?",
      "How does consumer behavior differ across product types?",
      "What trends can be observed in e-commerce product offerings?",
      "How do discounts affect sales on Flipkart?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "India",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/flipkart-products.png",
    "embedding_text": "The Flipkart Products dataset is a comprehensive collection of product information sourced from the Flipkart e-commerce platform, one of India's largest online retail marketplaces. This dataset is structured in a tabular format, containing rows that represent individual products and columns that include various attributes such as product name, category, price, brand, specifications, and customer ratings. The data is collected through web scraping techniques, ensuring a wide coverage of products available on the platform at the time of scraping. The dataset primarily focuses on the Indian market, reflecting the diverse range of products offered to consumers in this region. Key variables in the dataset include product categories, pricing information, and customer feedback, which can be leveraged to measure market trends, consumer preferences, and pricing dynamics. Researchers utilizing this dataset can expect to encounter data quality challenges, such as missing values or inconsistencies in product descriptions, which may require preprocessing steps like data cleaning and normalization. Common research questions that can be addressed using this dataset include analyzing the relationship between product features and pricing, exploring consumer preferences across different categories, and assessing the impact of promotional strategies on sales performance. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a valuable resource for both academic and industry research in the field of e-commerce.",
    "tfidf_keywords": [
      "web-scraping",
      "product-categorization",
      "consumer-preferences",
      "price-optimization",
      "market-trends",
      "customer-feedback",
      "e-commerce-analytics",
      "data-cleaning",
      "feature-engineering",
      "sales-promotion"
    ],
    "semantic_cluster": "e-commerce-analytics",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "pricing-strategies",
      "market-analysis",
      "data-preprocessing",
      "product-analytics"
    ],
    "canonical_topics": [
      "marketplaces",
      "consumer-behavior",
      "pricing",
      "data-engineering"
    ]
  },
  {
    "name": "OTTO Session-based Recommendations",
    "description": "12M+ e-commerce sessions with click \u2192 cart \u2192 order sequences. Real multi-stage conversion funnel data from German retailer",
    "category": "E-Commerce",
    "url": "https://www.kaggle.com/datasets/otto/recsys-dataset",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "sessions",
      "conversion funnel",
      "Kaggle",
      "recommendations",
      "e-commerce"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "recommendations"
    ],
    "summary": "The OTTO Session-based Recommendations dataset consists of over 12 million e-commerce sessions, capturing the click, cart, and order sequences of users. This dataset provides valuable insights into the multi-stage conversion funnel of a German retailer, enabling researchers to analyze consumer behavior and improve recommendation systems.",
    "use_cases": [
      "Analyzing user behavior in e-commerce",
      "Improving recommendation algorithms",
      "Optimizing marketing strategies",
      "Studying conversion funnel efficiency"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the click-to-cart conversion rates in the OTTO dataset?",
      "How can session data improve recommendation algorithms?",
      "What patterns can be identified in the multi-stage conversion funnel?",
      "How does user behavior change across different sessions?",
      "What insights can be drawn from the order sequences in e-commerce?",
      "How can this dataset be used to optimize marketing strategies?",
      "What are the implications of session data on inventory management?",
      "How do different product categories perform in the conversion funnel?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Germany",
    "size_category": "massive",
    "model_score": 0.0,
    "image_url": "/images/datasets/otto-session-based-recommendations.png",
    "embedding_text": "The OTTO Session-based Recommendations dataset is a comprehensive collection of over 12 million e-commerce sessions, specifically designed to capture the intricate dynamics of user interactions within a multi-stage conversion funnel. This dataset includes detailed sequences of user actions, such as clicks, adding items to the cart, and completing orders, providing a rich source of information for researchers and practitioners in the field of e-commerce. The data is structured in a tabular format, with rows representing individual sessions and columns containing variables that detail user actions, timestamps, product identifiers, and session identifiers. The collection methodology involves tracking real user interactions on the OTTO platform, a prominent German retailer, ensuring that the dataset reflects authentic consumer behavior in a competitive online marketplace. While the dataset offers extensive coverage of user interactions, it is important to note that it is limited to the German market, which may affect the generalizability of findings to other regions. Key variables in the dataset include session ID, user ID, product ID, action type (click, cart, order), and timestamps, which together allow for a nuanced analysis of the customer journey. Researchers can leverage this dataset to address a variety of research questions, such as identifying factors that influence conversion rates, understanding the impact of product placement on user behavior, and developing predictive models for recommendation systems. Common preprocessing steps may include data cleaning to handle missing values, normalization of timestamps, and feature engineering to create new variables that capture user engagement metrics. The dataset supports various types of analyses, including regression analysis to identify relationships between variables, machine learning techniques for predictive modeling, and descriptive statistics to summarize user behavior patterns. Researchers typically use this dataset to enhance their understanding of consumer behavior, optimize marketing strategies, and improve the effectiveness of recommendation algorithms, making it a valuable resource for both academic research and practical applications in the e-commerce sector.",
    "tfidf_keywords": [
      "e-commerce",
      "session-based recommendations",
      "conversion funnel",
      "user behavior",
      "clickstream data",
      "recommendation systems",
      "consumer insights",
      "data preprocessing",
      "predictive modeling",
      "marketing optimization"
    ],
    "semantic_cluster": "e-commerce-analytics",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "consumer-behavior",
      "recommendation-systems",
      "marketing-analytics",
      "data-preprocessing",
      "predictive-modeling"
    ],
    "canonical_topics": [
      "recommendation-systems",
      "consumer-behavior",
      "machine-learning",
      "data-engineering"
    ],
    "benchmark_usage": [
      "Common uses include analyzing consumer behavior and improving recommendation systems."
    ]
  },
  {
    "name": "Inside Airbnb Raw Data",
    "description": "Raw data files from Inside Airbnb project",
    "category": "Data Portals",
    "url": "http://insideairbnb.com/get-the-data/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Airbnb",
      "raw data",
      "rentals"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Inside Airbnb Raw Data provides comprehensive datasets that detail the Airbnb rental market, including information on listings, reviews, and host characteristics. Researchers and analysts can utilize this data to explore trends in rental pricing, occupancy rates, and consumer behavior in the short-term rental market.",
    "use_cases": [
      "Analyzing pricing strategies of Airbnb listings",
      "Examining the impact of location on rental success",
      "Studying consumer behavior in short-term rentals"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the characteristics of Airbnb listings?",
      "How do rental prices vary across different neighborhoods?",
      "What factors influence occupancy rates in Airbnb rentals?",
      "How has the Airbnb market changed over time?",
      "What are the demographics of Airbnb hosts?",
      "How do reviews impact rental prices?"
    ],
    "domain_tags": [
      "retail",
      "hospitality"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/logos/insideairbnb.png",
    "embedding_text": "The Inside Airbnb Raw Data is a rich repository of information collected from the Airbnb platform, designed to facilitate research and analysis in the short-term rental market. The dataset typically includes various structured files containing rows and columns that represent listings, reviews, and host details. Each listing may have attributes such as price, location, number of bedrooms, and amenities, while reviews can provide insights into guest satisfaction and experiences. The collection methodology involves scraping data directly from the Airbnb website, ensuring that the information is up-to-date and reflective of current market conditions. However, researchers should be aware of potential data quality issues, such as missing values or inconsistencies in the data due to the nature of web scraping. Common preprocessing steps may include cleaning the data, handling missing values, and transforming categorical variables into numerical formats for analysis. This dataset supports a range of analyses, including regression models to predict pricing, machine learning techniques for clustering listings, and descriptive statistics to summarize key trends. Researchers typically use this data to answer questions related to market dynamics, pricing strategies, and consumer preferences, making it a valuable resource for those studying the intersection of technology and economics in the hospitality sector.",
    "tfidf_keywords": [
      "Airbnb",
      "short-term rentals",
      "pricing strategies",
      "occupancy rates",
      "consumer behavior",
      "data scraping",
      "market analysis",
      "host characteristics",
      "listing attributes",
      "review sentiment"
    ],
    "semantic_cluster": "marketplace-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "marketplaces",
      "consumer-behavior",
      "pricing",
      "data-scraping",
      "econometrics"
    ],
    "canonical_topics": [
      "marketplaces",
      "consumer-behavior",
      "pricing",
      "data-engineering",
      "econometrics"
    ]
  },
  {
    "name": "Microsoft Research",
    "description": "Research tools and datasets across multiple domains",
    "category": "Data Portals",
    "url": "https://www.microsoft.com/en-us/research/tools/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Microsoft",
      "research",
      "various domains"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Microsoft Research provides a comprehensive collection of research tools and datasets across various domains, enabling users to explore and analyze data effectively. It serves as a valuable resource for researchers and practitioners looking to leverage diverse datasets for their studies.",
    "use_cases": [],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What datasets are available from Microsoft Research?",
      "How can I access research tools from Microsoft?",
      "What domains does Microsoft Research cover?",
      "Are there datasets for machine learning in Microsoft Research?",
      "What types of research tools does Microsoft Research offer?",
      "How can I utilize Microsoft Research datasets for my project?"
    ],
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/microsoft-research.jpg",
    "embedding_text": "Microsoft Research is a pivotal hub for researchers and practitioners, offering a wide array of research tools and datasets that span multiple domains. The datasets available through Microsoft Research are structured in various formats, accommodating different types of analysis and research needs. Typically, these datasets are organized in a tabular format, featuring rows and columns that represent different variables pertinent to the research questions at hand. The collection methodology employed by Microsoft Research involves rigorous data gathering techniques, ensuring that the datasets are both reliable and relevant. Researchers can expect to find data sourced from a variety of fields, reflecting the interdisciplinary nature of the research conducted at Microsoft. While specific temporal and geographic coverage details are not explicitly mentioned, the datasets are designed to be applicable across a broad spectrum of research inquiries. Key variables within the datasets often measure critical aspects of the research topics, providing insights that can drive innovative solutions and advancements in technology. However, users should be aware of potential limitations in data quality, which may arise from the diverse sources and methodologies employed in data collection. Common preprocessing steps may include data cleaning, normalization, and transformation to prepare the datasets for analysis. Researchers typically leverage these datasets to address a range of research questions, employing various analytical techniques such as regression analysis, machine learning, and descriptive statistics. The versatility of the datasets allows for extensive exploration and analysis, making Microsoft Research an invaluable resource for those engaged in data-driven research. Overall, Microsoft Research stands out as a significant contributor to the academic and professional landscape, empowering users to harness the power of data in their respective fields.",
    "data_modality": "mixed",
    "tfidf_keywords": [
      "research-tools",
      "datasets",
      "data-collection",
      "data-quality",
      "data-analysis",
      "interdisciplinary-research",
      "data-sources",
      "data-preprocessing",
      "machine-learning",
      "regression-analysis"
    ],
    "semantic_cluster": "research-data-resources",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "data-science",
      "machine-learning",
      "data-analysis",
      "research-methodology",
      "data-visualization"
    ],
    "canonical_topics": [
      "machine-learning",
      "data-engineering",
      "statistics",
      "consumer-behavior",
      "experimentation"
    ],
    "domain_tags": [
      "technology",
      "research"
    ]
  },
  {
    "name": "Hugging Face Datasets",
    "description": "ML/NLP datasets hub with 100K+ datasets. Easy loading via Python library. Community-driven repository",
    "category": "Data Portals",
    "url": "https://huggingface.co/datasets",
    "docs_url": "https://huggingface.co/docs/datasets",
    "github_url": null,
    "tags": [
      "ML",
      "NLP",
      "datasets",
      "Hugging Face",
      "community"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "ML",
      "NLP",
      "datasets"
    ],
    "summary": "Hugging Face Datasets is a comprehensive hub for machine learning and natural language processing datasets, featuring over 100,000 datasets. It allows users to easily load datasets via a Python library, making it accessible for various ML and NLP tasks.",
    "use_cases": [
      "Training machine learning models for NLP tasks",
      "Exploring various datasets for research purposes",
      "Comparing performance across different datasets",
      "Community contributions to dataset curation"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the available datasets on Hugging Face?",
      "How can I load datasets from Hugging Face using Python?",
      "What types of ML/NLP datasets can I find on Hugging Face?",
      "Is there a community-driven aspect to Hugging Face Datasets?",
      "How many datasets are available on Hugging Face?",
      "What is the purpose of Hugging Face Datasets?"
    ],
    "domain_tags": [
      "technology"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/hugging-face-datasets.png",
    "embedding_text": "Hugging Face Datasets serves as a pivotal resource in the machine learning and natural language processing communities, offering a vast repository of over 100,000 datasets. The datasets encompass a wide array of structures, including text, images, and more, facilitating diverse applications in research and development. The collection methodology is community-driven, allowing users to contribute and curate datasets, thus enriching the repository continuously. Each dataset typically contains various rows and columns, representing different variables pertinent to ML and NLP tasks. While the specific schema may vary across datasets, users can expect to find key variables that measure aspects such as text length, sentiment, and other relevant features. Data quality can vary, as the datasets are contributed by the community, which may introduce limitations in terms of consistency and completeness. Common preprocessing steps often include tokenization, normalization, and data cleaning to prepare the datasets for analysis. Researchers utilize Hugging Face Datasets to address a multitude of research questions, from sentiment analysis to language translation and beyond. The platform supports various types of analyses, including regression, machine learning model training, and descriptive statistics. Overall, Hugging Face Datasets is an invaluable tool for both novice and experienced researchers looking to leverage existing datasets for their machine learning and NLP projects.",
    "tfidf_keywords": [
      "machine-learning",
      "natural-language-processing",
      "datasets",
      "community-driven",
      "Python-library",
      "data-curation",
      "text-analysis",
      "dataset-repository",
      "ML-tasks",
      "data-loading",
      "research-resources",
      "data-quality",
      "preprocessing",
      "data-collection",
      "dataset-exploration"
    ],
    "semantic_cluster": "nlp-dataset-repository",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "machine-learning",
      "natural-language-processing",
      "data-curation",
      "dataset-exploration",
      "community-contributions"
    ],
    "canonical_topics": [
      "machine-learning",
      "natural-language-processing",
      "data-engineering"
    ]
  },
  {
    "name": "Hugging Face Datasets",
    "description": "659,000+ datasets across text, image, audio, and tabular with one-line data loaders",
    "category": "Dataset Aggregators",
    "url": "https://huggingface.co/datasets",
    "docs_url": "https://huggingface.co/docs/datasets",
    "github_url": "https://github.com/huggingface/datasets",
    "tags": [
      "NLP",
      "AI",
      "ML",
      "transformers",
      "streaming"
    ],
    "best_for": "Modern AI/NLP research with seamless ML framework integration",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "NLP",
      "AI",
      "ML",
      "transformers",
      "streaming"
    ],
    "summary": "The Hugging Face Datasets collection offers over 659,000 datasets spanning various modalities including text, image, audio, and tabular formats. Researchers and developers can leverage these datasets for training machine learning models, conducting experiments, and enhancing natural language processing applications.",
    "use_cases": [
      "Training machine learning models for text classification",
      "Conducting experiments in natural language processing",
      "Evaluating performance of AI models on diverse datasets",
      "Exploring datasets for image recognition tasks"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What datasets are available for NLP tasks?",
      "How can I access image datasets on Hugging Face?",
      "Are there audio datasets for machine learning?",
      "What are the best datasets for training transformers?",
      "How do I load datasets from Hugging Face?",
      "What types of datasets are included in the Hugging Face collection?",
      "Can I find tabular datasets for data analysis?",
      "What are the most popular datasets on Hugging Face?"
    ],
    "domain_tags": [
      "AI",
      "NLP"
    ],
    "data_modality": "mixed",
    "size_category": "massive",
    "model_score": 0.0,
    "image_url": "/images/datasets/hugging-face-datasets.png",
    "embedding_text": "The Hugging Face Datasets repository is a comprehensive collection of over 659,000 datasets that cater to a wide range of applications in artificial intelligence and machine learning. This extensive dataset library encompasses various data structures and schemas, including text, image, audio, and tabular formats, making it a versatile resource for researchers and practitioners alike. Each dataset within this collection is designed to facilitate seamless integration with popular machine learning frameworks, providing one-line data loaders that simplify the process of accessing and utilizing the data. The datasets are sourced from a variety of contributors, ensuring a rich diversity in the types of data available. Users can expect to find datasets that cover numerous domains, enabling them to address a multitude of research questions and analytical scenarios. Key variables within the datasets measure different attributes relevant to their respective domains, and the quality of the data is generally high, although users should be aware of potential limitations such as missing values or biases inherent in the data collection processes. Common preprocessing steps may include data cleaning, normalization, and transformation to prepare the datasets for analysis. Researchers typically use these datasets to conduct a wide range of analyses, including regression, machine learning model training, and descriptive statistics. The flexibility and breadth of the Hugging Face Datasets collection make it an invaluable tool for anyone looking to advance their work in AI and machine learning.",
    "tfidf_keywords": [
      "dataset-collection",
      "data-loaders",
      "machine-learning",
      "natural-language-processing",
      "image-datasets",
      "audio-datasets",
      "tabular-data",
      "transformers",
      "data-sourcing",
      "data-diversity",
      "AI-research",
      "data-preprocessing",
      "model-training",
      "data-analysis",
      "data-quality"
    ],
    "semantic_cluster": "nlp-dataset-collection",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "machine-learning",
      "natural-language-processing",
      "data-engineering",
      "computer-vision",
      "audio-processing"
    ],
    "canonical_topics": [
      "machine-learning",
      "natural-language-processing",
      "computer-vision",
      "data-engineering"
    ]
  },
  {
    "name": "Alibaba Personalized Re-Ranking",
    "description": "Mobile shopping user click data on recommended items",
    "category": "E-Commerce",
    "url": "http://yongfeng.me/dataset/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "re-ranking",
      "personalization",
      "recommendations"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "recommendations"
    ],
    "summary": "The Alibaba Personalized Re-Ranking dataset contains mobile shopping user click data on recommended items, allowing researchers to analyze user behavior and improve recommendation systems. It can be utilized to develop personalized marketing strategies and enhance user engagement through tailored product suggestions.",
    "use_cases": [
      "Analyzing user click patterns to enhance recommendation algorithms.",
      "Developing personalized marketing strategies based on user behavior.",
      "Evaluating the effectiveness of different re-ranking techniques.",
      "Studying the impact of personalized recommendations on user engagement."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Alibaba Personalized Re-Ranking dataset?",
      "How can I access mobile shopping user click data?",
      "What insights can be derived from user click data on recommended items?",
      "How does personalization affect user engagement in e-commerce?",
      "What are the key variables in the Alibaba dataset?",
      "How can this dataset improve recommendation algorithms?",
      "What analysis can be performed on mobile shopping user data?",
      "What are the applications of re-ranking in e-commerce?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "embedding_text": "The Alibaba Personalized Re-Ranking dataset is a valuable resource for researchers and practitioners in the field of e-commerce, specifically focusing on mobile shopping user click data related to recommended items. This dataset is structured in a tabular format, containing rows that represent individual user interactions with recommended products and columns that capture various attributes of these interactions, such as user ID, item ID, click timestamp, and potentially other contextual features. The data collection methodology involves aggregating user click data from Alibaba's mobile shopping platform, ensuring a rich dataset that reflects real-world user behavior in an online shopping environment. The dataset's coverage is primarily focused on user interactions, and while it does not explicitly mention temporal or geographic coverage, it provides insights into user behavior patterns that can be generalized across various demographics. Key variables in the dataset include user ID, item ID, and click timestamps, which measure user engagement with recommended items. However, known limitations may include potential biases in user behavior, such as the influence of marketing campaigns or seasonal trends, which could affect the generalizability of findings derived from the dataset. Common preprocessing steps may involve cleaning the data to handle missing values, normalizing click timestamps, and encoding categorical variables for analysis. Researchers can utilize this dataset to address various research questions, such as understanding the factors that influence user clicks on recommended items, evaluating the effectiveness of different personalization strategies, and exploring the relationship between user engagement and sales outcomes. The dataset supports a range of analyses, including regression modeling, machine learning techniques for predictive analytics, and descriptive statistics to summarize user behavior. Typically, researchers leverage this dataset in studies aimed at enhancing recommendation systems, improving user experience in e-commerce, and developing targeted marketing strategies that align with user preferences.",
    "tfidf_keywords": [
      "personalization",
      "re-ranking",
      "user engagement",
      "recommendation systems",
      "e-commerce",
      "click data",
      "mobile shopping",
      "user behavior",
      "data analysis",
      "machine learning",
      "consumer preferences",
      "marketing strategies",
      "data preprocessing",
      "predictive analytics",
      "user interactions"
    ],
    "semantic_cluster": "recommendation-systems",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "machine-learning",
      "consumer-behavior",
      "recommendation-systems",
      "data-analysis",
      "e-commerce"
    ],
    "canonical_topics": [
      "recommendation-systems",
      "consumer-behavior",
      "machine-learning",
      "data-engineering"
    ]
  },
  {
    "name": "German Used Cars",
    "description": "Used car listings or sales in Germany",
    "category": "Automotive",
    "url": "https://www.kaggle.com/datasets/gogotchuri/myautogecardetails",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "used cars",
      "Germany",
      "listings"
    ],
    "best_for": "Learning automotive analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The German Used Cars dataset contains listings or sales data for used cars in Germany. It can be utilized to analyze market trends, consumer preferences, and pricing strategies in the automotive sector.",
    "use_cases": [
      "Analyzing pricing trends of used cars",
      "Studying consumer preferences in the automotive market",
      "Evaluating the impact of car age on resale value"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the current used car listings in Germany?",
      "How do prices of used cars vary by make and model?",
      "What are the most popular used car brands in Germany?",
      "How does the age of a car affect its resale value?",
      "What trends can be observed in used car sales over the past year?",
      "How do geographic locations influence used car prices in Germany?"
    ],
    "domain_tags": [
      "automotive"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Germany",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/german-used-cars.jpg",
    "embedding_text": "The German Used Cars dataset is a comprehensive collection of used car listings and sales data specifically from Germany. This dataset is structured in a tabular format, consisting of rows that represent individual car listings or sales transactions, and columns that capture key variables such as make, model, year, price, mileage, and location. The data is likely collected from various online platforms and dealerships that list used cars for sale, providing a rich source for analysis. Researchers and analysts can leverage this dataset to explore a variety of research questions, such as the impact of car age on resale value, the correlation between mileage and pricing, and the popularity of different car brands within the German market. Common preprocessing steps may include handling missing values, normalizing price data, and encoding categorical variables for analysis. The dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it a valuable resource for those studying consumer behavior and market dynamics in the automotive sector. However, users should be aware of potential limitations regarding data quality, such as inconsistencies in listing details and the possibility of outdated information. Overall, this dataset serves as a crucial tool for understanding the used car market in Germany and can inform both academic research and practical applications in the automotive industry.",
    "tfidf_keywords": [
      "used cars",
      "Germany",
      "market trends",
      "consumer preferences",
      "pricing strategies",
      "automotive sector",
      "resale value",
      "car age",
      "mileage",
      "listing details"
    ],
    "semantic_cluster": "automotive-market-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "pricing",
      "market-analysis",
      "data-collection",
      "automotive-industry"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "pricing",
      "marketplaces"
    ]
  },
  {
    "name": "Gamified Learning",
    "description": "Experiments on gamification in learning environments",
    "category": "Education",
    "url": "https://data.mendeley.com/datasets/7kgpn39m8w/1",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "gamification",
      "education",
      "experiments"
    ],
    "best_for": "Learning education analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "education",
      "gamification",
      "learning"
    ],
    "summary": "The Gamified Learning dataset consists of experiments conducted to explore the effects of gamification within educational settings. Researchers can utilize this dataset to analyze how gamification influences learning outcomes, engagement, and motivation among students.",
    "use_cases": [
      "Analyzing the impact of gamification on student performance.",
      "Evaluating engagement levels in gamified versus traditional learning environments.",
      "Investigating the relationship between gamification and motivation in education."
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the effects of gamification on learning outcomes?",
      "How does gamification influence student engagement?",
      "What experiments have been conducted on gamification in education?",
      "What data is available on gamified learning environments?",
      "How can gamification improve educational experiences?",
      "What metrics are used to measure gamification effectiveness in learning?"
    ],
    "domain_tags": [
      "education"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0,
    "embedding_text": "The Gamified Learning dataset is a collection of experiments designed to investigate the role of gamification in educational contexts. This dataset is structured to include various rows and columns that capture different variables related to gamification techniques and their impacts on learning. The collection methodology involves conducting controlled experiments in educational settings, where participants engage with gamified learning materials. Key variables in this dataset may include measures of student engagement, performance metrics, and feedback on the gamified elements. Researchers often face challenges regarding data quality, as the subjective nature of engagement and motivation can lead to variability in responses. Common preprocessing steps may involve normalizing performance scores and categorizing qualitative feedback. This dataset supports a range of analyses, including regression analysis to determine the effectiveness of gamification strategies, as well as machine learning techniques for predictive modeling of student outcomes. Researchers typically use this dataset to address questions related to the efficacy of gamification in enhancing educational experiences and outcomes, making it a valuable resource for those exploring innovative teaching methodologies.",
    "tfidf_keywords": [
      "gamification",
      "learning outcomes",
      "student engagement",
      "educational experiments",
      "motivation",
      "performance metrics",
      "feedback analysis",
      "controlled experiments",
      "qualitative data",
      "quantitative analysis"
    ],
    "semantic_cluster": "gamification-in-education",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "educational-psychology",
      "learning-theories",
      "behavioral-economics",
      "student-motivation",
      "experimental-design"
    ],
    "canonical_topics": [
      "experimentation",
      "consumer-behavior",
      "education"
    ]
  },
  {
    "name": "KDD Cup",
    "description": "ACM SIGKDD annual data mining competition",
    "category": "Data Portals",
    "url": "https://kdd.org/kdd-cup",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "KDD",
      "data mining",
      "ACM"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "data mining",
      "machine learning"
    ],
    "topic_tags": [
      "data mining",
      "competition",
      "machine learning"
    ],
    "summary": "The KDD Cup is an annual data mining competition organized by ACM SIGKDD, featuring diverse datasets that challenge participants to develop innovative solutions using data analysis and machine learning techniques. Participants can leverage the datasets to explore various data mining methodologies and improve their predictive modeling skills.",
    "use_cases": [
      "Predictive modeling",
      "Data classification",
      "Anomaly detection",
      "Clustering analysis"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What datasets are available in the KDD Cup?",
      "How can I participate in the KDD Cup competition?",
      "What are the challenges in the KDD Cup?",
      "What techniques are commonly used in KDD Cup analyses?",
      "Where can I find past KDD Cup datasets?",
      "What are the evaluation metrics used in KDD Cup competitions?",
      "How do I prepare for the KDD Cup?",
      "What are the winning strategies for KDD Cup?"
    ],
    "domain_tags": [
      "technology",
      "education"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/logos/kdd.png",
    "embedding_text": "The KDD Cup is a prestigious annual data mining competition organized by the Association for Computing Machinery's Special Interest Group on Knowledge Discovery and Data Mining (ACM SIGKDD). It serves as a platform for researchers and practitioners to showcase their skills in data analysis and machine learning. The competition typically features a variety of datasets that are structured in a tabular format, consisting of rows and columns representing different variables and observations. Participants are tasked with developing innovative algorithms to solve specific problems presented in the competition. The datasets are often derived from real-world scenarios, encompassing various domains such as e-commerce, healthcare, and social networks, providing a rich source of information for analysis. Data collection methodologies vary, but they generally involve aggregating data from multiple sources, ensuring a diverse representation of the underlying phenomena. Key variables within the datasets may include features such as user behavior metrics, transaction records, and demographic information, which are essential for building predictive models. However, participants should be aware of potential data quality issues, such as missing values or noise, which may require preprocessing steps like normalization, imputation, or feature selection to enhance model performance. The KDD Cup datasets allow researchers to address a wide range of research questions, including but not limited to predicting user behavior, identifying trends, and detecting anomalies. Common types of analyses supported by the datasets include regression analysis, classification tasks, and clustering techniques, making them suitable for various machine learning applications. Researchers typically utilize these datasets to benchmark their algorithms against established methods, contributing to the advancement of data mining practices and fostering innovation within the field.",
    "tfidf_keywords": [
      "data mining",
      "machine learning",
      "predictive modeling",
      "classification",
      "anomaly detection",
      "KDD Cup",
      "benchmarking",
      "feature selection",
      "data preprocessing",
      "algorithm evaluation"
    ],
    "semantic_cluster": "data-mining-competitions",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "machine-learning",
      "predictive-analytics",
      "data-preprocessing",
      "algorithm-evaluation",
      "feature-engineering"
    ],
    "canonical_topics": [
      "machine-learning",
      "experimentation",
      "consumer-behavior"
    ],
    "benchmark_usage": [
      "Commonly used for benchmarking machine learning algorithms"
    ]
  },
  {
    "name": "RecSys Challenge 2024 (EB-NeRD)",
    "description": "2.3M users, 380M+ news impressions from Ekstra Bladet for news recommendation research",
    "category": "Data Portals",
    "url": "https://www.recsyschallenge.com/2024/",
    "docs_url": null,
    "github_url": "https://github.com/recsyspolimi/recsys-challenge-2024-ekstrabladet",
    "tags": [
      "RecSys",
      "news",
      "large-scale",
      "real-world",
      "2024",
      "impressions"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "news",
      "recommendation-systems",
      "large-scale"
    ],
    "summary": "The RecSys Challenge 2024 (EB-NeRD) dataset consists of 2.3 million users and over 380 million news impressions collected from Ekstra Bladet, aimed at facilitating research in news recommendation systems. Researchers can utilize this dataset to develop and test algorithms for personalized news recommendations, analyze user behavior, and evaluate the effectiveness of various recommendation strategies.",
    "use_cases": [
      "Developing personalized news recommendation algorithms",
      "Analyzing user engagement with news content",
      "Evaluating the performance of recommendation strategies",
      "Studying the impact of news recommendations on user behavior"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What is the RecSys Challenge 2024 dataset?",
      "How can I use the EB-NeRD dataset for news recommendation research?",
      "What are the key features of the RecSys Challenge 2024 dataset?",
      "How many users and impressions are included in the EB-NeRD dataset?",
      "What types of analyses can be performed with the RecSys Challenge 2024 data?",
      "What are the potential applications of the EB-NeRD dataset in machine learning?",
      "How does the dataset support large-scale recommendation systems?",
      "What insights can be gained from analyzing news impressions in the EB-NeRD dataset?"
    ],
    "domain_tags": [
      "media",
      "technology"
    ],
    "data_modality": "tabular",
    "size_category": "massive",
    "model_score": 0.0,
    "image_url": "/images/logos/recsyschallenge.png",
    "embedding_text": "The RecSys Challenge 2024 (EB-NeRD) dataset provides a rich resource for researchers interested in news recommendation systems. It comprises a substantial collection of data, featuring 2.3 million users and over 380 million news impressions sourced from Ekstra Bladet, a prominent Danish news outlet. The dataset is structured in a tabular format, where each row represents a unique news impression associated with a user. Key variables may include user IDs, news article IDs, timestamps of impressions, and possibly engagement metrics such as clicks or time spent on articles. The collection methodology involves tracking user interactions with news articles, ensuring a comprehensive representation of user behavior in a real-world setting. This dataset is particularly valuable for addressing research questions related to user preferences in news consumption, the effectiveness of various recommendation algorithms, and the dynamics of user engagement over time. Researchers can perform a variety of analyses, including regression modeling, machine learning applications, and descriptive statistics to uncover patterns in user behavior and content interaction. Common preprocessing steps may involve data cleaning, normalization, and feature engineering to prepare the data for analysis. However, researchers should be aware of potential limitations, such as biases in user representation or the temporal relevance of the impressions. Overall, the EB-NeRD dataset serves as a foundational tool for advancing the field of recommendation systems, offering insights into user behavior and the effectiveness of news delivery mechanisms.",
    "tfidf_keywords": [
      "news-recommendation",
      "user-engagement",
      "personalization",
      "machine-learning",
      "data-collection",
      "impression-analysis",
      "algorithm-evaluation",
      "behavioral-insights",
      "large-scale-data",
      "real-world-application"
    ],
    "semantic_cluster": "news-recommendation-systems",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "user-behavior",
      "recommendation-algorithms",
      "data-mining",
      "machine-learning",
      "user-engagement"
    ],
    "canonical_topics": [
      "recommendation-systems",
      "machine-learning",
      "consumer-behavior"
    ]
  },
  {
    "name": "NGSIM Vehicle Trajectories",
    "description": "Vehicle trajectory data for traffic flow modeling",
    "category": "Transportation & Mobility",
    "url": "https://data.transportation.gov/Automobiles/Next-Generation-Simulation-NGSIM-Vehicle-Trajector/8ect-6jqj/about_data",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "trajectories",
      "traffic",
      "simulation"
    ],
    "best_for": "Learning transportation & mobility analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The NGSIM Vehicle Trajectories dataset provides detailed vehicle trajectory data essential for traffic flow modeling. Researchers can utilize this dataset to analyze traffic patterns, simulate vehicle interactions, and improve traffic management systems.",
    "use_cases": [
      "Analyzing traffic flow patterns in urban areas",
      "Simulating vehicle interactions for traffic management",
      "Developing algorithms for autonomous vehicle navigation"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the NGSIM Vehicle Trajectories dataset?",
      "How can I use vehicle trajectory data for traffic flow modeling?",
      "What types of analyses can be performed with NGSIM data?",
      "Where can I find vehicle trajectory datasets?",
      "What are the key variables in the NGSIM Vehicle Trajectories dataset?",
      "How does vehicle trajectory data impact traffic simulation studies?"
    ],
    "domain_tags": [
      "transportation",
      "mobility"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/logos/transportation.png",
    "embedding_text": "The NGSIM Vehicle Trajectories dataset is a comprehensive collection of vehicle trajectory data designed for traffic flow modeling and analysis. This dataset captures the movement of vehicles in various traffic scenarios, providing researchers with the necessary information to study and improve traffic systems. The data is structured in a tabular format, consisting of rows representing individual vehicle trajectories and columns detailing key variables such as time, position, speed, and acceleration. Each entry in the dataset corresponds to a specific vehicle's movement over time, allowing for detailed analyses of traffic behavior and interactions. \n\nThe collection methodology for the NGSIM dataset involves the use of advanced tracking technologies to capture vehicle movements in real-time. This data is sourced from various traffic scenarios, ensuring a diverse representation of driving behaviors and conditions. The dataset is particularly valuable for its high temporal and spatial resolution, which enables researchers to conduct in-depth analyses of traffic dynamics. \n\nKey variables within the dataset include vehicle ID, timestamp, position coordinates (X, Y), speed, and acceleration. These variables are crucial for measuring vehicle performance and understanding traffic flow characteristics. However, users should be aware of potential limitations in data quality, such as missing values or inaccuracies in vehicle positioning, which may arise from the tracking technology used. \n\nCommon preprocessing steps for utilizing this dataset include cleaning the data to handle missing values, normalizing speed and acceleration measurements, and segmenting trajectories for specific analyses. Researchers typically employ this dataset to address various research questions, such as the impact of vehicle interactions on traffic flow, the effectiveness of traffic management strategies, and the development of predictive models for traffic behavior. \n\nThe NGSIM Vehicle Trajectories dataset supports a range of analyses, including regression analyses to identify relationships between variables, machine learning techniques for predictive modeling, and descriptive statistics to summarize traffic patterns. Researchers often leverage this dataset in studies focused on traffic simulation, autonomous vehicle navigation, and urban planning, making it a vital resource for those interested in transportation and mobility research.",
    "tfidf_keywords": [
      "vehicle trajectories",
      "traffic flow modeling",
      "simulation",
      "urban traffic",
      "vehicle interactions",
      "data collection methodology",
      "traffic dynamics",
      "predictive modeling",
      "real-time tracking",
      "traffic management"
    ],
    "semantic_cluster": "traffic-flow-modeling",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "traffic simulation",
      "autonomous vehicles",
      "urban planning",
      "data analysis",
      "predictive analytics"
    ],
    "canonical_topics": [
      "machine-learning",
      "statistics",
      "transportation",
      "data-engineering"
    ]
  },
  {
    "name": "ASSISTments Dataset",
    "description": "Data from online tutoring platform for educational data mining",
    "category": "Education",
    "url": "https://sites.google.com/site/las2016data/home",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "education",
      "tutoring",
      "learning analytics"
    ],
    "best_for": "Learning education analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The ASSISTments Dataset comprises data collected from an online tutoring platform designed for educational data mining. Researchers can utilize this dataset to analyze learning behaviors, assess tutoring effectiveness, and develop predictive models for student performance.",
    "use_cases": [
      "Analyzing student learning patterns",
      "Evaluating tutoring effectiveness",
      "Developing predictive models for student performance"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What is the ASSISTments Dataset?",
      "How can I access the ASSISTments Dataset?",
      "What types of analyses can be performed with the ASSISTments Dataset?",
      "What variables are included in the ASSISTments Dataset?",
      "How does the ASSISTments Dataset support educational research?",
      "What insights can be gained from the ASSISTments Dataset?"
    ],
    "domain_tags": [
      "education"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "embedding_text": "The ASSISTments Dataset is a rich source of educational data collected from an online tutoring platform that focuses on enhancing learning through personalized feedback and assessments. The dataset is structured in a tabular format, consisting of rows representing individual student interactions and columns detailing various attributes such as student ID, problem ID, time spent on each problem, correctness of answers, and feedback provided. This structure allows researchers to perform a variety of analyses aimed at understanding student behavior and learning outcomes. The data collection methodology involves tracking student interactions on the platform, capturing detailed logs of their activities, which provides a comprehensive view of their learning journey. Key variables include the number of attempts per problem, the time taken to solve each problem, and the correctness of responses, which are crucial for measuring student engagement and learning efficacy. However, researchers should be aware of potential limitations in data quality, such as missing values or biases in student engagement levels. Common preprocessing steps may include handling missing data, normalizing time spent on tasks, and encoding categorical variables for analysis. The dataset can address various research questions, such as identifying factors that influence student success, evaluating the impact of different tutoring strategies, and exploring the relationship between time spent on tasks and performance outcomes. It supports various types of analyses, including regression analysis, machine learning modeling, and descriptive statistics. Researchers typically use this dataset to inform educational practices, improve tutoring systems, and contribute to the field of educational data mining by providing insights into effective learning strategies.",
    "tfidf_keywords": [
      "educational-data-mining",
      "student-engagement",
      "tutoring-effectiveness",
      "predictive-modeling",
      "learning-analytics",
      "personalized-feedback",
      "data-collection-methodology",
      "interaction-logs",
      "performance-outcomes",
      "analysis-scenarios"
    ],
    "semantic_cluster": "educational-data-mining",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "learning-analytics",
      "student-performance",
      "educational-research",
      "tutoring-systems",
      "data-collection"
    ],
    "canonical_topics": [
      "machine-learning",
      "education",
      "statistics"
    ]
  },
  {
    "name": "Amazon Fraud Detection Benchmark",
    "description": "9 consolidated fraud datasets with unified format. Includes IEEE-CIS, credit card, e-commerce fraud. Benchmark for fraud ML research",
    "category": "Financial Services",
    "url": "https://github.com/amazon-science/fraud-dataset-benchmark",
    "docs_url": null,
    "github_url": "https://github.com/amazon-science/fraud-dataset-benchmark",
    "tags": [
      "fraud detection",
      "benchmark",
      "Amazon",
      "ML",
      "fintech"
    ],
    "best_for": "Learning financial services analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "machine-learning",
      "data-cleaning"
    ],
    "topic_tags": [
      "e-commerce",
      "fraud detection",
      "financial services"
    ],
    "summary": "The Amazon Fraud Detection Benchmark consists of nine consolidated fraud datasets formatted uniformly, including sources like IEEE-CIS and credit card fraud datasets. This benchmark serves as a valuable resource for researchers in the field of fraud detection and machine learning, enabling them to develop and evaluate algorithms aimed at identifying fraudulent activities in various domains.",
    "use_cases": [
      "Developing machine learning models for fraud detection",
      "Benchmarking algorithm performance against established datasets",
      "Conducting comparative studies on fraud detection techniques"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the Amazon Fraud Detection Benchmark?",
      "How can I access the fraud datasets?",
      "What types of fraud does this dataset cover?",
      "What machine learning techniques can be applied to this benchmark?",
      "How does the Amazon Fraud Detection Benchmark support research?",
      "What are the key features of the fraud datasets included?"
    ],
    "domain_tags": [
      "fintech"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "benchmark_usage": [
      "Benchmark for fraud ML research"
    ],
    "model_score": 0.0,
    "image_url": "/images/datasets/amazon-fraud-detection-benchmark.png",
    "embedding_text": "The Amazon Fraud Detection Benchmark is a comprehensive collection of nine distinct fraud datasets that have been consolidated into a unified format, making it easier for researchers and practitioners to utilize them in their studies. The datasets encompass a variety of fraud types, including those related to credit card transactions and e-commerce platforms, and are derived from reputable sources such as the IEEE-CIS dataset. Each dataset within the benchmark is structured in a tabular format, featuring rows representing individual transactions and columns detailing various attributes associated with those transactions. Key variables may include transaction amount, time of transaction, user behavior metrics, and flags indicating whether a transaction is fraudulent or legitimate. This rich structure allows for a wide range of analyses, including regression modeling, machine learning classification tasks, and descriptive statistics. The datasets are designed to support the development and evaluation of advanced fraud detection algorithms, providing a robust platform for benchmarking different approaches in the field. Researchers typically engage with this benchmark to address critical questions surrounding the effectiveness of various fraud detection methodologies, the characteristics of fraudulent transactions, and the overall performance of machine learning models in identifying fraud. However, users should be aware of potential data quality issues, such as missing values or imbalanced classes, which may necessitate preprocessing steps like data imputation or resampling. Overall, the Amazon Fraud Detection Benchmark serves as an essential resource for advancing the study of fraud detection in financial services and beyond.",
    "tfidf_keywords": [
      "fraud detection",
      "machine learning",
      "benchmarking",
      "e-commerce",
      "credit card fraud",
      "data preprocessing",
      "algorithm evaluation",
      "transaction analysis",
      "data quality",
      "classification tasks"
    ],
    "semantic_cluster": "fraud-detection-benchmarking",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "machine-learning",
      "data-cleaning",
      "algorithm-evaluation",
      "classification",
      "anomaly-detection"
    ],
    "canonical_topics": [
      "machine-learning",
      "finance",
      "data-engineering"
    ]
  },
  {
    "name": "Grab Driving GPS Traces",
    "description": "GPS trace data from Grab ride-hailing platform",
    "category": "Transportation & Mobility",
    "url": "https://engineering.grab.com/grab-posisi",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "GPS",
      "ride-hailing",
      "Southeast Asia"
    ],
    "best_for": "Learning transportation & mobility analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "transportation",
      "mobility",
      "data-analysis"
    ],
    "summary": "The Grab Driving GPS Traces dataset contains GPS trace data from the Grab ride-hailing platform, primarily used for analyzing transportation patterns and mobility trends in Southeast Asia. Researchers can utilize this data to explore ride-hailing behaviors, optimize routes, and study urban mobility dynamics.",
    "use_cases": [
      "Analyzing ride patterns in urban areas",
      "Optimizing routes for ride-hailing services",
      "Studying the impact of ride-hailing on traffic congestion",
      "Exploring user behavior in transportation choices"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the GPS traces from Grab?",
      "How can I analyze ride-hailing data from Grab?",
      "What insights can be gained from Grab's transportation data?",
      "Where can I find GPS data for Southeast Asia?",
      "How does Grab's ride-hailing data reflect urban mobility?",
      "What patterns exist in Grab's driving GPS traces?"
    ],
    "domain_tags": [
      "transportation",
      "mobility"
    ],
    "data_modality": "time-series",
    "geographic_scope": "Southeast Asia",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/grab-driving-gps-traces.png",
    "embedding_text": "The Grab Driving GPS Traces dataset provides a rich source of GPS trace data collected from the Grab ride-hailing platform, which operates extensively in Southeast Asia. This dataset is structured in a tabular format, consisting of rows representing individual trips and columns capturing various attributes such as trip ID, start and end coordinates, timestamps, and possibly other variables related to the ride experience. The collection methodology involves the aggregation of GPS data from Grab's operational database, ensuring a comprehensive representation of ride-hailing activities across different urban settings. Coverage of this dataset is geographically focused on Southeast Asia, encompassing major cities where Grab operates. However, specific temporal coverage details are not provided. Key variables in the dataset include trip duration, distance traveled, and geographical coordinates, which are crucial for measuring mobility patterns and understanding user behavior in the ride-hailing context. Researchers may encounter data quality issues such as missing values or inaccuracies in GPS readings, necessitating common preprocessing steps like data cleaning, normalization, and outlier detection. The dataset supports various types of analyses, including regression analysis to identify factors influencing ride duration, machine learning techniques for predictive modeling, and descriptive statistics to summarize ride patterns. Researchers typically leverage this dataset to address questions related to urban mobility, the efficiency of ride-hailing services, and the socio-economic implications of transportation choices in rapidly urbanizing regions. Overall, the Grab Driving GPS Traces dataset serves as a valuable resource for scholars and practitioners interested in transportation studies, urban planning, and data-driven decision-making in the mobility sector.",
    "tfidf_keywords": [
      "GPS-traces",
      "ride-hailing",
      "urban-mobility",
      "transportation-patterns",
      "data-analysis",
      "Southeast-Asia",
      "route-optimization",
      "user-behavior",
      "trip-duration",
      "geographical-coordinates",
      "data-quality",
      "machine-learning",
      "predictive-modeling",
      "socio-economic-implications"
    ],
    "semantic_cluster": "transportation-data-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "urban-planning",
      "mobility-patterns",
      "data-cleaning",
      "predictive-analytics",
      "transportation-economics"
    ],
    "canonical_topics": [
      "machine-learning",
      "consumer-behavior",
      "transportation",
      "econometrics"
    ]
  },
  {
    "name": "NYC TLC Trip Records",
    "description": "3B+ taxi and rideshare trips since 2009. Fares, tips, surge pricing, driver pay. The gold standard for marketplace analytics",
    "category": "Transportation & Mobility",
    "url": "https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page",
    "docs_url": "https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf",
    "github_url": "https://github.com/toddwschneider/nyc-taxi-data",
    "tags": [
      "taxi",
      "Uber",
      "Lyft",
      "surge pricing",
      "NYC",
      "large-scale"
    ],
    "best_for": "Learning transportation & mobility analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "transportation",
      "marketplace-analytics",
      "urban-mobility"
    ],
    "summary": "The NYC TLC Trip Records dataset encompasses over 3 billion taxi and rideshare trips recorded since 2009, providing insights into fares, tips, surge pricing, and driver pay. This dataset serves as a vital resource for analyzing marketplace dynamics and urban transportation patterns.",
    "use_cases": [
      "Analyzing the impact of surge pricing on driver earnings",
      "Examining trends in taxi and rideshare usage over time",
      "Investigating the relationship between trip distance and fare",
      "Assessing the effects of external events on transportation patterns"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the trends in taxi fares in NYC since 2009?",
      "How does surge pricing affect driver earnings?",
      "What is the distribution of tips given to drivers?",
      "How do rideshare services like Uber and Lyft compare to traditional taxis?",
      "What factors influence the demand for rideshare services in NYC?",
      "How has the number of trips changed over the years in NYC?",
      "What are the peak hours for taxi and rideshare services in NYC?",
      "How do different neighborhoods in NYC compare in terms of rideshare usage?"
    ],
    "domain_tags": [
      "transportation",
      "urban-planning"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2009-present",
    "geographic_scope": "New York City",
    "size_category": "massive",
    "model_score": 0.0,
    "image_url": "/images/datasets/nyc-tlc-trip-records.png",
    "embedding_text": "The NYC TLC Trip Records dataset is a comprehensive collection of over 3 billion taxi and rideshare trips that have been recorded since 2009. This dataset is structured in a tabular format, containing rows that represent individual trips and columns that capture various attributes such as trip duration, fare amount, tip amount, surge pricing indicators, and driver pay. The data is collected through the New York City Taxi and Limousine Commission (TLC), which mandates that all taxi and rideshare operators submit trip records, ensuring a high level of coverage and accuracy. The dataset spans a significant temporal range, starting from 2009 to the present, and is geographically focused on New York City, making it an invaluable resource for researchers and analysts interested in urban transportation dynamics. Key variables within the dataset include trip distance, fare amount, tip amount, and time of day, each providing insights into the economic aspects of taxi and rideshare services. However, users should be aware of potential limitations such as data quality issues related to missing or erroneous entries, which may require preprocessing steps like data cleaning and normalization. Researchers typically employ this dataset for various analyses, including regression modeling to understand fare determinants, machine learning for demand forecasting, and descriptive statistics to summarize trip characteristics. The dataset supports a wide range of research questions, such as exploring the effects of surge pricing on driver income or analyzing the impact of city events on transportation patterns. Overall, the NYC TLC Trip Records dataset is a gold standard for marketplace analytics, offering a rich source of information for understanding the complexities of urban mobility.",
    "benchmark_usage": [
      "Common uses include marketplace analytics, fare modeling, and urban mobility studies."
    ],
    "tfidf_keywords": [
      "surge-pricing",
      "trip-duration",
      "fare-amount",
      "driver-pay",
      "urban-mobility",
      "taxi-analytics",
      "rideshare-comparison",
      "demand-forecasting",
      "data-cleaning",
      "marketplace-dynamics"
    ],
    "semantic_cluster": "urban-transportation-analytics",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "marketplace-analytics",
      "urban-mobility",
      "data-cleaning",
      "regression-analysis",
      "demand-forecasting"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "pricing",
      "marketplaces",
      "econometrics",
      "transportation"
    ]
  },
  {
    "name": "IBM Developer Data",
    "description": "AI, data science, healthcare, and weather datasets from IBM",
    "category": "Data Portals",
    "url": "https://developer.ibm.com/technologies/artificial-intelligence/data/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "IBM",
      "AI",
      "healthcare",
      "weather"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "AI",
      "data science",
      "healthcare",
      "weather"
    ],
    "summary": "The IBM Developer Data provides a collection of datasets related to AI, data science, healthcare, and weather. Researchers and practitioners can utilize these datasets for various analyses, including predictive modeling and data visualization.",
    "use_cases": [
      "Predictive modeling in healthcare using AI datasets",
      "Weather forecasting analysis with IBM weather data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What datasets does IBM provide for AI research?",
      "Where can I find healthcare datasets from IBM?",
      "Are there weather datasets available from IBM Developer?",
      "How can I access data science datasets from IBM?",
      "What types of AI datasets are available on tech-econ.org?",
      "Can I find IBM datasets for healthcare analytics?"
    ],
    "domain_tags": [
      "healthcare",
      "weather"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/logos/ibm.png",
    "embedding_text": "The IBM Developer Data offers a diverse range of datasets that encompass various fields such as artificial intelligence, data science, healthcare, and weather. These datasets are structured in a tabular format, consisting of multiple rows and columns that represent different variables pertinent to each domain. The collection methodology for these datasets typically involves aggregating data from various reliable sources, ensuring a high level of data integrity and relevance. While specific temporal and geographic coverage details are not explicitly mentioned, the datasets are designed to support a wide array of research questions and analytical approaches. Key variables within these datasets may include patient health metrics, weather conditions, and AI performance indicators, which are crucial for conducting in-depth analyses. Researchers often utilize these datasets for tasks such as regression analysis, machine learning modeling, and descriptive statistics. Common preprocessing steps may involve data cleaning, normalization, and transformation to prepare the data for analysis. The quality of the data is generally high, but users should be aware of potential limitations such as missing values or biases inherent in the data collection process. Overall, the IBM Developer Data serves as a valuable resource for researchers and practitioners looking to leverage data in their studies, enabling them to address complex questions and derive insights across various sectors.",
    "tfidf_keywords": [
      "artificial-intelligence",
      "data-science",
      "healthcare-datasets",
      "weather-data",
      "predictive-modeling",
      "data-visualization",
      "data-integration",
      "data-quality",
      "data-preprocessing",
      "machine-learning"
    ],
    "semantic_cluster": "ai-healthcare-weather",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "data-integration",
      "predictive-modeling",
      "data-visualization",
      "machine-learning",
      "data-quality"
    ],
    "canonical_topics": [
      "machine-learning",
      "healthcare",
      "forecasting"
    ]
  },
  {
    "name": "NYC TLC Trip Records",
    "description": "Complete trip-level data for all NYC taxi and for-hire vehicle trips including Uber and Lyft. Billions of records since 2009 with pickups, dropoffs, fares, and tips.",
    "category": "Transportation Economics & Technology",
    "url": "https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page",
    "docs_url": "https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records.pdf",
    "github_url": null,
    "tags": [
      "taxi",
      "rideshare",
      "NYC",
      "trip-data",
      "surge-pricing"
    ],
    "best_for": "Ridesharing analysis, surge pricing research, and urban mobility patterns",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "transportation",
      "urban-studies",
      "data-analysis"
    ],
    "summary": "The NYC TLC Trip Records dataset provides comprehensive trip-level data for all taxi and for-hire vehicle trips in New York City, including rides from services like Uber and Lyft. This dataset allows researchers to analyze patterns in transportation, fare structures, and consumer behavior over time.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the characteristics of NYC taxi trips?",
      "How do fare prices vary by time of day in NYC?",
      "What trends can be observed in for-hire vehicle usage in NYC?",
      "How does surge pricing affect consumer behavior in rideshare services?",
      "What is the average trip distance for NYC taxis?",
      "How do pickup and dropoff locations correlate with trip fares?"
    ],
    "use_cases": [
      "Analyzing the impact of surge pricing on ridership patterns.",
      "Examining the relationship between trip distance and fare amounts.",
      "Investigating temporal trends in taxi usage over the years.",
      "Studying the geographic distribution of taxi pickups and dropoffs."
    ],
    "domain_tags": [
      "transportation",
      "urban-planning"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2009-present",
    "geographic_scope": "New York City",
    "size_category": "massive",
    "model_score": 0.0,
    "image_url": "/images/datasets/nyc-tlc-trip-records.png",
    "embedding_text": "The NYC TLC Trip Records dataset is a rich repository of trip-level data encompassing all taxi and for-hire vehicle trips in New York City, including significant contributions from rideshare services such as Uber and Lyft. This dataset has been meticulously compiled since 2009, resulting in billions of records that provide invaluable insights into urban transportation dynamics. Each record typically includes key variables such as pickup and dropoff locations, fare amounts, tips, and timestamps, allowing for a comprehensive analysis of transportation patterns and consumer behavior in one of the world's busiest cities. The data is structured in a tabular format, making it accessible for various analytical techniques, including regression analysis, machine learning, and descriptive statistics. Researchers can leverage this dataset to explore a multitude of research questions, such as the effects of surge pricing on ridership, the average trip distances, and the correlation between geographic locations and fare prices. However, users should be aware of potential data quality issues, including missing values and outliers, which may necessitate preprocessing steps like data cleaning and normalization. The dataset's extensive temporal coverage allows for longitudinal studies, while its geographic specificity to New York City makes it particularly relevant for urban studies and transportation economics. Overall, the NYC TLC Trip Records dataset serves as a foundational resource for researchers aiming to understand the complexities of urban transportation systems and their economic implications.",
    "benchmark_usage": [
      "Commonly used for transportation demand analysis",
      "Used in studies regarding urban mobility and economic impact of ride-sharing"
    ],
    "tfidf_keywords": [
      "trip-level data",
      "rideshare",
      "surge pricing",
      "fare analysis",
      "pickup locations",
      "dropoff locations",
      "urban transportation",
      "consumer behavior",
      "data preprocessing",
      "temporal trends",
      "geographic distribution",
      "data quality",
      "machine learning",
      "regression analysis",
      "descriptive statistics"
    ],
    "semantic_cluster": "urban-transportation-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "transportation-economics",
      "urban-planning",
      "data-analysis",
      "consumer-behavior",
      "pricing-strategies"
    ],
    "canonical_topics": [
      "econometrics",
      "consumer-behavior",
      "pricing",
      "marketplaces",
      "data-engineering"
    ]
  },
  {
    "name": "Ele.me Search",
    "description": "Search log dataset from Ele.me (Chinese food delivery)",
    "category": "Food & Delivery",
    "url": "https://tianchi.aliyun.com/dataset/120281",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "food delivery",
      "search logs",
      "China"
    ],
    "best_for": "Learning food & delivery analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Ele.me Search dataset consists of search logs from Ele.me, a prominent food delivery service in China. This dataset can be utilized to analyze consumer search behavior, preferences, and trends in the food delivery market.",
    "use_cases": [
      "Analyzing consumer preferences in food delivery",
      "Identifying seasonal trends in food searches",
      "Evaluating the impact of promotions on search behavior"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the most common search queries in Ele.me?",
      "How do search trends vary by time of day?",
      "What food categories are most frequently searched?",
      "How do seasonal trends affect search behavior?",
      "What are the top searched dishes in China?",
      "How do user demographics influence search patterns?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "China",
    "size_category": "medium",
    "model_score": 0.0,
    "embedding_text": "The Ele.me Search dataset is a comprehensive collection of search logs generated by users of Ele.me, a leading food delivery platform in China. This dataset captures various aspects of consumer behavior through search queries, providing insights into preferences, trends, and the dynamics of the food delivery market. The data structure typically includes rows representing individual search queries, with columns detailing key variables such as search time, user ID, search terms, and possibly the category of food being searched. The collection methodology involves logging user interactions with the search feature on the Ele.me platform, ensuring a rich dataset that reflects real-time consumer behavior. Coverage is primarily geographic, focusing on urban areas in China where Ele.me operates, and it may encompass a diverse demographic of users, although specific demographic data may not be included. Key variables in the dataset measure aspects such as frequency of searches for specific food items, time of searches, and patterns of user engagement with the platform. Data quality is generally high, but limitations may arise from potential biases in user behavior or incomplete data due to privacy concerns. Common preprocessing steps include cleaning search terms, normalizing data formats, and possibly aggregating data to analyze trends over time. Researchers can leverage this dataset to address various research questions, such as identifying popular food items, understanding the impact of external factors on search behavior, and exploring consumer preferences in the food delivery sector. The dataset supports a range of analyses, including regression analysis to identify factors influencing search frequency, machine learning techniques for predictive modeling, and descriptive statistics to summarize search trends. Typically, researchers use this dataset in studies focused on consumer behavior, market analysis, and the optimization of food delivery services.",
    "tfidf_keywords": [
      "search logs",
      "consumer preferences",
      "food delivery",
      "user engagement",
      "search behavior",
      "data preprocessing",
      "temporal trends",
      "urban demographics",
      "food categories",
      "market analysis"
    ],
    "semantic_cluster": "consumer-behavior-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "marketplaces",
      "e-commerce",
      "data-analysis",
      "user-experience"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "marketplaces",
      "data-engineering"
    ]
  },
  {
    "name": "LOBSTER Order Book",
    "description": "NASDAQ limit order book data at millisecond precision. Level 1-10 depth, message-by-message reconstruction. Market microstructure research",
    "category": "Financial Services",
    "url": "https://lobsterdata.com/",
    "docs_url": "https://lobsterdata.com/info/DataStructure.php",
    "github_url": null,
    "tags": [
      "order book",
      "NASDAQ",
      "high-frequency",
      "market microstructure",
      "trading"
    ],
    "best_for": "Learning financial services analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "time-series-analysis",
      "statistical-modeling"
    ],
    "topic_tags": [
      "financial-services",
      "market-microstructure",
      "high-frequency-trading"
    ],
    "summary": "The LOBSTER Order Book dataset provides detailed NASDAQ limit order book data with millisecond precision, allowing researchers to analyze market microstructure and trading behaviors. This dataset supports various analyses, including the study of order book dynamics and high-frequency trading strategies.",
    "use_cases": [
      "Analyzing the impact of order book depth on price movements",
      "Studying the behavior of high-frequency traders in response to market events",
      "Investigating liquidity dynamics in the NASDAQ market",
      "Evaluating trading strategies based on order book signals"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the LOBSTER Order Book dataset?",
      "How can I access NASDAQ limit order book data?",
      "What insights can be gained from market microstructure research?",
      "What are the applications of high-frequency trading data?",
      "How is order book data structured in LOBSTER?",
      "What variables are included in the LOBSTER dataset?",
      "How can I use LOBSTER data for trading strategy analysis?",
      "What preprocessing steps are needed for LOBSTER Order Book data?"
    ],
    "domain_tags": [
      "fintech"
    ],
    "data_modality": "time-series",
    "size_category": "medium",
    "model_score": 0.0,
    "embedding_text": "The LOBSTER Order Book dataset is a comprehensive collection of NASDAQ limit order book data captured at millisecond precision. It includes detailed information about the order book's Level 1-10 depth, allowing researchers to reconstruct the market's message-by-message activity. The dataset is structured in a tabular format, with each row representing a specific time point and columns detailing various aspects of the order book, such as bid and ask prices, order sizes, and timestamps. This rich dataset is particularly valuable for market microstructure research, as it enables the analysis of trading behaviors and the dynamics of liquidity in real-time. The collection methodology involves capturing live market data directly from NASDAQ, ensuring high fidelity and accuracy. However, researchers should be aware of potential limitations, including data quality issues that may arise from market anomalies or technical glitches during data collection. Common preprocessing steps include handling missing values, normalizing timestamps, and aggregating data for specific time intervals. The LOBSTER dataset supports a wide range of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it a versatile tool for researchers interested in financial markets. Typical research questions addressed using this dataset include the effects of order book depth on price volatility, the behavior of high-frequency traders in response to market news, and the overall impact of liquidity on trading efficiency. By leveraging the insights gained from this dataset, researchers can contribute to a deeper understanding of market mechanisms and inform trading strategies.",
    "tfidf_keywords": [
      "limit-order-book",
      "high-frequency-trading",
      "market-microstructure",
      "order-book-dynamics",
      "liquidity-analysis",
      "trading-strategies",
      "price-volatility",
      "message-reconstruction",
      "NASDAQ-data",
      "time-series-analysis",
      "bid-ask-spread",
      "order-flow",
      "market-anomalies",
      "data-preprocessing",
      "statistical-modeling"
    ],
    "semantic_cluster": "market-microstructure-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "financial-markets",
      "algorithmic-trading",
      "data-analysis",
      "quantitative-finance",
      "statistical-inference"
    ],
    "canonical_topics": [
      "finance",
      "machine-learning",
      "econometrics",
      "statistics",
      "data-engineering"
    ],
    "benchmark_usage": [
      "Market microstructure research",
      "High-frequency trading analysis"
    ]
  },
  {
    "name": "Cainiao Last-Mile (MSOM18)",
    "description": "Cainiao Last-Mile Delivery dataset from MSOM 2018",
    "category": "Food & Delivery",
    "url": "https://tianchi.aliyun.com/competition/entrance/231623/information",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "last-mile",
      "delivery",
      "Cainiao",
      "MSOM"
    ],
    "best_for": "Learning food & delivery analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "delivery"
    ],
    "summary": "The Cainiao Last-Mile Delivery dataset from MSOM 2018 provides insights into the logistics and operational aspects of last-mile delivery services. Researchers can utilize this dataset to analyze delivery efficiency, consumer behavior in logistics, and the impact of delivery methods on customer satisfaction.",
    "use_cases": [
      "Analyzing delivery time efficiency",
      "Studying consumer preferences in delivery options",
      "Evaluating the impact of delivery methods on customer satisfaction"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Cainiao Last-Mile Delivery dataset?",
      "How can I analyze last-mile delivery efficiency?",
      "What variables are included in the Cainiao dataset?",
      "What insights can be gained from the MSOM 2018 delivery dataset?",
      "How does last-mile delivery affect consumer behavior?",
      "What are the key metrics for evaluating delivery performance?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/cainiao-last-mile-msom18.png",
    "embedding_text": "The Cainiao Last-Mile Delivery dataset from MSOM 2018 is a comprehensive resource for understanding the intricacies of last-mile logistics in e-commerce. This dataset is structured in a tabular format, consisting of rows that represent individual delivery instances and columns that capture various attributes related to each delivery. Key variables may include delivery times, distances, customer ratings, and operational metrics, providing a rich schema for analysis. The dataset was collected through Cainiao's logistics operations, ensuring that the data reflects real-world delivery scenarios. While specific temporal and geographic coverage details are not provided, the dataset is expected to encompass a range of delivery instances across different regions and timeframes, reflecting the dynamic nature of last-mile delivery services. Researchers utilizing this dataset can explore a variety of research questions, such as the factors influencing delivery efficiency, the relationship between delivery methods and consumer satisfaction, and the overall impact of logistics on e-commerce performance. Common preprocessing steps may involve cleaning the data for missing values, normalizing delivery times, and categorizing delivery methods for comparative analysis. The dataset supports various types of analyses, including regression modeling to predict delivery times, machine learning algorithms for clustering delivery performance, and descriptive statistics to summarize customer feedback. Researchers typically leverage this dataset in studies focused on logistics optimization, consumer behavior analysis, and operational efficiency in the retail sector. By analyzing the Cainiao Last-Mile Delivery dataset, scholars can contribute valuable insights to the field of logistics and supply chain management, enhancing our understanding of how last-mile delivery impacts overall customer experience and operational success in e-commerce.",
    "tfidf_keywords": [
      "last-mile",
      "delivery efficiency",
      "consumer satisfaction",
      "logistics optimization",
      "operational metrics",
      "e-commerce",
      "delivery methods",
      "customer behavior",
      "data analysis",
      "Cainiao"
    ],
    "semantic_cluster": "last-mile-logistics",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "logistics",
      "supply-chain-management",
      "consumer-behavior",
      "operational-efficiency",
      "data-analysis"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "marketplaces",
      "statistics",
      "data-engineering"
    ]
  },
  {
    "name": "MSOM Data Challenges",
    "description": "Manufacturing & Service Operations Management challenges",
    "category": "Data Portals",
    "url": "https://pubsonline.informs.org/journal/msom",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "MSOM",
      "operations",
      "INFORMS"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The MSOM Data Challenges dataset focuses on challenges in Manufacturing and Service Operations Management. It provides a structured collection of data that can be utilized for various analyses related to operational efficiency and service delivery in manufacturing and service sectors.",
    "use_cases": [
      "Analyzing operational efficiency in manufacturing",
      "Evaluating service delivery challenges",
      "Identifying trends in operations management"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What are the MSOM Data Challenges?",
      "How can I access the MSOM Data Challenges dataset?",
      "What types of analyses can be performed with MSOM data?",
      "What are common challenges in Manufacturing and Service Operations Management?",
      "How does MSOM data contribute to operational efficiency?",
      "What insights can be gained from the MSOM Data Challenges?"
    ],
    "domain_tags": [
      "manufacturing",
      "service-operations"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "embedding_text": "The MSOM Data Challenges dataset is a valuable resource for researchers and practitioners in the fields of Manufacturing and Service Operations Management. It is structured in a tabular format, allowing for easy manipulation and analysis using data analysis tools. The dataset typically includes various columns that represent key variables related to operational challenges, such as efficiency metrics, service delivery times, and resource allocation. The collection methodology may involve aggregating data from industry reports, surveys, or operational logs, ensuring a comprehensive view of the challenges faced in these sectors. While specific temporal and geographic coverage details are not provided, the dataset is designed to address a wide range of operational scenarios, making it applicable to various contexts within manufacturing and service industries. Key variables may include production rates, service response times, and customer satisfaction scores, which are crucial for understanding operational performance. However, users should be aware of potential limitations such as data quality issues or biases in reporting. Common preprocessing steps might involve cleaning the dataset, handling missing values, and normalizing data for comparative analysis. Researchers can leverage this dataset to explore questions related to operational efficiency, service quality, and resource optimization. The types of analyses supported include regression analysis, machine learning applications, and descriptive statistics, allowing users to derive actionable insights that can inform decision-making in operations management.",
    "tfidf_keywords": [
      "operational-efficiency",
      "service-delivery",
      "manufacturing-challenges",
      "resource-allocation",
      "efficiency-metrics",
      "customer-satisfaction",
      "data-analysis",
      "service-operations",
      "production-rates",
      "performance-evaluation"
    ],
    "semantic_cluster": "operations-management-challenges",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "operations-research",
      "process-optimization",
      "supply-chain-management",
      "quality-control",
      "data-analytics"
    ],
    "canonical_topics": [
      "operations-management",
      "industrial-organization",
      "data-engineering"
    ]
  },
  {
    "name": "PatentsView",
    "description": "13M+ US patents (1976-present) with citations, inventors, assignees. Full patent text and claims. innovation research at scale",
    "category": "Data Portals",
    "url": "https://patentsview.org/download/data-download-tables",
    "docs_url": "https://patentsview.org/download/data-download-dictionary",
    "github_url": null,
    "tags": [
      "patents",
      "innovation",
      "USPTO",
      "citations",
      "intellectual property"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "patents",
      "innovation",
      "intellectual property"
    ],
    "summary": "PatentsView is a comprehensive dataset containing over 13 million US patents from 1976 to the present. It includes detailed information such as citations, inventors, and assignees, making it a valuable resource for conducting innovation research at scale.",
    "use_cases": [
      "Analyzing trends in patent filings over time.",
      "Studying the impact of patents on innovation and economic growth.",
      "Investigating the relationships between inventors and their assignees.",
      "Exploring citation networks among patents."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the trends in US patent filings over the years?",
      "How do citations correlate with the success of patents?",
      "What are the most cited patents in a specific technology area?",
      "Who are the leading inventors in the field of renewable energy patents?",
      "How does patent assignment affect innovation in different industries?",
      "What is the relationship between patent filings and economic growth?"
    ],
    "domain_tags": [
      "technology",
      "innovation",
      "intellectual property"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1976-present",
    "geographic_scope": "United States",
    "size_category": "massive",
    "model_score": 0.0,
    "image_url": "/images/logos/patentsview.png",
    "embedding_text": "PatentsView is a rich dataset that provides access to over 13 million US patents, covering a broad temporal range from 1976 to the present. The dataset is structured in a tabular format, with rows representing individual patents and columns detailing various attributes such as patent numbers, titles, abstracts, full text, claims, inventors, assignees, and citation counts. This comprehensive schema allows researchers to conduct in-depth analyses of patent data, exploring trends, relationships, and impacts of patents on innovation and economic activity. The collection methodology involves aggregating data from the United States Patent and Trademark Office (USPTO), ensuring that the dataset is both extensive and reliable. Key variables within the dataset include patent citations, which measure the influence of patents on subsequent innovations, and inventor and assignee information, which can be used to analyze collaboration patterns and the distribution of innovation across different entities. While the dataset is robust, researchers should be aware of potential limitations such as data completeness and the evolving nature of patent laws that may affect interpretation. Common preprocessing steps may include cleaning text data, normalizing citation counts, and filtering patents by specific criteria such as date or technology class. PatentsView supports various types of analyses, including regression, machine learning, and descriptive statistics, making it a versatile tool for researchers in fields such as economics, technology studies, and policy evaluation. Researchers typically utilize this dataset to answer questions related to innovation dynamics, the economic impact of patents, and the role of intellectual property in fostering technological advancement.",
    "benchmark_usage": [
      "Innovation research",
      "Patent analysis",
      "Intellectual property studies"
    ],
    "tfidf_keywords": [
      "patent-citations",
      "innovation-research",
      "USPTO",
      "intellectual-property",
      "assignee-analysis",
      "inventor-networks",
      "patent-trends",
      "economic-impact",
      "technology-classification",
      "data-aggregation"
    ],
    "semantic_cluster": "patent-analysis-methods",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "innovation-economics",
      "intellectual-property-law",
      "technology-transfer",
      "economic-development",
      "data-analysis"
    ],
    "canonical_topics": [
      "innovation",
      "econometrics",
      "data-engineering",
      "policy-evaluation"
    ]
  },
  {
    "name": "OpenML",
    "description": "Platform for sharing datasets, tasks, and ML code",
    "category": "Data Portals",
    "url": "https://www.openml.org/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "ML",
      "open science",
      "benchmarks"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "OpenML is a platform designed for sharing datasets, tasks, and machine learning code, facilitating collaboration and innovation in the machine learning community. Users can access a wide variety of datasets and tasks, enabling them to benchmark their algorithms and share their findings with others.",
    "use_cases": [
      "Benchmarking machine learning algorithms",
      "Collaborating on machine learning tasks",
      "Sharing datasets with the community"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is OpenML?",
      "How can I access datasets on OpenML?",
      "What types of machine learning tasks are available on OpenML?",
      "How does OpenML support open science?",
      "What are the benchmarks available on OpenML?",
      "How can I share my machine learning code on OpenML?"
    ],
    "domain_tags": [
      "technology"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "benchmark_usage": [
      "Benchmarking machine learning algorithms"
    ],
    "model_score": 0.0,
    "embedding_text": "OpenML is an innovative platform that serves as a hub for sharing datasets, tasks, and machine learning (ML) code, fostering an environment of collaboration and transparency in the field of machine learning. The platform allows researchers and practitioners to upload, share, and access a diverse array of datasets, which can be utilized for various machine learning tasks. The data structure within OpenML typically includes a wide range of datasets characterized by rows and columns, where each row represents an individual observation and each column corresponds to a specific variable. The variables can include features relevant to the tasks at hand, such as numerical values, categorical data, or text entries, depending on the nature of the dataset. OpenML's collection methodology relies on contributions from the global research community, enabling a rich repository of datasets that span numerous domains and applications. Users can leverage these datasets for benchmarking their algorithms, allowing for comparative analysis and performance evaluation across different machine learning models. The platform also supports a variety of data modalities, including tabular data, images, and text, making it a versatile resource for researchers. While OpenML provides a wealth of data, users should be aware of potential limitations regarding data quality and completeness, as the datasets are contributed by various sources and may vary in their preprocessing and documentation. Common preprocessing steps may include handling missing values, normalizing data, and encoding categorical variables to prepare the datasets for analysis. Researchers typically use OpenML to address a range of research questions, from exploring the effectiveness of different machine learning algorithms to understanding the underlying patterns within the data. The platform supports various types of analyses, including regression, classification, and clustering, enabling users to apply their preferred methodologies to the datasets available. Overall, OpenML stands as a vital resource for the machine learning community, promoting open science and collaboration through its extensive collection of datasets and tasks.",
    "image_url": "/images/datasets/openml.png",
    "tfidf_keywords": [
      "machine learning",
      "datasets",
      "benchmarking",
      "open science",
      "collaboration",
      "data sharing",
      "algorithm evaluation",
      "data modalities",
      "preprocessing",
      "performance analysis"
    ],
    "semantic_cluster": "ml-data-sharing",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "open science",
      "data sharing",
      "machine learning",
      "algorithm benchmarking",
      "collaborative research"
    ],
    "canonical_topics": [
      "machine-learning",
      "experimentation",
      "data-engineering"
    ]
  },
  {
    "name": "OpenML",
    "description": "ML benchmarking platform with standardized train-test splits for reproducible comparisons",
    "category": "Dataset Aggregators",
    "url": "https://www.openml.org",
    "docs_url": "https://docs.openml.org",
    "github_url": "https://github.com/openml/openml-python",
    "tags": [
      "benchmarking",
      "reproducibility",
      "ML",
      "AutoML"
    ],
    "best_for": "Reproducible ML benchmarking with standardized experimental setups",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "OpenML is a machine learning benchmarking platform that provides standardized train-test splits for reproducible comparisons across various machine learning models. It allows researchers and practitioners to evaluate their algorithms on a consistent basis, facilitating fair comparisons and enhancing the reproducibility of results in machine learning experiments.",
    "use_cases": [
      "Comparing the performance of different machine learning algorithms.",
      "Conducting reproducible experiments in machine learning research.",
      "Evaluating AutoML frameworks using standardized datasets.",
      "Benchmarking new algorithms against established models."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is OpenML and how does it support ML benchmarking?",
      "How can I access datasets on OpenML for reproducible comparisons?",
      "What are the features of OpenML for machine learning practitioners?",
      "How does OpenML ensure reproducibility in machine learning experiments?",
      "What types of datasets are available on OpenML?",
      "How can I use OpenML for AutoML tasks?",
      "What are the benefits of using OpenML for ML research?",
      "How does OpenML standardize train-test splits?"
    ],
    "domain_tags": [
      "technology"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "benchmark_usage": [
      "Standardized comparisons of machine learning models"
    ],
    "model_score": 0.0,
    "image_url": "/images/datasets/openml.png",
    "embedding_text": "OpenML serves as a comprehensive machine learning benchmarking platform that is designed to facilitate reproducible comparisons among various machine learning models. The platform provides a rich repository of datasets, each with standardized train-test splits, which allows researchers and practitioners to evaluate their algorithms under consistent conditions. This is particularly important in the field of machine learning, where the ability to replicate results is crucial for validating the effectiveness of different approaches. The data structure in OpenML typically consists of a variety of datasets that include rows representing individual observations and columns that define the features or variables of interest. Each dataset is meticulously curated to ensure that it meets the standards necessary for rigorous analysis. The collection methodology employed by OpenML involves aggregating datasets from diverse sources, ensuring a wide-ranging coverage of machine learning tasks. While OpenML does not specify temporal or geographic coverage in its description, the datasets available may span various domains and applications, providing a rich resource for machine learning research. Key variables within the datasets often include features that are relevant to the specific tasks at hand, such as input variables for predictive modeling or categorical variables for classification tasks. However, users should be aware of potential limitations regarding data quality, as some datasets may contain missing values or inconsistencies that require preprocessing. Common preprocessing steps may include handling missing data, normalizing features, and encoding categorical variables to prepare the datasets for analysis. Researchers typically leverage OpenML to address a variety of research questions, such as comparing the efficacy of different algorithms on the same dataset or exploring the impact of feature selection on model performance. The platform supports a range of analyses, including regression, classification, and clustering, making it a versatile tool for machine learning practitioners. Overall, OpenML is an invaluable resource for those looking to engage in machine learning research, providing the necessary infrastructure to conduct fair and reproducible experiments.",
    "tfidf_keywords": [
      "benchmarking",
      "reproducibility",
      "train-test splits",
      "machine learning",
      "datasets",
      "AutoML",
      "algorithm comparison",
      "data quality",
      "preprocessing",
      "evaluation metrics"
    ],
    "semantic_cluster": "ml-benchmarking-platform",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "machine-learning",
      "benchmarking",
      "reproducibility",
      "algorithm-evaluation",
      "data-preprocessing"
    ],
    "canonical_topics": [
      "machine-learning",
      "experimentation",
      "statistics"
    ]
  },
  {
    "name": "Google Dataset Search",
    "description": "Universal search engine for datasets across the web. Meta-tool for discovering research data",
    "category": "Data Portals",
    "url": "https://datasetsearch.research.google.com/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "search engine",
      "datasets",
      "discovery",
      "Google"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Google Dataset Search is a universal search engine designed to help users discover datasets across the web. It serves as a meta-tool for researchers and data enthusiasts to find relevant research data efficiently.",
    "use_cases": [],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What datasets are available on climate change?",
      "How can I find datasets related to economic indicators?",
      "Where can I locate datasets for machine learning projects?",
      "What are the datasets available for healthcare research?",
      "How do I search for datasets on social media trends?",
      "What datasets can I find for urban planning studies?"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0,
    "embedding_text": "Google Dataset Search is a powerful tool that aggregates datasets from various sources across the internet, providing a centralized platform for researchers, data scientists, and curious individuals to discover and access a wide range of datasets. The data structure typically includes metadata about the datasets, such as titles, descriptions, and links to the original sources, allowing users to quickly assess the relevance of the datasets to their research needs. The collection methodology involves crawling the web and indexing datasets from numerous repositories, ensuring a diverse and comprehensive coverage of available data. While the tool does not host datasets itself, it provides links to external sources where the datasets can be accessed. Researchers can use Google Dataset Search to address a variety of research questions, from analyzing trends in social media to exploring economic data and healthcare statistics. The platform supports various types of analyses, including regression, machine learning, and descriptive statistics, depending on the datasets accessed through it. Users typically engage with the tool to enhance their studies by finding relevant data that can support their hypotheses or provide insights into their areas of interest. However, it is important to note that the quality of the datasets found through Google Dataset Search can vary significantly, as the tool does not vet the data sources. Therefore, users should critically evaluate the datasets they choose to utilize, considering factors such as data quality, completeness, and potential biases. Common preprocessing steps may include cleaning the data, transforming variables, and ensuring compatibility with analytical tools. Overall, Google Dataset Search serves as an invaluable resource for anyone looking to harness the power of data in their research endeavors.",
    "tfidf_keywords": [
      "dataset-discovery",
      "data-aggregation",
      "metadata-indexing",
      "research-data",
      "data-repositories",
      "data-access",
      "data-quality",
      "data-collection-methodology",
      "data-structure",
      "data-analysis",
      "data-science",
      "data-visualization",
      "data-preprocessing",
      "data-sourcing",
      "data-compatibility"
    ],
    "semantic_cluster": "dataset-discovery-tools",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [],
    "canonical_topics": []
  },
  {
    "name": "Google Dataset Search",
    "description": "Search engine indexing 45M+ datasets from 13,000+ websites using schema.org metadata",
    "category": "Dataset Aggregators",
    "url": "https://datasetsearch.research.google.com",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "search",
      "discovery",
      "meta-search",
      "aggregator"
    ],
    "best_for": "Discovering datasets across government portals, research institutions, and commercial providers",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Google Dataset Search is a powerful search engine that indexes over 45 million datasets from more than 13,000 websites, utilizing schema.org metadata to enhance discoverability. Researchers and data enthusiasts can use this tool to find datasets across various domains, facilitating data-driven research and analysis.",
    "use_cases": [
      "Finding datasets for academic research",
      "Exploring datasets for data science projects",
      "Locating datasets for machine learning applications",
      "Identifying datasets for market analysis"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What datasets are indexed by Google Dataset Search?",
      "How can I find datasets related to machine learning?",
      "What are the most popular datasets available on Google Dataset Search?",
      "How does Google Dataset Search utilize schema.org metadata?",
      "Can I search for datasets by specific topics or keywords?",
      "What types of datasets can I discover using Google Dataset Search?"
    ],
    "domain_tags": [
      "General"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0,
    "embedding_text": "Google Dataset Search serves as a comprehensive search engine designed to index a vast array of datasets, exceeding 45 million, sourced from over 13,000 websites. Utilizing schema.org metadata, it enhances the visibility and accessibility of datasets across various domains, making it an invaluable resource for researchers, data scientists, and anyone interested in data-driven insights. The data structure consists of a diverse range of datasets, each characterized by various rows and columns that represent different variables pertinent to the dataset's subject matter. The collection methodology involves aggregating datasets from multiple sources, ensuring a broad coverage of topics and domains. However, the specific temporal and geographic coverage of the datasets is not explicitly detailed, which may limit the contextual understanding of the data. Key variables within the datasets can include attributes such as dataset title, description, publisher, and access information, which collectively measure the dataset's relevance and usability for research purposes. While the data quality is generally high due to the reputable sources, users should be aware of potential limitations such as outdated information or incomplete metadata. Common preprocessing steps may involve cleaning the dataset descriptions, standardizing formats, and filtering irrelevant datasets based on user queries. Researchers typically leverage Google Dataset Search to address a variety of research questions, from identifying trends in specific fields to conducting comparative analyses across different datasets. The platform supports various types of analyses, including regression, machine learning, and descriptive statistics, enabling users to extract meaningful insights from the datasets they discover. Overall, Google Dataset Search is a pivotal tool for facilitating data discovery, empowering users to engage in data-driven research and analysis across a multitude of disciplines.",
    "tfidf_keywords": [
      "dataset-indexing",
      "schema.org",
      "data-discovery",
      "search-engine",
      "metadata-aggregation",
      "dataset-visibility",
      "research-resources",
      "data-accessibility",
      "data-sourcing",
      "dataset-categorization",
      "information-retrieval",
      "data-collection-methodology",
      "dataset-variables",
      "data-quality-assessment",
      "data-preprocessing"
    ],
    "semantic_cluster": "dataset-discovery-tools",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [],
    "canonical_topics": []
  },
  {
    "name": "Rakuten Data Release",
    "description": "E-commerce, advertising, and multimedia datasets from Rakuten",
    "category": "Data Portals",
    "url": "https://rit.rakuten.com/data_release/#access",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Rakuten",
      "e-commerce",
      "multimedia"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "advertising",
      "multimedia"
    ],
    "summary": "The Rakuten Data Release provides a comprehensive collection of datasets related to e-commerce, advertising, and multimedia. Researchers can utilize this data to analyze consumer behavior, pricing strategies, and advertising effectiveness within the e-commerce sector.",
    "use_cases": [
      "Analyzing consumer purchasing patterns in e-commerce.",
      "Evaluating the effectiveness of advertising campaigns.",
      "Exploring multimedia content engagement metrics."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What datasets are available from Rakuten?",
      "How can I access Rakuten's e-commerce data?",
      "What multimedia datasets does Rakuten provide?",
      "Are there advertising datasets from Rakuten?",
      "What insights can be gained from Rakuten's data release?",
      "How does Rakuten's data support e-commerce research?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/rakuten-data-release.png",
    "embedding_text": "The Rakuten Data Release encompasses a diverse array of datasets that are pivotal for understanding the dynamics of e-commerce, advertising, and multimedia content. The datasets are structured in a tabular format, featuring rows that represent individual transactions, user interactions, or content engagements, and columns that capture various attributes such as user demographics, product categories, pricing information, and engagement metrics. The collection methodology involves aggregating data from Rakuten's extensive e-commerce platform, which includes user-generated content, transaction records, and advertising performance metrics. While the specific temporal and geographic coverage of the datasets is not explicitly detailed, they are likely to reflect trends and behaviors relevant to contemporary e-commerce practices. Key variables within the datasets may include user IDs, product IDs, transaction timestamps, and engagement rates, each measuring aspects of consumer behavior and advertising effectiveness. Researchers should be aware of potential limitations in data quality, such as missing values or biases in user representation, which may necessitate common preprocessing steps like data cleaning and normalization. The datasets can address a variety of research questions, including inquiries into consumer purchasing behavior, the impact of advertising on sales, and the effectiveness of different multimedia strategies in engaging users. Supported analyses range from regression modeling to machine learning applications and descriptive statistics, allowing researchers to derive actionable insights from the data. Typically, researchers leverage this data to inform marketing strategies, optimize pricing, and enhance user engagement through targeted advertising efforts.",
    "tfidf_keywords": [
      "e-commerce",
      "consumer-behavior",
      "advertising-performance",
      "multimedia-engagement",
      "transaction-data",
      "user-demographics",
      "pricing-strategies",
      "data-aggregation",
      "data-cleaning",
      "user-interaction"
    ],
    "semantic_cluster": "e-commerce-data-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "advertising-strategies",
      "data-analysis",
      "market-research",
      "user-engagement"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "marketplaces",
      "advertising",
      "data-engineering",
      "statistics"
    ]
  },
  {
    "name": "Marketing Science Databases",
    "description": "INFORMS conference with data-focused opportunities",
    "category": "Data Portals",
    "url": "https://pubsonline.informs.org/page/mksc/online-databases",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "marketing",
      "INFORMS",
      "databases"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "marketing",
      "data-analysis"
    ],
    "summary": "The Marketing Science Databases provide a collection of data-focused opportunities from the INFORMS conference, aimed at enhancing marketing research and analytics. Researchers can leverage this dataset to explore various marketing strategies, consumer behaviors, and data-driven decision-making processes.",
    "use_cases": [
      "Analyzing consumer behavior trends",
      "Evaluating marketing campaign effectiveness",
      "Exploring data-driven marketing strategies"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What data is available from the INFORMS conference?",
      "How can I access marketing science databases?",
      "What types of marketing data are included in the dataset?",
      "What opportunities does the INFORMS conference provide for data analysis?",
      "How can marketing researchers utilize the Marketing Science Databases?",
      "What are the key features of the Marketing Science Databases?"
    ],
    "domain_tags": [
      "marketing"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0,
    "embedding_text": "The Marketing Science Databases serve as a vital resource for researchers and practitioners in the field of marketing, particularly those attending the INFORMS conference. This dataset encompasses a variety of data structures and schemas, including both quantitative and qualitative data that can be used to analyze marketing strategies and consumer behavior. The collection methodology likely involves contributions from conference participants and industry experts, ensuring a diverse range of data sources. While specific temporal and geographic coverage details are not provided, the dataset is designed to support various research questions related to marketing effectiveness and consumer insights. Key variables may include metrics on marketing performance, consumer demographics, and engagement levels, although specific measurements are not detailed. Users should be aware of potential limitations in data quality, as datasets from conferences may vary in comprehensiveness and accuracy. Common preprocessing steps might involve cleaning and normalizing the data to prepare it for analysis. Researchers typically utilize this dataset to conduct various analyses, including regression analysis and machine learning techniques, to derive actionable insights and inform marketing strategies. The Marketing Science Databases thus represent a rich resource for advancing knowledge in marketing science and fostering data-driven decision-making.",
    "tfidf_keywords": [
      "consumer-behavior",
      "marketing-strategy",
      "data-driven",
      "INFORMS",
      "analytics",
      "performance-metrics",
      "engagement-levels",
      "quantitative-data",
      "qualitative-data",
      "marketing-research"
    ],
    "semantic_cluster": "marketing-data-resources",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "data-analysis",
      "consumer-insights",
      "marketing-research",
      "data-collection",
      "performance-evaluation"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "marketing",
      "data-engineering"
    ]
  },
  {
    "name": "FI-2010 Limit Order Book",
    "description": "4.3M samples of NASDAQ Nordic limit order book data. 10 depth levels, 5 stocks, normalized features. Benchmark for price prediction",
    "category": "Financial Services",
    "url": "https://etsin.fairdata.fi/dataset/73eb48d7-4dbc-4a10-a52a-da745b47a649",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "order book",
      "limit orders",
      "NASDAQ",
      "price prediction",
      "benchmark"
    ],
    "best_for": "Learning financial services analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "financial-services",
      "price-prediction"
    ],
    "summary": "The FI-2010 Limit Order Book dataset contains 4.3 million samples of limit order book data from NASDAQ Nordic, covering 10 depth levels for 5 different stocks. It serves as a benchmark for price prediction tasks, allowing researchers to explore and develop predictive models based on the dynamics of limit orders.",
    "use_cases": [
      "Price prediction modeling",
      "Market behavior analysis",
      "Algorithmic trading strategy development"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the FI-2010 Limit Order Book dataset?",
      "How can I use NASDAQ limit order book data for price prediction?",
      "What features are included in the FI-2010 dataset?",
      "What are the depth levels in the limit order book?",
      "How does the dataset support regression analysis?",
      "What stocks are covered in the FI-2010 dataset?",
      "What preprocessing steps are needed for limit order book data?",
      "How can I benchmark price prediction models using this dataset?"
    ],
    "domain_tags": [
      "fintech"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "benchmark_usage": [
      "Benchmark for price prediction"
    ],
    "model_score": 0.0,
    "image_url": "/images/logos/fairdata.png",
    "embedding_text": "The FI-2010 Limit Order Book dataset is a comprehensive collection of 4.3 million samples of limit order book data sourced from NASDAQ Nordic. It provides a detailed view of market dynamics through 10 depth levels for 5 different stocks, offering normalized features that facilitate comparative analysis. The dataset is structured in a tabular format, with each row representing a unique snapshot of the limit order book at a specific time, while the columns capture various attributes such as price levels, order sizes, and timestamps. This rich structure allows researchers to delve into the intricacies of market behavior and develop predictive models that can forecast price movements based on historical order book data. The collection methodology involves aggregating data from live market feeds, ensuring that the dataset reflects real-time trading conditions. However, users should be aware of potential limitations, such as data quality issues related to missing values or outliers, which may require preprocessing steps like normalization and imputation. Researchers can leverage this dataset to address critical questions related to price prediction, market efficiency, and trading strategies. The dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it a versatile tool for both academic and practical applications in finance. By utilizing the FI-2010 Limit Order Book dataset, researchers can benchmark their price prediction models against established standards, contributing to the broader field of financial analytics and algorithmic trading.",
    "tfidf_keywords": [
      "limit-order-book",
      "NASDAQ",
      "price-prediction",
      "depth-levels",
      "normalized-features",
      "market-dynamics",
      "algorithmic-trading",
      "data-preprocessing",
      "financial-analytics",
      "predictive-modeling"
    ],
    "semantic_cluster": "financial-data-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "market-efficiency",
      "algorithmic-trading",
      "predictive-modeling",
      "time-series-analysis",
      "financial-markets"
    ],
    "canonical_topics": [
      "finance",
      "machine-learning",
      "econometrics",
      "pricing"
    ]
  },
  {
    "name": "RecSys Datasets Collection",
    "description": "Datasets from ACM Recommender Systems challenges",
    "category": "Data Portals",
    "url": "https://github.com/RUCAIBox/RecSysDatasets",
    "docs_url": null,
    "github_url": "https://github.com/RUCAIBox/RecSysDatasets",
    "tags": [
      "RecSys",
      "recommendations",
      "ACM"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "recommendations",
      "consumer-behavior"
    ],
    "summary": "The RecSys Datasets Collection features datasets derived from ACM Recommender Systems challenges, providing a rich resource for researchers and practitioners interested in recommendation systems. Users can leverage these datasets to develop, test, and improve algorithms for personalized recommendations across various domains.",
    "use_cases": [
      "Developing recommendation algorithms",
      "Testing machine learning models for recommendations",
      "Analyzing user behavior in recommendation systems"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What datasets are available for recommender systems?",
      "How can I access ACM Recommender Systems datasets?",
      "What are the challenges in recommendation systems?",
      "What data is used in ACM RecSys competitions?",
      "Where can I find datasets for recommendation algorithms?",
      "What types of data are included in the RecSys Datasets Collection?"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/recsys-datasets-collection.png",
    "embedding_text": "The RecSys Datasets Collection is a compilation of datasets sourced from the ACM Recommender Systems challenges, which are renowned for their focus on advancing the field of recommendation systems. This collection serves as a vital resource for researchers and practitioners aiming to explore various facets of recommender systems. The datasets typically include user-item interaction data, which may consist of ratings, clicks, or purchase history, structured in a tabular format. Each dataset may vary in terms of the number of users, items, and interactions, providing a diverse range of scenarios for analysis. The collection methodology involves aggregating data from competitions where participants develop algorithms to solve specific recommendation problems, ensuring that the datasets are relevant and challenging. Researchers can utilize these datasets to address key research questions such as how to improve recommendation accuracy, understand user preferences, and evaluate the effectiveness of different recommendation strategies. Common preprocessing steps may include data cleaning, normalization, and transformation to prepare the datasets for analysis. The datasets support various types of analyses, including regression, machine learning, and descriptive statistics, allowing users to apply a range of methodologies to extract insights. However, users should be aware of potential limitations, such as sparsity in user-item interactions and biases in the data collection process. Overall, the RecSys Datasets Collection is instrumental for those looking to delve into the intricacies of recommendation systems and contribute to the ongoing discourse in this dynamic field.",
    "domain_tags": [
      "technology"
    ],
    "tfidf_keywords": [
      "user-item interactions",
      "recommendation algorithms",
      "data sparsity",
      "collaborative filtering",
      "content-based filtering",
      "evaluation metrics",
      "machine learning",
      "personalization",
      "ACM challenges",
      "data preprocessing"
    ],
    "semantic_cluster": "recommendation-systems-datasets",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "collaborative-filtering",
      "content-based-recommendation",
      "user-behavior-analysis",
      "machine-learning",
      "data-mining"
    ],
    "canonical_topics": [
      "recommendation-systems",
      "machine-learning",
      "consumer-behavior"
    ]
  },
  {
    "name": "USASpending Federal Awards",
    "description": "All federal contracts, grants, loans since 2001. 400+ variables, $50T+ in awards. Government procurement analytics",
    "category": "Data Portals",
    "url": "https://www.usaspending.gov/download_center/award_data_archive",
    "docs_url": "https://www.usaspending.gov/data-dictionary",
    "github_url": null,
    "tags": [
      "government",
      "procurement",
      "contracts",
      "grants",
      "federal spending"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The USASpending Federal Awards dataset provides comprehensive information on all federal contracts, grants, and loans issued since 2001, encompassing over 400 variables and more than $50 trillion in awards. This dataset is invaluable for government procurement analytics, allowing users to analyze spending patterns, evaluate program effectiveness, and conduct research on federal financial activities.",
    "use_cases": [
      "Analyzing trends in federal spending over time",
      "Evaluating the impact of federal grants on local economies",
      "Comparing contract awards across different federal agencies",
      "Assessing the effectiveness of federal loan programs"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the total federal awards since 2001?",
      "How do federal contracts vary by state?",
      "What trends can be observed in federal grant allocations?",
      "Which agencies are the largest recipients of federal loans?",
      "How does federal spending correlate with economic indicators?",
      "What are the most common types of federal contracts?"
    ],
    "domain_tags": [
      "government",
      "procurement"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2001-present",
    "geographic_scope": "United States",
    "size_category": "massive",
    "model_score": 0.0,
    "embedding_text": "The USASpending Federal Awards dataset is a comprehensive repository of federal financial transactions, including contracts, grants, and loans, dating back to 2001. This dataset features over 400 variables, providing a rich schema that includes details such as award amounts, recipient organizations, types of awards, and the agencies responsible for disbursement. The data is collected from various federal agencies and is made available to the public to promote transparency and accountability in government spending. Researchers and analysts can leverage this dataset to explore a wide range of research questions, such as identifying trends in federal spending, evaluating the impact of federal programs on local economies, and comparing the distribution of awards across different regions and agencies. Key variables in the dataset include the award amount, award type, recipient name, and agency name, each of which provides insights into federal financial activities. However, users should be aware of potential limitations in data quality, such as inconsistencies in reporting and variations in how different agencies categorize awards. Common preprocessing steps may include data cleaning, normalization, and the handling of missing values. The dataset supports various types of analyses, including descriptive statistics, regression analysis, and machine learning applications, making it a versatile tool for researchers in fields such as economics, public policy, and data science. Overall, the USASpending Federal Awards dataset serves as a vital resource for understanding federal financial flows and their implications for economic research and policy evaluation.",
    "tfidf_keywords": [
      "federal contracts",
      "grants",
      "loans",
      "procurement analytics",
      "government spending",
      "award amounts",
      "recipient organizations",
      "data transparency",
      "financial transactions",
      "economic impact"
    ],
    "semantic_cluster": "government-spending-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "public finance",
      "government accountability",
      "economic analysis",
      "data visualization",
      "policy evaluation"
    ],
    "canonical_topics": [
      "policy-evaluation",
      "econometrics",
      "finance"
    ],
    "benchmark_usage": [
      "Government procurement analytics"
    ]
  },
  {
    "name": "Yongfeng Dataset Collection",
    "description": "E-commerce and recommendation system datasets",
    "category": "Data Portals",
    "url": "https://www.yongfeng.me/dataset/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "recommendations",
      "e-commerce",
      "academic"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "recommendations"
    ],
    "summary": "The Yongfeng Dataset Collection comprises various datasets related to e-commerce and recommendation systems. Researchers can utilize this collection to analyze consumer behavior, develop recommendation algorithms, and improve e-commerce strategies.",
    "use_cases": [
      "Analyzing consumer purchasing patterns",
      "Developing recommendation algorithms",
      "Evaluating e-commerce strategies"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What datasets are available for e-commerce recommendations?",
      "How can I analyze consumer behavior in e-commerce?",
      "What data is included in the Yongfeng Dataset Collection?",
      "Where can I find datasets for recommendation systems?",
      "What are the applications of e-commerce datasets?",
      "How to use the Yongfeng Dataset for research?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "embedding_text": "The Yongfeng Dataset Collection is a comprehensive repository of datasets specifically designed for research in e-commerce and recommendation systems. This collection includes various datasets that capture consumer interactions, purchasing behaviors, and product recommendations across different platforms. The data is structured in a tabular format, comprising rows that represent individual transactions or interactions and columns that capture key variables such as user IDs, product IDs, timestamps, ratings, and contextual information. Researchers can leverage this dataset to explore a wide range of research questions related to consumer behavior, including how different factors influence purchasing decisions and the effectiveness of various recommendation strategies. \n\nThe collection methodology involves aggregating data from real-world e-commerce platforms, ensuring that the datasets reflect actual consumer behavior in a variety of contexts. However, users should be aware of potential limitations, such as missing values or biases inherent in the data collection process. Common preprocessing steps may include data cleaning, normalization, and feature engineering to prepare the data for analysis. \n\nThe datasets support various types of analyses, including regression modeling, machine learning applications, and descriptive statistics. Researchers typically use this collection to develop and validate recommendation algorithms, assess the impact of marketing strategies, and conduct exploratory analyses of consumer trends. By utilizing the Yongfeng Dataset Collection, researchers can gain valuable insights into the dynamics of e-commerce and enhance their understanding of recommendation systems.",
    "tfidf_keywords": [
      "recommendation-systems",
      "consumer-behavior",
      "e-commerce",
      "data-collection",
      "machine-learning",
      "data-preprocessing",
      "purchasing-patterns",
      "algorithm-validation",
      "user-interactions",
      "data-quality"
    ],
    "semantic_cluster": "e-commerce-recommendations",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "machine-learning",
      "consumer-behavior",
      "data-preprocessing",
      "recommendation-systems",
      "e-commerce"
    ],
    "canonical_topics": [
      "recommendation-systems",
      "consumer-behavior",
      "machine-learning",
      "econometrics"
    ]
  },
  {
    "name": "Julian McAuley Datasets",
    "description": "Reviews, recommendations, and social network data",
    "category": "Data Portals",
    "url": "https://cseweb.ucsd.edu/~jmcauley/datasets.html",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "reviews",
      "recommendations",
      "UCSD",
      "academic"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Julian McAuley Datasets provide a rich collection of reviews, recommendations, and social network data, primarily sourced from academic research. Researchers can utilize this dataset to analyze consumer behavior, develop recommendation systems, and explore social interactions within e-commerce platforms.",
    "use_cases": [
      "Analyzing consumer preferences through review data.",
      "Building and testing recommendation algorithms.",
      "Studying the impact of social networks on purchasing decisions."
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What are the available datasets for reviews and recommendations?",
      "How can I access social network data for academic research?",
      "Where can I find datasets related to consumer behavior?",
      "What data is available from Julian McAuley for e-commerce analysis?",
      "Are there datasets for studying recommendations in online platforms?",
      "How to analyze reviews from the Julian McAuley Datasets?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/logos/ucsd.png",
    "embedding_text": "The Julian McAuley Datasets encompass a diverse array of reviews, recommendations, and social network data, primarily aimed at facilitating research in the fields of e-commerce and consumer behavior. The datasets are structured in a tabular format, with rows representing individual reviews or user interactions and columns detailing various attributes such as user ID, product ID, review text, rating, and timestamps. This structure allows for comprehensive analysis and modeling of consumer preferences and behaviors. The data is collected from various online platforms, ensuring a rich source of information that reflects real-world interactions and opinions. Researchers can leverage this dataset to address critical research questions related to consumer decision-making, the effectiveness of recommendation systems, and the dynamics of social influence in purchasing behavior. Common preprocessing steps may include text normalization for review analysis, handling missing values, and encoding categorical variables for machine learning applications. The dataset supports a range of analyses, including regression modeling to understand factors influencing ratings, machine learning techniques for building predictive models, and descriptive statistics for summarizing user behavior. However, researchers should be aware of potential limitations, such as biases in user-generated content and the representativeness of the sampled data. Overall, the Julian McAuley Datasets serve as a valuable resource for academics and practitioners interested in exploring the intersection of technology and consumer behavior.",
    "tfidf_keywords": [
      "recommendation-systems",
      "consumer-preferences",
      "social-network-analysis",
      "e-commerce",
      "user-generated-content",
      "data-collection-methodology",
      "text-normalization",
      "predictive-modeling",
      "machine-learning",
      "consumer-behavior",
      "data-preprocessing",
      "rating-prediction",
      "user-interaction",
      "behavioral-analysis"
    ],
    "semantic_cluster": "recommendation-systems",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "recommendation-systems",
      "social-network-analysis",
      "data-mining",
      "text-analysis"
    ],
    "canonical_topics": [
      "recommendation-systems",
      "consumer-behavior",
      "data-engineering"
    ]
  },
  {
    "name": "Wayfair Search (WANDS)",
    "description": "233k human-annotated query-product judgments, 43k products",
    "category": "E-Commerce",
    "url": "https://github.com/wayfair/WANDS",
    "docs_url": null,
    "github_url": "https://github.com/wayfair/WANDS",
    "tags": [
      "search relevance",
      "annotations",
      "furniture"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "search-relevance"
    ],
    "summary": "The Wayfair Search (WANDS) dataset consists of 233k human-annotated query-product judgments across 43k products, providing a rich resource for analyzing search relevance in the e-commerce sector. Researchers can utilize this dataset to explore consumer behavior, improve search algorithms, and enhance product recommendations.",
    "use_cases": [
      "Analyzing the effectiveness of search algorithms in e-commerce.",
      "Improving product recommendations based on consumer search behavior.",
      "Studying the relationship between query types and product relevance.",
      "Evaluating the impact of search relevance on sales performance."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the query-product judgments in the Wayfair Search dataset?",
      "How can I analyze search relevance using the WANDS dataset?",
      "What products are included in the Wayfair Search dataset?",
      "How many human annotations are present in the Wayfair Search dataset?",
      "What insights can be gained from the Wayfair Search dataset?",
      "How does the Wayfair Search dataset help in understanding consumer behavior?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/wayfair-search-wands.png",
    "embedding_text": "The Wayfair Search (WANDS) dataset is a comprehensive collection of 233,000 human-annotated query-product judgments, covering 43,000 distinct products within the e-commerce domain, specifically focusing on furniture. This dataset is structured in a tabular format, where each row represents a unique query-product pairing, and the columns include variables such as query text, product identifiers, and relevance scores assigned by human annotators. The collection methodology involved gathering search queries from users and matching them with relevant products, which were then evaluated by human annotators to ensure high-quality judgments. This rigorous annotation process enhances the dataset's reliability for research purposes. While the dataset does not specify temporal or geographic coverage, it provides a rich source of information for analyzing consumer behavior in online shopping environments. Key variables in the dataset measure the relevance of products to specific search queries, allowing researchers to explore various research questions, such as the effectiveness of different search algorithms or the impact of product attributes on search relevance. Common preprocessing steps may include cleaning the query text, normalizing product identifiers, and handling missing values. The dataset supports a range of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it a valuable resource for researchers interested in e-commerce, search relevance, and consumer behavior. Researchers typically utilize this dataset to enhance search functionalities, improve user experience, and develop better recommendation systems.",
    "tfidf_keywords": [
      "search-relevance",
      "human-annotations",
      "e-commerce",
      "consumer-behavior",
      "query-product",
      "furniture",
      "product-recommendation",
      "search-algorithms",
      "data-annotation",
      "user-experience"
    ],
    "semantic_cluster": "search-relevance-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "search-engine-optimization",
      "user-experience",
      "product-analytics",
      "recommendation-systems",
      "consumer-insights"
    ],
    "canonical_topics": [
      "machine-learning",
      "consumer-behavior",
      "recommendation-systems",
      "data-engineering"
    ]
  },
  {
    "name": "Ele.me Clickstream",
    "description": "Clickstream data from Ele.me food delivery platform",
    "category": "Food & Delivery",
    "url": "https://tianchi.aliyun.com/dataset/131047",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "clickstream",
      "food delivery",
      "user behavior"
    ],
    "best_for": "Learning food & delivery analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "user-experience"
    ],
    "summary": "The Ele.me Clickstream dataset provides insights into user interactions on the Ele.me food delivery platform. Researchers can analyze user behavior, preferences, and trends in food delivery, enabling businesses to optimize their services and marketing strategies.",
    "use_cases": [
      "Analyzing user behavior patterns on food delivery platforms",
      "Identifying popular food items and trends over time",
      "Optimizing marketing strategies based on user interactions",
      "Studying the impact of promotions on user engagement"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Ele.me Clickstream dataset?",
      "How can I access Ele.me user behavior data?",
      "What insights can be gained from Ele.me Clickstream data?",
      "How does user behavior vary on food delivery platforms?",
      "What are the trends in food delivery user interactions?",
      "How can clickstream data improve food delivery services?"
    ],
    "domain_tags": [
      "food delivery"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "embedding_text": "The Ele.me Clickstream dataset consists of structured clickstream data collected from the Ele.me food delivery platform. This dataset typically includes various rows representing individual user interactions, with columns detailing key variables such as user ID, timestamp, food items viewed or ordered, and other relevant user actions. The data is collected through the platform's user interface, capturing real-time interactions as users navigate the app or website. Coverage of the dataset encompasses a variety of user demographics, reflecting a broad spectrum of consumer behavior in the food delivery sector. Key variables in the dataset measure aspects like frequency of orders, time spent on the platform, and user engagement metrics, providing valuable insights into user preferences and trends. However, data quality may vary due to factors such as incomplete user sessions or missing data points, which researchers should be aware of when conducting analyses. Common preprocessing steps may include data cleaning to handle missing values, normalization of timestamps, and categorization of food items for more effective analysis. Researchers can leverage this dataset to address various research questions, such as understanding the factors influencing food choices, the impact of marketing campaigns on user behavior, and the overall trends in food delivery services. The dataset supports a range of analyses, including regression analysis to identify relationships between user behavior and ordering patterns, machine learning techniques for predictive modeling, and descriptive statistics to summarize user interactions. Typically, researchers utilize this dataset in studies aimed at enhancing user experience, optimizing delivery logistics, and informing marketing strategies within the food delivery industry.",
    "tfidf_keywords": [
      "clickstream",
      "user behavior",
      "food delivery",
      "consumer preferences",
      "user engagement",
      "data preprocessing",
      "behavioral analysis",
      "marketing optimization",
      "trend analysis",
      "user demographics"
    ],
    "semantic_cluster": "user-behavior-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "consumer-behavior",
      "data-analytics",
      "user-experience",
      "marketing-strategies",
      "behavioral-economics"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "data-engineering",
      "product-analytics"
    ]
  },
  {
    "name": "CodaLab",
    "description": "Platform for competitions, benchmarks, and reproducible research",
    "category": "Data Portals",
    "url": "https://codalab.lisn.upsaclay.fr/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "benchmarks",
      "reproducibility",
      "competitions"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "CodaLab is a platform designed to facilitate competitions, benchmarks, and reproducible research in various fields. It allows users to create, share, and participate in competitions, providing a structured environment for testing algorithms and methodologies against standardized benchmarks.",
    "use_cases": [],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is CodaLab?",
      "How can I participate in competitions on CodaLab?",
      "What types of benchmarks are available on CodaLab?",
      "How does CodaLab support reproducible research?",
      "What features does CodaLab offer for researchers?",
      "Can I create my own competition on CodaLab?",
      "What are the benefits of using CodaLab for benchmarks?",
      "What types of data can be used in CodaLab competitions?"
    ],
    "domain_tags": [
      "research",
      "technology"
    ],
    "size_category": "medium",
    "benchmark_usage": [
      "facilitating competitions",
      "standardizing benchmarks",
      "supporting reproducibility"
    ],
    "model_score": 0.0,
    "embedding_text": "CodaLab is an innovative platform that serves as a hub for competitions, benchmarks, and reproducible research, primarily aimed at researchers and practitioners in various fields. The platform is structured to enable users to create and participate in competitions that challenge participants to develop algorithms or methodologies that can solve specific problems or tasks. CodaLab's architecture supports a variety of data structures and formats, allowing for flexibility in the types of challenges that can be hosted. Users can upload datasets, define evaluation metrics, and set rules for participation, fostering an environment conducive to rigorous testing and validation of research hypotheses. The platform emphasizes reproducibility, a critical aspect of scientific research, by providing tools that ensure that results can be replicated by others. This is achieved through standardized benchmarks that allow different approaches to be compared fairly. The data collected from competitions can be analyzed to draw insights into the effectiveness of various methods, making CodaLab a valuable resource for both academic and industry researchers. The platform's user-friendly interface and comprehensive documentation make it accessible to a wide audience, from curious learners to seasoned data scientists. Researchers typically use CodaLab to validate their models against established benchmarks, explore new methodologies, and contribute to the broader research community by sharing their findings and insights. Common preprocessing steps may include data cleaning, normalization, and feature extraction, which are essential for ensuring high-quality inputs for the algorithms being tested. While CodaLab provides a robust framework for conducting research, users should be aware of potential limitations, such as the variability in data quality across different competitions and the need for careful interpretation of results. Overall, CodaLab represents a significant advancement in the quest for reproducible research and robust benchmarking in the tech and research community.",
    "data_modality": "mixed",
    "tfidf_keywords": [
      "competitions",
      "benchmarks",
      "reproducibility",
      "algorithm-testing",
      "data-validation",
      "research-methodologies",
      "standardized-evaluation",
      "data-structures",
      "user-participation",
      "evaluation-metrics"
    ],
    "semantic_cluster": "research-benchmarking",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "algorithm-development",
      "data-science",
      "machine-learning",
      "research-validation",
      "performance-evaluation"
    ],
    "canonical_topics": [
      "experimentation",
      "machine-learning",
      "statistics",
      "data-engineering"
    ]
  },
  {
    "name": "Stanford GSB Experiment Collection",
    "description": "Datasets for experimentation analysis from Stanford Graduate School of Business",
    "category": "Education",
    "url": "https://github.com/gsbDBI/ExperimentData",
    "docs_url": null,
    "github_url": "https://github.com/gsbDBI/ExperimentData",
    "tags": [
      "Stanford",
      "causal inference",
      "experiments"
    ],
    "best_for": "Learning education analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Stanford GSB Experiment Collection provides datasets specifically designed for experimentation analysis, allowing researchers to explore causal relationships and test hypotheses in various contexts. This collection can be utilized for conducting rigorous analyses in education and business settings.",
    "use_cases": [
      "Analyzing causal effects of educational interventions",
      "Testing hypotheses in business experiments",
      "Conducting statistical analyses on experimental data"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What datasets are available from Stanford GSB for experimentation?",
      "How can I analyze causal inference using Stanford GSB datasets?",
      "What experiments are included in the Stanford GSB Experiment Collection?",
      "Where can I find datasets for business experimentation?",
      "What are the applications of Stanford GSB datasets in education?",
      "How to access Stanford GSB Experiment Collection datasets?"
    ],
    "domain_tags": [
      "education"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/stanford-gsb-experiment-collection.png",
    "embedding_text": "The Stanford GSB Experiment Collection is a valuable resource for researchers interested in experimentation analysis, particularly in the fields of education and business. This collection includes a variety of datasets that facilitate the exploration of causal relationships through rigorous experimental designs. The datasets are structured in a tabular format, typically comprising rows representing individual observations or experimental units and columns that capture various variables of interest. Key variables may include treatment indicators, outcome measures, and demographic information, depending on the specific dataset. Researchers can leverage these datasets to address a range of research questions, such as evaluating the effectiveness of educational programs or understanding consumer behavior in business contexts. The collection is particularly useful for conducting regression analyses, machine learning applications, and descriptive statistics. Common preprocessing steps may involve cleaning the data, handling missing values, and transforming variables to meet the assumptions of statistical models. While the datasets are curated for quality, researchers should be aware of potential limitations, such as sample size constraints or biases in experimental design. Overall, the Stanford GSB Experiment Collection serves as a foundational tool for those looking to engage in empirical research and contribute to the understanding of causal inference in various domains.",
    "tfidf_keywords": [
      "causal-inference",
      "experimentation",
      "statistical-analysis",
      "educational-interventions",
      "business-experiments",
      "treatment-effects",
      "regression-analysis",
      "data-collection",
      "hypothesis-testing",
      "empirical-research"
    ],
    "semantic_cluster": "experiment-analysis-education",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "causal-inference",
      "experimentation",
      "statistical-analysis",
      "hypothesis-testing",
      "treatment-effects"
    ],
    "canonical_topics": [
      "causal-inference",
      "experimentation",
      "statistics",
      "consumer-behavior",
      "education"
    ]
  },
  {
    "name": "Natural Driving in Ohio",
    "description": "ADAS-equipped vehicles with driving behavior events",
    "category": "Transportation & Mobility",
    "url": "https://data.transportation.gov/Automobiles/Advanced-Driver-Assistance-System-ADAS-Equipped-Si/iie8-uenj/about_data",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "driving",
      "ADAS",
      "behavior",
      "government"
    ],
    "best_for": "Learning transportation & mobility analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The dataset 'Natural Driving in Ohio' contains information on ADAS-equipped vehicles and their driving behavior events. It can be used to analyze driving patterns, assess the effectiveness of advanced driver-assistance systems, and inform government policies related to transportation safety.",
    "use_cases": [
      "Analyzing the impact of ADAS on driving safety",
      "Evaluating driving behavior trends in Ohio",
      "Informing government transportation policies",
      "Studying the correlation between ADAS features and driving events"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the driving behavior events captured in ADAS-equipped vehicles?",
      "How does ADAS impact driving patterns in Ohio?",
      "What government policies relate to driving behavior and ADAS?",
      "What data is available on vehicle safety in Ohio?",
      "How can driving behavior data inform transportation policy?",
      "What are the implications of ADAS technology on road safety?"
    ],
    "domain_tags": [
      "transportation",
      "government"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Ohio",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/logos/transportation.png",
    "embedding_text": "The 'Natural Driving in Ohio' dataset provides a comprehensive view of driving behavior events associated with advanced driver-assistance systems (ADAS) in Ohio. It is structured in a tabular format, containing rows that represent individual driving events and columns that detail various attributes of these events, such as vehicle speed, braking patterns, and the specific ADAS features engaged during each event. The data is collected from a variety of ADAS-equipped vehicles, ensuring a diverse representation of driving behaviors across different vehicle types. Researchers can leverage this dataset to explore a range of research questions, such as the effectiveness of ADAS in preventing accidents, the variability of driving behavior under different conditions, and the overall impact of technology on road safety. Common preprocessing steps may include data cleaning to handle missing values, normalization of driving metrics, and categorization of driving events based on severity. The dataset supports various types of analyses, including regression analysis to identify factors influencing driving behavior, machine learning models to predict driving events, and descriptive statistics to summarize driving patterns. Researchers typically use this dataset to inform studies on transportation safety, evaluate the performance of ADAS technologies, and contribute to policy discussions aimed at improving road safety in Ohio. However, users should be aware of potential limitations, such as the representativeness of the sample and the specific contexts in which the data was collected, which may affect the generalizability of findings.",
    "tfidf_keywords": [
      "ADAS",
      "driving behavior",
      "vehicle safety",
      "transportation policy",
      "driving events",
      "Ohio",
      "advanced driver-assistance systems",
      "data analysis",
      "machine learning",
      "regression analysis",
      "safety metrics",
      "driving patterns",
      "data collection",
      "event categorization"
    ],
    "semantic_cluster": "transportation-safety-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "transportation-safety",
      "machine-learning",
      "data-analysis",
      "policy-evaluation",
      "behavioral-economics"
    ],
    "canonical_topics": [
      "causal-inference",
      "machine-learning",
      "policy-evaluation",
      "consumer-behavior",
      "transportation"
    ]
  },
  {
    "name": "Prosper Loan Data",
    "description": "113K P2P loans with borrower characteristics and outcomes",
    "category": "Financial Services",
    "url": "https://www.kaggle.com/datasets/henryokam/prosper-loan-data",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "P2P",
      "lending",
      "loans"
    ],
    "best_for": "Learning financial services analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "P2P",
      "lending",
      "loans"
    ],
    "summary": "The Prosper Loan Data consists of 113,000 peer-to-peer loans, providing insights into borrower characteristics and loan outcomes. This dataset can be utilized for various analyses, including understanding lending patterns, borrower behavior, and loan performance metrics.",
    "use_cases": [
      "Analyzing borrower demographics and their impact on loan performance",
      "Predicting loan defaults using machine learning techniques",
      "Examining trends in peer-to-peer lending over time",
      "Evaluating the effectiveness of different lending strategies"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the characteristics of borrowers in the Prosper Loan Data?",
      "How do loan outcomes vary by borrower demographics?",
      "What trends can be observed in P2P lending over time?",
      "How does loan amount affect repayment rates?",
      "What factors influence borrower default rates?",
      "How can machine learning be applied to predict loan outcomes?",
      "What are the common characteristics of successful loans?",
      "How does the Prosper Loan Data compare to traditional lending data?"
    ],
    "domain_tags": [
      "financial services"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/prosper-loan-data.jpg",
    "embedding_text": "The Prosper Loan Data is a comprehensive dataset that encompasses 113,000 peer-to-peer loans, detailing various borrower characteristics and the outcomes associated with these loans. This dataset is structured in a tabular format, with rows representing individual loans and columns capturing key variables such as loan amount, interest rate, borrower credit score, employment status, and repayment status. The data is collected from the Prosper platform, a leading peer-to-peer lending marketplace, which connects borrowers with individual and institutional investors. The dataset provides a rich source of information for researchers and analysts interested in the financial services sector, particularly in the realm of alternative lending. Key variables in the dataset include borrower characteristics such as income, credit history, and loan purpose, which are essential for understanding the factors that influence loan performance and borrower behavior. The dataset is particularly valuable for addressing research questions related to lending patterns, borrower demographics, and the effectiveness of peer-to-peer lending compared to traditional financial institutions. Common preprocessing steps may include handling missing values, normalizing data, and encoding categorical variables to prepare the dataset for analysis. Researchers typically use this dataset to conduct various types of analyses, including regression analysis, machine learning modeling, and descriptive statistics, to derive insights into the dynamics of peer-to-peer lending. However, users should be aware of potential limitations, such as data quality issues and the representativeness of the dataset, which may affect the generalizability of findings. Overall, the Prosper Loan Data serves as a valuable resource for exploring the evolving landscape of peer-to-peer lending and its implications for borrowers and lenders alike.",
    "tfidf_keywords": [
      "peer-to-peer lending",
      "borrower characteristics",
      "loan outcomes",
      "default prediction",
      "credit score",
      "interest rate",
      "loan amount",
      "repayment status",
      "financial services",
      "alternative lending"
    ],
    "semantic_cluster": "peer-to-peer-lending",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "financial-analysis",
      "credit-risk-assessment",
      "machine-learning",
      "data-preprocessing",
      "consumer-finance"
    ],
    "canonical_topics": [
      "finance",
      "consumer-behavior",
      "machine-learning"
    ]
  },
  {
    "name": "Prosper Loans",
    "description": "113K P2P loans with borrower characteristics, credit grades, and loan outcomes. Alternative to LendingClub for P2P lending research",
    "category": "Financial Services",
    "url": "https://www.prosper.com/plp/about/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "P2P lending",
      "loans",
      "credit",
      "fintech"
    ],
    "best_for": "Learning financial services analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "financial-services",
      "peer-to-peer-lending",
      "credit-analysis"
    ],
    "summary": "The Prosper Loans dataset consists of 113,000 peer-to-peer loans, providing insights into borrower characteristics, credit grades, and loan outcomes. This dataset serves as an alternative to LendingClub for researchers interested in analyzing P2P lending dynamics and borrower behavior.",
    "use_cases": [
      "Analyzing the impact of borrower characteristics on loan outcomes.",
      "Comparing credit grades and their predictive power in P2P lending.",
      "Investigating trends in peer-to-peer lending over time.",
      "Evaluating the performance of loans based on borrower demographics."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the characteristics of borrowers in the Prosper Loans dataset?",
      "How do credit grades affect loan outcomes in P2P lending?",
      "What insights can be drawn from analyzing 113K P2P loans?",
      "How does the Prosper Loans dataset compare to LendingClub data?",
      "What trends can be observed in peer-to-peer lending?",
      "How can borrower characteristics predict loan performance?",
      "What are the common credit grades in the Prosper Loans dataset?",
      "What factors influence the success of P2P loans?"
    ],
    "domain_tags": [
      "fintech"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/prosper-loans.png",
    "embedding_text": "The Prosper Loans dataset is a comprehensive collection of 113,000 peer-to-peer loans, offering a rich source of information for researchers and analysts interested in the dynamics of P2P lending. The dataset includes various borrower characteristics, credit grades, and loan outcomes, making it a valuable alternative to other lending datasets such as those from LendingClub. The data is structured in a tabular format, with rows representing individual loans and columns capturing key variables such as borrower income, credit score, loan amount, interest rate, and loan status. This structure allows for straightforward analysis using common data manipulation and statistical techniques. The dataset was collected through Prosper's online lending platform, which connects borrowers with investors, and reflects a diverse range of borrower profiles and loan types.\n\nKey variables in the dataset include borrower characteristics such as income, employment status, and credit score, which are crucial for understanding the factors that influence loan approval and repayment. Additionally, the dataset captures loan-specific details such as the amount borrowed, interest rates, and repayment status, providing a comprehensive view of the lending process. Researchers can utilize this dataset to address various research questions, such as the relationship between borrower characteristics and loan outcomes, the effectiveness of credit grading systems, and the overall trends in peer-to-peer lending.\n\nCommon preprocessing steps may include handling missing values, normalizing data, and encoding categorical variables to prepare the dataset for analysis. The dataset supports a variety of analytical approaches, including regression analysis, machine learning models, and descriptive statistics, enabling researchers to derive insights into borrower behavior and lending patterns. Overall, the Prosper Loans dataset is an essential resource for those looking to explore the evolving landscape of peer-to-peer lending and its implications for the financial services industry.",
    "tfidf_keywords": [
      "peer-to-peer lending",
      "borrower characteristics",
      "credit grades",
      "loan outcomes",
      "P2P loans",
      "fintech",
      "loan performance",
      "credit risk",
      "demographic analysis",
      "lending dynamics"
    ],
    "semantic_cluster": "peer-to-peer-lending",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "credit risk",
      "financial analysis",
      "loan performance",
      "demographic analysis",
      "P2P lending dynamics"
    ],
    "canonical_topics": [
      "finance",
      "consumer-behavior",
      "econometrics"
    ]
  },
  {
    "name": "DiDi GAIA Open Data",
    "description": "Billions of GPS points and ride trajectories from China's largest ride-hailing platform. Driver behavior and urban mobility patterns",
    "category": "Transportation & Mobility",
    "url": "https://gaia.didichuxing.com/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "rideshare",
      "GPS",
      "trajectories",
      "China",
      "DiDi",
      "mobility"
    ],
    "best_for": "Learning transportation & mobility analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "transportation",
      "urban-mobility",
      "data-analysis"
    ],
    "summary": "The DiDi GAIA Open Data consists of billions of GPS points and ride trajectories collected from China's largest ride-hailing platform, DiDi. This dataset provides insights into driver behavior and urban mobility patterns, allowing researchers to analyze transportation trends and improve urban planning.",
    "use_cases": [
      "Analyzing urban mobility patterns in major Chinese cities",
      "Studying the impact of ride-hailing services on traffic congestion",
      "Examining driver behavior and its implications for safety",
      "Evaluating the effectiveness of transportation policies"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the ride patterns in urban China?",
      "How does driver behavior vary across different cities in China?",
      "What insights can be drawn from DiDi's GPS data?",
      "How can urban mobility be improved using ride-hailing data?",
      "What are the trends in ride-sharing usage in China?",
      "How does DiDi's data reflect changes in urban transportation?"
    ],
    "domain_tags": [
      "transportation",
      "urban-planning"
    ],
    "data_modality": "tabular",
    "geographic_scope": "China",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/logos/didichuxing.png",
    "embedding_text": "The DiDi GAIA Open Data is a comprehensive dataset that captures billions of GPS points and ride trajectories from DiDi, China's largest ride-hailing platform. This dataset is structured in a tabular format, with rows representing individual ride instances and columns detailing various attributes such as pickup and drop-off locations, timestamps, and ride durations. The collection methodology involves aggregating real-time GPS data from DiDi's extensive network of drivers, providing a rich source of information for researchers interested in urban mobility and transportation dynamics. While the dataset covers a wide range of geographic areas within China, specific temporal coverage is not explicitly mentioned, which may limit certain longitudinal analyses. Key variables in the dataset include GPS coordinates, ride timestamps, and possibly driver identification numbers, which can be used to measure and analyze patterns in ride-sharing behavior and urban traffic flow. However, researchers should be aware of potential data quality issues, such as missing values or inaccuracies in GPS readings, which may necessitate common preprocessing steps like data cleaning and normalization. The dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it a versatile resource for addressing research questions related to transportation efficiency, urban planning, and the socio-economic impacts of ride-hailing services. Researchers typically utilize this dataset to explore the implications of ride-sharing on urban infrastructure, assess the effectiveness of transportation policies, and derive insights into consumer behavior in the context of mobility services.",
    "tfidf_keywords": [
      "ride-hailing",
      "GPS-data",
      "urban-mobility",
      "driver-behavior",
      "transportation-patterns",
      "data-collection-methodology",
      "ride-trajectory",
      "traffic-congestion",
      "urban-planning",
      "data-quality",
      "preprocessing",
      "socio-economic-impact",
      "machine-learning",
      "regression-analysis"
    ],
    "semantic_cluster": "urban-mobility-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "urban-planning",
      "transportation-economics",
      "data-analysis",
      "mobility-patterns",
      "ride-sharing",
      "geospatial-analysis",
      "traffic-flow",
      "consumer-behavior"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "transportation",
      "machine-learning",
      "data-engineering"
    ]
  },
  {
    "name": "Hashed Multimodal Banking",
    "description": "Banking transactions and product purchases with hashed identifiers",
    "category": "Financial Services",
    "url": "https://github.com/dzhambo/mbd",
    "docs_url": null,
    "github_url": "https://github.com/dzhambo/mbd",
    "tags": [
      "banking",
      "transactions",
      "privacy-preserving"
    ],
    "best_for": "Learning financial services analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "financial-services",
      "privacy-preserving",
      "consumer-behavior"
    ],
    "summary": "The Hashed Multimodal Banking dataset contains banking transactions and product purchases identified by hashed identifiers, ensuring privacy while enabling analysis of consumer behavior and financial services. Researchers can utilize this dataset to explore trends in banking transactions and the impact of product purchases on consumer spending.",
    "use_cases": [
      "Analyzing consumer spending patterns",
      "Evaluating the impact of product purchases on banking transactions",
      "Studying privacy-preserving techniques in financial datasets"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the trends in banking transactions?",
      "How do product purchases affect consumer spending?",
      "What insights can be drawn from hashed banking data?",
      "How can privacy-preserving techniques be applied in financial datasets?",
      "What patterns exist in consumer behavior related to banking?",
      "How can transaction data inform financial service improvements?"
    ],
    "domain_tags": [
      "fintech"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/hashed-multimodal-banking.png",
    "embedding_text": "The Hashed Multimodal Banking dataset is a comprehensive collection of banking transactions and product purchases, each associated with hashed identifiers to ensure privacy and security. This dataset is structured in a tabular format, consisting of rows representing individual transactions and purchases, and columns detailing various attributes such as transaction amount, date, product type, and hashed identifiers. The collection methodology involves aggregating transaction data from financial institutions while implementing hashing techniques to anonymize sensitive information. While the dataset does not specify temporal or geographic coverage, it is designed to facilitate analyses that explore consumer behavior and financial trends. Key variables within the dataset include transaction amounts, product categories, and timestamps, which collectively measure consumer spending patterns and purchasing decisions. Researchers may encounter data quality limitations, particularly concerning the completeness of transaction records or potential biases in the data collection process. Common preprocessing steps may include data cleaning, normalization of transaction amounts, and the conversion of categorical variables into numerical formats for analysis. This dataset supports a variety of research questions, such as examining the relationship between product purchases and banking transactions, identifying trends in consumer spending, and assessing the effectiveness of privacy-preserving techniques in financial data analysis. Analysts can employ methods ranging from regression analysis to machine learning algorithms and descriptive statistics to derive insights from this dataset. Researchers typically utilize the Hashed Multimodal Banking dataset to inform studies on consumer behavior, evaluate the impact of financial products, and explore the implications of privacy in financial transactions.",
    "tfidf_keywords": [
      "hashed-identifiers",
      "consumer-spending",
      "financial-services",
      "privacy-preserving",
      "transaction-analysis",
      "data-anonymization",
      "banking-transactions",
      "product-purchases",
      "data-aggregation",
      "consumer-behavior"
    ],
    "semantic_cluster": "privacy-in-financial-data",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "data-anonymization",
      "consumer-behavior",
      "financial-analysis",
      "transaction-data",
      "privacy-preserving-techniques"
    ],
    "canonical_topics": [
      "finance",
      "consumer-behavior",
      "data-engineering"
    ]
  },
  {
    "name": "Google BigQuery Crypto",
    "description": "8 complete blockchain histories (Bitcoin, Ethereum, etc.) with daily updates. Transaction-level data for crypto analytics research",
    "category": "Financial Services",
    "url": "https://cloud.google.com/bigquery/public-data",
    "docs_url": "https://cloud.google.com/blog/products/data-analytics/introducing-six-new-cryptocurrencies-in-bigquery-public-datasets",
    "github_url": null,
    "tags": [
      "blockchain",
      "cryptocurrency",
      "Bitcoin",
      "Ethereum",
      "BigQuery"
    ],
    "best_for": "Learning financial services analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Google BigQuery Crypto dataset provides comprehensive blockchain histories for major cryptocurrencies such as Bitcoin and Ethereum, updated daily. This dataset enables researchers to conduct in-depth crypto analytics and explore transaction-level data for various research inquiries.",
    "use_cases": [
      "Analyzing transaction trends over time for Bitcoin and Ethereum.",
      "Comparing transaction volumes between different cryptocurrencies.",
      "Conducting research on the impact of blockchain technology in financial services."
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the daily transaction trends in Bitcoin?",
      "How does Ethereum's transaction volume compare to Bitcoin?",
      "What insights can be derived from blockchain data for financial services?",
      "How can I analyze cryptocurrency transaction patterns using BigQuery?",
      "What are the historical price movements of Bitcoin and Ethereum?",
      "How do blockchain transactions impact market behavior?"
    ],
    "domain_tags": [
      "financial services"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/google-bigquery-crypto.png",
    "embedding_text": "The Google BigQuery Crypto dataset is a rich resource for researchers interested in blockchain technology and cryptocurrency analytics. It encompasses complete blockchain histories for major cryptocurrencies, including Bitcoin and Ethereum, and is updated daily to reflect the latest transaction data. This dataset is structured in a tabular format, with rows representing individual transactions and columns containing key variables such as transaction IDs, timestamps, amounts, and associated addresses. The data is collected from the respective blockchain networks, ensuring that it is comprehensive and accurate. Researchers can leverage this dataset to explore a variety of research questions, such as analyzing transaction trends, understanding the dynamics of cryptocurrency markets, and examining the implications of blockchain technology on financial services. Common preprocessing steps may include data cleaning, normalization, and the transformation of raw blockchain data into a more analyzable format. The dataset supports various types of analyses, including regression, machine learning, and descriptive statistics, making it a versatile tool for both academic and industry research. However, users should be aware of potential limitations in data quality, such as missing values or discrepancies in transaction records, which may arise from the decentralized nature of blockchain technology. Overall, the Google BigQuery Crypto dataset serves as a foundational resource for those looking to delve into the world of cryptocurrencies and blockchain analytics.",
    "tfidf_keywords": [
      "blockchain",
      "cryptocurrency",
      "Bitcoin",
      "Ethereum",
      "transaction-level data",
      "crypto analytics",
      "BigQuery",
      "financial services",
      "data structure",
      "data quality",
      "preprocessing",
      "temporal analysis",
      "market behavior",
      "decentralized networks"
    ],
    "semantic_cluster": "blockchain-analytics",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "financial-technology",
      "data-analysis",
      "market-trends",
      "cryptoeconomics",
      "transaction-analysis"
    ],
    "canonical_topics": [
      "finance",
      "machine-learning",
      "data-engineering",
      "consumer-behavior"
    ]
  },
  {
    "name": "SEC EDGAR Filings",
    "description": "21M+ public company filings since 1994. 10-Ks, 8-Ks, proxy statements. Full text + structured XBRL data",
    "category": "Financial Services",
    "url": "https://www.sec.gov/cgi-bin/browse-edgar?action=getcurrent",
    "docs_url": "https://www.sec.gov/os/accessing-edgar-data",
    "github_url": "https://github.com/datasets/edgar",
    "tags": [
      "SEC",
      "corporate filings",
      "10-K",
      "disclosure",
      "large-scale"
    ],
    "best_for": "Learning financial services analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The SEC EDGAR Filings dataset contains over 21 million public company filings dating back to 1994, including 10-Ks, 8-Ks, and proxy statements. This dataset provides both full text and structured XBRL data, enabling users to analyze corporate disclosures and financial performance over time.",
    "use_cases": [
      "Analyzing trends in corporate financial disclosures over time.",
      "Comparing financial performance across different companies using 10-K filings.",
      "Examining the impact of regulatory changes on corporate reporting.",
      "Studying the relationship between corporate governance and stock performance using proxy statements."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the SEC EDGAR filings?",
      "How can I access public company filings?",
      "What types of documents are included in the SEC EDGAR dataset?",
      "How can I analyze 10-K reports using this dataset?",
      "What is the significance of 8-K filings in corporate governance?",
      "How can structured XBRL data be utilized for financial analysis?",
      "What trends can be observed in corporate disclosures since 1994?"
    ],
    "domain_tags": [
      "financial services"
    ],
    "data_modality": "mixed",
    "temporal_coverage": "1994-present",
    "size_category": "massive",
    "model_score": 0.0,
    "image_url": "/images/datasets/sec-edgar-filings.png",
    "embedding_text": "The SEC EDGAR Filings dataset is a comprehensive collection of over 21 million public company filings that have been made available since 1994. This dataset includes a variety of document types, such as 10-Ks, 8-Ks, and proxy statements, which are essential for understanding corporate financial health and governance. The data is structured in both full text and XBRL formats, allowing for detailed financial analysis and reporting. Researchers and analysts can leverage this dataset to explore a wide range of research questions, such as the impact of regulatory changes on corporate disclosures, trends in financial reporting practices, and the relationship between governance structures and firm performance. The dataset is organized into rows and columns, with key variables measuring financial metrics, corporate actions, and compliance with regulatory requirements. Common preprocessing steps may include text normalization, extraction of relevant financial metrics, and conversion of XBRL data into usable formats for analysis. The dataset supports various types of analyses, including regression, machine learning, and descriptive statistics, making it a valuable resource for both academic research and practical applications in finance and corporate governance. However, users should be aware of potential data quality issues, such as inconsistencies in reporting practices across different companies and changes in regulatory requirements over time. Overall, the SEC EDGAR Filings dataset is an indispensable tool for anyone interested in the financial services sector, providing insights into corporate behavior and the evolving landscape of public company reporting.",
    "benchmark_usage": [
      "Common uses include financial analysis, corporate governance research, and regulatory compliance studies."
    ],
    "tfidf_keywords": [
      "SEC",
      "EDGAR",
      "10-K",
      "8-K",
      "proxy statements",
      "XBRL",
      "corporate governance",
      "financial disclosures",
      "public company filings",
      "regulatory compliance",
      "financial analysis",
      "data structure",
      "temporal trends",
      "data quality",
      "reporting practices"
    ],
    "semantic_cluster": "financial-disclosure-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "corporate governance",
      "financial reporting",
      "regulatory compliance",
      "data analysis",
      "financial metrics"
    ],
    "canonical_topics": [
      "finance",
      "econometrics",
      "data-engineering"
    ]
  },
  {
    "name": "Computational Neuroscience",
    "description": "Experimental data from neural recordings and behavior",
    "category": "Education",
    "url": "https://crcns.org/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "neuroscience",
      "neural data",
      "behavior"
    ],
    "best_for": "Learning education analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Computational Neuroscience dataset includes experimental data derived from neural recordings and associated behavioral observations. Researchers can utilize this dataset to explore the relationships between neural activity and behavioral outcomes, potentially leading to insights in both neuroscience and psychology.",
    "use_cases": [
      "Analyzing the correlation between neural activity and behavior",
      "Investigating the impact of specific stimuli on neural responses",
      "Exploring behavioral patterns in relation to neural data"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the relationship between neural recordings and behavior?",
      "How can neural data inform our understanding of behavioral patterns?",
      "What experimental data is available in computational neuroscience?",
      "What insights can be gained from analyzing neural data?",
      "How does neural activity correlate with specific behaviors?",
      "What methodologies are used in computational neuroscience research?"
    ],
    "domain_tags": [
      "healthcare"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/logos/crcns.png",
    "embedding_text": "The Computational Neuroscience dataset encompasses a rich collection of experimental data that captures neural recordings alongside behavioral observations. This dataset is structured to include various rows and columns that represent different experimental trials, neural activity measurements, and behavioral responses. The collection methodology typically involves recording neural signals using advanced techniques such as electrophysiology or functional imaging, while behavioral data may be gathered through controlled experiments or observational studies. Coverage of the dataset may vary, but it is primarily focused on capturing the dynamic interplay between neural processes and behavior. Key variables within the dataset include measures of neural activity (e.g., firing rates, signal amplitudes) and behavioral metrics (e.g., response times, accuracy), which together facilitate a comprehensive analysis of how neural mechanisms underpin behavioral outcomes. Researchers often face challenges related to data quality, such as noise in neural recordings or variability in behavioral responses, which necessitates careful preprocessing steps like filtering and normalization. The dataset supports a range of analyses, including regression modeling, machine learning approaches, and descriptive statistics, enabling researchers to address critical research questions about the neural basis of behavior. Common applications of this dataset include investigating the effects of specific stimuli on neural responses, understanding the neural correlates of decision-making processes, and exploring the mechanisms of learning and memory. Overall, the Computational Neuroscience dataset serves as a valuable resource for advancing our understanding of the intricate connections between neural activity and behavior.",
    "tfidf_keywords": [
      "neural recordings",
      "behavioral observations",
      "electrophysiology",
      "functional imaging",
      "neural activity",
      "response times",
      "data preprocessing",
      "machine learning",
      "regression modeling",
      "decision-making processes"
    ],
    "semantic_cluster": "neuroscience-behavior-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "neural-networks",
      "behavioral-science",
      "cognitive-psychology",
      "experimental-design",
      "data-collection-methods"
    ],
    "canonical_topics": [
      "healthcare",
      "machine-learning",
      "statistics"
    ]
  },
  {
    "name": "Baidu AI Datasets",
    "description": "AI, NLP, computer vision, and autonomous driving datasets",
    "category": "Data Portals",
    "url": "https://aistudio.baidu.com/datasetoverview",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Baidu",
      "NLP",
      "computer vision",
      "autonomous"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "AI",
      "NLP",
      "computer vision",
      "autonomous driving"
    ],
    "summary": "The Baidu AI Datasets encompass a wide range of datasets focused on artificial intelligence applications, including natural language processing, computer vision, and autonomous driving. Researchers and developers can utilize these datasets to train models, conduct experiments, and enhance their understanding of AI technologies.",
    "use_cases": [
      "Training machine learning models for NLP tasks",
      "Developing computer vision applications",
      "Conducting research in autonomous driving technology"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What datasets does Baidu provide for AI research?",
      "How can I access Baidu's NLP datasets?",
      "Are there datasets available for computer vision from Baidu?",
      "What autonomous driving datasets are offered by Baidu?",
      "Where can I find AI datasets for machine learning?",
      "What types of data does Baidu's AI datasets include?"
    ],
    "domain_tags": [
      "technology",
      "transportation"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/logos/baidu.png",
    "embedding_text": "The Baidu AI Datasets provide a comprehensive collection of datasets tailored for various applications in artificial intelligence, particularly in the fields of natural language processing (NLP), computer vision, and autonomous driving. These datasets are structured to support a range of research and development activities, making them valuable resources for both academic and industry practitioners. The datasets typically include a variety of data types, such as text, images, and structured data, which can be used to train machine learning models, conduct experiments, and develop new algorithms. The collection methodology for these datasets often involves the aggregation of data from multiple sources, including web scraping, user-generated content, and partnerships with other organizations. This ensures a diverse and rich set of data that can be leveraged for various AI applications. Key variables within the datasets may include textual features for NLP tasks, image attributes for computer vision applications, and sensor data for autonomous driving research. While the datasets are designed to be comprehensive, users should be aware of potential limitations regarding data quality, such as noise, bias, and missing values, which may require preprocessing steps like data cleaning, normalization, and augmentation. Researchers typically utilize these datasets to address a wide range of research questions, from improving model accuracy to exploring new methodologies in AI. The types of analyses supported by these datasets include regression analysis, machine learning model training, and descriptive statistics, making them versatile tools for advancing the field of artificial intelligence.",
    "tfidf_keywords": [
      "natural-language-processing",
      "computer-vision",
      "autonomous-driving",
      "machine-learning",
      "data-collection",
      "dataset-aggregation",
      "data-preprocessing",
      "model-training",
      "AI-applications",
      "data-quality"
    ],
    "semantic_cluster": "ai-datasets-collection",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "machine-learning",
      "data-preprocessing",
      "computer-vision",
      "natural-language-processing",
      "autonomous-systems"
    ],
    "canonical_topics": [
      "machine-learning",
      "natural-language-processing",
      "computer-vision"
    ]
  },
  {
    "name": "MIMIC-IV",
    "description": "Gold standard for freely accessible critical care EHR data from MIT/Beth Israel. Contains 364,627 unique patients, 546,028 hospitalizations, and 94,458 ICU stays (2008-2022). Includes demographics, vitals, labs, medications, procedures, and clinical notes.",
    "category": "Healthcare Economics & Health-Tech",
    "url": "https://mimic.mit.edu/",
    "source": "MIT Laboratory for Computational Physiology",
    "type": "EHR Database",
    "access": "Free (PhysioNet credentialing required)",
    "format": "PostgreSQL/CSV",
    "tags": [
      "Healthcare",
      "EHR",
      "ICU",
      "Clinical",
      "Free"
    ],
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "MIMIC-IV is a comprehensive dataset providing critical care electronic health record (EHR) data, encompassing a wide array of patient information including demographics, vital signs, laboratory results, medications, and clinical notes. Researchers can utilize this dataset for various analyses in healthcare economics, clinical research, and machine learning applications.",
    "use_cases": [
      "Analyzing patient outcomes in critical care settings.",
      "Developing predictive models for ICU admissions.",
      "Examining the impact of interventions on patient health.",
      "Studying the relationships between demographics and health outcomes."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the MIMIC-IV dataset?",
      "How can I access the MIMIC-IV EHR data?",
      "What types of analyses can be performed with MIMIC-IV?",
      "What variables are included in the MIMIC-IV dataset?",
      "How many patients are represented in the MIMIC-IV dataset?",
      "What is the time range of the MIMIC-IV dataset?",
      "What are the key features of the MIMIC-IV dataset?",
      "How is the MIMIC-IV dataset structured?"
    ],
    "update_frequency": "Periodic releases",
    "geographic_coverage": "Boston, MA (single hospital)",
    "domain_tags": [
      "healthcare"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2008-2022",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/mimic-iv.jpg",
    "embedding_text": "The MIMIC-IV dataset is a rich source of critical care electronic health record (EHR) data, meticulously curated to provide insights into patient care within intensive care units (ICUs). This dataset encompasses a total of 364,627 unique patients, 546,028 hospitalizations, and 94,458 ICU stays recorded between 2008 and 2022. The data structure is primarily tabular, consisting of numerous rows and columns that capture a diverse range of variables including patient demographics, vital signs, laboratory test results, medication prescriptions, procedures performed, and clinical notes. Each entry in the dataset corresponds to a specific patient encounter, allowing for detailed longitudinal studies of patient health trajectories. The collection methodology involved extracting data from the electronic health records of patients treated at the Beth Israel Deaconess Medical Center, ensuring a robust and comprehensive dataset that reflects real-world clinical practices. Key variables within the dataset include age, gender, ethnicity, admission and discharge times, diagnoses, and treatment regimens, which are essential for conducting various types of analyses. Researchers often utilize MIMIC-IV to address critical research questions related to patient outcomes, treatment efficacy, and healthcare resource utilization. The dataset supports a range of analytical approaches, including regression analysis, machine learning, and descriptive statistics, making it a versatile tool for both academic and clinical research. However, users should be aware of potential limitations, such as missing data and the need for preprocessing steps like normalization and handling of categorical variables. Overall, MIMIC-IV serves as a foundational resource for advancing knowledge in healthcare economics and improving patient care through data-driven insights.",
    "tfidf_keywords": [
      "critical-care",
      "EHR",
      "ICU",
      "patient-outcomes",
      "longitudinal-studies",
      "predictive-modeling",
      "healthcare-resource-utilization",
      "demographics",
      "clinical-notes",
      "medication-prescriptions",
      "data-preprocessing",
      "vital-signs",
      "laboratory-results",
      "treatment-efficacy"
    ],
    "semantic_cluster": "healthcare-data-analysis",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "healthcare-economics",
      "clinical-research",
      "machine-learning",
      "data-analytics",
      "patient-care"
    ],
    "canonical_topics": [
      "healthcare",
      "machine-learning",
      "statistics"
    ]
  },
  {
    "name": "Indian Automobiles (Telangana)",
    "description": "Vehicle sales data for Telangana, India (2023)",
    "category": "Automotive",
    "url": "https://www.kaggle.com/datasets/zubairatha/revving-up-telangana-vehicle-sales-2023",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "vehicles",
      "India",
      "regional sales"
    ],
    "best_for": "Learning automotive analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [
      "pandas-dataframes"
    ],
    "topic_tags": [
      "automotive",
      "regional-sales"
    ],
    "summary": "The Indian Automobiles dataset provides comprehensive vehicle sales data specifically for the Telangana region in India for the year 2023. This dataset can be utilized for various analyses, including understanding market trends, consumer preferences, and regional sales performance in the automotive sector.",
    "use_cases": [
      "Analyzing trends in vehicle sales over time.",
      "Comparing sales performance of different vehicle types.",
      "Assessing the impact of economic conditions on vehicle sales.",
      "Identifying consumer preferences in the automotive market."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the vehicle sales trends in Telangana for 2023?",
      "How do regional sales of automobiles in Telangana compare to other states in India?",
      "What types of vehicles are most popular in Telangana based on sales data?",
      "How can I analyze the impact of economic factors on vehicle sales in Telangana?",
      "What demographic factors influence vehicle purchases in Telangana?",
      "How does the sales data vary across different vehicle categories in Telangana?"
    ],
    "domain_tags": [
      "automotive"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2023",
    "geographic_scope": "Telangana, India",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/indian-automobiles-telangana.png",
    "embedding_text": "The Indian Automobiles dataset is a rich source of vehicle sales data specifically focused on the Telangana region of India for the year 2023. This dataset is structured in a tabular format, containing rows that represent individual vehicle sales transactions and columns that capture various attributes such as vehicle type, sales volume, price, and possibly demographic information of the buyers. The data collection methodology typically involves aggregating sales records from dealerships, manufacturers, and possibly government databases that track vehicle registrations. Coverage is limited to the year 2023, providing a snapshot of the automotive market during this period, and is geographically confined to Telangana, allowing for region-specific analysis. Key variables in the dataset may include vehicle make and model, sales figures, pricing information, and buyer demographics, which can be instrumental in measuring market demand and consumer behavior. However, potential limitations may arise from data quality issues, such as incomplete records or inconsistencies in reporting across different sources. Common preprocessing steps might involve cleaning the data to handle missing values, normalizing price data, and categorizing vehicle types for analysis. Researchers can utilize this dataset to address various questions, such as identifying trends in vehicle sales, understanding the impact of economic factors on purchasing decisions, and analyzing consumer preferences based on demographic data. The dataset supports a range of analyses, including regression analysis to identify relationships between variables, descriptive statistics to summarize sales performance, and machine learning techniques for predictive modeling. Researchers typically leverage this dataset in studies focused on market analysis, consumer behavior, and economic impact assessments within the automotive sector.",
    "tfidf_keywords": [
      "vehicle-sales",
      "Telangana",
      "automotive-market",
      "consumer-preferences",
      "sales-trends",
      "regional-analysis",
      "market-demand",
      "demographic-factors",
      "data-collection-methodology",
      "price-analysis",
      "sales-volume",
      "vehicle-types",
      "economic-factors",
      "data-quality",
      "preprocessing-steps"
    ],
    "semantic_cluster": "automotive-market-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "market-analysis",
      "sales-forecasting",
      "data-preprocessing",
      "regression-analysis"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "econometrics",
      "statistics",
      "data-engineering"
    ]
  },
  {
    "name": "PEP Experimental Research",
    "description": "Experimental research datasets from Partnership for Economic Policy",
    "category": "Education",
    "url": "https://www.pep-net.org/publications/datasets/experimental-research-datasets",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "development",
      "RCT",
      "policy"
    ],
    "best_for": "Learning education analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The PEP Experimental Research dataset comprises experimental research datasets provided by the Partnership for Economic Policy. It is designed to facilitate analysis in the field of economic policy, particularly through randomized controlled trials (RCTs) that assess the impact of various development policies.",
    "use_cases": [
      "Evaluating the effectiveness of development policies",
      "Conducting randomized controlled trials",
      "Analyzing the impact of interventions on economic outcomes"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the experimental datasets available from PEP?",
      "How can I access the PEP Experimental Research datasets?",
      "What types of RCTs are included in the PEP dataset?",
      "What policies are evaluated in the PEP Experimental Research?",
      "How can I use PEP datasets for economic policy analysis?",
      "What is the Partnership for Economic Policy?",
      "What methodologies are used in PEP experimental research?"
    ],
    "domain_tags": [
      "policy"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "embedding_text": "The PEP Experimental Research dataset is a collection of experimental research datasets curated by the Partnership for Economic Policy, aimed at providing valuable insights into the effectiveness of various economic policies through rigorous experimentation. The dataset typically includes a variety of experimental designs, primarily focusing on randomized controlled trials (RCTs) that assess the impact of specific interventions on economic outcomes. Researchers can leverage this dataset to explore a wide range of research questions related to development policies, including the effectiveness of educational programs, health interventions, and social welfare initiatives. The data structure is generally tabular, consisting of rows representing individual observations or experimental units, and columns capturing key variables such as treatment assignments, demographic information, and outcome measures. The collection methodology often involves field experiments conducted in collaboration with local partners, ensuring that the data reflects real-world conditions and challenges. While the dataset is rich in information, researchers should be aware of potential limitations, such as sample size constraints and the generalizability of findings across different contexts. Common preprocessing steps may include data cleaning, handling missing values, and transforming variables for analysis. The dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it a versatile resource for economists and policy analysts. Researchers typically use this dataset to inform policy decisions, evaluate program effectiveness, and contribute to the broader discourse on economic development.",
    "tfidf_keywords": [
      "randomized-controlled-trials",
      "experimental-design",
      "policy-evaluation",
      "impact-assessment",
      "development-interventions",
      "field-experiments",
      "causal-inference",
      "treatment-effects",
      "economic-outcomes",
      "data-collection-methodology"
    ],
    "semantic_cluster": "policy-evaluation-methods",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "causal-inference",
      "treatment-effects",
      "experimental-design",
      "policy-analysis",
      "field-experiments"
    ],
    "canonical_topics": [
      "experimentation",
      "policy-evaluation",
      "causal-inference"
    ]
  },
  {
    "name": "Yandex Datasets",
    "description": "Search ranking, translation quality, and ML task datasets",
    "category": "Data Portals",
    "url": "https://research.yandex.com/datasets",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "search ranking",
      "translation",
      "ML",
      "Russia"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "search ranking",
      "translation",
      "machine learning"
    ],
    "summary": "The Yandex Datasets provide a collection of datasets focused on search ranking, translation quality, and various machine learning tasks. Researchers and practitioners can utilize these datasets to improve algorithms, evaluate model performance, and conduct comparative studies in natural language processing and machine learning.",
    "use_cases": [
      "Evaluating translation models using Yandex translation datasets.",
      "Improving search algorithms with Yandex search ranking data.",
      "Conducting machine learning experiments with Yandex datasets."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What datasets does Yandex provide for search ranking?",
      "How can I access Yandex datasets for translation quality?",
      "Are there machine learning datasets available from Yandex?",
      "What types of data are included in Yandex datasets?",
      "How can Yandex datasets be used in ML tasks?",
      "What is the focus of Yandex datasets in terms of data quality?"
    ],
    "domain_tags": [
      "technology",
      "data science",
      "machine learning"
    ],
    "data_modality": "mixed",
    "geographic_scope": "Russia",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/yandex-datasets.png",
    "embedding_text": "The Yandex Datasets encompass a diverse array of datasets that focus on critical areas such as search ranking, translation quality, and various machine learning tasks. These datasets are structured in a tabular format, typically consisting of rows representing individual data points and columns corresponding to different variables or features relevant to the tasks at hand. The data collection methodology employed by Yandex involves aggregating information from various sources, ensuring a rich and comprehensive dataset that can be leveraged for multiple research and practical applications. While specific temporal and demographic coverage details are not explicitly mentioned, the datasets are primarily focused on the Russian context, reflecting the linguistic and cultural nuances inherent in the data. Key variables within these datasets may include metrics for translation accuracy, search result rankings, and performance indicators for machine learning algorithms. Researchers should be aware of potential limitations in data quality, such as biases in the data collection process or the representativeness of the datasets for broader applications. Common preprocessing steps may involve cleaning the data, normalizing values, and transforming categorical variables into numerical formats suitable for analysis. The Yandex Datasets can address a variety of research questions, particularly those related to the effectiveness of translation models or the optimization of search algorithms. They support various types of analyses, including regression, machine learning model training, and descriptive statistics. Researchers typically utilize these datasets in studies aimed at enhancing algorithmic performance, conducting comparative analyses, and exploring the intricacies of language processing tasks.",
    "tfidf_keywords": [
      "search ranking",
      "translation quality",
      "machine learning tasks",
      "data collection methodology",
      "data quality",
      "algorithm performance",
      "natural language processing",
      "evaluation metrics",
      "data preprocessing",
      "comparative studies"
    ],
    "semantic_cluster": "nlp-datasets-collection",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "natural-language-processing",
      "machine-learning",
      "data-quality",
      "algorithm-evaluation",
      "data-preprocessing"
    ],
    "canonical_topics": [
      "machine-learning",
      "natural-language-processing",
      "data-engineering"
    ]
  },
  {
    "name": "Makridakis Competitions",
    "description": "Time series data for forecasting competitions (M1-M5)",
    "category": "Data Portals",
    "url": "https://www.mcompetitions.unic.ac.cy/the-dataset/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "time series",
      "forecasting",
      "M competitions"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "time series analysis",
      "forecasting techniques"
    ],
    "topic_tags": [
      "forecasting",
      "time series",
      "competitions"
    ],
    "summary": "The Makridakis Competitions dataset provides time series data specifically designed for forecasting competitions (M1-M5). Researchers and practitioners can utilize this dataset to benchmark forecasting methods, compare model performances, and enhance their understanding of time series forecasting techniques.",
    "use_cases": [
      "Benchmarking forecasting models",
      "Comparing performance of different forecasting techniques",
      "Conducting time series analysis",
      "Developing new forecasting algorithms"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the Makridakis Competitions dataset?",
      "How can I use the M1-M5 time series data for forecasting?",
      "What are the key variables in the Makridakis Competitions dataset?",
      "What forecasting techniques can be applied to this dataset?",
      "How do the M competitions compare in terms of forecasting accuracy?",
      "What preprocessing steps are needed for the Makridakis dataset?"
    ],
    "domain_tags": [
      "forecasting",
      "data science"
    ],
    "data_modality": "time-series",
    "size_category": "medium",
    "model_score": 0.0,
    "embedding_text": "The Makridakis Competitions dataset is a well-known collection of time series data specifically designed for evaluating forecasting methods through competitions, notably the M1 to M5 competitions. This dataset features a variety of time series data that researchers and practitioners can use to benchmark their forecasting models. The data structure typically includes multiple time series, each represented by various attributes such as the time index, observed values, and potentially other relevant features that could influence forecasting accuracy. The collection methodology for the Makridakis dataset involves gathering time series data from diverse sources, ensuring a rich variety of datasets that reflect different forecasting challenges. While the dataset does not specify temporal or geographic coverage, it is designed to encompass a wide range of forecasting scenarios, making it applicable to numerous research questions in the field of time series analysis. Key variables within the dataset measure observed values over time, allowing for the application of various forecasting techniques, including regression analysis and machine learning methods. However, users should be aware of potential limitations in data quality, such as missing values or outliers, which may require preprocessing steps like imputation or normalization. Researchers typically utilize this dataset to address questions related to forecasting accuracy, model comparison, and the effectiveness of different forecasting approaches. The types of analyses supported by the dataset include regression analysis, machine learning applications, and descriptive statistics, making it a versatile resource for both academic and practical applications in forecasting. Overall, the Makridakis Competitions dataset serves as a crucial tool for advancing knowledge and practices in the field of time series forecasting.",
    "benchmark_usage": [
      "Common uses include benchmarking various forecasting methods against each other."
    ],
    "tfidf_keywords": [
      "time series",
      "forecasting competitions",
      "M1-M5",
      "benchmarking",
      "forecasting accuracy",
      "regression analysis",
      "machine learning",
      "data preprocessing",
      "observed values",
      "model comparison"
    ],
    "semantic_cluster": "forecasting-competitions",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "time series analysis",
      "forecasting methods",
      "model evaluation",
      "data preprocessing",
      "performance metrics"
    ],
    "canonical_topics": [
      "forecasting",
      "machine-learning",
      "statistics"
    ]
  },
  {
    "name": "Home Depot Product Search",
    "description": "Human-rated relevance scores (1-3) for search terms and products",
    "category": "E-Commerce",
    "url": "https://www.kaggle.com/datasets/thedevastator/the-home-depot-products-dataset",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "search relevance",
      "retail",
      "Kaggle"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Home Depot Product Search dataset provides human-rated relevance scores for various search terms and products, allowing users to analyze how well products match search queries. This dataset can be utilized to improve search algorithms, enhance user experience, and conduct market research in the retail sector.",
    "use_cases": [
      "Analyzing the effectiveness of search algorithms in e-commerce.",
      "Improving product recommendations based on search relevance.",
      "Conducting market research to understand consumer search behavior."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the relevance scores for search terms related to home improvement?",
      "How do different products rank for specific search queries in Home Depot?",
      "What search terms yield the highest relevance scores for tools?",
      "How can I analyze the relationship between search terms and product categories?",
      "What trends can be identified in search relevance over time?",
      "How do user ratings correlate with product search relevance?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/home-depot-product-search.png",
    "embedding_text": "The Home Depot Product Search dataset is structured in a tabular format, consisting of rows representing individual search terms and products, with columns that include relevance scores, product identifiers, and associated search queries. The relevance scores are rated on a scale from 1 to 3, indicating the degree to which a product matches a search term. This dataset is particularly valuable for researchers and practitioners in the e-commerce sector, as it provides insights into consumer behavior and search effectiveness. The data is likely collected through user interactions on the Home Depot website, where human raters assess the relevance of products to specific search queries. While the dataset offers rich information for analysis, it may have limitations in terms of data quality, as human ratings can be subjective and may vary over time or across different raters. Common preprocessing steps may include normalizing relevance scores, handling missing values, and categorizing products based on their attributes. Researchers can use this dataset to address various questions, such as how well products align with consumer search intent, the effectiveness of different search terms, and trends in product visibility over time. The dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics, enabling users to derive actionable insights that can enhance search functionalities and improve user experience on e-commerce platforms. Overall, the Home Depot Product Search dataset serves as a crucial resource for understanding the dynamics of search relevance in the retail industry.",
    "tfidf_keywords": [
      "search relevance",
      "human-rated scores",
      "e-commerce",
      "consumer behavior",
      "product matching",
      "retail analytics",
      "search algorithms",
      "market research",
      "user experience",
      "product identifiers"
    ],
    "semantic_cluster": "search-relevance-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "search algorithms",
      "consumer behavior",
      "product analytics",
      "market research",
      "e-commerce strategies"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "recommendation-systems",
      "product-analytics"
    ]
  },
  {
    "name": "Russian Car Market",
    "description": "Car sales information in Russia",
    "category": "Automotive",
    "url": "https://www.kaggle.com/datasets/ekibee/car-sales-information",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "cars",
      "Russia",
      "sales"
    ],
    "best_for": "Learning automotive analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Russian Car Market dataset provides detailed information on car sales in Russia, including various metrics related to sales performance and consumer preferences. Researchers can utilize this dataset to analyze trends in the automotive market, understand consumer behavior, and evaluate pricing strategies.",
    "use_cases": [
      "Analyzing sales trends over time",
      "Evaluating the impact of economic factors on car sales",
      "Comparing sales performance of different car brands",
      "Understanding consumer preferences in the automotive sector"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the trends in car sales in Russia?",
      "How do consumer preferences affect car sales?",
      "What factors influence car pricing in the Russian market?",
      "What is the market share of different car brands in Russia?",
      "How has the Russian car market changed over the years?",
      "What are the demographics of car buyers in Russia?"
    ],
    "domain_tags": [
      "automotive"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Russia",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/russian-car-market.jpg",
    "embedding_text": "The Russian Car Market dataset is a comprehensive collection of car sales information specifically focused on the automotive sector in Russia. This dataset typically includes a variety of variables such as sales volume, car models, pricing, and possibly demographic information about the buyers. The data structure is likely organized in a tabular format, with rows representing individual sales transactions and columns capturing key attributes such as the make and model of the car, sale price, date of sale, and possibly the location of the sale. The collection methodology for this dataset may involve aggregating data from various sources, including dealership sales reports, market research firms, and government databases that track automotive sales. While the dataset provides valuable insights into the automotive market, researchers should be aware of potential limitations, such as data completeness and accuracy, as well as any biases introduced during data collection. Common preprocessing steps may include cleaning the data to handle missing values, normalizing price data, and categorizing car models for analysis. Researchers can use this dataset to address various research questions, such as identifying trends in car sales over time, analyzing the impact of economic conditions on consumer purchasing decisions, and evaluating the effectiveness of marketing strategies employed by car manufacturers. The types of analyses supported by this dataset include regression analysis to understand the relationship between pricing and sales volume, machine learning techniques for predictive modeling, and descriptive statistics to summarize sales performance. Overall, the Russian Car Market dataset serves as a valuable resource for researchers and analysts interested in exploring the dynamics of the automotive market in Russia.",
    "tfidf_keywords": [
      "car sales",
      "automotive market",
      "consumer preferences",
      "pricing strategies",
      "sales trends",
      "market share",
      "demographic analysis",
      "economic factors",
      "brand performance",
      "sales volume"
    ],
    "semantic_cluster": "automotive-market-analysis",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "consumer-behavior",
      "market-analysis",
      "sales-forecasting",
      "pricing-strategies",
      "economic-impact"
    ],
    "canonical_topics": [
      "consumer-behavior",
      "econometrics",
      "pricing",
      "marketplaces"
    ]
  },
  {
    "name": "National Health Interview Survey (NHIS)",
    "description": "CDC's flagship health survey covering ~35,000 households annually since 1957. Monitors health status, healthcare access, and health behaviors. Free public use files with extensive documentation.",
    "category": "Healthcare Economics & Health-Tech",
    "url": "https://www.cdc.gov/nchs/nhis/",
    "source": "CDC National Center for Health Statistics",
    "type": "Survey",
    "access": "Free public use files",
    "format": "SAS/Stata/CSV",
    "tags": [
      "Healthcare",
      "Survey",
      "Health Status",
      "Free"
    ],
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The National Health Interview Survey (NHIS) is a comprehensive health survey conducted by the CDC, capturing data from approximately 35,000 households annually since 1957. It provides insights into health status, healthcare access, and health behaviors, making it a valuable resource for researchers and policymakers in the healthcare sector.",
    "use_cases": [
      "Analyzing trends in health behaviors over time.",
      "Examining disparities in healthcare access among different demographics.",
      "Investigating the relationship between health status and socioeconomic factors."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the National Health Interview Survey?",
      "How can I access the NHIS public use files?",
      "What types of health behaviors are monitored in the NHIS?",
      "What demographic information is collected in the NHIS?",
      "How has the NHIS evolved since its inception in 1957?",
      "What are the key findings from the latest NHIS data?",
      "How does the NHIS measure healthcare access?",
      "What documentation is available for NHIS data users?"
    ],
    "update_frequency": "Annual",
    "geographic_coverage": "United States (national)",
    "domain_tags": [
      "healthcare"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/national-health-interview-survey-nhis.png",
    "embedding_text": "The National Health Interview Survey (NHIS) is a pivotal health survey conducted by the Centers for Disease Control and Prevention (CDC), which has been collecting data from approximately 35,000 households annually since its inception in 1957. This survey serves as a cornerstone for understanding the health status, healthcare access, and health behaviors of the U.S. population. The NHIS data is structured in a tabular format, with rows representing individual respondents and columns capturing various health-related variables, including demographic information, health conditions, healthcare utilization, and health behaviors. The collection methodology involves a multi-stage probability sampling design, ensuring that the survey results are representative of the U.S. population. Key variables measured in the NHIS include self-reported health status, access to healthcare services, insurance coverage, and lifestyle factors such as smoking and physical activity. While the NHIS is a rich source of health data, researchers should be aware of potential limitations, including self-reporting biases and the evolving nature of health questions over time. Common preprocessing steps for NHIS data may include data cleaning, handling missing values, and variable transformations to facilitate analysis. Researchers typically use NHIS data to address a variety of research questions related to public health, healthcare policy, and epidemiology. The dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it a versatile tool for both academic and policy-oriented research. Overall, the NHIS is an invaluable resource for anyone interested in exploring the dynamics of health and healthcare in the United States.",
    "tfidf_keywords": [
      "health survey",
      "CDC",
      "health status",
      "healthcare access",
      "health behaviors",
      "demographic information",
      "self-reported health",
      "insurance coverage",
      "public health",
      "epidemiology",
      "data collection",
      "sampling design",
      "data preprocessing",
      "regression modeling",
      "machine learning"
    ],
    "semantic_cluster": "health-survey-data",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "public health",
      "healthcare policy",
      "epidemiology",
      "socioeconomic factors",
      "health disparities"
    ],
    "canonical_topics": [
      "healthcare",
      "policy-evaluation",
      "statistics",
      "consumer-behavior"
    ]
  },
  {
    "name": "LendingClub Loans",
    "description": "2.7M loans (2007-2019) with 151 features. Interest rates, credit scores, defaults. The canonical P2P lending dataset for credit risk modeling",
    "category": "Financial Services",
    "url": "https://www.kaggle.com/datasets/wordsforthewise/lending-club",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "P2P lending",
      "credit risk",
      "loans",
      "defaults",
      "fintech"
    ],
    "best_for": "Learning financial services analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "P2P lending",
      "credit risk",
      "fintech"
    ],
    "summary": "The LendingClub Loans dataset comprises 2.7 million loans issued between 2007 and 2019, featuring 151 attributes including interest rates, credit scores, and default statuses. This dataset serves as a canonical resource for credit risk modeling, enabling researchers and analysts to explore various aspects of peer-to-peer lending and assess factors influencing loan performance.",
    "use_cases": [
      "Analyzing the impact of credit scores on loan default rates.",
      "Building predictive models for loan performance based on borrower characteristics.",
      "Exploring trends in peer-to-peer lending over time.",
      "Assessing the effectiveness of different lending strategies."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the interest rates in the LendingClub Loans dataset?",
      "How do credit scores affect loan defaults in LendingClub data?",
      "What features are included in the LendingClub Loans dataset?",
      "What trends can be observed in peer-to-peer lending from 2007 to 2019?",
      "How can I model credit risk using the LendingClub Loans dataset?",
      "What is the distribution of loan amounts in the LendingClub dataset?",
      "How does the LendingClub Loans dataset compare to other lending datasets?",
      "What preprocessing steps are needed for analyzing LendingClub loan data?"
    ],
    "domain_tags": [
      "fintech"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2007-2019",
    "size_category": "massive",
    "model_score": 0.0,
    "image_url": "/images/datasets/lendingclub-loans.jpg",
    "embedding_text": "The LendingClub Loans dataset is a comprehensive collection of 2.7 million loans issued from 2007 to 2019, providing a rich resource for researchers and practitioners in the field of financial services, particularly in peer-to-peer lending and credit risk assessment. This dataset includes 151 features that capture essential aspects of each loan, such as interest rates, credit scores, loan amounts, and default statuses. The data is structured in a tabular format, with each row representing a unique loan and each column corresponding to a specific variable related to that loan. Key variables include the borrower\u2019s credit score, which measures their creditworthiness, and the interest rate, which reflects the cost of borrowing. The dataset's extensive temporal coverage allows for the analysis of trends and changes in lending practices over more than a decade, making it a valuable asset for longitudinal studies. Researchers typically utilize this dataset to address various research questions, such as the factors influencing loan defaults, the relationship between borrower characteristics and loan performance, and the overall effectiveness of peer-to-peer lending platforms. Common preprocessing steps may include handling missing values, normalizing numerical features, and encoding categorical variables to prepare the data for analysis. The dataset supports a range of analytical techniques, including regression analysis, machine learning, and descriptive statistics, enabling users to build predictive models and derive insights into the dynamics of credit risk. However, users should be aware of potential limitations, such as the representativeness of the sample and the impact of external economic factors on loan performance. Overall, the LendingClub Loans dataset serves as a foundational resource for those interested in exploring the complexities of lending and credit risk in the modern financial landscape.",
    "benchmark_usage": [
      "Credit risk modeling",
      "Loan performance analysis"
    ],
    "tfidf_keywords": [
      "peer-to-peer lending",
      "credit risk",
      "loan defaults",
      "interest rates",
      "credit scores",
      "predictive modeling",
      "financial services",
      "data preprocessing",
      "longitudinal analysis",
      "loan performance"
    ],
    "semantic_cluster": "credit-risk-modeling",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "credit-scoring",
      "risk-assessment",
      "financial-analysis",
      "machine-learning",
      "data-preprocessing"
    ],
    "canonical_topics": [
      "finance",
      "machine-learning",
      "econometrics"
    ]
  },
  {
    "name": "Behavioral Risk Factor Surveillance System (BRFSS)",
    "description": "World's largest ongoing health survey with 400,000+ adults annually across all states. Covers chronic conditions, risk behaviors, and preventive health. State-level estimates available.",
    "category": "Healthcare Economics & Health-Tech",
    "url": "https://www.cdc.gov/brfss/",
    "source": "CDC",
    "type": "Survey",
    "access": "Free public use files",
    "format": "SAS/ASCII",
    "tags": [
      "Healthcare",
      "Survey",
      "Behavioral",
      "State-level",
      "Free"
    ],
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Behavioral Risk Factor Surveillance System (BRFSS) is the world's largest ongoing health survey, collecting data from over 400,000 adults annually across all states. It provides insights into chronic conditions, risk behaviors, and preventive health measures, allowing for state-level estimates that can inform public health policies and interventions.",
    "use_cases": [
      "Analyzing the prevalence of chronic diseases across different states.",
      "Examining the relationship between risk behaviors and health outcomes.",
      "Evaluating the effectiveness of public health interventions.",
      "Conducting longitudinal studies on health trends over time."
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Behavioral Risk Factor Surveillance System?",
      "How can I access BRFSS data for health research?",
      "What types of chronic conditions are covered in the BRFSS?",
      "What are the risk behaviors tracked by the BRFSS survey?",
      "How does BRFSS data inform public health policy?",
      "What is the methodology behind the BRFSS data collection?",
      "What demographic information is available in the BRFSS dataset?",
      "How can state-level estimates from BRFSS be used in research?"
    ],
    "update_frequency": "Annual",
    "geographic_coverage": "United States (state-level)",
    "domain_tags": [
      "healthcare"
    ],
    "data_modality": "tabular",
    "geographic_scope": "United States",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/logos/cdc.png",
    "embedding_text": "The Behavioral Risk Factor Surveillance System (BRFSS) is a comprehensive health survey that stands as the largest ongoing data collection effort in the realm of public health. It gathers information from more than 400,000 adults each year, providing a wealth of data on chronic health conditions, risk behaviors, and preventive health practices. The survey's design allows for state-level estimates, making it a vital resource for health policymakers and researchers alike. The data structure typically includes a variety of variables, such as demographic information (age, gender, race, income), health status indicators (presence of chronic conditions like diabetes or hypertension), and behavioral risk factors (smoking, alcohol consumption, physical activity levels). The collection methodology involves telephone interviews, ensuring a broad representation of the adult population across all states. However, it is important to note that while BRFSS aims for comprehensive coverage, there may be limitations in data quality, such as self-reported biases or variations in response rates across different demographic groups. Researchers often preprocess BRFSS data to handle missing values, standardize responses, and create composite variables for analysis. The dataset supports a range of analyses, including regression modeling, machine learning applications, and descriptive statistics, enabling researchers to explore various health-related research questions. Common research inquiries include the impact of socioeconomic factors on health outcomes, the effectiveness of health promotion strategies, and trends in health behaviors over time. Overall, BRFSS serves as a crucial tool for understanding public health dynamics and informing evidence-based interventions.",
    "tfidf_keywords": [
      "chronic conditions",
      "risk behaviors",
      "preventive health",
      "public health policy",
      "state-level estimates",
      "health survey methodology",
      "demographic variables",
      "self-reported data",
      "health trends",
      "longitudinal studies",
      "data preprocessing",
      "regression modeling",
      "machine learning",
      "descriptive statistics"
    ],
    "semantic_cluster": "health-survey-data",
    "content_format": "dataset",
    "depth_level": "intro",
    "related_concepts": [
      "public health",
      "epidemiology",
      "health policy",
      "behavioral health",
      "data analysis"
    ],
    "canonical_topics": [
      "healthcare",
      "policy-evaluation",
      "consumer-behavior"
    ]
  },
  {
    "name": "Statcast / Baseball Savant",
    "description": "MLB's official tracking data including pitch velocity, spin rate, exit velocity, launch angle, and player positioning from 2015-present",
    "category": "Sports & Athletics",
    "url": "https://baseballsavant.mlb.com/",
    "docs_url": "https://baseballsavant.mlb.com/csv-docs",
    "github_url": null,
    "tags": [
      "baseball",
      "tracking-data",
      "MLB",
      "pitch-tracking",
      "batted-ball"
    ],
    "best_for": "Modern baseball analytics, player evaluation, and biomechanical analysis",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "sports",
      "data-analysis"
    ],
    "summary": "The Statcast / Baseball Savant dataset provides detailed tracking data from Major League Baseball (MLB) games, including metrics such as pitch velocity, spin rate, exit velocity, launch angle, and player positioning. This rich dataset allows analysts and researchers to explore player performance, game strategies, and trends in baseball over time.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is Statcast data?",
      "How to analyze MLB pitch tracking data?",
      "What metrics are included in Baseball Savant?",
      "How does player positioning affect game outcomes?",
      "What trends can be identified in MLB tracking data?",
      "How to visualize baseball tracking data?"
    ],
    "use_cases": [
      "Analyzing player performance metrics over seasons",
      "Studying the impact of pitch types on batting outcomes",
      "Evaluating team strategies based on player positioning",
      "Identifying trends in batted-ball data"
    ],
    "domain_tags": [
      "sports",
      "analytics"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2015-present",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/statcast-baseball-savant.png",
    "embedding_text": "The Statcast / Baseball Savant dataset is a comprehensive collection of tracking data from Major League Baseball (MLB) games, capturing a wide array of metrics that are crucial for understanding player performance and game dynamics. The dataset includes variables such as pitch velocity, spin rate, exit velocity, launch angle, and player positioning, all of which are essential for in-depth analysis of baseball games. The data is structured in a tabular format, with rows representing individual plays or events during games and columns detailing various metrics associated with each play. This structure allows for straightforward manipulation and analysis using data analysis tools such as Python's pandas library or R. \n\nThe collection methodology for Statcast data involves advanced tracking technology installed in MLB stadiums, which captures high-resolution data during games. This technology utilizes a combination of radar and optical systems to track the movement of the ball and players in real-time. The resulting data is then aggregated and made available through the Baseball Savant platform, providing researchers and analysts with a rich resource for exploring baseball analytics. \n\nKey variables in this dataset measure critical aspects of gameplay. For instance, pitch velocity and spin rate provide insights into the effectiveness of pitchers, while exit velocity and launch angle are vital for evaluating hitters' performance. Player positioning data allows for the analysis of defensive strategies and their impact on game outcomes. \n\nWhile the dataset is rich in information, it is important to acknowledge potential limitations in data quality. Factors such as tracking errors, variations in data collection methods across different stadiums, and the inherent variability in player performance can affect the accuracy of analyses. Common preprocessing steps may include cleaning the data to remove outliers, normalizing metrics for comparison, and merging datasets for comprehensive analyses. \n\nResearchers typically use this dataset to address various research questions, such as evaluating the effectiveness of different pitching strategies, analyzing the impact of player performance on team success, and exploring trends in player development over time. The dataset supports a range of analyses, including regression analysis to identify relationships between variables, machine learning techniques for predictive modeling, and descriptive statistics to summarize player performance. Overall, the Statcast / Baseball Savant dataset is an invaluable resource for anyone interested in the intersection of sports and data analytics, providing a foundation for both academic research and practical applications in the field of baseball analytics.",
    "tfidf_keywords": [
      "pitch velocity",
      "spin rate",
      "exit velocity",
      "launch angle",
      "player positioning",
      "MLB analytics",
      "tracking technology",
      "data visualization",
      "performance metrics",
      "game strategy"
    ],
    "semantic_cluster": "sports-data-analytics",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "sports-analytics",
      "performance-evaluation",
      "data-visualization",
      "predictive-modeling",
      "statistical-analysis"
    ],
    "canonical_topics": [
      "statistics",
      "machine-learning",
      "data-engineering"
    ]
  },
  {
    "name": "LOBSTER (Limit Order Book System)",
    "description": "High-frequency limit order book tick data for NASDAQ stocks reconstructed from ITCH messages. Gold standard for market microstructure research.",
    "category": "Financial Services",
    "url": "https://lobsterdata.com/",
    "docs_url": "https://lobsterdata.com/info/DataStructure.php",
    "github_url": null,
    "tags": [
      "high-frequency",
      "limit-order-book",
      "NASDAQ",
      "tick-data",
      "market-microstructure"
    ],
    "best_for": "Market microstructure research, algorithmic trading backtesting, LOB simulation validation",
    "model_score": 0.0,
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "time-series-analysis",
      "market-microstructure"
    ],
    "topic_tags": [
      "financial-services",
      "high-frequency-trading",
      "data-analysis"
    ],
    "summary": "The LOBSTER dataset provides high-frequency limit order book tick data for NASDAQ stocks, reconstructed from ITCH messages. It serves as a gold standard for market microstructure research, allowing researchers to analyze trading behaviors, order dynamics, and price formation processes.",
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the LOBSTER dataset?",
      "How can I analyze NASDAQ tick data?",
      "What are the applications of limit order book data?",
      "What methodologies are used in market microstructure research?",
      "How does high-frequency trading affect market dynamics?",
      "What variables are included in the LOBSTER dataset?",
      "What preprocessing steps are needed for limit order book data?",
      "What research questions can be addressed using LOBSTER data?"
    ],
    "use_cases": [
      "Analyzing order book dynamics",
      "Studying price impact of trades",
      "Investigating market liquidity",
      "Evaluating trading strategies"
    ],
    "embedding_text": "The LOBSTER (Limit Order Book System) dataset is a comprehensive collection of high-frequency limit order book tick data specifically for NASDAQ stocks, meticulously reconstructed from ITCH messages. This dataset is recognized as a gold standard in the realm of market microstructure research, providing invaluable insights into the intricate dynamics of financial markets. The data structure typically consists of rows representing individual trades and order book updates, with columns detailing key variables such as price, volume, order type, and timestamps. This rich dataset allows researchers to explore a multitude of research questions related to trading behaviors, order dynamics, and price formation processes.\n\nThe collection methodology involves aggregating and processing ITCH messages, which capture real-time trading activity on the NASDAQ exchange. This ensures that the dataset reflects the most accurate and detailed view of market transactions, making it an essential resource for understanding high-frequency trading mechanisms. While the dataset excels in providing detailed insights, researchers should be aware of potential limitations, such as data quality issues arising from market anomalies or system errors during data collection.\n\nCommon preprocessing steps include cleaning the data to remove any erroneous entries, normalizing timestamps for consistency, and structuring the data into a format suitable for analysis. Researchers typically employ various analytical techniques, including regression analysis, machine learning algorithms, and descriptive statistics, to extract meaningful patterns and insights from the data. The LOBSTER dataset supports a wide range of analyses, from studying the impact of high-frequency trading on market liquidity to evaluating the effectiveness of different trading strategies.\n\nIn conclusion, the LOBSTER dataset stands as a pivotal resource for academics and practitioners alike, facilitating a deeper understanding of market microstructure and the factors that influence trading behavior. Its detailed and high-frequency nature allows for sophisticated analyses that can inform both theoretical research and practical trading applications.",
    "domain_tags": [
      "fintech"
    ],
    "data_modality": "time-series",
    "size_category": "medium",
    "benchmark_usage": [
      "Market microstructure research",
      "High-frequency trading analysis"
    ],
    "tfidf_keywords": [
      "limit-order-book",
      "high-frequency-trading",
      "NASDAQ",
      "market-microstructure",
      "order-dynamics",
      "price-formation",
      "trading-strategies",
      "liquidity-analysis",
      "tick-data",
      "ITCH-messages"
    ],
    "semantic_cluster": "market-microstructure-research",
    "content_format": "dataset",
    "depth_level": "intermediate",
    "related_concepts": [
      "market-liquidity",
      "order-execution",
      "trade-impact",
      "financial-markets",
      "algorithmic-trading"
    ],
    "canonical_topics": [
      "finance",
      "econometrics",
      "machine-learning"
    ]
  }
]