[
  {
    "name": "BayesianBandits",
    "description": "Lightweight microframework for Bayesian bandits (Thompson Sampling) with support for contextual/restless/delayed rewards.",
    "category": "Adaptive Experimentation & Bandits",
    "docs_url": "https://rukulkarni.com/projects/bayesianbandits/",
    "github_url": "https://github.com/IntelyCare/bayesianbandits",
    "url": "https://github.com/IntelyCare/bayesianbandits",
    "install": "pip install bayesianbandits",
    "tags": [
      "A/B testing",
      "experimentation",
      "Bayesian"
    ],
    "best_for": "Online A/B testing, multi-armed bandits, adaptive allocation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-scipy",
      "bayesian-inference",
      "A/B-testing"
    ],
    "topic_tags": [
      "thompson-sampling",
      "contextual-bandits",
      "online-experiments",
      "bayesian-optimization",
      "adaptive-testing"
    ],
    "summary": "BayesianBandits is a lightweight Python framework implementing Thompson Sampling for multi-armed bandit problems. It supports advanced scenarios like contextual bandits, restless bandits, and delayed reward feedback. Data scientists use it to optimize real-time decision making in experiments where traditional A/B testing is too slow or inefficient.",
    "use_cases": [
      "Dynamically allocating traffic to website variants based on real-time conversion performance",
      "Personalizing content recommendations by learning user preferences through contextual bandit feedback"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python package for thompson sampling bandits",
      "contextual bandit library with delayed rewards",
      "bayesian bandit framework for online experiments",
      "thompson sampling implementation for A/B testing"
    ]
  },
  {
    "name": "ContextualBandits",
    "description": "Implements a wide range of contextual bandit algorithms (linear, tree-based, neural) and off-policy evaluation methods.",
    "category": "Adaptive Experimentation & Bandits",
    "docs_url": "https://contextual-bandits.readthedocs.io/",
    "github_url": "https://github.com/david-cortes/contextualbandits",
    "url": "https://github.com/david-cortes/contextualbandits",
    "install": "pip install contextualbandits",
    "tags": [
      "A/B testing",
      "experimentation",
      "machine learning"
    ],
    "best_for": "Online A/B testing, multi-armed bandits, adaptive allocation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-scikit-learn",
      "multi-armed-bandits",
      "numpy-arrays"
    ],
    "topic_tags": [
      "contextual-bandits",
      "off-policy-evaluation",
      "adaptive-algorithms",
      "reinforcement-learning",
      "python-package"
    ],
    "summary": "A comprehensive Python package that implements contextual bandit algorithms including linear, tree-based, and neural network approaches, plus off-policy evaluation methods. Used by data scientists and researchers to optimize sequential decision-making problems where actions depend on context. Enables practical implementation of adaptive experimentation beyond traditional A/B testing.",
    "use_cases": [
      "Personalizing content recommendations where user features influence which articles/products to show",
      "Optimizing ad targeting by adapting bidding strategies based on user demographics and behavior patterns"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "contextual bandits python implementation",
      "off-policy evaluation methods package",
      "adaptive experimentation beyond A/B testing",
      "multi-armed bandits with user features"
    ]
  },
  {
    "name": "MABWiser",
    "description": "Production-ready, scikit-learn style library for contextual & stochastic bandits with parallelism and simulation tools.",
    "category": "Adaptive Experimentation & Bandits",
    "docs_url": "https://fidelity.github.io/mabwiser/",
    "github_url": "https://github.com/fidelity/mabwiser",
    "url": "https://github.com/fidelity/mabwiser",
    "install": "pip install mabwiser",
    "tags": [
      "A/B testing",
      "experimentation"
    ],
    "best_for": "Online A/B testing, multi-armed bandits, adaptive allocation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-scikit-learn",
      "hypothesis-testing",
      "statistical-inference"
    ],
    "topic_tags": [
      "multi-armed-bandits",
      "contextual-bandits",
      "production-ready",
      "experimentation-framework",
      "adaptive-testing"
    ],
    "summary": "MABWiser is a production-ready Python library that implements multi-armed bandit algorithms with contextual and stochastic capabilities, built in scikit-learn style for easy adoption. It provides parallelized execution and simulation tools for running adaptive experiments that optimize decisions in real-time. The library is designed for practitioners who need to move beyond traditional A/B testing to more sophisticated adaptive experimentation methods.",
    "use_cases": [
      "Optimizing product recommendation algorithms by learning which recommendations work best for different user segments",
      "Dynamic pricing experiments where prices adapt based on user behavior and market conditions"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for contextual bandits production",
      "MABWiser multi-armed bandit implementation",
      "adaptive experimentation beyond A/B testing",
      "scikit-learn style bandit algorithms"
    ]
  },
  {
    "name": "Open Bandit Pipeline (OBP)",
    "description": "Framework for **offline evaluation (OPE)** of bandit policies using logged data. Implements IPS, DR, DM estimators.",
    "category": "Adaptive Experimentation & Bandits",
    "docs_url": "https://zr-obp.readthedocs.io/en/latest/",
    "github_url": "https://github.com/st-tech/zr-obp",
    "url": "https://github.com/st-tech/zr-obp",
    "install": "pip install obp",
    "tags": [
      "A/B testing",
      "experimentation"
    ],
    "best_for": "Online A/B testing, multi-armed bandits, adaptive allocation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "multi-armed-bandits",
      "python-scikit-learn",
      "causal-inference"
    ],
    "topic_tags": [
      "offline-policy-evaluation",
      "bandit-algorithms",
      "causal-inference",
      "python-package",
      "experimentation-framework"
    ],
    "summary": "Open Bandit Pipeline (OBP) is a Python framework for evaluating bandit policies using historical logged data without running live experiments. It implements key offline policy evaluation methods like Inverse Propensity Scoring (IPS), Doubly Robust (DR), and Direct Method (DM) estimators. Data scientists and researchers use it to safely test new recommendation or treatment assignment policies before deployment.",
    "use_cases": [
      "Evaluating new recommendation algorithms using past user interaction logs before A/B testing",
      "Assessing personalized treatment policies in healthcare using historical patient data"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "offline policy evaluation bandit python",
      "evaluate recommendation policy without AB test",
      "IPS doubly robust estimator package",
      "bandit offline evaluation logged data"
    ]
  },
  {
    "name": "PyXAB",
    "description": "Library for advanced bandit problems: X-armed bandits (continuous/structured action spaces) and online optimization.",
    "category": "Adaptive Experimentation & Bandits",
    "docs_url": "https://pyxab.readthedocs.io/en/latest/",
    "github_url": "https://github.com/huanzhang12/pyxab",
    "url": "https://github.com/huanzhang12/pyxab",
    "install": "pip install pyxab",
    "tags": [
      "A/B testing",
      "experimentation"
    ],
    "best_for": "Online A/B testing, multi-armed bandits, adaptive allocation",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "multi-armed-bandits",
      "convex-optimization",
      "python-numpy"
    ],
    "topic_tags": [
      "continuous-bandits",
      "optimization",
      "reinforcement-learning",
      "python-package"
    ],
    "summary": "PyXAB is a Python library for solving X-armed bandit problems where actions exist in continuous or structured spaces rather than discrete options. It's designed for researchers and practitioners working on online optimization problems that require sequential decision-making with complex action spaces. The library implements advanced algorithms for scenarios where traditional multi-armed bandits are insufficient due to infinite or highly structured action sets.",
    "use_cases": [
      "Dynamic pricing optimization where price points form a continuous space",
      "Hyperparameter tuning for machine learning models with structured parameter spaces"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "continuous action space bandits python",
      "X-armed bandit optimization library",
      "structured bandit problems implementation",
      "online optimization continuous actions"
    ]
  },
  {
    "name": "SMPyBandits",
    "description": "Comprehensive research framework for single/multi-player MAB algorithms (stochastic, adversarial, contextual).",
    "category": "Adaptive Experimentation & Bandits",
    "docs_url": "https://smpybandits.github.io/",
    "github_url": "https://github.com/SMPyBandits/SMPyBandits",
    "url": "https://github.com/SMPyBandits/SMPyBandits",
    "install": "pip install SMPyBandits",
    "tags": [
      "A/B testing",
      "experimentation"
    ],
    "best_for": "Online A/B testing, multi-armed bandits, adaptive allocation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-numpy",
      "multi-armed-bandits",
      "hypothesis-testing"
    ],
    "topic_tags": [
      "multi-armed-bandits",
      "online-learning",
      "reinforcement-learning",
      "python-package"
    ],
    "summary": "SMPyBandits is a comprehensive Python research framework implementing various multi-armed bandit algorithms for both single and multi-player scenarios. It supports stochastic, adversarial, and contextual bandits with extensive simulation capabilities. Researchers and data scientists use it to prototype, benchmark, and deploy adaptive experimentation strategies.",
    "use_cases": [
      "Dynamic pricing optimization where algorithms learn optimal prices through customer interactions",
      "Content recommendation systems that adaptively allocate traffic to different article types based on engagement"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "Python library for multi-armed bandit experiments",
      "How to implement contextual bandits in Python",
      "Multi-player bandit algorithms comparison framework",
      "Stochastic vs adversarial bandit implementation tools"
    ]
  },
  {
    "name": "abracadabra",
    "description": "Sequential testing with always-valid inference. Supports continuous monitoring of A/B tests.",
    "category": "Adaptive Experimentation & Bandits",
    "docs_url": "https://github.com/quizlet/abracadabra",
    "github_url": "https://github.com/quizlet/abracadabra",
    "url": "https://pypi.org/project/abracadabra/",
    "install": "pip install abracadabra",
    "tags": [
      "sequential testing",
      "A/B testing",
      "always-valid"
    ],
    "best_for": "Continuous experiment monitoring",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "hypothesis-testing",
      "python-programming",
      "A-B-testing-fundamentals"
    ],
    "topic_tags": [
      "sequential-testing",
      "A-B-testing",
      "continuous-monitoring",
      "always-valid-inference",
      "experimentation"
    ],
    "summary": "Abracadabra is a Python package for sequential A/B testing that allows continuous monitoring of experiments without inflating Type I error rates. It implements always-valid inference methods, enabling data scientists to peek at results anytime and stop experiments early when significant effects are detected. The package is particularly useful for teams running online experiments who want flexibility in monitoring without statistical penalties.",
    "use_cases": [
      "Monitoring conversion rate experiments on e-commerce platforms where you need to check results daily and stop early if a clear winner emerges",
      "Running product feature tests where business stakeholders want regular updates and the ability to halt experiments based on interim results"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "sequential A/B testing python package",
      "continuous monitoring A/B tests without multiple testing problems",
      "always valid inference experimentation tools",
      "early stopping A/B tests statistical significance"
    ]
  },
  {
    "name": "crewai",
    "description": "Framework for orchestrating role-playing autonomous AI agents. Multi-agent collaboration made intuitive.",
    "category": "Agentic AI",
    "docs_url": "https://docs.crewai.com/",
    "github_url": "https://github.com/crewAIInc/crewAI",
    "url": "https://www.crewai.com/",
    "install": "pip install crewai",
    "tags": [
      "agents",
      "multi-agent",
      "orchestration",
      "roles"
    ],
    "best_for": "Role-based multi-agent teams",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-programming",
      "large-language-models",
      "API-integration"
    ],
    "topic_tags": [
      "multi-agent-systems",
      "ai-orchestration",
      "autonomous-agents",
      "llm-workflows",
      "collaborative-ai"
    ],
    "summary": "CrewAI is a Python framework that enables multiple AI agents to work together on complex tasks, with each agent having specific roles and responsibilities. It simplifies the coordination of autonomous AI agents that can collaborate, delegate tasks, and combine their outputs. Data scientists and AI practitioners use it to build sophisticated multi-agent systems without dealing with low-level orchestration complexity.",
    "use_cases": [
      "Building a research pipeline where different agents handle data collection, analysis, and report writing",
      "Creating a customer service system where agents specialize in different domains and escalate issues between each other"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "multi agent AI framework Python",
      "how to coordinate multiple AI agents",
      "CrewAI vs LangChain agents",
      "autonomous AI agent orchestration tools"
    ]
  },
  {
    "name": "langchain",
    "description": "Framework for developing LLM-powered applications. Chains, tools, memory, and retrieval.",
    "category": "Agentic AI",
    "docs_url": "https://python.langchain.com/",
    "github_url": "https://github.com/langchain-ai/langchain",
    "url": "https://python.langchain.com/",
    "install": "pip install langchain",
    "tags": [
      "LLM",
      "chains",
      "tools",
      "RAG"
    ],
    "best_for": "LLM framework \u2014 chains, tools, memory",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-programming",
      "OpenAI-API",
      "prompt-engineering"
    ],
    "topic_tags": [
      "large-language-models",
      "agentic-ai",
      "retrieval-augmented-generation",
      "application-framework"
    ],
    "summary": "LangChain is a Python framework for building applications powered by large language models, providing modular components for chaining prompts, integrating external tools, and managing conversation memory. It's widely used by data scientists and developers to create chatbots, document Q&A systems, and AI agents that can interact with databases and APIs. The framework simplifies complex LLM workflows through pre-built chains and retrieval-augmented generation capabilities.",
    "use_cases": [
      "Building a customer support chatbot that queries company databases and documentation",
      "Creating a research assistant that can summarize papers and answer questions about uploaded documents"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "how to build LLM applications with langchain",
      "langchain framework for chatbot development",
      "retrieval augmented generation with langchain",
      "langchain vs building custom LLM apps"
    ]
  },
  {
    "name": "langgraph",
    "description": "Framework for building stateful, multi-actor LLM applications. Graph-based agent workflows with persistence.",
    "category": "Agentic AI",
    "docs_url": "https://langchain-ai.github.io/langgraph/",
    "github_url": "https://github.com/langchain-ai/langgraph",
    "url": "https://langchain-ai.github.io/langgraph/",
    "install": "pip install langgraph",
    "tags": [
      "agents",
      "LLM",
      "workflows",
      "multi-agent"
    ],
    "best_for": "Production agent workflows with state management",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-langchain",
      "LLM-APIs",
      "graph-theory-basics"
    ],
    "topic_tags": [
      "multi-agent-systems",
      "LLM-orchestration",
      "stateful-workflows",
      "graph-based-AI",
      "conversational-AI"
    ],
    "summary": "LangGraph is a framework for building complex, stateful applications with multiple LLM agents that can interact and maintain conversation history. It enables developers to create graph-based workflows where different agents handle specialized tasks and coordinate through defined relationships. The framework is particularly useful for building sophisticated AI assistants and automated reasoning systems that require persistent state management.",
    "use_cases": [
      "Building a customer service system where different AI agents handle billing, technical support, and sales inquiries while maintaining conversation context",
      "Creating a research assistant that coordinates multiple specialized agents for literature search, data analysis, and report generation with persistent workflow state"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "how to build multi agent LLM workflows",
      "stateful AI agent framework python",
      "LangGraph vs LangChain for agent orchestration",
      "graph based LLM application development"
    ]
  },
  {
    "name": "openai-agents",
    "description": "OpenAI's lightweight, production-ready SDK for building agentic AI applications. Fast prototyping.",
    "category": "Agentic AI",
    "docs_url": "https://openai.github.io/openai-agents-python/",
    "github_url": "https://github.com/openai/openai-agents-python",
    "url": "https://openai.github.io/openai-agents-python/",
    "install": "pip install openai-agents",
    "tags": [
      "agents",
      "OpenAI",
      "tools",
      "lightweight"
    ],
    "best_for": "OpenAI's lightweight SDK \u2014 fast prototyping",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-programming",
      "OpenAI-API",
      "async-programming"
    ],
    "topic_tags": [
      "agentic-ai",
      "openai-sdk",
      "production-tools",
      "ai-agents",
      "rapid-prototyping"
    ],
    "summary": "OpenAI's official SDK for building AI agents that can use tools and maintain conversations across multiple interactions. Designed for production use with minimal setup, making it easy to create agents that can perform tasks like web searches, data analysis, or API calls. Ideal for developers who want to quickly prototype and deploy agentic AI applications without heavy framework overhead.",
    "use_cases": [
      "Building a customer support agent that can search knowledge bases and escalate to humans",
      "Creating a data analysis assistant that can query databases and generate reports"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "lightweight OpenAI agent framework",
      "how to build production AI agents",
      "OpenAI SDK for agentic applications",
      "fast prototyping AI agents tools"
    ]
  },
  {
    "name": "bsts",
    "description": "Bayesian Structural Time Series providing the foundation for CausalImpact. Supports spike-and-slab variable selection, multiple state components (trend, seasonality, regression), and non-Gaussian outcomes. Developed at Google.",
    "category": "Bayesian Causal Inference",
    "docs_url": "https://cran.r-project.org/web/packages/bsts/bsts.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=bsts",
    "install": "install.packages(\"bsts\")",
    "tags": [
      "Bayesian",
      "structural-time-series",
      "spike-and-slab",
      "state-space",
      "Google"
    ],
    "best_for": "Bayesian structural time series with spike-and-slab selection\u2014foundation for CausalImpact",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "time-series-analysis",
      "bayesian-statistics",
      "R-programming"
    ],
    "topic_tags": [
      "bayesian-inference",
      "time-series",
      "causal-impact",
      "state-space-models",
      "R-package"
    ],
    "summary": "BSTS is an R package for Bayesian structural time series modeling that serves as the foundation for Google's CausalImpact package. It enables flexible time series analysis with automatic variable selection through spike-and-slab priors and supports complex state components like trends and seasonality. Particularly valuable for causal inference in time series data and intervention analysis.",
    "use_cases": [
      "Measuring the causal impact of a marketing campaign on sales by modeling pre-intervention time series",
      "Forecasting business metrics while accounting for multiple seasonal patterns and external covariates"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "bayesian time series modeling R package",
      "causal impact analysis time series",
      "spike and slab variable selection time series",
      "Google BSTS package structural time series"
    ]
  },
  {
    "name": "Bambi",
    "description": "High-level interface for building Bayesian GLMMs, built on top of PyMC. Uses formula syntax similar to R's `lme4`.",
    "category": "Bayesian Econometrics",
    "docs_url": "https://bambinos.github.io/bambi/",
    "github_url": "https://github.com/bambinos/bambi",
    "url": "https://github.com/bambinos/bambi",
    "install": "pip install bambi",
    "tags": [
      "Bayesian",
      "inference"
    ],
    "best_for": "Uncertainty quantification, prior-informed inference, probabilistic modeling",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pymc",
      "generalized-linear-models",
      "R-formula-syntax"
    ],
    "topic_tags": [
      "bayesian-inference",
      "mixed-effects-models",
      "generalized-linear-models",
      "hierarchical-modeling",
      "python-package"
    ],
    "summary": "Bambi provides a high-level, R-like interface for building Bayesian generalized linear mixed models (GLMMs) using PyMC as the backend. It allows economists and data scientists to specify complex hierarchical models using familiar formula syntax without writing low-level PyMC code. The package is particularly useful for multilevel regression analysis with partial pooling and uncertainty quantification.",
    "use_cases": [
      "Analyzing user engagement across different product features with random effects for user groups",
      "Estimating treatment effects in A/B tests with hierarchical structure across market segments"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "high level interface for bayesian mixed effects models",
      "PyMC wrapper with R-like formula syntax",
      "hierarchical regression with uncertainty quantification python",
      "Bayesian GLMM package for economists"
    ]
  },
  {
    "name": "LightweightMMM",
    "description": "Bayesian Marketing Mix Modeling (see Marketing Mix Models section).",
    "category": "Bayesian Econometrics",
    "docs_url": null,
    "github_url": "https://github.com/google/lightweight_mmm",
    "url": "https://github.com/google/lightweight_mmm",
    "install": "pip install lightweight_mmm",
    "tags": [
      "Bayesian",
      "inference"
    ],
    "best_for": "Uncertainty quantification, prior-informed inference, probabilistic modeling",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-programming",
      "bayesian-statistics",
      "marketing-attribution"
    ],
    "topic_tags": [
      "marketing-mix-modeling",
      "bayesian-inference",
      "media-attribution",
      "python-package",
      "causal-inference"
    ],
    "summary": "LightweightMMM is a Google-developed Python package for Bayesian Marketing Mix Modeling that helps quantify the incremental impact of different marketing channels on business outcomes. It uses probabilistic methods to estimate media effectiveness, budget allocation, and diminishing returns curves while accounting for uncertainty in the estimates.",
    "use_cases": [
      "Optimizing marketing budget allocation across channels like TV, digital ads, and social media",
      "Measuring incrementality and ROI of marketing campaigns while controlling for seasonality and external factors"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "bayesian marketing mix modeling python",
      "how to measure marketing channel effectiveness",
      "lightweight mmm implementation guide",
      "marketing attribution bayesian inference"
    ]
  },
  {
    "name": "NumPyro",
    "description": "Probabilistic programming library built on JAX for scalable Bayesian inference, often faster than PyMC.",
    "category": "Bayesian Econometrics",
    "docs_url": "https://num.pyro.ai/",
    "github_url": "https://github.com/pyro-ppl/numpyro",
    "url": "https://github.com/pyro-ppl/numpyro",
    "install": "pip install numpyro",
    "tags": [
      "Bayesian",
      "inference"
    ],
    "best_for": "Uncertainty quantification, prior-informed inference, probabilistic modeling",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-programming",
      "bayesian-statistics",
      "MCMC-sampling"
    ],
    "topic_tags": [
      "probabilistic-programming",
      "bayesian-inference",
      "JAX",
      "MCMC",
      "scalable-computing"
    ],
    "summary": "NumPyro is a probabilistic programming library that leverages JAX's GPU acceleration and automatic differentiation for fast Bayesian inference. It's particularly useful for economists who need to fit complex hierarchical models or run computationally intensive MCMC sampling. The library offers similar modeling capabilities to PyMC but with significantly better performance on large datasets.",
    "use_cases": [
      "Estimating hierarchical demand models with thousands of products and markets",
      "Running sensitivity analysis on structural economic models with multiple parameters"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "fast bayesian inference library for economics",
      "NumPyro vs PyMC performance comparison",
      "JAX probabilistic programming for econometrics",
      "scalable MCMC sampling for large datasets"
    ]
  },
  {
    "name": "PyMC",
    "description": "Flexible probabilistic programming library for Bayesian modeling and inference using MCMC algorithms (NUTS).",
    "category": "Bayesian Econometrics",
    "docs_url": "https://www.pymc.io/",
    "github_url": "https://github.com/pymc-devs/pymc",
    "url": "https://github.com/pymc-devs/pymc",
    "install": "pip install pymc",
    "tags": [
      "Bayesian",
      "inference"
    ],
    "best_for": "Uncertainty quantification, prior-informed inference, probabilistic modeling",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-basics",
      "probability-distributions",
      "linear-regression"
    ],
    "topic_tags": [
      "bayesian-inference",
      "mcmc-sampling",
      "probabilistic-programming",
      "uncertainty-quantification",
      "hierarchical-modeling"
    ],
    "summary": "PyMC is a Python library for Bayesian statistical modeling that allows users to build complex probabilistic models and perform inference using advanced MCMC sampling algorithms. It's particularly powerful for handling uncertainty quantification and hierarchical models in econometric applications. The library provides an intuitive interface for specifying priors, likelihoods, and posterior sampling.",
    "use_cases": [
      "Estimating treatment effects with uncertainty bounds in A/B testing",
      "Building hierarchical models for customer lifetime value across different segments"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "bayesian modeling python package",
      "MCMC sampling library for econometrics",
      "how to quantify uncertainty in causal inference",
      "PyMC vs Stan for bayesian analysis"
    ]
  },
  {
    "name": "bayesplot",
    "description": "Extensive library of ggplot2-based plotting functions for posterior analysis, MCMC diagnostics, and prior/posterior predictive checks supporting the applied Bayesian workflow for any MCMC-fitted model.",
    "category": "Bayesian Inference",
    "docs_url": "https://mc-stan.org/bayesplot/",
    "github_url": "https://github.com/stan-dev/bayesplot",
    "url": "https://cran.r-project.org/package=bayesplot",
    "install": "install.packages(\"bayesplot\")",
    "tags": [
      "visualization",
      "MCMC-diagnostics",
      "posterior-predictive-checks",
      "ggplot2",
      "Bayesian"
    ],
    "best_for": "Diagnostic plots and posterior visualization for MCMC-based Bayesian models, implementing Gabry et al. (2019)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "R-ggplot2",
      "MCMC-sampling",
      "Bayesian-statistics"
    ],
    "topic_tags": [
      "Bayesian-visualization",
      "MCMC-diagnostics",
      "posterior-analysis",
      "R-package",
      "statistical-graphics"
    ],
    "summary": "bayesplot is an R package that provides comprehensive visualization tools for Bayesian analysis workflows. It offers specialized plots for diagnosing MCMC chains, visualizing posterior distributions, and conducting predictive checks. The package integrates seamlessly with popular Bayesian modeling packages like Stan, brms, and rstanarm.",
    "use_cases": [
      "Diagnosing convergence issues in MCMC chains from Stan models",
      "Creating publication-ready posterior distribution plots for research papers"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "best R package for Bayesian plotting",
      "how to visualize MCMC diagnostics in R",
      "posterior predictive check visualization tools",
      "ggplot2 based Bayesian analysis plots"
    ]
  },
  {
    "name": "brms",
    "description": "High-level interface for fitting Bayesian generalized multilevel models using Stan, with lme4-style formula syntax supporting linear, count, survival, ordinal, zero-inflated, hurdle, and mixture models with flexible prior specification.",
    "category": "Bayesian Inference",
    "docs_url": "https://paul-buerkner.github.io/brms/",
    "github_url": "https://github.com/paul-buerkner/brms",
    "url": "https://cran.r-project.org/package=brms",
    "install": "install.packages(\"brms\")",
    "tags": [
      "Bayesian",
      "multilevel-models",
      "Stan",
      "regression",
      "distributional-regression"
    ],
    "best_for": "Complex hierarchical Bayesian regression with familiar R formula syntax, implementing B\u00fcrkner (2017)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "hierarchical-models",
      "R-programming",
      "MCMC-basics"
    ],
    "topic_tags": [
      "bayesian-regression",
      "multilevel-models",
      "stan-interface",
      "mixed-effects",
      "distributional-modeling"
    ],
    "summary": "brms is an R package that provides an intuitive interface for fitting complex Bayesian multilevel models using Stan as the computational backend. It allows data scientists and researchers to specify sophisticated hierarchical models using familiar lme4-style syntax while leveraging Stan's powerful MCMC sampling capabilities. The package supports a wide range of response distributions and model types, making it accessible for practitioners who want Bayesian inference without writing Stan code directly.",
    "use_cases": [
      "Analyzing user engagement across different product features with varying baseline rates by geographic region",
      "Modeling patient treatment responses in clinical trials accounting for hospital-level effects and individual covariates"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for Bayesian multilevel models",
      "Stan interface with lme4 syntax",
      "how to fit hierarchical Bayesian models in R",
      "brms vs rstanarm for mixed effects models"
    ]
  },
  {
    "name": "rstan",
    "description": "Core R interface to the Stan probabilistic programming language, providing full Bayesian inference via NUTS/HMC, approximate inference via ADVI, and penalized maximum likelihood via L-BFGS for custom Bayesian models.",
    "category": "Bayesian Inference",
    "docs_url": "https://mc-stan.org/rstan/",
    "github_url": "https://github.com/stan-dev/rstan",
    "url": "https://cran.r-project.org/package=rstan",
    "install": "install.packages(\"rstan\")",
    "tags": [
      "Stan",
      "MCMC",
      "HMC",
      "probabilistic-programming",
      "Bayesian"
    ],
    "best_for": "Custom Bayesian models requiring direct Stan language access for maximum flexibility, implementing Carpenter et al. (2017)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "R-programming",
      "Bayesian-statistics",
      "MCMC-fundamentals"
    ],
    "topic_tags": [
      "probabilistic-programming",
      "Bayesian-inference",
      "MCMC-sampling",
      "hierarchical-models",
      "R-package"
    ],
    "summary": "RStan is the R interface to Stan, a powerful probabilistic programming language for Bayesian statistical modeling. It enables data scientists and researchers to specify custom Bayesian models using intuitive syntax and performs efficient inference using advanced sampling algorithms like Hamiltonian Monte Carlo. The package is widely used in academic research and industry for complex statistical modeling where standard frequentist approaches fall short.",
    "use_cases": [
      "Hierarchical modeling for A/B test analysis with user-level random effects",
      "Custom attribution modeling with prior knowledge about marketing channel effectiveness"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "Stan R package for Bayesian modeling",
      "how to do MCMC sampling in R",
      "probabilistic programming languages for data science",
      "Bayesian inference tools for custom models"
    ]
  },
  {
    "name": "rstanarm",
    "description": "Pre-compiled Bayesian regression models using Stan that mimic familiar R functions (lm, glm, lmer) with customary formula syntax, weakly informative default priors, and zero model compilation time.",
    "category": "Bayesian Inference",
    "docs_url": "https://mc-stan.org/rstanarm/",
    "github_url": "https://github.com/stan-dev/rstanarm",
    "url": "https://cran.r-project.org/package=rstanarm",
    "install": "install.packages(\"rstanarm\")",
    "tags": [
      "Bayesian",
      "Stan",
      "regression",
      "mixed-effects",
      "pre-compiled"
    ],
    "best_for": "Applied Bayesian regression with minimal learning curve for lm/glm/lmer users",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "R-programming",
      "linear-regression",
      "glm-models"
    ],
    "topic_tags": [
      "bayesian-regression",
      "stan",
      "R-package",
      "mixed-effects",
      "pre-compiled"
    ],
    "summary": "rstanarm provides pre-compiled Bayesian regression models that work like familiar R functions (lm, glm, lmer) but with built-in uncertainty quantification and credible intervals. It eliminates the complexity of writing Stan code while providing access to Bayesian inference with sensible default priors. Perfect for analysts who want Bayesian benefits without the steep learning curve of probabilistic programming.",
    "use_cases": [
      "A/B testing analysis where you need credible intervals and want to incorporate prior beliefs about effect sizes",
      "Hierarchical modeling of user behavior across different market segments with automatic shrinkage"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "easy bayesian regression in R without writing Stan code",
      "rstanarm vs regular lm for A/B testing",
      "pre-compiled bayesian models R package",
      "how to get credible intervals from regression in R"
    ]
  },
  {
    "name": "boot",
    "description": "Classic bootstrap methods implementing the approaches described in Davison & Hinkley (1997). Provides functions for both parametric and nonparametric resampling with various confidence interval methods.",
    "category": "Bootstrap & Inference",
    "docs_url": "https://cran.r-project.org/web/packages/boot/boot.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=boot",
    "install": "install.packages(\"boot\")",
    "tags": [
      "bootstrap",
      "resampling",
      "confidence-intervals",
      "nonparametric",
      "parametric"
    ],
    "best_for": "Classic bootstrap methods from Davison & Hinkley (1997) for general resampling inference",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "R-programming",
      "sampling-distributions",
      "hypothesis-testing"
    ],
    "topic_tags": [
      "bootstrap",
      "resampling",
      "confidence-intervals",
      "nonparametric-statistics",
      "R-package"
    ],
    "summary": "The boot package implements classical bootstrap resampling methods from Davison & Hinkley's seminal 1997 textbook. It provides functions for generating bootstrap samples, calculating bias-corrected estimates, and constructing various types of confidence intervals without distributional assumptions. This is the go-to R package for practitioners needing robust uncertainty quantification when traditional parametric methods don't apply.",
    "use_cases": [
      "Estimating confidence intervals for complex statistics like correlation coefficients when sample sizes are small",
      "Creating robust uncertainty estimates for machine learning model performance metrics when distributional assumptions are violated"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "bootstrap confidence intervals R package",
      "nonparametric resampling methods",
      "how to bootstrap statistics in R",
      "Davison Hinkley bootstrap implementation"
    ]
  },
  {
    "name": "fwildclusterboot",
    "description": "Fast wild cluster bootstrap implementation following Roodman et al. (2019)\u2014up to 1000\u00d7 faster than alternatives. Critical for panel data with few clusters. Integrates with fixest and lfe for efficient inference.",
    "category": "Bootstrap & Inference",
    "docs_url": "https://s3alfisc.github.io/fwildclusterboot/",
    "github_url": "https://github.com/s3alfisc/fwildclusterboot",
    "url": "https://cran.r-project.org/package=fwildclusterboot",
    "install": "install.packages(\"fwildclusterboot\")",
    "tags": [
      "wild-bootstrap",
      "cluster-robust",
      "few-clusters",
      "panel-data",
      "fixest"
    ],
    "best_for": "Fast wild cluster bootstrap for panel data with few clusters, implementing Roodman et al. (2019)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "fixed-effects-regression",
      "clustered-standard-errors",
      "R-programming"
    ],
    "topic_tags": [
      "wild-bootstrap",
      "cluster-inference",
      "panel-data",
      "standard-errors",
      "econometrics"
    ],
    "summary": "Fast implementation of wild cluster bootstrap for statistical inference when you have few clusters in panel data. Solves the problem where traditional clustered standard errors become unreliable with small numbers of clusters. Integrates seamlessly with popular R packages like fixest and lfe for efficient econometric analysis.",
    "use_cases": [
      "Running difference-in-differences with only 8-15 states as treatment units",
      "Analyzing firm-level panel data where clustering by industry leaves only 12 sectors"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "wild bootstrap few clusters R",
      "clustered standard errors small sample",
      "bootstrap inference panel data",
      "fwildclusterboot vs traditional clustering"
    ]
  },
  {
    "name": "rsample",
    "description": "Modern tidyverse-compatible resampling infrastructure. Provides functions for creating resamples (bootstrap, cross-validation, time series splits) that integrate seamlessly with tidymodels workflows.",
    "category": "Bootstrap & Inference",
    "docs_url": "https://rsample.tidymodels.org/",
    "github_url": "https://github.com/tidymodels/rsample",
    "url": "https://cran.r-project.org/package=rsample",
    "install": "install.packages(\"rsample\")",
    "tags": [
      "resampling",
      "cross-validation",
      "bootstrap",
      "tidymodels",
      "time-series-cv"
    ],
    "best_for": "Tidyverse-native resampling for bootstrap, cross-validation, and time series splits",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "R-dplyr",
      "cross-validation",
      "bootstrap-sampling"
    ],
    "topic_tags": [
      "resampling",
      "tidymodels",
      "cross-validation",
      "bootstrap",
      "model-validation"
    ],
    "summary": "rsample is an R package that provides a consistent framework for creating resamples like bootstrap samples, cross-validation folds, and time series splits. It integrates seamlessly with the tidymodels ecosystem, making it easy to incorporate resampling into machine learning workflows. The package handles common resampling scenarios with intuitive functions that return tidy data structures.",
    "use_cases": [
      "Setting up 10-fold cross-validation to evaluate model performance before production deployment",
      "Creating bootstrap samples to estimate confidence intervals for A/B test effect sizes"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "how to do cross validation in R tidymodels",
      "bootstrap sampling package for R",
      "rsample tutorial for model validation",
      "time series cross validation in tidymodels"
    ]
  },
  {
    "name": "bunching",
    "description": "Implements Kleven-Waseem style bunching estimation for kink and notch designs. Calculates parametric elasticities from bunching at tax thresholds with publication-ready output.",
    "category": "Bunching Estimation",
    "docs_url": "https://cran.r-project.org/web/packages/bunching/bunching.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=bunching",
    "install": "install.packages(\"bunching\")",
    "tags": [
      "bunching",
      "kink-design",
      "notch-design",
      "tax-research",
      "elasticity"
    ],
    "best_for": "Kleven-Waseem bunching estimation at kinks and notches for tax research",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "regression-discontinuity",
      "python-pandas",
      "tax-policy-basics"
    ],
    "topic_tags": [
      "bunching-estimation",
      "tax-elasticity",
      "kink-design",
      "notch-design",
      "public-economics"
    ],
    "summary": "This package implements the Kleven-Waseem bunching estimation method for measuring behavioral responses at tax kinks and notches. It's designed for economists studying how taxpayers respond to tax thresholds by calculating elasticities from observed bunching patterns. The package produces publication-ready results for academic research in public economics.",
    "use_cases": [
      "Measuring taxpayer responses to earned income tax credit kinks",
      "Analyzing bunching behavior at corporate tax rate thresholds"
    ],
    "audience": [
      "Early-PhD",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "bunching estimation python package",
      "Kleven Waseem bunching method implementation",
      "tax kink elasticity calculation tool",
      "notch design bunching analysis"
    ]
  },
  {
    "name": "bnlearn",
    "description": "Bayesian network structure learning, parameter estimation, and inference. Implements constraint-based (PC, GS), score-based (HC, TABU), and hybrid algorithms for DAG learning with discrete and continuous data.",
    "category": "Causal Discovery",
    "docs_url": "https://www.bnlearn.com/",
    "github_url": null,
    "url": "https://cran.r-project.org/package=bnlearn",
    "install": "install.packages(\"bnlearn\")",
    "tags": [
      "Bayesian-networks",
      "structure-learning",
      "parameter-estimation",
      "probabilistic-graphical-models",
      "inference"
    ],
    "best_for": "Bayesian network learning and inference with constraint-based and score-based algorithms",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "directed-acyclic-graphs",
      "conditional-independence",
      "maximum-likelihood-estimation"
    ],
    "topic_tags": [
      "bayesian-networks",
      "causal-discovery",
      "probabilistic-graphical-models",
      "structure-learning",
      "R-package"
    ],
    "summary": "bnlearn is an R package for learning Bayesian network structures from data and performing probabilistic inference. It provides multiple algorithms for discovering causal relationships and estimating parameters in directed acyclic graphs. The package is widely used by researchers and practitioners for causal discovery and probabilistic modeling tasks.",
    "use_cases": [
      "discovering causal relationships between customer behavior variables from transaction data",
      "building probabilistic models for medical diagnosis by learning dependencies between symptoms and conditions"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "how to learn Bayesian network structure from data",
      "R package for causal discovery with DAGs",
      "probabilistic graphical models structure learning algorithms",
      "bnlearn vs other Bayesian network packages"
    ]
  },
  {
    "name": "dagitty",
    "description": "Analysis of structural causal models represented as DAGs. Computes adjustment sets, identifies instrumental variables, tests conditional independencies, and finds minimal sufficient adjustment sets for causal identification.",
    "category": "Causal Discovery",
    "docs_url": "http://www.dagitty.net/",
    "github_url": "https://github.com/jtextor/dagitty",
    "url": "https://cran.r-project.org/package=dagitty",
    "install": "install.packages(\"dagitty\")",
    "tags": [
      "DAG",
      "causal-graphs",
      "adjustment-sets",
      "d-separation",
      "instrumental-variables"
    ],
    "best_for": "DAG-based causal analysis with adjustment set computation and d-separation testing",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "directed-acyclic-graphs",
      "causal-inference-theory",
      "R-programming"
    ],
    "topic_tags": [
      "causal-graphs",
      "DAG-analysis",
      "adjustment-sets",
      "causal-identification",
      "d-separation"
    ],
    "summary": "dagitty is an R package for analyzing directed acyclic graphs (DAGs) to support causal inference. It helps researchers identify which variables to control for in observational studies by computing adjustment sets and testing causal assumptions. The package is widely used by economists and data scientists working on causal identification problems.",
    "use_cases": [
      "Determining which covariates to include when estimating the causal effect of a treatment on an outcome",
      "Testing whether your causal assumptions are consistent with observed data patterns using conditional independence tests"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "how to find adjustment sets for causal inference",
      "R package for analyzing causal DAGs",
      "tools for causal identification with confounders",
      "dagitty tutorial for adjustment sets"
    ]
  },
  {
    "name": "ggdag",
    "description": "Visualize and analyze causal DAGs using ggplot2. Provides tidy interface to dagitty with publication-quality DAG plots, path highlighting, and adjustment set visualization.",
    "category": "Causal Discovery",
    "docs_url": "https://ggdag.malco.io/",
    "github_url": "https://github.com/malcolmbarrett/ggdag",
    "url": "https://cran.r-project.org/package=ggdag",
    "install": "install.packages(\"ggdag\")",
    "tags": [
      "DAG",
      "visualization",
      "ggplot2",
      "causal-diagrams",
      "adjustment-sets"
    ],
    "best_for": "Publication-quality DAG visualization using ggplot2 with dagitty integration",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "ggplot2",
      "basic-causal-inference",
      "R-programming"
    ],
    "topic_tags": [
      "causal-diagrams",
      "DAG-visualization",
      "ggplot2",
      "causal-inference",
      "R-package"
    ],
    "summary": "ggdag is an R package that creates publication-quality causal directed acyclic graphs (DAGs) using ggplot2 syntax. It provides an intuitive interface for drawing causal diagrams, identifying confounders, and visualizing adjustment sets needed for causal identification. The package is especially useful for researchers who need to communicate causal assumptions clearly in papers and presentations.",
    "use_cases": [
      "Drawing causal diagrams to identify which variables to control for in a regression analyzing the effect of user engagement features on retention",
      "Creating publication-ready DAGs to illustrate potential confounders when studying the causal impact of recommendation algorithms on user behavior"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "how to draw causal diagrams in R",
      "ggplot2 package for DAGs",
      "visualize confounders and adjustment sets",
      "R tool for causal inference diagrams"
    ]
  },
  {
    "name": "pcalg",
    "description": "Causal structure learning from observational data using the PC algorithm and variants. Estimates Markov equivalence class of DAGs from conditional independence tests with intervention support.",
    "category": "Causal Discovery",
    "docs_url": "https://cran.r-project.org/web/packages/pcalg/pcalg.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=pcalg",
    "install": "install.packages(\"pcalg\")",
    "tags": [
      "causal-discovery",
      "PC-algorithm",
      "structure-learning",
      "DAG",
      "conditional-independence"
    ],
    "best_for": "Causal structure learning from observational data using PC algorithm",
    "language": "R",
    "difficulty": "advanced",
    "prerequisites": [
      "conditional-independence-testing",
      "directed-acyclic-graphs",
      "python-networkx"
    ],
    "topic_tags": [
      "causal-discovery",
      "PC-algorithm",
      "structure-learning",
      "observational-data",
      "DAG-estimation"
    ],
    "summary": "The pcalg package implements the PC algorithm and its variants for learning causal structure from observational data through conditional independence testing. It estimates the Markov equivalence class of directed acyclic graphs (DAGs) that could have generated the observed data, with support for interventional data. This is a foundational tool for causal discovery when you need to infer causal relationships without experimental manipulation.",
    "use_cases": [
      "Discovering causal relationships in healthcare data where randomized experiments are unethical or impossible",
      "Learning market structure and causal dependencies between economic variables from observational business data"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "PC algorithm causal discovery implementation",
      "learn causal structure from observational data",
      "estimate DAG from conditional independence tests",
      "causal graph discovery without experiments"
    ]
  },
  {
    "name": "Ananke",
    "description": "Causal inference using graphical models (DAGs), including identification theory and effect estimation.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": "https://ananke.readthedocs.io/",
    "github_url": "[https://github.com/py-why/Ananke](https://github.com/ghosthamlet/ananke",
    "url": "[https://github.com/py-why/Ananke](https://github.com/ghosthamlet/ananke",
    "install": "pip install ananke-causal",
    "tags": [
      "causal inference",
      "graphs"
    ],
    "best_for": "Learning causal structure from data, DAG estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-networkx",
      "directed-acyclic-graphs",
      "instrumental-variables"
    ],
    "topic_tags": [
      "causal-inference",
      "dag-identification",
      "graphical-models",
      "python-package",
      "effect-estimation"
    ],
    "summary": "Ananke is a Python package for causal inference that leverages directed acyclic graphs (DAGs) to identify causal effects and estimate treatment impacts. It implements formal identification theory to determine whether causal effects can be computed from observational data given assumptions encoded in graphical models. The package is particularly useful for researchers who need to move beyond simple correlation analysis to establish causal relationships.",
    "use_cases": [
      "Determining whether a marketing intervention's effect can be identified from observational data given confounding variables",
      "Estimating the causal impact of a product feature on user retention while accounting for selection bias using instrumental variables"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python package for causal inference with DAGs",
      "how to identify causal effects from observational data",
      "graphical models for causal identification",
      "DAG-based causal effect estimation tools"
    ]
  },
  {
    "name": "Benchpress",
    "description": "Benchmarking 41+ structure learning algorithms for causal discovery. Standardized evaluation framework.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": "https://benchpressdocs.readthedocs.io/",
    "github_url": "https://github.com/felixleopoldo/benchpress",
    "url": "https://github.com/felixleopoldo/benchpress",
    "install": "pip install benchpress",
    "tags": [
      "causal discovery",
      "benchmarking",
      "structure learning"
    ],
    "best_for": "Comparing causal discovery algorithms",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-scikit-learn",
      "directed-acyclic-graphs",
      "statistical-hypothesis-testing"
    ],
    "topic_tags": [
      "causal-discovery",
      "structure-learning",
      "benchmarking",
      "graph-algorithms",
      "evaluation-framework"
    ],
    "summary": "Benchpress is a standardized evaluation framework that benchmarks 41+ algorithms for learning causal graph structures from observational data. It provides consistent comparison metrics and evaluation protocols for causal discovery methods. Researchers and data scientists use it to select the best structure learning algorithm for their specific dataset and problem constraints.",
    "use_cases": [
      "Comparing multiple causal discovery algorithms on your dataset to identify which performs best before committing to one approach",
      "Evaluating a new structure learning method you've developed against established baselines using standardized metrics"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "benchmark causal discovery algorithms",
      "compare structure learning methods",
      "evaluate causal graph learning performance",
      "standardized causal discovery evaluation"
    ]
  },
  {
    "name": "Causal Discovery Toolbox (CDT)",
    "description": "Implements algorithms for causal discovery (recovering causal graph structure) from observational data.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": "https://fentechsolutions.github.io/CausalDiscoveryToolbox/html/index.html",
    "github_url": "https://github.com/FenTechSolutions/CausalDiscoveryToolbox",
    "url": "https://github.com/FenTechSolutions/CausalDiscoveryToolbox",
    "install": "pip install cdt",
    "tags": [
      "causal inference",
      "graphs"
    ],
    "best_for": "Learning causal structure from data, DAG estimation",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "python-networkx",
      "directed-acyclic-graphs",
      "hypothesis-testing"
    ],
    "topic_tags": [
      "causal-discovery",
      "graph-structure",
      "observational-data",
      "causal-graphs",
      "structure-learning"
    ],
    "summary": "CDT is a Python package that implements algorithms to automatically discover causal relationships and graph structures from observational data without prior knowledge of the causal model. It's primarily used by researchers and data scientists working on causal inference problems where the underlying causal structure is unknown. The toolkit provides various algorithms like PC, GES, and constraint-based methods to recover directed acyclic graphs representing causal relationships.",
    "use_cases": [
      "Discovering which features causally influence customer churn from historical user behavior data",
      "Identifying causal pathways between genes, proteins, and disease outcomes from genomics datasets"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "python package for causal graph discovery",
      "how to find causal structure from observational data",
      "algorithms for learning causal graphs automatically",
      "causal discovery methods implementation python"
    ]
  },
  {
    "name": "CausalNex",
    "description": "Uses Bayesian Networks for causal reasoning, combining ML with expert knowledge to model relationships.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": null,
    "github_url": "https://github.com/microsoft/causalnex",
    "url": "https://github.com/microsoft/causalnex",
    "install": "pip install causalnex",
    "tags": [
      "causal inference",
      "graphs",
      "Bayesian"
    ],
    "best_for": "Learning causal structure from data, DAG estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "directed-acyclic-graphs",
      "bayesian-inference"
    ],
    "topic_tags": [
      "causal-discovery",
      "bayesian-networks",
      "structural-causal-models",
      "graph-learning",
      "python-package"
    ],
    "summary": "CausalNex is a Python library that combines machine learning with domain expertise to build Bayesian Networks for causal reasoning. It enables practitioners to discover causal relationships from data while incorporating prior knowledge through structural constraints. The package is particularly useful for understanding complex systems where relationships between variables need to be modeled and interpreted causally.",
    "use_cases": [
      "Building a causal model to understand which marketing channels actually drive customer conversions versus just correlate with them",
      "Modeling supply chain relationships to identify which factors causally impact delivery delays for operational improvements"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python package for causal discovery with bayesian networks",
      "how to combine expert knowledge with data for causal modeling",
      "bayesian network library for causal inference python",
      "tools for learning causal graphs from observational data"
    ]
  },
  {
    "name": "LiNGAM",
    "description": "Specialized package for learning non-Gaussian linear causal models, implementing various versions of the LiNGAM algorithm including ICA-based methods.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": "https://lingam.readthedocs.io/",
    "github_url": "https://github.com/cdt15/lingam",
    "url": "https://github.com/cdt15/lingam",
    "install": "pip install lingam",
    "tags": [
      "causal inference",
      "graphs"
    ],
    "best_for": "Learning causal structure from data, DAG estimation",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "independent-component-analysis",
      "directed-acyclic-graphs",
      "maximum-likelihood-estimation"
    ],
    "topic_tags": [
      "lingam",
      "causal-discovery",
      "non-gaussian",
      "structural-equations",
      "ica-based"
    ],
    "summary": "LiNGAM (Linear Non-Gaussian Acyclic Model) is a specialized package for discovering causal structure in observational data by exploiting non-Gaussian noise assumptions. It implements various algorithms including ICA-based, DirectLiNGAM, and other variants to learn directed acyclic graphs from linear structural equation models. Primarily used by researchers and advanced practitioners working on causal discovery problems where traditional correlation-based methods fail.",
    "use_cases": [
      "Discovering causal relationships in economic time series data where variables have non-Gaussian distributions",
      "Learning the causal structure of gene regulatory networks from expression data with non-Gaussian noise"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "How to discover causal structure with non-Gaussian data",
      "LiNGAM algorithm implementation Python",
      "Causal discovery methods for non-Gaussian linear models",
      "DirectLiNGAM vs ICA-based causal inference"
    ]
  },
  {
    "name": "MCD",
    "description": "Mixture of Causal Graphs discovery for heterogeneous time series (ICML 2024). Finds time-varying causal structures.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": null,
    "github_url": "https://github.com/causal-disentanglement/mcd",
    "url": "https://pypi.org/project/MCD/",
    "install": "pip install mcd",
    "tags": [
      "causal discovery",
      "time series",
      "heterogeneous"
    ],
    "best_for": "Time-varying causal structure discovery",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "structural-causal-models",
      "graph-neural-networks",
      "time-series-analysis"
    ],
    "topic_tags": [
      "causal-discovery",
      "time-varying-graphs",
      "heterogeneous-time-series",
      "mixture-models",
      "ICML-2024"
    ],
    "summary": "MCD discovers time-varying causal structures in heterogeneous time series data using a mixture of causal graphs approach. It identifies different causal regimes that may exist at different time periods or across different entities in the data. This is particularly valuable for understanding how causal relationships evolve over time or differ across subgroups.",
    "use_cases": [
      "Analyzing how causal relationships between economic variables change during different market regimes",
      "Understanding time-varying causal effects in A/B testing when user behavior patterns shift over time"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "time varying causal discovery methods",
      "causal graphs for heterogeneous time series",
      "mixture models for causal structure learning",
      "ICML 2024 causal discovery papers"
    ]
  },
  {
    "name": "SDCI",
    "description": "State-dependent causal inference for conditionally stationary processes (ICML 2025). Handles regime-switching causal graphs.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": null,
    "github_url": "https://github.com/causal-disentanglement/sdci",
    "url": "https://pypi.org/project/SDCI/",
    "install": "pip install sdci",
    "tags": [
      "causal discovery",
      "time series",
      "regime switching"
    ],
    "best_for": "State-dependent causal discovery",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "causal-inference",
      "graphical-models",
      "time-series-analysis"
    ],
    "topic_tags": [
      "causal-discovery",
      "time-series",
      "regime-switching",
      "graphical-models",
      "state-dependent"
    ],
    "summary": "SDCI implements state-dependent causal inference methods for time series data where causal relationships change across different regimes or states. This advanced package handles conditionally stationary processes where the underlying causal graph structure switches between different configurations over time. Particularly useful for economists and data scientists analyzing structural breaks or regime changes in causal relationships.",
    "use_cases": [
      "Analyzing how monetary policy transmission mechanisms change during different economic regimes (recession vs expansion)",
      "Discovering causal relationships in user behavior data that vary across different platform states or market conditions"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "causal discovery with regime switching",
      "time series causal inference state dependent",
      "structural break causal graph discovery",
      "regime switching causal relationships python"
    ]
  },
  {
    "name": "Tigramite",
    "description": "Specialized package for causal inference in time series data implementing PCMCI, PCMCIplus, LPCMCI algorithms with conditional independence tests.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": "https://tigramite.readthedocs.io/",
    "github_url": "https://github.com/jakobrunge/tigramite",
    "url": "https://github.com/jakobrunge/tigramite",
    "install": "pip install tigramite",
    "tags": [
      "causal inference",
      "graphs"
    ],
    "best_for": "Learning causal structure from data, DAG estimation",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "time-series-analysis",
      "directed-acyclic-graphs",
      "conditional-independence-testing"
    ],
    "topic_tags": [
      "causal-discovery",
      "time-series",
      "pcmci",
      "granger-causality",
      "python-package"
    ],
    "summary": "Tigramite is a Python package for discovering causal relationships in time series data using advanced algorithms like PCMCI and PCMCIplus. It's designed for researchers who need to identify causal structures from temporal data while accounting for confounders and complex dependencies. The package implements state-of-the-art conditional independence tests specifically tailored for time series causal discovery.",
    "use_cases": [
      "Identifying causal relationships between economic indicators over time",
      "Discovering feedback loops in climate variables from observational data"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "python package for causal discovery in time series",
      "PCMCI algorithm implementation",
      "time series causal inference tools",
      "discover causal graphs from temporal data"
    ]
  },
  {
    "name": "causal-learn",
    "description": "Comprehensive Python package serving as Python translation and extension of Java-based Tetrad toolkit for causal discovery algorithms.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": "https://causal-learn.readthedocs.io/",
    "github_url": "https://github.com/py-why/causal-learn",
    "url": "https://github.com/py-why/causal-learn",
    "install": "pip install causal-learn",
    "tags": [
      "causal inference",
      "graphs"
    ],
    "best_for": "Learning causal structure from data, DAG estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "directed-acyclic-graphs",
      "hypothesis-testing"
    ],
    "topic_tags": [
      "causal-discovery",
      "graphical-models",
      "python-package",
      "structural-learning",
      "tetrad"
    ],
    "summary": "Causal-learn is a comprehensive Python package that implements causal discovery algorithms to automatically learn causal relationships from observational data. It serves as the Python version of the established Tetrad toolkit, providing methods to discover causal graphs, estimate causal effects, and validate causal assumptions. The package is widely used by researchers and data scientists who need to move beyond correlational analysis to understand underlying causal mechanisms.",
    "use_cases": [
      "Learning causal structure from observational healthcare data to identify which treatments actually cause improved patient outcomes",
      "Discovering causal relationships in marketing attribution data to understand which touchpoints truly drive conversions versus mere correlation"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python package for causal discovery from data",
      "how to learn causal graphs automatically",
      "tetrad python implementation causal inference",
      "tools for discovering causal relationships observational data"
    ]
  },
  {
    "name": "causal-llm-bfs",
    "description": "LLM + BFS hybrid for efficient causal graph discovery. Uses language models to guide structure search.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": "https://github.com/superkaiba/causal-llm-bfs",
    "github_url": "https://github.com/superkaiba/causal-llm-bfs",
    "url": "https://github.com/superkaiba/causal-llm-bfs",
    "install": "pip install causal-llm-bfs",
    "tags": [
      "causal discovery",
      "LLM",
      "graphs"
    ],
    "best_for": "LLM-assisted causal discovery",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "causal-inference-methods",
      "graph-neural-networks",
      "transformer-architectures"
    ],
    "topic_tags": [
      "causal-discovery",
      "large-language-models",
      "graph-search",
      "structure-learning",
      "hybrid-methods"
    ],
    "summary": "A novel approach that combines large language models with breadth-first search algorithms to efficiently discover causal graph structures from data. The LLM provides intelligent guidance to the search process, potentially reducing the exponential complexity of traditional causal structure learning. This cutting-edge method is particularly useful for researchers working on automated causal discovery in complex domains.",
    "use_cases": [
      "Discovering causal relationships in high-dimensional observational datasets where domain knowledge can guide structure search",
      "Automating causal graph construction for A/B testing frameworks in tech platforms with hundreds of potential confounders"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "LLM guided causal discovery methods",
      "hybrid approaches for causal graph learning",
      "efficient algorithms for causal structure discovery",
      "language models for causal inference automation"
    ]
  },
  {
    "name": "gCastle",
    "description": "Huawei Noah's Ark Lab end-to-end causal structure learning toolchain emphasizing gradient-based methods with GPU acceleration (NOTEARS, GOLEM).",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": "https://gcastle.readthedocs.io/",
    "github_url": "https://github.com/huawei-noah/trustworthyAI",
    "url": "https://github.com/huawei-noah/trustworthyAI",
    "install": "pip install gcastle",
    "tags": [
      "causal inference",
      "graphs"
    ],
    "best_for": "Learning causal structure from data, DAG estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-programming",
      "networkx",
      "pytorch-basics"
    ],
    "topic_tags": [
      "causal-discovery",
      "directed-acyclic-graphs",
      "gradient-optimization",
      "gpu-computing",
      "python-package"
    ],
    "summary": "gCastle is Huawei Noah's Ark Lab's comprehensive Python toolchain for learning causal structure from observational data using gradient-based optimization methods. It provides GPU-accelerated implementations of popular algorithms like NOTEARS and GOLEM for discovering directed acyclic graphs (DAGs) that represent causal relationships. The package offers an end-to-end pipeline from data preprocessing to causal graph visualization and evaluation.",
    "use_cases": [
      "Discovering causal relationships in business metrics to understand which factors drive customer churn or revenue growth",
      "Learning gene regulatory networks from genomics data to identify which genes causally influence disease outcomes"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "gradient based causal discovery python",
      "NOTEARS GOLEM implementation GPU",
      "causal structure learning toolchain",
      "DAG discovery from observational data"
    ]
  },
  {
    "name": "py-tetrad",
    "description": "Python interface to Tetrad Java library using JPype, providing direct access to Tetrad's causal discovery algorithms with efficient data translation.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": null,
    "github_url": "https://github.com/py-why/py-tetrad",
    "url": "https://github.com/py-why/py-tetrad",
    "install": "Available on GitHub (installation via git clone)",
    "tags": [
      "causal inference",
      "graphs"
    ],
    "best_for": "Learning causal structure from data, DAG estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "directed-acyclic-graphs",
      "java-installation"
    ],
    "topic_tags": [
      "causal-discovery",
      "graphical-models",
      "python-wrapper",
      "tetrad-library",
      "causal-inference"
    ],
    "summary": "py-tetrad is a Python wrapper that provides access to the comprehensive Tetrad Java library for causal discovery and graphical modeling. It allows researchers to use Tetrad's powerful algorithms like PC, FCI, and GES directly from Python environments with seamless data conversion. The package bridges Python's data science ecosystem with Tetrad's mature causal inference implementations.",
    "use_cases": [
      "Learning causal structure from observational data to identify potential interventions in business processes",
      "Applying constraint-based algorithms to discover causal relationships in experimental datasets"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "python wrapper for tetrad causal discovery",
      "how to use tetrad algorithms in python",
      "causal structure learning python package",
      "tetrad java library python interface"
    ]
  },
  {
    "name": "CATENets",
    "description": "JAX-accelerated neural network CATE estimators implementing SNet, FlexTENet, TARNet, CFRNet, and DragonNet architectures.",
    "category": "Causal Inference & Matching",
    "docs_url": null,
    "github_url": "https://github.com/AliciaCurth/CATENets",
    "url": "https://github.com/AliciaCurth/CATENets",
    "install": "pip install catenets",
    "tags": [
      "causal inference",
      "deep learning",
      "JAX"
    ],
    "best_for": "GPU-accelerated neural CATE estimation",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "causal-inference-fundamentals",
      "neural-networks",
      "JAX-programming"
    ],
    "topic_tags": [
      "neural-cate-estimation",
      "treatment-effects",
      "deep-causal-inference",
      "JAX-acceleration"
    ],
    "summary": "CATENets provides JAX-accelerated implementations of state-of-the-art neural network architectures for estimating Conditional Average Treatment Effects (CATE). The package includes multiple deep learning approaches like TARNet, DragonNet, and FlexTENet for heterogeneous treatment effect estimation. It's designed for researchers and practitioners who need scalable, GPU-accelerated causal inference methods for complex, high-dimensional data.",
    "use_cases": [
      "Estimating personalized treatment effects in A/B tests with complex user features and interactions",
      "Analyzing heterogeneous policy impacts across different demographic groups using observational data"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "neural network CATE estimation JAX",
      "deep learning treatment effects package",
      "TARNet DragonNet implementation",
      "heterogeneous treatment effects neural networks"
    ]
  },
  {
    "name": "CausalInference",
    "description": "Implements classical causal inference methods like propensity score matching, inverse probability weighting, stratification.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://causalinferenceinpython.org",
    "github_url": "https://github.com/laurencium/causalinference",
    "url": "https://github.com/laurencium/causalinference",
    "install": "pip install CausalInference",
    "tags": [
      "causal inference",
      "matching"
    ],
    "best_for": "Estimating treatment effects, propensity score matching, observational studies",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "regression-analysis",
      "observational-studies"
    ],
    "topic_tags": [
      "propensity-score-matching",
      "treatment-effects",
      "causal-methods",
      "python-package"
    ],
    "summary": "CausalInference is a Python package that implements foundational causal inference methods for estimating treatment effects from observational data. It provides accessible implementations of propensity score matching, inverse probability weighting, and stratification techniques. The package is ideal for data scientists who need to move beyond correlation to establish causal relationships in non-experimental settings.",
    "use_cases": [
      "Measuring the impact of a marketing campaign on customer retention using observational data",
      "Evaluating the effect of a product feature launch on user engagement when randomized experiments weren't possible"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python package for propensity score matching",
      "how to implement inverse probability weighting",
      "causal inference methods for observational data",
      "treatment effect estimation with matching"
    ]
  },
  {
    "name": "CausalLib",
    "description": "IBM-developed package that provides a scikit-learn-inspired API for causal inference with meta-algorithms supporting arbitrary machine learning models.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://causallib.readthedocs.io/",
    "github_url": "https://github.com/IBM/causallib",
    "url": "https://github.com/IBM/causallib",
    "install": "pip install causallib",
    "tags": [
      "causal inference",
      "matching"
    ],
    "best_for": "Estimating treatment effects, propensity score matching, observational studies",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-scikit-learn",
      "basic-causal-inference",
      "propensity-score-matching"
    ],
    "topic_tags": [
      "causal-inference",
      "python-package",
      "matching-methods",
      "meta-algorithms",
      "sklearn-api"
    ],
    "summary": "CausalLib is IBM's Python package that brings causal inference methods into a familiar scikit-learn framework. It provides standardized implementations of matching, weighting, and other causal methods that can work with any machine learning model as the underlying estimator. The package is designed for practitioners who want to apply causal inference techniques without building everything from scratch.",
    "use_cases": [
      "Estimating treatment effects in A/B tests with confounding variables using propensity score matching",
      "Evaluating the causal impact of marketing campaigns by combining machine learning models with causal inference methods"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "scikit-learn style causal inference package",
      "IBM CausalLib tutorial",
      "python package for causal inference with machine learning",
      "causal inference library that works with sklearn models"
    ]
  },
  {
    "name": "CausalML",
    "description": "Focuses on uplift modeling and heterogeneous treatment effect estimation using machine learning techniques.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://causalml.readthedocs.io/",
    "github_url": "https://github.com/uber/causalml",
    "url": "https://github.com/uber/causalml",
    "install": "pip install causalml",
    "tags": [
      "causal inference",
      "matching"
    ],
    "best_for": "Estimating treatment effects, propensity score matching, observational studies",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-scikit-learn",
      "randomized-controlled-trials",
      "regression-analysis"
    ],
    "topic_tags": [
      "uplift-modeling",
      "heterogeneous-treatment-effects",
      "causal-ml",
      "personalization",
      "python-package"
    ],
    "summary": "CausalML is a Python package that enables practitioners to estimate heterogeneous treatment effects and perform uplift modeling using machine learning approaches. It provides implementations of meta-learners like T-learner, S-learner, and X-learner to identify which users or segments respond differently to treatments. The package is particularly valuable for personalizing interventions in marketing, product features, and policy decisions.",
    "use_cases": [
      "Determining which customers should receive marketing promotions based on their likelihood to respond positively",
      "Identifying user segments that benefit most from a new product feature to optimize rollout strategy"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "uplift modeling python package",
      "heterogeneous treatment effects machine learning",
      "CausalML tutorial implementation",
      "personalized treatment effect estimation tools"
    ]
  },
  {
    "name": "CausalMatch",
    "description": "Implements Propensity Score Matching (PSM) and Coarsened Exact Matching (CEM) with ML flexibility for propensity score estimation.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://github.com/bytedance/CausalMatch",
    "github_url": null,
    "url": "https://github.com/bytedance/CausalMatch",
    "install": "pip install causalmatch",
    "tags": [
      "causal inference",
      "matching"
    ],
    "best_for": "Estimating treatment effects, propensity score matching, observational studies",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-scikit-learn",
      "logistic-regression",
      "pandas-dataframes"
    ],
    "topic_tags": [
      "propensity-score-matching",
      "causal-inference",
      "observational-data",
      "treatment-effects",
      "python-package"
    ],
    "summary": "CausalMatch is a Python package that implements propensity score matching and coarsened exact matching methods for causal inference from observational data. It allows data scientists to estimate treatment effects by matching treated and control units with similar characteristics, incorporating machine learning models for flexible propensity score estimation. The package is particularly useful for reducing selection bias when randomized experiments aren't feasible.",
    "use_cases": [
      "Measuring impact of marketing campaigns on customer retention using historical user data",
      "Evaluating effectiveness of new product features by matching users who adopted the feature with similar non-adopters"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "propensity score matching python package",
      "causal inference matching methods implementation",
      "how to do PSM with machine learning models",
      "coarsened exact matching python library"
    ]
  },
  {
    "name": "CausalPlayground",
    "description": "Python library for causal research that addresses the scarcity of real-world datasets with known causal relations. Provides fine-grained control over structural causal models.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://causal-playground.readthedocs.io/",
    "github_url": "https://github.com/causal-playground/causal-playground",
    "url": "https://github.com/causal-playground/causal-playground",
    "install": "pip install causal-playground",
    "tags": [
      "causal inference",
      "matching"
    ],
    "best_for": "Estimating treatment effects, propensity score matching, observational studies",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "causal-diagrams",
      "structural-causal-models"
    ],
    "topic_tags": [
      "causal-inference",
      "simulation",
      "synthetic-data",
      "python-package",
      "experimental-design"
    ],
    "summary": "CausalPlayground is a Python library that generates synthetic datasets with known causal relationships for causal inference research and education. It allows researchers to create controlled environments for testing causal methods when real-world data with ground truth causal effects is unavailable. The library provides fine-grained control over structural causal models, making it valuable for benchmarking causal inference algorithms.",
    "use_cases": [
      "Testing and comparing different causal inference methods on datasets where true causal effects are known",
      "Teaching causal inference concepts by generating examples with controllable confounding and treatment effects"
    ],
    "audience": [
      "Early-PhD",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for generating synthetic causal datasets",
      "how to create fake data with known causal relationships",
      "causal inference simulation package python",
      "testing causal methods with ground truth data"
    ]
  },
  {
    "name": "CausalPy",
    "description": "Developed by PyMC Labs, focuses specifically on causal inference in quasi-experimental settings. Specializes in scenarios where randomization is impossible or expensive.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://www.pymc-marketing.io/",
    "github_url": "https://github.com/pymc-labs/pymc-marketing",
    "url": "https://github.com/pymc-labs/pymc-marketing",
    "install": "pip install CausalPy",
    "tags": [
      "causal inference",
      "matching"
    ],
    "best_for": "Estimating treatment effects, propensity score matching, observational studies",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "bayesian-statistics",
      "difference-in-differences"
    ],
    "topic_tags": [
      "causal-inference",
      "quasi-experimental",
      "bayesian-methods",
      "pymc",
      "econometrics"
    ],
    "summary": "CausalPy is a Python package by PyMC Labs designed for causal inference in quasi-experimental settings using Bayesian methods. It's particularly valuable when randomized controlled trials aren't feasible, offering tools for difference-in-differences, interrupted time series, and regression discontinuity designs. The package integrates with PyMC's probabilistic programming framework to provide uncertainty quantification for causal estimates.",
    "use_cases": [
      "Evaluating the impact of a policy change on business metrics using interrupted time series analysis",
      "Measuring treatment effects in observational data where randomization wasn't possible using synthetic control methods"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "Python package for causal inference without randomization",
      "Bayesian quasi-experimental analysis tools",
      "CausalPy vs other causal inference libraries",
      "How to do difference-in-differences with uncertainty quantification"
    ]
  },
  {
    "name": "DoWhy",
    "description": "End-to-end framework for causal inference based on causal graphs (DAGs) and potential outcomes. Covers identification, estimation, refutation.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://www.pywhy.org/dowhy/",
    "github_url": "https://github.com/py-why/dowhy",
    "url": "https://github.com/py-why/dowhy",
    "install": "pip install dowhy",
    "tags": [
      "causal inference",
      "matching"
    ],
    "best_for": "Estimating treatment effects, propensity score matching, observational studies",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "directed-acyclic-graphs",
      "regression-analysis"
    ],
    "topic_tags": [
      "causal-inference",
      "dag-modeling",
      "treatment-effects",
      "python-package",
      "experimental-design"
    ],
    "summary": "DoWhy is a comprehensive Python framework that implements the four-step causal inference process: modeling with DAGs, identification of causal effects, estimation using various methods, and robustness testing through refutation. It provides a unified interface for causal analysis that guides practitioners through proper methodology while supporting multiple estimation techniques from matching to instrumental variables.",
    "use_cases": [
      "Measuring impact of product feature changes on user engagement when randomized experiments aren't feasible",
      "Evaluating effectiveness of marketing campaigns or policy interventions using observational data"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python package for causal inference with DAGs",
      "end-to-end framework for measuring treatment effects",
      "DoWhy tutorial for causal analysis",
      "how to do causal inference in python observational data"
    ]
  },
  {
    "name": "KECENI",
    "description": "Doubly robust, non-parametric estimation of node-wise counterfactual means under network interference (arXiv 2024).",
    "category": "Causal Inference & Matching",
    "docs_url": null,
    "github_url": "https://github.com/HeejongBong/KECENI",
    "url": "https://pypi.org/project/KECENI/",
    "install": "pip install keceni",
    "tags": [
      "networks",
      "spillovers",
      "causal inference"
    ],
    "best_for": "Network interference with node heterogeneity",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "causal-inference",
      "network-analysis",
      "doubly-robust-estimation"
    ],
    "topic_tags": [
      "network-interference",
      "counterfactual-estimation",
      "doubly-robust",
      "spillover-effects",
      "non-parametric"
    ],
    "summary": "KECENI implements doubly robust estimation for measuring counterfactual outcomes when treatments have spillover effects through network connections. It provides non-parametric methods to estimate what would have happened to each node under different treatment assignments, accounting for both direct effects and indirect effects from connected nodes. This is particularly valuable for economists and data scientists analyzing interventions in networked systems where traditional causal inference methods fail due to interference.",
    "use_cases": [
      "Estimating impact of targeted ads on social media users and their friends",
      "Measuring spillover effects of job training programs in communities with social connections"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "network interference causal inference estimation",
      "spillover effects counterfactual analysis package",
      "doubly robust network treatment effects",
      "causal inference with network spillovers python"
    ]
  },
  {
    "name": "NetworkCausalTree",
    "description": "Estimates both direct treatment effects and spillover effects under clustered network interference (Bargagli-Stoffi et al. 2025).",
    "category": "Causal Inference & Matching",
    "docs_url": null,
    "github_url": "https://github.com/fbargaglistoffi/NetworkCausalTree",
    "url": "https://github.com/fbargaglistoffi/NetworkCausalTree",
    "install": "pip install networkcausaltree",
    "tags": [
      "causal inference",
      "networks",
      "spillovers"
    ],
    "best_for": "Treatment effects with network interference",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "causal-forests",
      "graph-theory",
      "clustered-standard-errors"
    ],
    "topic_tags": [
      "network-effects",
      "treatment-spillovers",
      "causal-trees",
      "interference-estimation"
    ],
    "summary": "NetworkCausalTree extends causal forest methods to estimate both direct treatment effects and network spillover effects when units are connected in clusters or networks. It handles the complex interference patterns that arise when treating one unit affects connected units, which standard causal inference methods cannot capture. This is particularly valuable for economists and data scientists analyzing interventions in networked settings like social media, organizations, or geographic regions.",
    "use_cases": [
      "Measuring how a social media algorithm change affects both treated users and their connected friends",
      "Estimating spillover effects of employee training programs within organizational teams or departments"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "network spillover effects causal inference",
      "estimate treatment effects with interference",
      "causal forests for networked data",
      "clustered network interference estimation"
    ]
  },
  {
    "name": "PySensemakr",
    "description": "Implements Cinelli-Hazlett framework for assessing robustness to unobserved confounding. Computes confounder strength needed to invalidate results.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://github.com/carloscinelli/PySensemakr",
    "github_url": "https://github.com/carloscinelli/PySensemakr",
    "url": "https://github.com/carloscinelli/PySensemakr",
    "install": "pip install pysensemakr",
    "tags": [
      "causal inference",
      "sensitivity analysis",
      "robustness"
    ],
    "best_for": "Sensitivity analysis with publication-ready contour plots",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "causal-inference-basics",
      "python-pandas",
      "regression-analysis"
    ],
    "topic_tags": [
      "sensitivity-analysis",
      "confounding",
      "causal-inference",
      "robustness-checks"
    ],
    "summary": "PySensemakr implements the Cinelli-Hazlett sensitivity analysis framework to assess how robust causal estimates are to unobserved confounding. It calculates how strong an unmeasured confounder would need to be to overturn your causal conclusions. Essential for validating observational studies and strengthening causal arguments.",
    "use_cases": [
      "Testing robustness of A/B test results when you suspect unmeasured confounders like user engagement",
      "Validating causal effects in observational studies by quantifying confounder strength needed to nullify findings"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "how to test robustness unobserved confounding python",
      "sensitivity analysis causal inference package",
      "check if unmeasured variables affect causal results",
      "Cinelli Hazlett sensitivity analysis implementation"
    ]
  },
  {
    "name": "aipyw",
    "description": "Minimal, fast AIPW (Augmented Inverse Probability Weighting) implementation for discrete treatments. Sklearn-compatible with cross-fitting.",
    "category": "Causal Inference & Matching",
    "docs_url": null,
    "github_url": "https://github.com/apoorvalal/aipyw",
    "url": "https://github.com/apoorvalal/aipyw",
    "install": "pip install aipyw",
    "tags": [
      "causal inference",
      "AIPW",
      "treatment effects"
    ],
    "best_for": "Fast AIPW estimation with sklearn models",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-sklearn",
      "propensity-score-matching",
      "cross-validation"
    ],
    "topic_tags": [
      "AIPW",
      "treatment-effects",
      "doubly-robust",
      "causal-inference",
      "python-package"
    ],
    "summary": "AIPW is a doubly-robust method for estimating treatment effects that combines propensity score weighting with outcome modeling. This package provides a fast, sklearn-compatible implementation with cross-fitting to reduce overfitting bias. It's particularly useful for discrete treatment scenarios where you want robust causal estimates.",
    "use_cases": [
      "Estimating the effect of a marketing campaign on customer purchases using observational data",
      "Measuring impact of a feature rollout on user engagement when randomization wasn't possible"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "AIPW implementation python",
      "doubly robust treatment effect estimation",
      "sklearn compatible causal inference package",
      "augmented inverse probability weighting discrete treatments"
    ]
  },
  {
    "name": "causal-curve",
    "description": "Continuous treatment dose-response curve estimation. GPS and TMLE methods for continuous treatments.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://causal-curve.readthedocs.io/",
    "github_url": "https://github.com/ronikobrosly/causal-curve",
    "url": "https://github.com/ronikobrosly/causal-curve",
    "install": "pip install causal-curve",
    "tags": [
      "dose-response",
      "continuous treatment",
      "GPS"
    ],
    "best_for": "Dose-response curves for continuous treatments",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "propensity-score-matching",
      "python-scikit-learn",
      "TMLE"
    ],
    "topic_tags": [
      "dose-response",
      "continuous-treatment",
      "GPS",
      "TMLE",
      "causal-inference"
    ],
    "summary": "A Python package for estimating dose-response curves when treatments are continuous rather than binary. Implements Generalized Propensity Score (GPS) and Targeted Maximum Likelihood Estimation (TMLE) methods to estimate causal effects across different treatment intensities. Useful for understanding how varying levels of intervention affect outcomes.",
    "use_cases": [
      "Measuring how different advertising spend levels affect customer acquisition",
      "Estimating optimal drug dosage effects on patient recovery rates"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "continuous treatment dose response curve python",
      "GPS TMLE implementation continuous treatments",
      "how to estimate causal effects with continuous interventions",
      "dose response analysis package python"
    ]
  },
  {
    "name": "fastmatch",
    "description": "Fast k-nearest-neighbor matching for large datasets using Facebook's FAISS library.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://github.com/py-econometrics/fastmatch",
    "github_url": null,
    "url": "https://github.com/py-econometrics/fastmatch",
    "install": "pip install fastmatch",
    "tags": [
      "causal inference",
      "matching"
    ],
    "best_for": "Estimating treatment effects, propensity score matching, observational studies",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "propensity-score-matching",
      "python-scikit-learn",
      "nearest-neighbors-algorithms"
    ],
    "topic_tags": [
      "k-nearest-neighbors",
      "propensity-matching",
      "causal-inference",
      "faiss-library",
      "python-package"
    ],
    "summary": "A Python package that accelerates k-nearest-neighbor matching for causal inference by leveraging Facebook's FAISS library for efficient similarity search. It's particularly useful for matching treatment and control units in observational studies when dealing with large datasets that would be computationally expensive with traditional matching methods. The package enables researchers to perform propensity score matching and other distance-based matching at scale.",
    "use_cases": [
      "Matching treated customers to control customers in a large e-commerce dataset for causal impact analysis",
      "Finding similar firms as controls when evaluating the effect of a policy intervention on company outcomes"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "fast knn matching large datasets python",
      "propensity score matching performance optimization",
      "FAISS library causal inference matching",
      "scalable nearest neighbor matching treatment control"
    ]
  },
  {
    "name": "mcf (Modified Causal Forest)",
    "description": "Comprehensive Python implementation for heterogeneous treatment effect estimation. Handles binary/multiple discrete treatments with optimal policy learning via Policy Trees.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://mcfpy.github.io/mcf/",
    "github_url": "https://github.com/MCFpy/mcf",
    "url": "https://github.com/MCFpy/mcf",
    "install": "pip install mcf",
    "tags": [
      "causal inference",
      "treatment effects",
      "policy learning"
    ],
    "best_for": "CATE estimation with policy tree optimization",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-scikit-learn",
      "random-forests",
      "causal-inference-fundamentals"
    ],
    "topic_tags": [
      "heterogeneous-treatment-effects",
      "causal-forests",
      "policy-optimization",
      "python-implementation"
    ],
    "summary": "MCF is a Python package that extends causal forests to estimate how treatment effects vary across different subgroups in your data. It's designed for data scientists running A/B tests or policy evaluations who need to understand not just average treatment effects, but which users or segments benefit most from interventions. The package includes policy tree functionality to automatically recommend optimal treatment assignment rules.",
    "use_cases": [
      "Personalizing product features by estimating which user segments respond best to different UI changes",
      "Optimizing marketing campaigns by identifying customer characteristics that predict higher response to specific promotional strategies"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python package for heterogeneous treatment effects",
      "causal forest implementation policy trees",
      "estimate treatment effect variation subgroups",
      "personalized treatment assignment rules python"
    ]
  },
  {
    "name": "pydtr",
    "description": "Dynamic treatment regimes using Iterative Q-Learning. Scikit-learn compatible for multi-stage optimal treatment sequencing.",
    "category": "Causal Inference & Matching",
    "docs_url": null,
    "github_url": "https://github.com/fullflu/pydtr",
    "url": "https://pypi.org/project/pydtr/",
    "install": "pip install pydtr",
    "tags": [
      "dynamic treatment",
      "reinforcement learning",
      "causal inference"
    ],
    "best_for": "Multi-stage dynamic treatment regimes",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "scikit-learn",
      "q-learning",
      "causal-inference-methods"
    ],
    "topic_tags": [
      "dynamic-treatment",
      "q-learning",
      "multi-stage-treatment",
      "personalized-medicine",
      "python-package"
    ],
    "summary": "PyDTR implements dynamic treatment regimes using Iterative Q-Learning to optimize sequential treatment decisions over multiple time periods. It's designed for researchers and data scientists working on personalized treatment strategies where optimal actions depend on patient history and evolving conditions. The package provides scikit-learn compatible tools for learning adaptive treatment policies from observational or experimental data.",
    "use_cases": [
      "Optimizing sequential drug dosing regimens based on patient response history and biomarkers",
      "Designing adaptive mobile health interventions that adjust messaging frequency based on user engagement patterns"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "dynamic treatment regime python implementation",
      "iterative q-learning for personalized medicine",
      "multi-stage treatment optimization scikit-learn",
      "sequential treatment decision reinforcement learning"
    ]
  },
  {
    "name": "pyregadj",
    "description": "Regression and ML adjustments to treatment effects in RCTs. Implements List et al. (2024) methods.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://github.com/vyasenov/pyregadj",
    "github_url": "https://github.com/vyasenov/pyregadj",
    "url": "https://github.com/vyasenov/pyregadj",
    "install": "pip install pyregadj",
    "tags": [
      "RCT",
      "regression adjustment",
      "treatment effects"
    ],
    "best_for": "Covariate adjustment in experiments",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "randomized-controlled-trials",
      "python-scikit-learn",
      "linear-regression"
    ],
    "topic_tags": [
      "regression-adjustment",
      "RCT-analysis",
      "treatment-effects",
      "variance-reduction",
      "python-package"
    ],
    "summary": "A Python package implementing regression and machine learning adjustments for treatment effect estimation in randomized controlled trials, based on List et al. (2024) methods. It helps reduce variance in RCT analysis by incorporating baseline covariates and pre-treatment variables. Particularly useful for improving precision of treatment effect estimates when you have rich baseline data.",
    "use_cases": [
      "Analyzing A/B test results with user demographic and behavioral covariates to get more precise treatment effects",
      "Estimating impact of educational interventions while controlling for student baseline characteristics and school fixed effects"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "how to reduce variance in RCT treatment effect estimates",
      "regression adjustment methods for A/B testing",
      "pyregadj package for randomized experiments",
      "improving precision of treatment effects with covariates"
    ]
  },
  {
    "name": "scikit-uplift",
    "description": "Focuses on uplift modeling and estimating heterogeneous treatment effects using various ML-based methods.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://scikit-uplift.readthedocs.io/en/latest/",
    "github_url": "https://github.com/maks-sh/scikit-uplift",
    "url": "https://github.com/maks-sh/scikit-uplift",
    "install": "pip install scikit-uplift",
    "tags": [
      "causal inference",
      "matching"
    ],
    "best_for": "Estimating treatment effects, propensity score matching, observational studies",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "randomized-controlled-trials",
      "python-scikit-learn",
      "causal-inference-basics"
    ],
    "topic_tags": [
      "uplift-modeling",
      "heterogeneous-treatment-effects",
      "causal-ml",
      "personalization",
      "python-package"
    ],
    "summary": "scikit-uplift is a Python package designed for uplift modeling, which predicts the incremental impact of treatments on individual units rather than just overall treatment effects. It provides machine learning-based methods to identify which customers or users will respond most positively to interventions like marketing campaigns or product changes. The package is particularly valuable for personalization and targeting decisions in business applications.",
    "use_cases": [
      "Identifying which customers to target with promotional offers to maximize incremental revenue",
      "Determining which users should receive a new product feature to optimize engagement lift"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "uplift modeling python package",
      "heterogeneous treatment effects machine learning",
      "scikit-uplift tutorial personalization",
      "how to measure incremental impact of interventions"
    ]
  },
  {
    "name": "y0",
    "description": "Causal inference framework providing tools for causal graph manipulation and effect identification.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://y0.readthedocs.io/",
    "github_url": "https://github.com/y0-causal-inference/y0",
    "url": "https://github.com/y0-causal-inference/y0",
    "install": "pip install y0",
    "tags": [
      "causal inference",
      "graphs",
      "identification"
    ],
    "best_for": "Causal graph manipulation and do-calculus",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-programming",
      "directed-acyclic-graphs",
      "causal-identification-theory"
    ],
    "topic_tags": [
      "causal-inference",
      "graph-manipulation",
      "identification-algorithms",
      "python-package",
      "causal-graphs"
    ],
    "summary": "Y0 is a Python framework for causal inference that provides computational tools for manipulating causal graphs and identifying causal effects. It implements algorithms for causal identification, graph operations, and effect estimation from observational data. The package is designed for researchers and practitioners who need to work with causal DAGs and perform formal causal analysis.",
    "use_cases": [
      "Determining if a causal effect is identifiable from observational data given a causal graph",
      "Manipulating and analyzing complex causal graphs with confounders and mediators"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python package for causal graph manipulation",
      "how to identify causal effects from DAGs",
      "y0 causal inference framework tutorial",
      "tools for causal identification algorithms"
    ]
  },
  {
    "name": "ATbounds",
    "description": "Implements modern treatment effect bounds beyond basic Manski worst-case scenarios. Provides tighter bounds using monotonicity, mean independence, and other assumptions following Lee and Weidner (2021).",
    "category": "Causal Inference (Bounds)",
    "docs_url": "https://cran.r-project.org/web/packages/ATbounds/ATbounds.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=ATbounds",
    "install": "install.packages(\"ATbounds\")",
    "tags": [
      "partial-identification",
      "bounds",
      "treatment-effects",
      "Manski",
      "monotonicity"
    ],
    "best_for": "Modern treatment effect bounds with tighter identification under various assumptions, implementing Lee & Weidner (2021)",
    "language": "R",
    "difficulty": "advanced",
    "prerequisites": [
      "causal-inference",
      "manski-bounds",
      "potential-outcomes"
    ],
    "topic_tags": [
      "partial-identification",
      "treatment-effects",
      "bounds",
      "causal-inference",
      "monotonicity"
    ],
    "summary": "ATbounds implements modern treatment effect bounds that improve upon basic Manski worst-case bounds by incorporating additional assumptions like monotonicity and mean independence. The package follows Lee and Weidner (2021) methodology to provide tighter identification bounds when randomized experiments aren't feasible. It's particularly useful for causal inference researchers working with observational data where point identification is impossible.",
    "use_cases": [
      "Evaluating job training programs where participation is endogenous and randomization isn't possible",
      "Estimating returns to education when ability is unobserved and creates selection bias"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "treatment effect bounds beyond Manski",
      "partial identification with monotonicity assumptions",
      "Lee Weidner bounds implementation",
      "tighter bounds for causal inference"
    ]
  },
  {
    "name": "CausalGPS",
    "description": "Machine learning-based generalized propensity score estimation for continuous treatments. Uses SuperLearner ensemble methods for flexible estimation of dose-response curves.",
    "category": "Causal Inference (Continuous Treatment)",
    "docs_url": "https://cran.r-project.org/web/packages/CausalGPS/CausalGPS.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=CausalGPS",
    "install": "install.packages(\"CausalGPS\")",
    "tags": [
      "GPS",
      "continuous-treatment",
      "machine-learning",
      "SuperLearner",
      "dose-response"
    ],
    "best_for": "ML-based generalized propensity scores for continuous treatment dose-response estimation",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "propensity-score-matching",
      "python-scikit-learn",
      "causal-inference-fundamentals"
    ],
    "topic_tags": [
      "generalized-propensity-score",
      "continuous-treatment",
      "dose-response",
      "ensemble-methods",
      "causal-inference"
    ],
    "summary": "CausalGPS provides machine learning-based estimation of generalized propensity scores for continuous treatment variables, extending traditional binary propensity score methods. It leverages SuperLearner ensemble methods to flexibly model the relationship between covariates and continuous treatment doses. The package enables robust estimation of dose-response curves and causal effects when treatments vary in intensity rather than just presence/absence.",
    "use_cases": [
      "Estimating the causal effect of different advertising spend levels on customer acquisition",
      "Analyzing how varying dosages of a medical intervention affect patient outcomes"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "generalized propensity score for continuous treatment",
      "dose response curve estimation python",
      "causal inference continuous treatment variable",
      "SuperLearner propensity score matching"
    ]
  },
  {
    "name": "DRDID",
    "description": "Implements locally efficient doubly robust DiD estimators that combine inverse probability weighting and outcome regression for improved statistical properties. Handles both panel data and repeated cross-sections in the canonical 2x2 DiD setting with covariates, providing robustness against model misspecification.",
    "category": "Causal Inference (DiD)",
    "docs_url": "https://psantanna.com/DRDID/",
    "github_url": "https://github.com/pedrohcgs/DRDID",
    "url": "https://cran.r-project.org/package=DRDID",
    "install": "install.packages(\"DRDID\")",
    "tags": [
      "doubly-robust",
      "difference-in-differences",
      "inverse-probability-weighting",
      "ATT",
      "covariates"
    ],
    "best_for": "Two-period DiD with covariates requiring robust estimation against model misspecification, implementing Sant'Anna & Zhao (2020)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "difference-in-differences",
      "propensity-score-matching",
      "panel-data-analysis"
    ],
    "topic_tags": [
      "doubly-robust",
      "difference-in-differences",
      "causal-inference",
      "treatment-effects",
      "R-package"
    ],
    "summary": "DRDID is an R package that implements doubly robust difference-in-differences estimators, combining inverse probability weighting with outcome regression for more reliable causal effect estimates. It's designed for researchers who need robust DiD analysis with covariates in 2x2 treatment/control settings. The package provides protection against model misspecification by requiring only one of the two models (propensity score or outcome regression) to be correctly specified.",
    "use_cases": [
      "Evaluating the causal impact of a policy intervention when treatment assignment depends on observable characteristics",
      "Analyzing the effect of a product launch or business intervention when you have pre/post data and want to control for confounding variables"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "doubly robust difference in differences R package",
      "DiD with covariates and inverse probability weighting",
      "robust causal inference for policy evaluation",
      "DRDID package for treatment effect estimation"
    ]
  },
  {
    "name": "HonestDiD",
    "description": "Constructs robust confidence intervals for DiD and event-study designs under violations of parallel trends. Allows researchers to conduct sensitivity analysis by relaxing the parallel trends assumption using smoothness or relative magnitude restrictions on pre-trend violations.",
    "category": "Causal Inference (DiD)",
    "docs_url": "https://github.com/asheshrambachan/HonestDiD",
    "github_url": "https://github.com/asheshrambachan/HonestDiD",
    "url": "https://cran.r-project.org/package=HonestDiD",
    "install": "install.packages(\"HonestDiD\")",
    "tags": [
      "sensitivity-analysis",
      "parallel-trends",
      "robust-inference",
      "confidence-intervals",
      "event-study"
    ],
    "best_for": "Assessing how treatment effect conclusions change under plausible parallel trends violations, implementing Rambachan & Roth (2023)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "difference-in-differences",
      "event-study-methodology",
      "causal-inference-fundamentals"
    ],
    "topic_tags": [
      "sensitivity-analysis",
      "parallel-trends",
      "robust-inference",
      "difference-in-differences",
      "event-study"
    ],
    "summary": "HonestDiD is an R package that helps researchers test the robustness of difference-in-differences and event study results when the parallel trends assumption may be violated. It constructs confidence intervals that remain valid under specified violations of parallel trends, allowing for more credible causal inference. The package is particularly valuable for researchers who want to move beyond just testing parallel trends to actually accounting for potential violations in their inference.",
    "use_cases": [
      "Evaluating policy impact on employment outcomes when treated and control groups show different pre-treatment trends",
      "Analyzing the effect of a product launch on user engagement when baseline metrics aren't perfectly parallel across test regions"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "robust confidence intervals for difference in differences",
      "sensitivity analysis parallel trends assumption violation",
      "how to handle non-parallel pre-trends in DiD",
      "honest DiD package for robust causal inference"
    ]
  },
  {
    "name": "bacondecomp",
    "description": "Performs Goodman-Bacon decomposition showing how two-way fixed effects (TWFE) estimates are weighted averages of all possible 2\u00d72 DiD comparisons. Essential for diagnosing negative weights problems in staggered adoption designs.",
    "category": "Causal Inference (DiD)",
    "docs_url": "https://cran.r-project.org/web/packages/bacondecomp/bacondecomp.pdf",
    "github_url": "https://github.com/evanjflack/bacondecomp",
    "url": "https://cran.r-project.org/package=bacondecomp",
    "install": "install.packages(\"bacondecomp\")",
    "tags": [
      "DiD",
      "TWFE",
      "Goodman-Bacon",
      "decomposition",
      "staggered-adoption"
    ],
    "best_for": "Goodman-Bacon decomposition for diagnosing negative weights in TWFE staggered DiD designs",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "difference-in-differences",
      "fixed-effects-regression",
      "panel-data-analysis"
    ],
    "topic_tags": [
      "goodman-bacon",
      "twfe-decomposition",
      "staggered-adoption",
      "negative-weights",
      "causal-inference"
    ],
    "summary": "The bacondecomp package implements the Goodman-Bacon decomposition, which breaks down two-way fixed effects estimates into weighted averages of all possible 2x2 difference-in-differences comparisons. This diagnostic tool is essential for understanding when TWFE estimates may be biased due to negative weights in staggered treatment adoption settings. Researchers use it to evaluate whether their DiD design is compromised by treatment effect heterogeneity.",
    "use_cases": [
      "Diagnosing bias in a staggered rollout study where different states adopted a policy at different times",
      "Evaluating the validity of TWFE estimates in a corporate setting where business units received treatment interventions sequentially"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "goodman bacon decomposition python",
      "how to diagnose negative weights in difference in differences",
      "twfe decomposition staggered adoption",
      "bacon decomposition package implementation"
    ]
  },
  {
    "name": "did",
    "description": "Implements group-time average treatment effects (ATT(g,t)) for staggered DiD designs with multiple periods and variation in treatment timing. Provides flexible aggregation into event-study plots or overall treatment effect estimates, addressing the well-documented negative weighting issues with conventional TWFE under staggered adoption.",
    "category": "Causal Inference (DiD)",
    "docs_url": "https://bcallaway11.github.io/did/",
    "github_url": "https://github.com/bcallaway11/did",
    "url": "https://cran.r-project.org/package=did",
    "install": "install.packages(\"did\")",
    "tags": [
      "difference-in-differences",
      "staggered-adoption",
      "event-study",
      "treatment-effects",
      "panel-data"
    ],
    "best_for": "Staggered rollout designs where different units adopt treatment at different times, implementing the Callaway & Sant'Anna (2021) estimator",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "difference-in-differences",
      "panel-data-analysis",
      "R-programming"
    ],
    "topic_tags": [
      "difference-in-differences",
      "staggered-adoption",
      "event-study",
      "causal-inference",
      "panel-data"
    ],
    "summary": "The 'did' package implements modern difference-in-differences methods for staggered treatment adoption, solving the negative weighting problems in traditional two-way fixed effects models. It calculates group-time average treatment effects and aggregates them into event-study plots or overall estimates. Ideal for economists and data scientists analyzing policy interventions or product rollouts with varying treatment timing across units.",
    "use_cases": [
      "Evaluating the impact of minimum wage increases rolled out across different states at different times",
      "Measuring the effect of a new product feature launched to different user cohorts in a staggered fashion"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "staggered difference in differences R package",
      "how to fix negative weights in TWFE",
      "event study plot with multiple treatment periods",
      "group time average treatment effects implementation"
    ]
  },
  {
    "name": "didimputation",
    "description": "Implements the imputation-based DiD estimator that first estimates Y(0) counterfactuals from untreated observations using two-way fixed effects, then imputes treatment effects for treated units. Avoids negative weighting problems of conventional TWFE under heterogeneous treatment effects.",
    "category": "Causal Inference (DiD)",
    "docs_url": "https://github.com/kylebutts/didimputation",
    "github_url": "https://github.com/kylebutts/didimputation",
    "url": "https://cran.r-project.org/package=didimputation",
    "install": "install.packages(\"didimputation\")",
    "tags": [
      "imputation",
      "two-way-fixed-effects",
      "event-study",
      "counterfactual",
      "robust-estimation"
    ],
    "best_for": "Event-study designs where imputation-based correction for TWFE bias is preferred, implementing Borusyak, Jaravel & Spiess (2024)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "difference-in-differences",
      "two-way-fixed-effects",
      "panel-data-analysis"
    ],
    "topic_tags": [
      "did-imputation",
      "treatment-effects",
      "causal-inference",
      "heterogeneous-effects",
      "econometrics"
    ],
    "summary": "An R package implementing imputation-based difference-in-differences estimation that avoids the negative weighting issues of traditional two-way fixed effects models. It first estimates counterfactual outcomes for untreated units, then imputes treatment effects, making it robust to heterogeneous treatment effects across time and units.",
    "use_cases": [
      "Evaluating staggered policy rollouts where treatment timing varies across states or regions",
      "Measuring impact of corporate interventions implemented at different times across business units"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "DiD estimator that handles heterogeneous treatment effects",
      "imputation based difference in differences R package",
      "how to avoid negative weights in TWFE DiD",
      "robust DiD estimation with staggered adoption"
    ]
  },
  {
    "name": "fastdid",
    "description": "High-performance implementation of Callaway & Sant'Anna estimators optimized for large datasets with millions of observations. Reduces computation time from hours to seconds while supporting time-varying covariates and multiple events per unit.",
    "category": "Causal Inference (DiD)",
    "docs_url": "https://tsailintung.github.io/fastdid",
    "github_url": "https://github.com/TsaiLintung/fastdid",
    "url": "https://cran.r-project.org/package=fastdid",
    "install": "install.packages(\"fastdid\")",
    "tags": [
      "high-performance",
      "large-scale",
      "staggered-DiD",
      "time-varying-covariates",
      "fast-computation"
    ],
    "best_for": "Large-scale applications where standard did package is computationally prohibitive, with support for time-varying covariates",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "difference-in-differences",
      "python-pandas",
      "callaway-santanna-estimator"
    ],
    "topic_tags": [
      "staggered-did",
      "high-performance-computing",
      "causal-inference",
      "large-datasets",
      "econometrics-package"
    ],
    "summary": "A high-performance Python package implementing Callaway & Sant'Anna difference-in-differences estimators, specifically optimized for massive datasets with millions of observations. It dramatically reduces computation time from hours to seconds while handling complex scenarios like time-varying covariates and multiple treatment events per unit.",
    "use_cases": [
      "Analyzing the staggered rollout of a product feature across millions of users with varying characteristics",
      "Evaluating policy impacts across thousands of geographic units over multiple time periods with large administrative datasets"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "fast difference in differences for large datasets",
      "callaway sant'anna implementation python millions observations",
      "high performance staggered did estimator",
      "scalable causal inference package time varying covariates"
    ]
  },
  {
    "name": "fect",
    "description": "Fixed Effects Counterfactual Estimators (v2.0+) incorporating gsynth functionality. Supports treatment switching on/off with carryover effects, matrix completion methods, and Rambachan & Roth sensitivity analysis for parallel trends violations.",
    "category": "Causal Inference (DiD)",
    "docs_url": "https://yiqingxu.org/packages/fect/",
    "github_url": "https://github.com/xuyiqing/fect",
    "url": "https://cran.r-project.org/package=fect",
    "install": "install.packages(\"fect\")",
    "tags": [
      "counterfactual",
      "matrix-completion",
      "interactive-fixed-effects",
      "sensitivity-analysis",
      "carryover"
    ],
    "best_for": "Counterfactual estimation with interactive fixed effects, treatment switching, and sensitivity analysis",
    "language": "R",
    "difficulty": "advanced",
    "prerequisites": [
      "difference-in-differences",
      "panel-data-methods",
      "matrix-completion"
    ],
    "topic_tags": [
      "counterfactual-estimation",
      "interactive-fixed-effects",
      "treatment-effects",
      "sensitivity-analysis",
      "panel-data"
    ],
    "summary": "Advanced R package for causal inference with panel data that extends traditional difference-in-differences to handle complex treatment patterns and violations of parallel trends. Combines matrix completion methods with interactive fixed effects models to estimate counterfactual outcomes when units can switch treatment status multiple times. Includes modern sensitivity analysis tools to assess robustness to parallel trends assumptions.",
    "use_cases": [
      "Evaluating policy interventions where regions can adopt and abandon policies over time with lasting effects",
      "Measuring impact of product feature rollouts across markets with staggered implementation and potential reversal"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "R package for difference in differences with treatment switching",
      "matrix completion methods for causal inference panel data",
      "sensitivity analysis for parallel trends assumption violations",
      "interactive fixed effects counterfactual estimation R"
    ]
  },
  {
    "name": "staggered",
    "description": "Provides the efficient estimator for randomized staggered rollout designs, offering optimal weighting schemes for treatment effect estimation. Also implements Callaway & Sant'Anna and Sun & Abraham estimators with design-based Fisher inference for randomized experiments.",
    "category": "Causal Inference (DiD)",
    "docs_url": "https://cran.r-project.org/web/packages/staggered/readme/README.html",
    "github_url": "https://github.com/jonathandroth/staggered",
    "url": "https://cran.r-project.org/package=staggered",
    "install": "install.packages(\"staggered\")",
    "tags": [
      "staggered-rollout",
      "randomized-experiments",
      "efficient-estimation",
      "event-study",
      "fisher-inference"
    ],
    "best_for": "Randomized experiments with staggered treatment timing where efficiency gains matter, implementing Roth & Sant'Anna (2023)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "difference-in-differences",
      "randomized-controlled-trials",
      "panel-data-analysis"
    ],
    "topic_tags": [
      "staggered-rollout",
      "difference-in-differences",
      "treatment-effects",
      "randomized-experiments",
      "causal-inference"
    ],
    "summary": "The staggered package implements efficient estimators for randomized staggered rollout experiments, where treatment is rolled out to different units at different times. It provides optimal weighting schemes and includes popular estimators like Callaway & Sant'Anna and Sun & Abraham with design-based inference. This is particularly useful for tech companies running gradual feature rollouts or policy experiments.",
    "use_cases": [
      "Analyzing gradual feature rollouts where different user cohorts receive treatment at staggered time periods",
      "Evaluating policy interventions implemented across different regions or departments at different dates"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "staggered rollout experiment analysis R package",
      "efficient estimator for randomized staggered design",
      "Callaway Sant'Anna estimator implementation",
      "how to analyze treatment effects with staggered adoption"
    ]
  },
  {
    "name": "DTRreg",
    "description": "Dynamic treatment regime estimation via G-estimation for sequential treatment decisions. Implements methods for finding optimal treatment rules that adapt over time based on patient characteristics.",
    "category": "Causal Inference (Dynamic Treatment)",
    "docs_url": "https://cran.r-project.org/web/packages/DTRreg/DTRreg.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=DTRreg",
    "install": "install.packages(\"DTRreg\")",
    "tags": [
      "dynamic-treatment",
      "G-estimation",
      "sequential-decisions",
      "optimal-treatment",
      "personalization"
    ],
    "best_for": "Dynamic treatment regime estimation via G-estimation for sequential treatment decisions",
    "language": "R",
    "difficulty": "advanced",
    "prerequisites": [
      "causal-inference",
      "regression-modeling",
      "longitudinal-data"
    ],
    "topic_tags": [
      "dynamic-treatment",
      "g-estimation",
      "sequential-decisions",
      "personalized-medicine",
      "causal-inference"
    ],
    "summary": "DTRreg implements G-estimation methods for finding optimal dynamic treatment regimes that adapt treatment decisions over time based on evolving patient characteristics. This package is essential for researchers developing personalized treatment strategies in clinical settings where treatment decisions must be made sequentially. It enables estimation of treatment rules that maximize long-term outcomes by accounting for time-varying confounders and treatment history.",
    "use_cases": [
      "Optimizing multi-stage cancer treatment protocols where drug choice depends on tumor response and patient health status",
      "Designing adaptive educational interventions where teaching methods adjust based on student progress and engagement metrics"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "dynamic treatment regime estimation R package",
      "G-estimation for sequential treatment decisions",
      "optimal treatment rules time-varying confounders",
      "personalized medicine dynamic decision making"
    ]
  },
  {
    "name": "DynTxRegime",
    "description": "Comprehensive package for dynamic treatment regimes implementing Q-learning, value search, and outcome-weighted learning methods. Accompanies the textbook 'Dynamic Treatment Regimes' (Tsiatis et al., 2020).",
    "category": "Causal Inference (Dynamic Treatment)",
    "docs_url": "https://cran.r-project.org/web/packages/DynTxRegime/DynTxRegime.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=DynTxRegime",
    "install": "install.packages(\"DynTxRegime\")",
    "tags": [
      "dynamic-treatment",
      "Q-learning",
      "value-search",
      "reinforcement-learning",
      "personalized-medicine"
    ],
    "best_for": "Comprehensive dynamic treatment regimes with Q-learning and value search, from Tsiatis et al. (2020) textbook",
    "language": "R",
    "difficulty": "advanced",
    "prerequisites": [
      "reinforcement-learning-basics",
      "causal-inference-fundamentals",
      "R-programming"
    ],
    "topic_tags": [
      "dynamic-treatment",
      "Q-learning",
      "personalized-medicine",
      "sequential-decision-making",
      "causal-inference"
    ],
    "summary": "DynTxRegime is an R package implementing advanced methods for dynamic treatment regimes, including Q-learning and outcome-weighted learning approaches. It's designed for researchers developing personalized treatment strategies where decisions adapt based on patient responses over time. The package accompanies a comprehensive textbook and provides tools for sequential decision-making in medical and behavioral interventions.",
    "use_cases": [
      "Optimizing multi-stage cancer treatment protocols where drug selection depends on tumor response",
      "Designing adaptive educational interventions that adjust based on student performance"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "dynamic treatment regime R package",
      "Q-learning for personalized medicine implementation",
      "sequential treatment decision making tools",
      "outcome weighted learning methods"
    ]
  },
  {
    "name": "eventstudyr",
    "description": "Implements event study best practices from Freyaldenhoven et al. (2021) including sup-t confidence bands for uniform inference and formal pre-trend testing. Provides robust methods for dynamic treatment effect estimation.",
    "category": "Causal Inference (Event Study)",
    "docs_url": "https://cran.r-project.org/web/packages/eventstudyr/eventstudyr.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=eventstudyr",
    "install": "install.packages(\"eventstudyr\")",
    "tags": [
      "event-study",
      "pre-trends",
      "sup-t-bands",
      "uniform-inference",
      "dynamic-effects"
    ],
    "best_for": "Event study best practices with sup-t confidence bands and formal pre-trend testing, implementing Freyaldenhoven et al. (2021)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "difference-in-differences",
      "panel-data-methods",
      "statistical-inference"
    ],
    "topic_tags": [
      "event-study",
      "causal-inference",
      "dynamic-treatment-effects",
      "pre-trends",
      "uniform-inference"
    ],
    "summary": "R package implementing state-of-the-art event study methods with formal pre-trend testing and sup-t confidence bands for uniform inference. Based on Freyaldenhoven et al. (2021) best practices for robust dynamic treatment effect estimation. Essential for researchers conducting rigorous event studies with proper statistical inference.",
    "use_cases": [
      "Evaluating policy impact over time with staggered treatment adoption",
      "Testing for pre-trends and estimating dynamic effects in corporate finance event studies"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "event study package with pre-trend testing",
      "sup-t confidence bands dynamic treatment effects",
      "robust event study methods R package",
      "uniform inference event study analysis"
    ]
  },
  {
    "name": "fixes",
    "description": "Streamlined event study workflows with simple run_es() and plot_es() functions built on fixest. New 2025 package providing convenient wrappers for common event study specifications.",
    "category": "Causal Inference (Event Study)",
    "docs_url": "https://cran.r-project.org/web/packages/fixes/fixes.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=fixes",
    "install": "install.packages(\"fixes\")",
    "tags": [
      "event-study",
      "fixest",
      "DiD",
      "streamlined",
      "visualization"
    ],
    "best_for": "Streamlined event study workflows with simple run_es() and plot_es() functions on fixest",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "fixest-package",
      "difference-in-differences",
      "R-programming"
    ],
    "topic_tags": [
      "event-study",
      "causal-inference",
      "R-package",
      "difference-in-differences",
      "visualization"
    ],
    "summary": "The fixes package provides streamlined R functions for event study analysis, offering simple run_es() and plot_es() wrappers around the fixest package. Released in 2025, it eliminates the complexity of setting up common event study specifications, making causal inference workflows more accessible to practitioners.",
    "use_cases": [
      "Evaluating the impact of a policy change on user engagement metrics across different treatment cohorts",
      "Analyzing the effect of a product launch on sales performance with automated visualization of pre/post-treatment trends"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "simple event study R package",
      "fixest wrapper for event studies",
      "easy event study analysis with visualization",
      "streamlined difference in differences R"
    ]
  },
  {
    "name": "inferference",
    "description": "Computes inverse probability weighted (IPW) causal effects under partial interference following Tchetgen Tchetgen and VanderWeele (2012). Handles spillover effects within groups while maintaining independence across groups.",
    "category": "Causal Inference (Interference)",
    "docs_url": "https://cran.r-project.org/web/packages/inferference/inferference.pdf",
    "github_url": "https://github.com/bsaul/inferference",
    "url": "https://cran.r-project.org/package=inferference",
    "install": "install.packages(\"inferference\")",
    "tags": [
      "interference",
      "spillovers",
      "IPW",
      "partial-interference",
      "SUTVA-violations"
    ],
    "best_for": "IPW causal effects under partial interference with within-group spillovers, implementing Tchetgen Tchetgen & VanderWeele (2012)",
    "language": "R",
    "difficulty": "advanced",
    "prerequisites": [
      "causal-inference",
      "inverse-probability-weighting",
      "randomized-experiments"
    ],
    "topic_tags": [
      "interference",
      "spillover-effects",
      "ipw-estimation",
      "causal-inference",
      "sutva-violations"
    ],
    "summary": "The inferference package implements inverse probability weighted estimation for causal effects when units interfere with each other, violating the standard SUTVA assumption. It handles partial interference settings where spillovers occur within predefined groups but independence is maintained across groups. This is essential for experiments where treatment of one unit affects outcomes of nearby units.",
    "use_cases": [
      "Analyzing social media experiments where friends influence each other's behavior",
      "Evaluating educational interventions where student outcomes depend on classroom peers' treatment status"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "How to handle interference in randomized experiments",
      "IPW estimation with spillover effects",
      "Causal inference when SUTVA is violated",
      "Partial interference analysis in R"
    ]
  },
  {
    "name": "latenetwork",
    "description": "Handles both noncompliance AND network interference of unknown form following Hoshino and Yanagi (2023 JASA). Provides valid inference when treatment effects spill over through network connections.",
    "category": "Causal Inference (Interference)",
    "docs_url": "https://cran.r-project.org/web/packages/latenetwork/latenetwork.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=latenetwork",
    "install": "install.packages(\"latenetwork\")",
    "tags": [
      "network-interference",
      "noncompliance",
      "LATE",
      "spillovers",
      "IV"
    ],
    "best_for": "LATE estimation with network interference and noncompliance, implementing Hoshino & Yanagi (2023 JASA)",
    "language": "R",
    "difficulty": "advanced",
    "prerequisites": [
      "instrumental-variables",
      "causal-inference",
      "network-analysis"
    ],
    "topic_tags": [
      "network-interference",
      "instrumental-variables",
      "LATE",
      "spillover-effects",
      "causal-inference"
    ],
    "summary": "Implements methods from Hoshino and Yanagi (2023) to estimate local average treatment effects when both treatment noncompliance and network spillovers are present. Provides valid statistical inference for randomized experiments where treatment effects propagate through social or economic networks of unknown structure.",
    "use_cases": [
      "A/B testing social media features where user interactions create spillovers between treatment and control groups",
      "Evaluating educational interventions in schools where peer effects contaminate the control group"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "How to handle network spillovers in randomized experiments",
      "LATE estimation with network interference",
      "Dealing with noncompliance and spillovers simultaneously",
      "Network interference in causal inference R package"
    ]
  },
  {
    "name": "EValue",
    "description": "Conducts sensitivity analyses for unmeasured confounding, selection bias, and measurement error in observational studies and meta-analyses. Computes E-values representing the minimum strength of association unmeasured confounders would need to fully explain away an observed effect.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://louisahsmith.github.io/evalue/",
    "github_url": "https://github.com/mayamathur/evalue_package",
    "url": "https://cran.r-project.org/package=EValue",
    "install": "install.packages(\"EValue\")",
    "tags": [
      "E-value",
      "unmeasured-confounding",
      "sensitivity-analysis",
      "selection-bias",
      "meta-analysis"
    ],
    "best_for": "Quantifying the minimum confounding strength on the risk ratio scale needed to explain away observed treatment-outcome associations, implementing VanderWeele & Ding (2017)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "observational-studies",
      "regression-analysis",
      "confounding-variables"
    ],
    "topic_tags": [
      "e-value",
      "sensitivity-analysis",
      "causal-inference",
      "observational-studies",
      "unmeasured-confounding"
    ],
    "summary": "EValue is a package for conducting sensitivity analyses to assess how robust causal conclusions are to potential unmeasured confounding, selection bias, and measurement error. It computes E-values that quantify the minimum strength of association that unmeasured confounders would need to have to completely explain away an observed treatment effect. This tool is essential for researchers working with observational data who need to evaluate the credibility of their causal claims.",
    "use_cases": [
      "Evaluating whether unmeasured confounders could explain away the observed effect of a medical treatment in an observational study",
      "Assessing robustness of findings in a tech company's analysis of user behavior interventions where randomization wasn't possible"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "How to test robustness of causal effects to unmeasured confounding",
      "E-value calculation for observational studies",
      "Sensitivity analysis for confounding bias",
      "Tools for evaluating causal inference assumptions"
    ]
  },
  {
    "name": "SuperLearner",
    "description": "Implements the Super Learner algorithm for optimal ensemble prediction via cross-validation. Creates weighted combinations of multiple ML algorithms (XGBoost, Random Forest, glmnet, neural networks, SVM, BART) with guaranteed asymptotic optimality.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://cran.r-project.org/web/packages/SuperLearner/vignettes/Guide-to-SuperLearner.html",
    "github_url": "https://github.com/ecpolley/SuperLearner",
    "url": "https://cran.r-project.org/package=SuperLearner",
    "install": "install.packages(\"SuperLearner\")",
    "tags": [
      "ensemble-learning",
      "cross-validation",
      "stacking",
      "prediction",
      "model-selection"
    ],
    "best_for": "Building optimal prediction ensembles for nuisance parameter estimation (propensity scores, outcome models) in causal inference, implementing van der Laan, Polley & Hubbard (2007)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-scikit-learn",
      "cross-validation",
      "ensemble-methods"
    ],
    "topic_tags": [
      "ensemble-learning",
      "super-learner",
      "model-stacking",
      "prediction",
      "causal-inference"
    ],
    "summary": "SuperLearner is an ensemble method that optimally combines multiple machine learning algorithms using cross-validation to create weighted predictions with theoretical guarantees. It's widely used in causal inference and prediction tasks where you want to leverage the strengths of different algorithms without manual tuning. The method automatically finds the best combination of base learners like XGBoost, Random Forest, and neural networks.",
    "use_cases": [
      "Estimating treatment effects in randomized trials where you need robust prediction of outcomes",
      "Building high-performance prediction models by combining multiple algorithms without overfitting"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "How to combine multiple ML models optimally",
      "Super learner ensemble method implementation",
      "Cross-validation stacking for causal inference",
      "Best ensemble learning package for prediction"
    ]
  },
  {
    "name": "causalToolbox",
    "description": "Implements meta-learner algorithms (S-learner, T-learner, X-learner) for heterogeneous treatment effect estimation using flexible base learners including honest Random Forests and BART for personalized CATE estimation.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://rdrr.io/github/soerenkuenzel/causalToolbox/",
    "github_url": "https://github.com/forestry-labs/causalToolbox",
    "url": "https://github.com/forestry-labs/causalToolbox",
    "install": "devtools::install_github(\"forestry-labs/causalToolbox\")",
    "tags": [
      "metalearners",
      "X-learner",
      "T-learner",
      "S-learner",
      "CATE"
    ],
    "best_for": "Comparing and benchmarking different CATE meta-learner strategies (S/T/X-learner) with BART or RF base learners, implementing K\u00fcnzel et al. (2019)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "random-forests",
      "causal-inference-basics",
      "python-scikit-learn"
    ],
    "topic_tags": [
      "heterogeneous-treatment-effects",
      "meta-learners",
      "personalized-medicine",
      "A-B-testing",
      "CATE-estimation"
    ],
    "summary": "causalToolbox provides implementations of popular meta-learner algorithms (S-learner, T-learner, X-learner) for estimating heterogeneous treatment effects across different subgroups. It combines flexible machine learning models like Random Forests and BART with causal inference methods to estimate personalized treatment effects (CATE). This package is particularly useful for practitioners who want to move beyond average treatment effects to understand how treatments work differently for different individuals or segments.",
    "use_cases": [
      "Personalizing marketing campaigns by estimating which customer segments respond best to different promotional strategies",
      "Clinical trial analysis to identify patient subgroups who benefit most from a new treatment based on their characteristics"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "How to estimate heterogeneous treatment effects with machine learning",
      "X-learner vs T-learner for personalized treatment effects",
      "CATE estimation using random forests",
      "Meta-learners for A/B test heterogeneity analysis"
    ]
  },
  {
    "name": "causalweight",
    "description": "Semiparametric causal inference methods based on inverse probability weighting and double machine learning for average treatment effects, causal mediation analysis (direct/indirect effects), and dynamic treatment evaluation. Supports LATE estimation with instrumental variables.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://cran.r-project.org/web/packages/causalweight/causalweight.pdf",
    "github_url": "https://github.com/hbodory/causalweight",
    "url": "https://cran.r-project.org/package=causalweight",
    "install": "install.packages(\"causalweight\")",
    "tags": [
      "inverse-probability-weighting",
      "causal-mediation",
      "double-machine-learning",
      "LATE",
      "instrumental-variables"
    ],
    "best_for": "Mediation analysis and LATE estimation using weighting-based approaches with flexible nuisance estimation, implementing Huber (2014) and Fr\u00f6lich & Huber (2017)",
    "language": "R",
    "difficulty": "advanced",
    "prerequisites": [
      "propensity-score-matching",
      "instrumental-variables",
      "cross-validation"
    ],
    "topic_tags": [
      "causal-inference",
      "treatment-effects",
      "mediation-analysis",
      "double-ml",
      "ipw"
    ],
    "summary": "A Python package implementing advanced semiparametric causal inference methods that combine inverse probability weighting with double machine learning techniques. It enables researchers to estimate average treatment effects, decompose causal pathways through mediation analysis, and evaluate dynamic treatment regimes while handling high-dimensional confounders. The package is particularly valuable for economists and data scientists working on complex causal questions where traditional methods may fall short.",
    "use_cases": [
      "Measuring the direct vs indirect effects of a job training program on earnings through skill acquisition",
      "Evaluating the causal impact of personalized marketing campaigns while controlling for selection bias using customer behavioral data"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "double machine learning causal inference python",
      "mediation analysis with high dimensional confounders",
      "LATE estimation instrumental variables package",
      "inverse probability weighting treatment effects implementation"
    ]
  },
  {
    "name": "ddml",
    "description": "Streamlined double/debiased machine learning estimation with emphasis on (short-)stacking to combine multiple base learners, increasing robustness to unknown data generating processes. Designed as a complement to DoubleML with simpler syntax.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://thomaswiemann.com/ddml/",
    "github_url": "https://github.com/thomaswiemann/ddml",
    "url": "https://cran.r-project.org/package=ddml",
    "install": "install.packages(\"ddml\")",
    "tags": [
      "double-machine-learning",
      "stacking",
      "model-averaging",
      "treatment-effects",
      "causal-inference"
    ],
    "best_for": "Quick, robust DML estimation using short-stacking to ensemble multiple ML learners without extensive tuning, implementing Ahrens, Hansen, Schaffer & Wiemann (2024)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-scikit-learn",
      "causal-inference-basics",
      "cross-validation"
    ],
    "topic_tags": [
      "double-machine-learning",
      "causal-inference",
      "model-stacking",
      "treatment-effects",
      "python-package"
    ],
    "summary": "ddml is a Python package for double/debiased machine learning that combines multiple base learners through stacking to estimate causal effects robustly. It offers simpler syntax than DoubleML while maintaining statistical rigor for treatment effect estimation. The package is particularly useful when the underlying data generating process is unknown and you want to hedge against model misspecification.",
    "use_cases": [
      "Estimating impact of price changes on demand when functional form is uncertain",
      "Measuring effect of marketing campaigns using ensemble of ML models for robustness"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "double machine learning python package simple syntax",
      "stacking multiple models for causal inference",
      "robust treatment effect estimation with ML",
      "ddml vs DoubleML comparison"
    ]
  },
  {
    "name": "grf",
    "description": "Forest-based statistical estimation and inference for heterogeneous treatment effects, supporting multiple treatment arms, instrumental variables, survival outcomes, and quantile regression\u2014all with honest estimation and valid confidence intervals. The most widely-used R package for CATE estimation.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://grf-labs.github.io/grf/",
    "github_url": "https://github.com/grf-labs/grf",
    "url": "https://cran.r-project.org/package=grf",
    "install": "install.packages(\"grf\")",
    "tags": [
      "causal-forest",
      "heterogeneous-treatment-effects",
      "CATE",
      "machine-learning",
      "econometrics"
    ],
    "best_for": "Estimating individual-level treatment effects (CATE) with valid statistical inference in RCTs or observational studies, implementing Athey, Tibshirani & Wager (2019)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "random-forests",
      "treatment-effects",
      "R-programming"
    ],
    "topic_tags": [
      "causal-forests",
      "heterogeneous-treatment-effects",
      "CATE-estimation",
      "R-package",
      "honest-inference"
    ],
    "summary": "The grf package is the leading R implementation for causal forest estimation, enabling researchers to identify how treatment effects vary across individuals using machine learning. It provides robust statistical inference with confidence intervals for heterogeneous treatment effects, supporting complex scenarios like multiple treatments and instrumental variables. Widely adopted by data scientists and economists for personalized treatment recommendations and policy analysis.",
    "use_cases": [
      "A/B testing platform wants to identify which user segments respond differently to a new feature recommendation",
      "Healthcare researchers analyzing how patient characteristics affect drug treatment effectiveness across subpopulations"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "causal forest R package for heterogeneous treatment effects",
      "how to estimate CATE with confidence intervals",
      "grf package tutorial for personalized treatment effects",
      "machine learning causal inference heterogeneous effects"
    ]
  },
  {
    "name": "hdm",
    "description": "High-dimensional statistical methods featuring heteroscedasticity-robust LASSO with theoretically-grounded penalty selection, post-double-selection inference, and treatment effect estimation under sparsity assumptions for high-dimensional controls.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://cran.r-project.org/web/packages/hdm/vignettes/hdm.html",
    "github_url": "https://github.com/MartinSpindler/hdm",
    "url": "https://cran.r-project.org/package=hdm",
    "install": "install.packages(\"hdm\")",
    "tags": [
      "lasso",
      "post-double-selection",
      "high-dimensional",
      "instrumental-variables",
      "sparsity"
    ],
    "best_for": "Post-double-selection LASSO inference and treatment effect estimation when the true model is sparse, implementing Belloni, Chernozhukov & Hansen (2014)",
    "language": "R",
    "difficulty": "advanced",
    "prerequisites": [
      "lasso-regression",
      "instrumental-variables",
      "asymptotic-theory"
    ],
    "topic_tags": [
      "high-dimensional-inference",
      "treatment-effects",
      "post-selection",
      "robust-standard-errors",
      "sparsity"
    ],
    "summary": "The hdm package implements theoretically rigorous methods for causal inference in high-dimensional settings where the number of potential confounders exceeds sample size. It combines LASSO variable selection with post-double-selection techniques to estimate treatment effects while maintaining valid statistical inference under sparsity assumptions.",
    "use_cases": [
      "Estimating advertising effectiveness when controlling for thousands of user features and behavioral variables",
      "Evaluating policy interventions using administrative data with rich covariate information from multiple linked datasets"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "high dimensional treatment effects lasso",
      "post double selection causal inference",
      "hdm package sparse confounders",
      "robust lasso heteroscedastic errors"
    ]
  },
  {
    "name": "ltmle",
    "description": "Targeted maximum likelihood estimation for treatment/censoring-specific mean outcomes with time-varying treatments and confounders. Supports longitudinal settings, marginal structural models, and dynamic treatment regimes alongside IPTW and G-computation.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://joshuaschwab.github.io/ltmle/",
    "github_url": "https://github.com/joshuaschwab/ltmle",
    "url": "https://cran.r-project.org/package=ltmle",
    "install": "install.packages(\"ltmle\")",
    "tags": [
      "TMLE",
      "longitudinal",
      "time-varying-treatment",
      "dynamic-regimes",
      "MSM"
    ],
    "best_for": "Causal inference with time-varying treatments, time-varying confounders, and right-censored longitudinal data, implementing Lendle et al. (2017)",
    "language": "R",
    "difficulty": "advanced",
    "prerequisites": [
      "causal-inference",
      "longitudinal-data-analysis",
      "tmle"
    ],
    "topic_tags": [
      "tmle",
      "longitudinal-causal-inference",
      "time-varying-treatment",
      "dynamic-treatment-regimes",
      "marginal-structural-models"
    ],
    "summary": "ltmle implements Longitudinal Targeted Maximum Likelihood Estimation for causal inference with time-varying treatments and confounders. It enables estimation of treatment effects in complex longitudinal settings where treatments, confounders, and outcomes evolve over time. The package supports dynamic treatment regimes, marginal structural models, and provides alternatives to standard IPTW and G-computation approaches.",
    "use_cases": [
      "Estimating the causal effect of varying medication dosages over time on patient outcomes while accounting for time-varying health status",
      "Analyzing the impact of dynamic marketing interventions on customer behavior when treatment assignment depends on previous responses and changing customer characteristics"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "longitudinal TMLE for time-varying treatments",
      "causal inference with dynamic treatment regimes",
      "marginal structural models implementation R",
      "time-varying confounders causal estimation"
    ]
  },
  {
    "name": "sensemakr",
    "description": "Suite of sensitivity analysis tools extending the traditional omitted variable bias framework, computing robustness values, bias-adjusted estimates, and sensitivity contour plots for OLS regression to assess how strong unmeasured confounders would need to be to overturn conclusions.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://carloscinelli.com/sensemakr/",
    "github_url": "https://github.com/carloscinelli/sensemakr",
    "url": "https://cran.r-project.org/package=sensemakr",
    "install": "install.packages(\"sensemakr\")",
    "tags": [
      "sensitivity-analysis",
      "omitted-variable-bias",
      "robustness-value",
      "causal-inference",
      "regression"
    ],
    "best_for": "Assessing how strong unmeasured confounders would need to be to overturn regression-based causal conclusions, implementing Cinelli & Hazlett (2020)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "linear-regression",
      "causal-inference-fundamentals",
      "R-programming"
    ],
    "topic_tags": [
      "sensitivity-analysis",
      "omitted-variable-bias",
      "causal-inference",
      "regression-diagnostics",
      "robustness-testing"
    ],
    "summary": "sensemakr is an R package for sensitivity analysis that helps researchers assess how robust their causal conclusions are to potential unmeasured confounders. It extends traditional omitted variable bias analysis by computing robustness values and creating sensitivity contour plots to quantify how strong hidden confounders would need to be to change your results. Particularly useful for observational studies where you can't randomize treatment assignment.",
    "use_cases": [
      "Testing whether your finding that education increases wages is robust to unmeasured ability or family background",
      "Assessing if your conclusion about a marketing campaign's effectiveness could be overturned by unmeasured customer characteristics"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "how to test robustness of causal estimates to unmeasured confounders",
      "sensitivity analysis for observational studies R package",
      "omitted variable bias sensitivity analysis tools",
      "robustness values for regression estimates"
    ]
  },
  {
    "name": "tmle",
    "description": "Implements targeted maximum likelihood estimation for point treatment effects with binary or continuous outcomes. Estimates ATE, ATT, ATC, and supports marginal structural models. Integrates SuperLearner for data-adaptive nuisance parameter estimation.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://cran.r-project.org/web/packages/tmle/tmle.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=tmle",
    "install": "install.packages(\"tmle\")",
    "tags": [
      "TMLE",
      "causal-inference",
      "ATE",
      "doubly-robust",
      "propensity-score"
    ],
    "best_for": "Estimating point treatment effects (ATE/ATT/ATC) in observational studies with binary treatments, implementing Gruber & van der Laan (2012)",
    "language": "R",
    "difficulty": "advanced",
    "prerequisites": [
      "propensity-score-matching",
      "super-learner",
      "double-machine-learning"
    ],
    "topic_tags": [
      "tmle",
      "treatment-effects",
      "doubly-robust",
      "causal-ml",
      "ate-estimation"
    ],
    "summary": "TMLE is a doubly robust method for estimating causal treatment effects that combines outcome modeling and propensity score estimation. It integrates machine learning methods like SuperLearner for flexible nuisance parameter estimation while maintaining statistical guarantees. Popular for estimating average treatment effects in observational studies with complex confounding.",
    "use_cases": [
      "Estimating effect of mobile app feature on user retention using observational data",
      "Measuring impact of pricing changes on customer churn with high-dimensional covariates"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "doubly robust treatment effect estimation R package",
      "TMLE implementation for causal inference",
      "targeted maximum likelihood estimation with machine learning",
      "SuperLearner integration for causal effects"
    ]
  },
  {
    "name": "tmle3",
    "description": "A modular, extensible framework for targeted minimum loss-based estimation supporting custom TMLE parameters through a unified interface. Part of the tlverse ecosystem, designed to be as general as the mathematical TMLE framework itself for complex analyses.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://tlverse.org/tmle3/",
    "github_url": "https://github.com/tlverse/tmle3",
    "url": "https://github.com/tlverse/tmle3",
    "install": "remotes::install_github(\"tlverse/tmle3\")",
    "tags": [
      "TMLE",
      "tlverse",
      "modular",
      "extensible",
      "stochastic-interventions"
    ],
    "best_for": "Complex TMLE analyses requiring custom parameters, mediation, stochastic interventions, or optimal treatment regimes",
    "language": "R",
    "difficulty": "advanced",
    "prerequisites": [
      "causal-inference-theory",
      "R-programming",
      "targeted-maximum-likelihood"
    ],
    "topic_tags": [
      "TMLE",
      "causal-inference",
      "tlverse",
      "targeted-estimation",
      "R-package"
    ],
    "summary": "tmle3 is an advanced R package providing a modular framework for targeted minimum loss-based estimation (TMLE) with support for custom parameters and complex causal analyses. It's part of the tlverse ecosystem and designed for researchers who need flexible, extensible TMLE implementations. The package allows for sophisticated causal inference analyses including stochastic interventions and custom estimands.",
    "use_cases": [
      "Estimating causal effects of continuous treatments with custom intervention distributions",
      "Building custom TMLE estimators for novel causal parameters not available in standard packages"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "How to implement custom TMLE parameters in R",
      "tlverse tmle3 package tutorial",
      "Modular TMLE framework for stochastic interventions",
      "R package for extensible targeted maximum likelihood estimation"
    ]
  },
  {
    "name": "CBPS",
    "description": "Implements Covariate Balancing Propensity Score, which estimates propensity scores by jointly optimizing treatment prediction and covariate balance via generalized method of moments (GMM). Supports binary, multi-valued, and continuous treatments, as well as longitudinal settings for marginal structural models.",
    "category": "Causal Inference (Matching)",
    "docs_url": "https://cran.r-project.org/web/packages/CBPS/CBPS.pdf",
    "github_url": "https://github.com/kosukeimai/CBPS",
    "url": "https://cran.r-project.org/package=CBPS",
    "install": "install.packages(\"CBPS\")",
    "tags": [
      "propensity-score",
      "covariate-balance",
      "GMM",
      "weighting",
      "treatment-effects"
    ],
    "best_for": "When propensity score model specification is uncertain and you want simultaneous balance optimization, implementing Imai & Ratkovic (2014)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "propensity-score-matching",
      "generalized-method-of-moments",
      "python-sklearn"
    ],
    "topic_tags": [
      "propensity-scoring",
      "covariate-balancing",
      "treatment-effects",
      "causal-inference",
      "weighting-methods"
    ],
    "summary": "CBPS is a Python package that implements Covariate Balancing Propensity Score methodology for causal inference. It improves upon traditional propensity score methods by simultaneously optimizing for treatment prediction accuracy and covariate balance between treatment groups. The package handles various treatment types (binary, multi-valued, continuous) and supports longitudinal analysis through marginal structural models.",
    "use_cases": [
      "Evaluating impact of marketing campaigns where you need better balance between treatment and control groups than standard propensity scoring",
      "Analyzing effectiveness of policy interventions with multiple treatment levels while ensuring comparable groups across covariates"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "covariate balancing propensity score python implementation",
      "better propensity score matching with balance optimization",
      "CBPS package for treatment effect estimation",
      "propensity score method that balances covariates automatically"
    ]
  },
  {
    "name": "MatchIt",
    "description": "Comprehensive matching package that selects matched samples of treated and control groups with similar covariate distributions. Provides a unified interface to multiple matching methods including nearest neighbor, optimal pair, optimal full, genetic, exact, coarsened exact (CEM), cardinality matching, and subclassification with propensity score estimation via GLM, GAM, random forest, and BART.",
    "category": "Causal Inference (Matching)",
    "docs_url": "https://kosukeimai.github.io/MatchIt/",
    "github_url": "https://github.com/kosukeimai/MatchIt",
    "url": "https://cran.r-project.org/package=MatchIt",
    "install": "install.packages(\"MatchIt\")",
    "tags": [
      "propensity-score-matching",
      "causal-inference",
      "observational-studies",
      "covariate-balance",
      "treatment-effects"
    ],
    "best_for": "Preprocessing observational data via matching to reduce confounding before estimating causal treatment effects, implementing Ho et al. (2007, 2011)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "logistic-regression",
      "python-pandas",
      "experimental-design"
    ],
    "topic_tags": [
      "propensity-score-matching",
      "causal-inference",
      "treatment-effects",
      "observational-studies",
      "covariate-balance"
    ],
    "summary": "MatchIt is a comprehensive R package that implements multiple matching methods to create balanced treatment and control groups from observational data. It provides a unified interface for techniques like nearest neighbor matching, optimal matching, and coarsened exact matching, with built-in propensity score estimation. Data scientists and researchers use it to reduce selection bias when estimating causal effects from non-experimental data.",
    "use_cases": [
      "Estimating the effect of a marketing campaign on customer retention using historical data with matched treatment and control groups",
      "Evaluating the impact of a policy change by matching similar units before and after implementation to control for confounding variables"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "how to match treatment and control groups in observational data",
      "propensity score matching implementation R package",
      "covariate balancing for causal inference",
      "MatchIt tutorial for treatment effect estimation"
    ]
  },
  {
    "name": "WeightIt",
    "description": "Unified interface for generating balancing weights for causal effect estimation in observational studies. Supports binary, multi-category, and continuous treatments for point and longitudinal/marginal structural models. Methods include inverse probability weighting (IPW), entropy balancing, covariate balancing propensity score (CBPS), energy balancing, stable balancing weights, BART, and SuperLearner.",
    "category": "Causal Inference (Matching)",
    "docs_url": "https://ngreifer.github.io/WeightIt/",
    "github_url": "https://github.com/ngreifer/WeightIt",
    "url": "https://cran.r-project.org/package=WeightIt",
    "install": "install.packages(\"WeightIt\")",
    "tags": [
      "propensity-score-weighting",
      "inverse-probability-weighting",
      "entropy-balancing",
      "CBPS",
      "marginal-structural-models"
    ],
    "best_for": "Generating balancing weights using modern weighting methods (IPW, entropy balancing, CBPS, etc.) for point or longitudinal treatments",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "propensity-score-matching",
      "regression-analysis",
      "observational-studies"
    ],
    "topic_tags": [
      "causal-inference",
      "propensity-scores",
      "treatment-effects",
      "observational-data",
      "r-package"
    ],
    "summary": "WeightIt is an R package that provides a unified interface for generating balancing weights to estimate causal effects from observational data. It supports multiple weighting methods including inverse probability weighting, entropy balancing, and covariate balancing propensity scores for various treatment types. The package is particularly useful for researchers who need to compare different weighting approaches or handle complex treatment scenarios like continuous treatments or longitudinal data.",
    "use_cases": [
      "Evaluating the causal effect of a marketing campaign on customer retention using observational data with multiple confounders",
      "Estimating the impact of employee training programs on productivity when randomized experiments are not feasible"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for inverse probability weighting causal inference",
      "how to generate balancing weights for observational studies",
      "entropy balancing vs propensity score weighting comparison",
      "causal inference package continuous treatments R"
    ]
  },
  {
    "name": "cobalt",
    "description": "Generates standardized balance tables and plots for covariates after preprocessing via matching, weighting, or subclassification. Provides unified balance assessment across multiple R packages (MatchIt, WeightIt, twang, Matching, optmatch, CBPS, ebal, cem, sbw, designmatch). Supports multi-category, continuous, and longitudinal treatments with clustered and multiply imputed data.",
    "category": "Causal Inference (Matching)",
    "docs_url": "https://ngreifer.github.io/cobalt/",
    "github_url": "https://github.com/ngreifer/cobalt",
    "url": "https://cran.r-project.org/package=cobalt",
    "install": "install.packages(\"cobalt\")",
    "tags": [
      "covariate-balance",
      "balance-diagnostics",
      "love-plot",
      "standardized-mean-difference",
      "balance-tables"
    ],
    "best_for": "Assessing and visualizing covariate balance before/after matching or weighting to validate causal inference preprocessing",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "causal-inference-fundamentals",
      "propensity-score-matching",
      "r-programming"
    ],
    "topic_tags": [
      "balance-assessment",
      "matching-diagnostics",
      "causal-inference",
      "love-plots",
      "r-package"
    ],
    "summary": "cobalt is an R package that creates standardized balance tables and visualizations to assess covariate balance after causal inference preprocessing steps like matching, weighting, or subclassification. It works with multiple popular R causal inference packages and provides unified diagnostics across different methods. The package is essential for validating that treatment and control groups are comparable before estimating causal effects.",
    "use_cases": [
      "Evaluating whether propensity score matching successfully balanced covariates between treated and control groups in an A/B test analysis",
      "Creating standardized balance diagnostics across multiple weighting methods to choose the best approach for estimating treatment effects"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "how to check covariate balance after matching in R",
      "love plot for propensity score matching diagnostics",
      "standardized mean difference balance tables R package",
      "assess balance after weighting causal inference"
    ]
  },
  {
    "name": "ebal",
    "description": "Implements entropy balancing, a reweighting method that finds weights for control units such that specified covariate moment conditions (means, variances) are exactly satisfied while staying as close as possible to uniform weights by minimizing Kullback-Leibler divergence. Primarily designed for ATT estimation.",
    "category": "Causal Inference (Matching)",
    "docs_url": "https://cran.r-project.org/web/packages/ebal/ebal.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=ebal",
    "install": "install.packages(\"ebal\")",
    "tags": [
      "entropy-balancing",
      "reweighting",
      "covariate-balance",
      "observational-studies",
      "ATT"
    ],
    "best_for": "When you need exact covariate balance on specified moments (means, variances) with minimal weight dispersion, implementing Hainmueller (2012)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "propensity-score-matching",
      "python-pandas",
      "observational-studies"
    ],
    "topic_tags": [
      "entropy-balancing",
      "causal-inference",
      "reweighting",
      "covariate-matching",
      "ATT-estimation"
    ],
    "summary": "Entropy balancing is a preprocessing method for causal inference that creates weights for control units to exactly match treatment group moments on observed covariates. It improves upon propensity score matching by directly optimizing covariate balance while maintaining interpretable weights. Data scientists and researchers use it to estimate treatment effects from observational data when randomized experiments aren't feasible.",
    "use_cases": [
      "Estimating the impact of a marketing campaign on customer behavior using historical transaction data",
      "Evaluating the effect of a policy intervention by reweighting control regions to match treated regions on demographic characteristics"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "entropy balancing for causal inference",
      "reweighting methods for observational studies",
      "alternative to propensity score matching",
      "covariate balancing for ATT estimation"
    ]
  },
  {
    "name": "optmatch",
    "description": "Distance-based bipartite matching using minimum cost network flow algorithms, oriented to matching treatment and control groups in observational studies. Provides optimal full matching and pair matching with support for propensity score distances, Mahalanobis distance, calipers, and exact matching constraints.",
    "category": "Causal Inference (Matching)",
    "docs_url": "https://markmfredrickson.github.io/optmatch",
    "github_url": "https://github.com/markmfredrickson/optmatch",
    "url": "https://cran.r-project.org/package=optmatch",
    "install": "install.packages(\"optmatch\")",
    "tags": [
      "optimal-matching",
      "propensity-score",
      "network-flow",
      "observational-studies",
      "full-matching"
    ],
    "best_for": "When you need mathematically optimal matching solutions that minimize total matched distance with flexible control:treatment ratios (full matching), implementing Hansen & Klopfer (2006)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "propensity-score-matching",
      "causal-inference-basics",
      "r-programming"
    ],
    "topic_tags": [
      "optimal-matching",
      "propensity-score",
      "causal-inference",
      "observational-studies",
      "treatment-effects"
    ],
    "summary": "R package implementing optimal matching algorithms for creating balanced treatment and control groups in observational studies. Uses minimum cost network flow to find the best possible matches based on propensity scores or other distance metrics. Essential tool for causal inference when randomized experiments aren't feasible.",
    "use_cases": [
      "Matching treated and control patients in medical studies to estimate treatment effects",
      "Creating balanced groups in A/B test analysis when randomization was imperfect or impossible"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "optimal matching package for causal inference",
      "how to do propensity score matching in R",
      "best R package for treatment control matching",
      "optmatch vs matchit for observational studies"
    ]
  },
  {
    "name": "CMAverse",
    "description": "Unified interface for six causal mediation approaches including traditional regression, inverse odds weighting, and g-formula. Supports multiple sequential mediators and exposure-mediator interactions.",
    "category": "Causal Inference (Mediation)",
    "docs_url": "https://bs1125.github.io/CMAverse/",
    "github_url": null,
    "url": "https://cran.r-project.org/package=CMAverse",
    "install": "install.packages(\"CMAverse\")",
    "tags": [
      "mediation",
      "g-formula",
      "multiple-mediators",
      "causal-mechanisms",
      "unified-interface"
    ],
    "best_for": "Unified causal mediation analysis with six approaches and multiple sequential mediators",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "linear-regression",
      "causal-inference-fundamentals",
      "r-programming"
    ],
    "topic_tags": [
      "mediation-analysis",
      "causal-mechanisms",
      "g-formula",
      "exposure-mediator-interactions",
      "sequential-mediators"
    ],
    "summary": "CMAverse provides a unified R interface for implementing six different causal mediation analysis methods, including traditional regression-based approaches, inverse odds weighting, and g-formula estimation. It's designed for researchers who need to estimate how much of a causal effect operates through intermediate variables (mediators), with support for complex scenarios involving multiple sequential mediators and interactions between exposures and mediators.",
    "use_cases": [
      "Analyzing how much of a digital marketing campaign's effect on sales operates through increased brand awareness versus direct purchase intent",
      "Decomposing the causal effect of a new feature rollout on user retention into pathways through engagement metrics, satisfaction scores, and usage frequency"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "unified interface for causal mediation analysis",
      "multiple mediator analysis R package",
      "g-formula mediation analysis implementation",
      "compare different mediation analysis methods"
    ]
  },
  {
    "name": "mediation",
    "description": "Estimates Average Causal Mediation Effects (ACME) with sensitivity analysis for unmeasured confounding. Implements Tingley et al. (2014 JSS) methods for understanding causal mechanisms.",
    "category": "Causal Inference (Mediation)",
    "docs_url": "https://cran.r-project.org/web/packages/mediation/mediation.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=mediation",
    "install": "install.packages(\"mediation\")",
    "tags": [
      "mediation",
      "ACME",
      "causal-mechanisms",
      "sensitivity-analysis",
      "indirect-effects"
    ],
    "best_for": "Average Causal Mediation Effects with sensitivity analysis, implementing Tingley et al. (2014 JSS)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "causal-inference-basics",
      "regression-analysis",
      "r-programming"
    ],
    "topic_tags": [
      "mediation-analysis",
      "causal-mechanisms",
      "sensitivity-analysis",
      "indirect-effects",
      "r-package"
    ],
    "summary": "The mediation package provides tools for estimating how much of a treatment effect operates through intermediate variables (mediators). It implements robust methods for calculating Average Causal Mediation Effects (ACME) and includes sensitivity analysis to assess how unmeasured confounding might affect results. Essential for researchers who need to understand not just whether treatments work, but how they work.",
    "use_cases": [
      "Analyzing whether a job training program improves wages directly or through increased skills",
      "Understanding if a marketing campaign affects sales through brand awareness or purchase intent"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "How to decompose treatment effects into direct and indirect pathways",
      "R package for mediation analysis with sensitivity testing",
      "Estimate average causal mediation effects ACME",
      "Tools for understanding causal mechanisms in experiments"
    ]
  },
  {
    "name": "PStrata",
    "description": "Principal stratification analysis for noncompliance and truncation-by-death using both Bayesian (Stan) and frequentist estimation. Implements Liu and Li (2023) methods for causal inference with post-treatment complications.",
    "category": "Causal Inference (Principal Stratification)",
    "docs_url": "https://cran.r-project.org/web/packages/PStrata/PStrata.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=PStrata",
    "install": "install.packages(\"PStrata\")",
    "tags": [
      "principal-stratification",
      "noncompliance",
      "truncation-by-death",
      "Bayesian",
      "Stan"
    ],
    "best_for": "Principal stratification for noncompliance and truncation-by-death with Bayesian/frequentist estimation",
    "language": "R",
    "difficulty": "advanced",
    "prerequisites": [
      "causal-inference-fundamentals",
      "Bayesian-statistics",
      "Stan-modeling"
    ],
    "topic_tags": [
      "principal-stratification",
      "causal-inference",
      "noncompliance",
      "truncation-by-death",
      "Bayesian-methods"
    ],
    "summary": "PStrata implements advanced principal stratification methods for handling two key challenges in causal inference: participant noncompliance and truncation-by-death. The package provides both Bayesian (Stan) and frequentist approaches based on Liu and Li (2023) methods, allowing researchers to estimate causal effects when post-treatment complications arise.",
    "use_cases": [
      "Clinical trial analysis where patients don't adhere to assigned treatments and some experience mortality",
      "Educational intervention studies with student dropout and varying compliance rates"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "principal stratification noncompliance R package",
      "truncation by death causal inference methods",
      "Bayesian principal stratification Stan implementation",
      "Liu Li 2023 principal stratification code"
    ]
  },
  {
    "name": "rddapp",
    "description": "Supports multi-assignment RDD with two running variables, power analysis for RDD designs, and includes a Shiny interface for interactive analysis. Handles both sharp and fuzzy designs with bandwidth selection.",
    "category": "Causal Inference (RDD)",
    "docs_url": "https://cran.r-project.org/web/packages/rddapp/rddapp.pdf",
    "github_url": "https://github.com/felixthoemmes/rddapp",
    "url": "https://cran.r-project.org/package=rddapp",
    "install": "install.packages(\"rddapp\")",
    "tags": [
      "RDD",
      "multi-assignment",
      "power-analysis",
      "Shiny",
      "fuzzy-RDD"
    ],
    "best_for": "Multi-assignment RDD with two running variables and power analysis with Shiny interface",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "regression-discontinuity-design",
      "R-programming",
      "ggplot2"
    ],
    "topic_tags": [
      "regression-discontinuity",
      "multi-assignment-RDD",
      "power-analysis",
      "causal-inference",
      "R-package"
    ],
    "summary": "An R package that extends standard regression discontinuity designs to handle complex scenarios with multiple assignment variables and fuzzy cutoffs. It provides power analysis tools to help researchers determine sample size requirements and includes an interactive Shiny interface for exploratory analysis. Particularly useful for policy evaluation where treatment assignment depends on multiple criteria or has probabilistic elements.",
    "use_cases": [
      "Evaluating education policies where student placement depends on both test scores and geographic location",
      "Analyzing healthcare interventions with fuzzy eligibility rules based on multiple patient characteristics"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "multi assignment regression discontinuity R package",
      "RDD with two running variables implementation",
      "power analysis for regression discontinuity designs",
      "fuzzy RDD analysis tools R"
    ]
  },
  {
    "name": "rddensity",
    "description": "Implements manipulation testing (density discontinuity testing) procedures using local polynomial density estimators to detect perfect self-selection around a cutoff. Provides rddensity() for hypothesis testing, rdbwdensity() for bandwidth selection, and rdplotdensity() for density plots with confidence bands.",
    "category": "Causal Inference (RDD)",
    "docs_url": "https://rdpackages.github.io/rddensity/",
    "github_url": "https://github.com/rdpackages/rddensity",
    "url": "https://cran.r-project.org/package=rddensity",
    "install": "install.packages(\"rddensity\")",
    "tags": [
      "manipulation-testing",
      "density-discontinuity",
      "McCrary-test",
      "falsification",
      "sorting"
    ],
    "best_for": "Testing RDD validity by detecting bunching/manipulation around the cutoff (McCrary-type tests), implementing Cattaneo, Jansson & Ma (2020)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "regression-discontinuity-design",
      "local-polynomial-regression",
      "hypothesis-testing"
    ],
    "topic_tags": [
      "manipulation-testing",
      "regression-discontinuity",
      "density-estimation",
      "falsification-tests",
      "causal-inference"
    ],
    "summary": "R package for testing whether individuals manipulate their assignment variable around a regression discontinuity cutoff by examining density discontinuities. Essential for validating RDD assumptions before running causal analyses. Provides McCrary-style tests with modern local polynomial methods and visualization tools.",
    "use_cases": [
      "Testing if students can manipulate test scores around scholarship eligibility thresholds",
      "Checking whether firms bunch around regulatory size cutoffs to avoid compliance costs"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "how to test for manipulation in regression discontinuity",
      "McCrary test R package",
      "density discontinuity testing RDD",
      "check sorting around RD cutoff"
    ]
  },
  {
    "name": "rddtools",
    "description": "Regression discontinuity design toolkit with clustered inference for geographic discontinuities. Provides bandwidth selection, specification tests, and visualization tools.",
    "category": "Causal Inference (RDD)",
    "docs_url": "https://cran.r-project.org/web/packages/rddtools/rddtools.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=rddtools",
    "install": "install.packages(\"rddtools\")",
    "tags": [
      "RDD",
      "clustered-inference",
      "bandwidth-selection",
      "geographic-discontinuity",
      "visualization"
    ],
    "best_for": "RDD with clustered inference for geographic discontinuities",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "regression-analysis",
      "causal-inference-basics",
      "R-programming"
    ],
    "topic_tags": [
      "regression-discontinuity",
      "causal-inference",
      "geographic-analysis",
      "bandwidth-selection",
      "statistical-toolkit"
    ],
    "summary": "rddtools is an R package that provides a comprehensive toolkit for implementing regression discontinuity designs, with specialized features for geographic discontinuities and clustered standard errors. It streamlines the RDD workflow by offering automated bandwidth selection, diagnostic tests, and publication-ready visualizations. The package is particularly useful for researchers analyzing policy changes or natural experiments with geographic boundaries.",
    "use_cases": [
      "Analyzing the causal impact of school district policies by comparing students just across district boundaries",
      "Evaluating the effect of tax policy changes by examining businesses on either side of state or municipal borders"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "R package for regression discontinuity design with geographic boundaries",
      "how to implement RDD with clustered standard errors",
      "bandwidth selection tools for regression discontinuity",
      "RDD toolkit with visualization and diagnostic tests"
    ]
  },
  {
    "name": "rdlocrand",
    "description": "Provides tools for RD analysis under local randomization: rdrandinf() performs hypothesis testing using randomization inference, rdwinselect() selects a window around the cutoff where randomization likely holds, rdsensitivity() assesses sensitivity to different windows, and rdrbounds() constructs Rosenbaum bounds for unobserved confounders.",
    "category": "Causal Inference (RDD)",
    "docs_url": "https://rdpackages.github.io/rdlocrand/",
    "github_url": "https://github.com/rdpackages/rdlocrand",
    "url": "https://cran.r-project.org/package=rdlocrand",
    "install": "install.packages(\"rdlocrand\")",
    "tags": [
      "local-randomization",
      "randomization-inference",
      "finite-sample",
      "window-selection",
      "sensitivity-analysis"
    ],
    "best_for": "Finite-sample inference in RDD when local randomization assumption is plausible near the cutoff, implementing Cattaneo, Frandsen & Titiunik (2015)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "regression-discontinuity-design",
      "randomization-inference",
      "R-programming"
    ],
    "topic_tags": [
      "regression-discontinuity",
      "local-randomization",
      "randomization-inference",
      "causal-inference",
      "R-package"
    ],
    "summary": "R package implementing local randomization methods for regression discontinuity designs as an alternative to conventional parametric approaches. Provides tools for window selection, randomization-based hypothesis testing, and sensitivity analysis when treatment assignment is as-good-as-random near the cutoff. Particularly useful when bandwidth selection and functional form assumptions are problematic in standard RDD.",
    "use_cases": [
      "Evaluating education policy interventions where students are assigned to programs based on test score cutoffs",
      "Analyzing the causal effects of financial aid eligibility based on income thresholds"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "local randomization regression discontinuity R",
      "rdlocrand package tutorial",
      "randomization inference RDD window selection",
      "alternative to bandwidth selection regression discontinuity"
    ]
  },
  {
    "name": "rdmulti",
    "description": "Provides tools for RD designs with multiple cutoffs or scores: rdmc() estimates pooled and cutoff-specific effects in multi-cutoff designs, rdmcplot() draws RD plots for multi-cutoff designs, and rdms() estimates effects in cumulative cutoffs or multi-score (geographic/boundary) designs.",
    "category": "Causal Inference (RDD)",
    "docs_url": "https://rdpackages.github.io/rdmulti/",
    "github_url": "https://github.com/rdpackages/rdmulti",
    "url": "https://cran.r-project.org/package=rdmulti",
    "install": "install.packages(\"rdmulti\")",
    "tags": [
      "multiple-cutoffs",
      "multi-score",
      "geographic-RD",
      "pooled-effects",
      "extrapolation"
    ],
    "best_for": "RDD with multiple cutoffs (e.g., different thresholds across regions) or multiple running variables (geographic boundaries), implementing Cattaneo, Titiunik & Vazquez-Bare (2020)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "regression-discontinuity",
      "causal-inference-fundamentals",
      "R-programming"
    ],
    "topic_tags": [
      "regression-discontinuity",
      "multiple-cutoffs",
      "geographic-boundaries",
      "causal-inference",
      "R-package"
    ],
    "summary": "rdmulti is an R package that extends regression discontinuity designs to handle multiple cutoffs or scoring dimensions. It's used by researchers analyzing policies with multiple thresholds or geographic boundaries where standard RD methods don't apply. The package enables pooled estimation across cutoffs and handles complex multi-dimensional discontinuities.",
    "use_cases": [
      "Analyzing education policies with different grade-level cutoffs across districts",
      "Estimating effects of zoning laws or electoral boundaries using geographic discontinuities"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "regression discontinuity multiple cutoffs R",
      "how to analyze RD with several thresholds",
      "geographic boundary regression discontinuity",
      "pooled RD estimation multiple scores"
    ]
  },
  {
    "name": "rdpower",
    "description": "Provides tools for power, sample size, and minimum detectable effects (MDE) calculations in RD designs using robust bias-corrected local polynomial inference: rdpower() calculates power, rdsampsi() calculates required sample size for desired power, and rdmde() computes minimum detectable effects.",
    "category": "Causal Inference (RDD)",
    "docs_url": "https://rdpackages.github.io/rdpower/",
    "github_url": "https://github.com/rdpackages/rdpower",
    "url": "https://cran.r-project.org/package=rdpower",
    "install": "install.packages(\"rdpower\")",
    "tags": [
      "power-analysis",
      "sample-size",
      "MDE",
      "study-design",
      "ex-ante-analysis"
    ],
    "best_for": "Planning RDD studies\u2014calculating required sample sizes, statistical power, or minimum detectable effects, implementing Cattaneo, Titiunik & Vazquez-Bare (2019)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "regression-discontinuity-design",
      "R-programming",
      "statistical-power-analysis"
    ],
    "topic_tags": [
      "power-analysis",
      "regression-discontinuity",
      "sample-size-calculation",
      "experimental-design",
      "R-package"
    ],
    "summary": "rdpower is an R package for calculating statistical power, required sample sizes, and minimum detectable effects in regression discontinuity (RD) designs. It uses robust bias-corrected local polynomial methods to provide accurate ex-ante calculations for RD studies. Essential for researchers planning RD experiments who need to determine feasibility and resource requirements.",
    "use_cases": [
      "Planning a policy evaluation study using school admission cutoffs and need to determine minimum sample size for 80% power",
      "Calculating the smallest effect size detectable with existing administrative data around eligibility thresholds"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "power analysis regression discontinuity R",
      "sample size calculation RD design",
      "minimum detectable effect regression discontinuity",
      "rdpower package tutorial"
    ]
  },
  {
    "name": "SCtools",
    "description": "Automates placebo tests and multi-treated-unit ATT calculations for synthetic control. Provides utilities for generating in-space and in-time placebos with visualization.",
    "category": "Causal Inference (Synthetic Control)",
    "docs_url": "https://cran.r-project.org/web/packages/SCtools/SCtools.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=SCtools",
    "install": "install.packages(\"SCtools\")",
    "tags": [
      "synthetic-control",
      "placebo-tests",
      "multi-unit",
      "ATT",
      "visualization"
    ],
    "best_for": "Automated placebo tests and multi-treated-unit ATT calculations for synthetic control",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "synthetic-control-method",
      "python-programming",
      "causal-inference-basics"
    ],
    "topic_tags": [
      "synthetic-control",
      "placebo-tests",
      "causal-inference",
      "python-package",
      "policy-evaluation"
    ],
    "summary": "SCtools is a Python package that automates synthetic control analysis by handling placebo tests and multi-unit treatment effects. It simplifies the implementation of robustness checks through automated in-space and in-time placebo generation with built-in visualization capabilities. The package is designed for researchers conducting policy evaluation and causal inference studies using synthetic control methods.",
    "use_cases": [
      "Evaluating the impact of a policy intervention across multiple states or regions simultaneously",
      "Conducting robustness checks for synthetic control studies by automating placebo tests and generating diagnostic plots"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "synthetic control placebo tests python",
      "multi unit synthetic control package",
      "how to automate synthetic control analysis",
      "synthetic control robustness checks tools"
    ]
  },
  {
    "name": "Synth",
    "description": "The original synthetic control method implementation for comparative case studies. Constructs a weighted combination of comparison units to create a synthetic counterfactual for estimating effects of interventions on a single treated unit, as used in seminal studies of California tobacco program and German reunification.",
    "category": "Causal Inference (Synthetic Control)",
    "docs_url": "https://web.stanford.edu/~jhain/",
    "github_url": "https://github.com/cran/Synth",
    "url": "https://cran.r-project.org/package=Synth",
    "install": "install.packages(\"Synth\")",
    "tags": [
      "synthetic-control",
      "comparative-case-studies",
      "counterfactual",
      "policy-evaluation",
      "single-unit-treatment"
    ],
    "best_for": "Classic single-treated-unit policy evaluations, implementing Abadie, Diamond & Hainmueller (2010, 2011, 2015)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "panel-data-analysis",
      "python-numpy",
      "linear-regression"
    ],
    "topic_tags": [
      "synthetic-control",
      "causal-inference",
      "policy-evaluation",
      "counterfactual-estimation",
      "comparative-case-studies"
    ],
    "summary": "The original synthetic control method implementation for creating artificial counterfactuals when studying the causal effect of interventions on a single treated unit. Used by researchers and data scientists to evaluate policy impacts by constructing a weighted combination of similar untreated units that mimics the pre-treatment behavior of the treated unit. Essential for comparative case studies where traditional experimental or quasi-experimental designs are not feasible.",
    "use_cases": [
      "evaluating-state-policy-impact-using-other-states-as-controls",
      "measuring-market-entry-effects-by-comparing-treated-geography-to-synthetic-control"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "synthetic control method implementation python",
      "how to create counterfactual for single treated unit",
      "comparative case study causal inference package",
      "policy evaluation when only one unit is treated"
    ]
  },
  {
    "name": "augsynth",
    "description": "Implements the Augmented Synthetic Control Method, which uses an outcome model (ridge regression by default) to correct for bias when pre-treatment fit is imperfect. Uniquely supports staggered adoption across multiple treated units via multisynth() function.",
    "category": "Causal Inference (Synthetic Control)",
    "docs_url": "https://github.com/ebenmichael/augsynth/blob/master/vignettes/singlesynth-vignette.md",
    "github_url": "https://github.com/ebenmichael/augsynth",
    "url": "https://github.com/ebenmichael/augsynth",
    "install": "devtools::install_github(\"ebenmichael/augsynth\")",
    "tags": [
      "augmented-synthetic-control",
      "bias-correction",
      "staggered-adoption",
      "ridge-regression",
      "imperfect-fit"
    ],
    "best_for": "SC applications with imperfect pre-treatment fit or staggered adoption across units, implementing Ben-Michael, Feller & Rothstein (2021)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "synthetic-control-method",
      "ridge-regression",
      "panel-data-analysis"
    ],
    "topic_tags": [
      "synthetic-control",
      "causal-inference",
      "staggered-adoption",
      "bias-correction",
      "policy-evaluation"
    ],
    "summary": "The augsynth package implements Augmented Synthetic Control, which improves upon traditional synthetic control by using ridge regression to correct for bias when pre-treatment fit between treated and synthetic units is poor. It's particularly valuable for tech economists evaluating policy interventions or feature rollouts where perfect pre-treatment matching isn't achievable, and uniquely handles staggered adoption scenarios where different units receive treatment at different times.",
    "use_cases": [
      "Evaluating the impact of a new product feature rolled out to different markets at different times",
      "Measuring the effect of a policy change across states when synthetic control fit is imperfect"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "How to handle poor pre-treatment fit in synthetic control",
      "Staggered adoption synthetic control method",
      "Bias correction for synthetic control analysis",
      "Augmented synthetic control vs traditional synthetic control"
    ]
  },
  {
    "name": "gsynth",
    "description": "Implements generalized synthetic control with interactive fixed effects, extending SCM to multiple treated units with variable treatment timing. Uses factor models to impute counterfactuals, handling unbalanced panels and complex treatment patterns with latent factor structures.",
    "category": "Causal Inference (Synthetic Control)",
    "docs_url": "https://yiqingxu.org/packages/gsynth/",
    "github_url": "https://github.com/xuyiqing/gsynth",
    "url": "https://cran.r-project.org/package=gsynth",
    "install": "install.packages(\"gsynth\")",
    "tags": [
      "generalized-synthetic-control",
      "interactive-fixed-effects",
      "factor-models",
      "multiple-treated-units",
      "unbalanced-panels"
    ],
    "best_for": "Multiple treated units with staggered treatment timing and latent factor structures, implementing Xu (2017)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "synthetic-control-method",
      "panel-data-regression",
      "factor-analysis"
    ],
    "topic_tags": [
      "generalized-synthetic-control",
      "interactive-fixed-effects",
      "causal-inference",
      "panel-data",
      "factor-models"
    ],
    "summary": "gsynth extends the synthetic control method to handle multiple treated units with staggered treatment timing using interactive fixed effects models. It addresses limitations of traditional SCM by incorporating latent factor structures to better model complex treatment patterns in unbalanced panel data. The package is particularly useful for policy evaluation when treatment rollout varies across units and time periods.",
    "use_cases": [
      "evaluating staggered policy rollouts across multiple states or regions",
      "analyzing product feature launches with different timing across user segments"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "generalized synthetic control multiple treatment units",
      "interactive fixed effects causal inference",
      "staggered treatment timing synthetic control",
      "factor model counterfactual estimation panel data"
    ]
  },
  {
    "name": "microsynth",
    "description": "Extends synthetic control method to micro-level data with many units. Implements permutation inference and handles high-dimensional settings where traditional SCM struggles.",
    "category": "Causal Inference (Synthetic Control)",
    "docs_url": "https://cran.r-project.org/web/packages/microsynth/microsynth.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=microsynth",
    "install": "install.packages(\"microsynth\")",
    "tags": [
      "synthetic-control",
      "micro-data",
      "permutation-inference",
      "high-dimensional",
      "many-units"
    ],
    "best_for": "Synthetic control for micro-level data with many units and permutation inference",
    "language": "R",
    "difficulty": "advanced",
    "prerequisites": [
      "synthetic-control-method",
      "panel-data-analysis",
      "permutation-tests"
    ],
    "topic_tags": [
      "synthetic-control",
      "micro-level-data",
      "permutation-inference",
      "causal-inference",
      "high-dimensional"
    ],
    "summary": "The microsynth package extends the synthetic control method to handle micro-level data with many units, addressing limitations of traditional SCM in high-dimensional settings. It implements permutation-based inference procedures to provide robust statistical testing for treatment effects. This tool is particularly valuable for researchers working with individual-level or firm-level panel data where creating synthetic controls from a small donor pool is challenging.",
    "use_cases": [
      "Evaluating impact of policy changes on individual households or firms using administrative microdata",
      "Assessing treatment effects in randomized experiments with many units where traditional synthetic control donor pools are insufficient"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "synthetic control for micro level data",
      "how to use synthetic control with many units",
      "microsynth package permutation inference",
      "synthetic control high dimensional panel data"
    ]
  },
  {
    "name": "pensynth",
    "description": "Implements penalized synthetic control method from Abadie & L'Hour (2021). Adds regularization to improve pre-treatment fit and reduce interpolation bias in sparse donor pools.",
    "category": "Causal Inference (Synthetic Control)",
    "docs_url": "https://cran.r-project.org/web/packages/pensynth/pensynth.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=pensynth",
    "install": "install.packages(\"pensynth\")",
    "tags": [
      "synthetic-control",
      "penalized",
      "regularization",
      "interpolation-bias",
      "sparse-donors"
    ],
    "best_for": "Penalized synthetic control with regularization, implementing Abadie & L'Hour (2021)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "synthetic-control-method",
      "ridge-regression",
      "python-scikit-learn"
    ],
    "topic_tags": [
      "synthetic-control",
      "causal-inference",
      "regularization",
      "policy-evaluation",
      "python-package"
    ],
    "summary": "Pensynth implements the penalized synthetic control method that adds L2 regularization to the traditional synthetic control approach. This helps improve pre-treatment fit and reduces interpolation bias when working with sparse donor pools, making synthetic control more robust for policy evaluation studies.",
    "use_cases": [
      "Evaluating policy impact when you have few suitable control units and traditional synthetic control produces poor pre-treatment fit",
      "Analyzing treatment effects in settings with high-dimensional covariates where standard synthetic control suffers from overfitting"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "penalized synthetic control python implementation",
      "how to fix poor pre-treatment fit in synthetic control",
      "synthetic control with regularization sparse donors",
      "pensynth package synthetic control method"
    ]
  },
  {
    "name": "scpi",
    "description": "Provides rigorous prediction intervals for synthetic control methods following Cattaneo et al. (2021, 2025). Supports staggered adoption designs with valid uncertainty quantification.",
    "category": "Causal Inference (Synthetic Control)",
    "docs_url": "https://nppackages.github.io/scpi/",
    "github_url": null,
    "url": "https://cran.r-project.org/package=scpi",
    "install": "install.packages(\"scpi\")",
    "tags": [
      "synthetic-control",
      "prediction-intervals",
      "uncertainty-quantification",
      "staggered-adoption",
      "inference"
    ],
    "best_for": "Rigorous prediction intervals for synthetic control, implementing Cattaneo et al. (2021, 2025)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "synthetic-control-method",
      "causal-inference-basics",
      "R-programming"
    ],
    "topic_tags": [
      "synthetic-control",
      "prediction-intervals",
      "causal-inference",
      "uncertainty-quantification",
      "staggered-adoption"
    ],
    "summary": "The scpi package implements rigorous prediction intervals for synthetic control methods, providing valid uncertainty quantification for treatment effect estimates. It supports both single-unit and staggered adoption designs, addressing a key limitation of traditional synthetic control methods that lack formal statistical inference procedures.",
    "use_cases": [
      "Evaluating policy impact with confidence intervals when a state implements new regulation",
      "Measuring marketing campaign effects across multiple markets with rollout uncertainty"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "synthetic control confidence intervals R",
      "uncertainty quantification synthetic control method",
      "staggered adoption synthetic control inference",
      "prediction intervals causal inference package"
    ]
  },
  {
    "name": "synthdid",
    "description": "Implements synthetic difference-in-differences, a hybrid method combining insights from both DiD and synthetic control that reweights and matches pre-treatment trends. Provides improved robustness properties compared to either method alone by combining their strengths.",
    "category": "Causal Inference (Synthetic Control)",
    "docs_url": "https://synth-inference.github.io/synthdid/",
    "github_url": "https://github.com/synth-inference/synthdid",
    "url": "https://cran.r-project.org/package=synthdid",
    "install": "install.packages(\"synthdid\")",
    "tags": [
      "synthetic-control",
      "difference-in-differences",
      "hybrid-estimator",
      "panel-data",
      "robust-estimation"
    ],
    "best_for": "Settings where neither pure DiD nor pure SC is ideal, implementing Arkhangelsky, Athey, Hirshberg, Imbens & Wager (2021)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "difference-in-differences",
      "synthetic-control-method",
      "panel-data-analysis"
    ],
    "topic_tags": [
      "synthetic-difference-in-differences",
      "causal-inference",
      "panel-data",
      "hybrid-estimator",
      "treatment-effects"
    ],
    "summary": "Synthetic DiD is a causal inference method that combines synthetic control and difference-in-differences approaches to estimate treatment effects in panel data. It improves upon traditional DiD by reweighting control units to better match pre-treatment trends, while maintaining the robustness properties of both parent methods. Data scientists and researchers use it when they need stronger identification assumptions than standard DiD but more flexibility than pure synthetic control.",
    "use_cases": [
      "Evaluating the impact of a new policy rollout across different states or regions when treatment assignment isn't random",
      "Measuring the effect of a product feature launch on user engagement when only some user segments received the feature"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "synthetic difference in differences implementation",
      "hybrid estimator combining DiD and synthetic control",
      "robust causal inference for panel data",
      "synthdid package for treatment effect estimation"
    ]
  },
  {
    "name": "tidysynth",
    "description": "Brings synthetic control method into the tidyverse with cleaner syntax and built-in placebo inference. Provides pipe-friendly workflows for SCM estimation and visualization.",
    "category": "Causal Inference (Synthetic Control)",
    "docs_url": "https://cran.r-project.org/web/packages/tidysynth/tidysynth.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=tidysynth",
    "install": "install.packages(\"tidysynth\")",
    "tags": [
      "synthetic-control",
      "tidyverse",
      "placebo-inference",
      "causal-inference",
      "policy-evaluation"
    ],
    "best_for": "Tidyverse-friendly synthetic control method with clean syntax and built-in placebo inference",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "synthetic-control-method",
      "R-tidyverse",
      "difference-in-differences"
    ],
    "topic_tags": [
      "synthetic-control",
      "causal-inference",
      "policy-evaluation",
      "tidyverse",
      "R-package"
    ],
    "summary": "tidysynth is an R package that brings the synthetic control method into the tidyverse ecosystem with cleaner, pipe-friendly syntax. It simplifies the process of creating synthetic controls for causal inference and includes built-in placebo testing functionality. The package is designed for researchers and analysts who need to evaluate policy interventions or treatment effects using synthetic control methodology.",
    "use_cases": [
      "Evaluating the causal impact of a new policy implemented in one state compared to other states",
      "Analyzing the effect of a product launch in one market while using other similar markets as synthetic controls"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "tidyverse synthetic control R package",
      "how to do synthetic control method in R",
      "placebo inference synthetic control",
      "policy evaluation synthetic control tidyverse"
    ]
  },
  {
    "name": "MAPIE",
    "description": "Scikit-learn-contrib library for conformal prediction intervals. Provides model-agnostic uncertainty quantification for regression and classification.",
    "category": "Conformal Prediction & Uncertainty",
    "docs_url": "https://mapie.readthedocs.io/",
    "github_url": "https://github.com/scikit-learn-contrib/MAPIE",
    "url": "https://github.com/scikit-learn-contrib/MAPIE",
    "install": "pip install mapie",
    "tags": [
      "conformal prediction",
      "uncertainty",
      "intervals"
    ],
    "best_for": "Model-agnostic prediction intervals",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "scikit-learn",
      "python-numpy",
      "statistical-inference"
    ],
    "topic_tags": [
      "conformal-prediction",
      "uncertainty-quantification",
      "prediction-intervals",
      "model-agnostic",
      "scikit-learn"
    ],
    "summary": "MAPIE is a Python library that implements conformal prediction methods to generate prediction intervals with statistical guarantees. It works with any scikit-learn compatible model to quantify uncertainty without making distributional assumptions. The library is particularly useful for practitioners who need reliable uncertainty estimates in production machine learning systems.",
    "use_cases": [
      "Adding confidence intervals to existing regression models in production to flag uncertain predictions",
      "Quantifying prediction uncertainty for medical diagnosis models where knowing confidence levels is critical for decision-making"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "how to add uncertainty quantification to scikit-learn models",
      "conformal prediction python library",
      "prediction intervals for machine learning models",
      "model-agnostic uncertainty estimation tools"
    ]
  },
  {
    "name": "TorchCP",
    "description": "PyTorch-native conformal prediction for DNNs, GNNs, and LLMs with GPU acceleration.",
    "category": "Conformal Prediction & Uncertainty",
    "docs_url": "https://torchcp.readthedocs.io/",
    "github_url": "https://github.com/ml-stat-Sustech/TorchCP",
    "url": "https://github.com/ml-stat-Sustech/TorchCP",
    "install": "pip install torchcp",
    "tags": [
      "conformal prediction",
      "PyTorch",
      "deep learning"
    ],
    "best_for": "Conformal prediction for neural networks",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "pytorch-tensors",
      "neural-network-training",
      "python-classes"
    ],
    "topic_tags": [
      "conformal-prediction",
      "uncertainty-quantification",
      "pytorch",
      "deep-learning",
      "prediction-intervals"
    ],
    "summary": "TorchCP is a PyTorch-native library that implements conformal prediction methods for deep neural networks, graph neural networks, and large language models. It provides GPU-accelerated uncertainty quantification with prediction intervals that have statistical guarantees. The package is designed for practitioners who need to add reliable uncertainty estimates to their existing PyTorch models.",
    "use_cases": [
      "Adding confidence intervals to production deep learning models for risk assessment",
      "Quantifying uncertainty in graph neural network predictions for molecular property prediction"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "pytorch conformal prediction library",
      "how to add uncertainty quantification to deep learning models",
      "conformal prediction for neural networks python",
      "GPU accelerated uncertainty estimation pytorch"
    ]
  },
  {
    "name": "crepes",
    "description": "Lightweight library for conformal regressors and predictive systems. Simple API for calibrated prediction intervals.",
    "category": "Conformal Prediction & Uncertainty",
    "docs_url": "https://github.com/henrikbostrom/crepes",
    "github_url": "https://github.com/henrikbostrom/crepes",
    "url": "https://github.com/henrikbostrom/crepes",
    "install": "pip install crepes",
    "tags": [
      "conformal prediction",
      "regression",
      "intervals"
    ],
    "best_for": "Simple conformal regressors",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "scikit-learn",
      "regression-analysis",
      "python-numpy"
    ],
    "topic_tags": [
      "conformal-prediction",
      "uncertainty-quantification",
      "prediction-intervals",
      "calibrated-models",
      "regression"
    ],
    "summary": "CREPES is a lightweight Python library that implements conformal prediction methods for regression tasks, providing calibrated prediction intervals with statistical guarantees. It offers a simple API to wrap existing regression models and generate reliable uncertainty estimates without making distributional assumptions. The library is particularly useful for practitioners who need trustworthy confidence bounds on their predictions.",
    "use_cases": [
      "Adding uncertainty estimates to production ML models for risk-sensitive decisions",
      "Validating model reliability by checking if prediction intervals contain true values at specified confidence levels"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "conformal prediction python library",
      "how to get prediction intervals for regression",
      "calibrated uncertainty estimates scikit-learn",
      "lightweight conformal regression package"
    ]
  },
  {
    "name": "fortuna",
    "description": "AWS library for uncertainty quantification in deep learning. Bayesian and conformal methods.",
    "category": "Conformal Prediction & Uncertainty",
    "docs_url": "https://aws-fortuna.readthedocs.io/",
    "github_url": "https://github.com/awslabs/fortuna",
    "url": "https://github.com/awslabs/fortuna",
    "install": "pip install fortuna",
    "tags": [
      "uncertainty",
      "Bayesian",
      "deep learning"
    ],
    "best_for": "Deep learning uncertainty quantification",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "pytorch",
      "bayesian-inference",
      "python-numpy"
    ],
    "topic_tags": [
      "uncertainty-quantification",
      "conformal-prediction",
      "bayesian-deep-learning",
      "aws-tools",
      "model-reliability"
    ],
    "summary": "Fortuna is AWS's library for quantifying uncertainty in deep learning models using both Bayesian and conformal prediction methods. It helps data scientists build more reliable ML systems by providing confidence intervals and uncertainty estimates for neural network predictions. The library integrates with popular deep learning frameworks and offers production-ready implementations of uncertainty quantification techniques.",
    "use_cases": [
      "Adding confidence intervals to production ML models to flag uncertain predictions",
      "Evaluating model reliability in high-stakes applications like medical diagnosis or autonomous systems"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "uncertainty quantification deep learning python",
      "bayesian neural networks AWS library",
      "conformal prediction implementation",
      "how to measure model confidence pytorch"
    ]
  },
  {
    "name": "puncc",
    "description": "IRT Lab's library for predictive uncertainty with conformal prediction. Supports various conformal methods.",
    "category": "Conformal Prediction & Uncertainty",
    "docs_url": "https://github.com/deel-ai/puncc",
    "github_url": "https://github.com/deel-ai/puncc",
    "url": "https://github.com/deel-ai/puncc",
    "install": "pip install puncc",
    "tags": [
      "conformal prediction",
      "uncertainty",
      "calibration"
    ],
    "best_for": "Calibrated prediction sets",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "scikit-learn",
      "prediction-intervals",
      "python-numpy"
    ],
    "topic_tags": [
      "conformal-prediction",
      "uncertainty-quantification",
      "prediction-intervals",
      "model-calibration",
      "python-library"
    ],
    "summary": "PUNCC is a Python library that implements conformal prediction methods for quantifying uncertainty in machine learning predictions. It provides various conformal prediction techniques to generate prediction intervals with statistical guarantees. The library is designed for practitioners who need reliable uncertainty estimates alongside their model predictions.",
    "use_cases": [
      "Adding uncertainty bounds to production ML models to flag low-confidence predictions",
      "Validating model reliability by checking if prediction intervals achieve target coverage rates"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "conformal prediction python library",
      "how to add uncertainty to sklearn predictions",
      "prediction intervals with statistical guarantees",
      "PUNCC conformal prediction tutorial"
    ]
  },
  {
    "name": "cjoint",
    "description": "Estimates Average Marginal Component Effects (AMCEs) for conjoint experiments following Hainmueller, Hopkins & Yamamoto (2014). Handles multi-dimensional preferences with clustered standard errors.",
    "category": "Conjoint Analysis",
    "docs_url": "https://cran.r-project.org/web/packages/cjoint/cjoint.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=cjoint",
    "install": "install.packages(\"cjoint\")",
    "tags": [
      "conjoint",
      "AMCE",
      "survey-experiments",
      "preferences",
      "political-science"
    ],
    "best_for": "AMCE estimation for conjoint experiments, implementing Hainmueller, Hopkins & Yamamoto (2014)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "survey-design",
      "linear-regression",
      "R-programming"
    ],
    "topic_tags": [
      "conjoint-analysis",
      "experimental-design",
      "survey-experiments",
      "causal-inference",
      "preference-elicitation"
    ],
    "summary": "The cjoint package implements the standard method for analyzing conjoint experiments, estimating how individual attributes influence choices or ratings. It's widely used by researchers in political science, marketing, and economics to understand multi-dimensional preferences and trade-offs. The package handles complex survey designs with proper clustering and follows established best practices from Hainmueller et al.",
    "use_cases": [
      "Measuring voter preferences across candidate attributes like experience, party, and policy positions",
      "Testing product features to understand which combinations drive consumer choice in market research"
    ],
    "audience": [
      "Early-PhD",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "How to analyze conjoint experiment data in R",
      "AMCE estimation for survey experiments",
      "cjoint package tutorial conjoint analysis",
      "Measuring preferences with conjoint experiments"
    ]
  },
  {
    "name": "cregg",
    "description": "Tidy interface for conjoint analysis with visualization. Provides functions for calculating and plotting marginal means and AMCEs with ggplot2-based output for publication-ready figures.",
    "category": "Conjoint Analysis",
    "docs_url": "https://thomasleeper.com/cregg/",
    "github_url": "https://github.com/leeper/cregg",
    "url": "https://cran.r-project.org/package=cregg",
    "install": "install.packages(\"cregg\")",
    "tags": [
      "conjoint",
      "visualization",
      "marginal-means",
      "ggplot2",
      "survey-experiments"
    ],
    "best_for": "Tidy conjoint analysis with ggplot2 visualization for marginal means and AMCEs",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "conjoint-experiments",
      "R-programming",
      "ggplot2"
    ],
    "topic_tags": [
      "conjoint-analysis",
      "causal-inference",
      "survey-experiments",
      "data-visualization",
      "R-package"
    ],
    "summary": "An R package that provides a streamlined workflow for analyzing conjoint experiments, with built-in functions for calculating Average Marginal Component Effects (AMCEs) and marginal means. It generates publication-ready visualizations using ggplot2, making it easy to interpret and communicate results from choice-based experiments.",
    "use_cases": [
      "Analyzing consumer preferences for product features in market research studies",
      "Measuring voter preferences for candidate attributes in political science research"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "R package for conjoint analysis with visualization",
      "how to calculate AMCE in conjoint experiments",
      "best tools for analyzing choice experiments",
      "conjoint analysis package with ggplot2 output"
    ]
  },
  {
    "name": "H2O Sparkling Water",
    "description": "H2O's distributed ML engine on Spark with GLM/GAM that provides p-values, confidence intervals, and Tweedie/Gamma distributions.",
    "category": "Core Libraries & Linear Models",
    "docs_url": "https://docs.h2o.ai/sparkling-water/3.3/latest-stable/doc/index.html",
    "github_url": "https://github.com/h2oai/sparkling-water",
    "url": "https://github.com/h2oai/sparkling-water",
    "install": "pip install h2o_pysparkling_3.4",
    "tags": [
      "spark",
      "GLM",
      "GAM",
      "distributed",
      "p-values"
    ],
    "best_for": "Econometric inference (p-values, CIs) at Spark scale",
    "language": "Spark",
    "difficulty": "intermediate",
    "prerequisites": [
      "apache-spark",
      "python-pyspark",
      "generalized-linear-models"
    ],
    "topic_tags": [
      "distributed-ml",
      "spark-integration",
      "statistical-inference",
      "glm-gam",
      "large-scale-modeling"
    ],
    "summary": "H2O Sparkling Water integrates H2O's machine learning capabilities with Apache Spark, enabling distributed GLM and GAM models with statistical inference features like p-values and confidence intervals. It's particularly valuable for data scientists working with large datasets who need both scalability and rigorous statistical testing. The library supports specialized distributions like Tweedie and Gamma for modeling non-normal data at scale.",
    "use_cases": [
      "Building interpretable insurance pricing models on terabytes of claims data with proper statistical significance testing",
      "Running large-scale A/B test analysis with GAM models to capture non-linear effects while maintaining distributed processing"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "distributed GLM with p-values on Spark",
      "H2O Sparkling Water vs regular Spark MLlib",
      "large scale GAM models with confidence intervals",
      "Tweedie regression on big data Spark"
    ]
  },
  {
    "name": "Linregress",
    "description": "Simple linear regression for Rust with R-style formula syntax, standard errors, t-stats, and p-values.",
    "category": "Core Libraries & Linear Models",
    "docs_url": "https://docs.rs/linregress",
    "github_url": "https://github.com/n1m3/linregress",
    "url": "https://crates.io/crates/linregress",
    "install": "cargo add linregress",
    "tags": [
      "rust",
      "regression",
      "OLS",
      "statistics"
    ],
    "best_for": "Simple no-frills OLS regression in Rust",
    "language": "Rust",
    "difficulty": "beginner",
    "prerequisites": [
      "rust-basics",
      "linear-regression-theory"
    ],
    "topic_tags": [
      "rust-statistics",
      "linear-regression",
      "OLS-implementation",
      "statistical-computing"
    ],
    "summary": "Linregress is a Rust library that provides simple linear regression functionality with an R-like formula syntax. It outputs standard statistical measures including standard errors, t-statistics, and p-values, making it accessible for users familiar with R who want to implement regression in Rust. The package is ideal for basic econometric analysis and statistical modeling in Rust applications.",
    "use_cases": [
      "Building a Rust application that needs to perform quick linear regressions on data streams",
      "Implementing basic econometric models in a high-performance Rust backend system"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "linear regression library for Rust",
      "Rust package for OLS regression with R syntax",
      "how to do regression analysis in Rust",
      "Rust statistical computing linear models"
    ]
  },
  {
    "name": "Polars",
    "description": "Blazingly fast DataFrame library for Rust and Python with SQL-like syntax, lazy evaluation, and excellent time series handling.",
    "category": "Core Libraries & Linear Models",
    "docs_url": "https://pola.rs/",
    "github_url": "https://github.com/pola-rs/polars",
    "url": "https://crates.io/crates/polars",
    "install": "cargo add polars",
    "tags": [
      "rust",
      "dataframe",
      "data manipulation",
      "performance"
    ],
    "best_for": "High-performance data manipulation (pandas alternative)",
    "language": "Rust",
    "difficulty": "beginner",
    "prerequisites": [
      "python-pandas",
      "basic-SQL",
      "dataframe-operations"
    ],
    "topic_tags": [
      "data-manipulation",
      "performance-optimization",
      "rust-backend",
      "lazy-evaluation",
      "time-series"
    ],
    "summary": "Polars is a high-performance DataFrame library built in Rust with Python bindings that offers pandas-like functionality with significantly faster execution speeds. It features lazy evaluation, memory efficiency, and SQL-like syntax making it ideal for large dataset processing. Tech economists and data scientists use it to accelerate data pipelines and handle computationally intensive analysis workflows.",
    "use_cases": [
      "Processing large financial datasets with millions of transactions for market analysis",
      "Accelerating existing pandas workflows that are hitting memory or speed bottlenecks"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "fast alternative to pandas for large datasets",
      "polars vs pandas performance comparison",
      "how to migrate from pandas to polars",
      "rust dataframe library for python data science"
    ]
  },
  {
    "name": "Scikit-learn",
    "description": "Foundational ML library with regression models (incl. regularized), model selection, cross-validation, evaluation metrics.",
    "category": "Core Libraries & Linear Models",
    "docs_url": "https://scikit-learn.org/",
    "github_url": "https://github.com/scikit-learn/scikit-learn",
    "url": "https://github.com/scikit-learn/scikit-learn",
    "install": "pip install scikit-learn",
    "tags": [
      "regression",
      "linear models"
    ],
    "best_for": "OLS regression, basic econometrics, data manipulation",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-basics",
      "numpy-arrays",
      "pandas-dataframes"
    ],
    "topic_tags": [
      "machine-learning",
      "supervised-learning",
      "model-evaluation",
      "cross-validation",
      "feature-selection"
    ],
    "summary": "Scikit-learn is Python's most widely-used machine learning library, providing simple and efficient tools for data mining and analysis. It offers comprehensive implementations of classification, regression, and clustering algorithms with consistent APIs. The library is essential for anyone starting in machine learning, offering everything from basic linear regression to advanced ensemble methods with built-in model evaluation tools.",
    "use_cases": [
      "Building predictive models for user conversion rates using logistic regression with cross-validation",
      "Implementing A/B test analysis with regularized regression models to control for multiple features"
    ],
    "audience": [
      "Junior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "python machine learning library for beginners",
      "how to do linear regression in python",
      "scikit-learn vs other ML libraries",
      "cross validation and model selection tools"
    ]
  },
  {
    "name": "Statsmodels",
    "description": "Comprehensive library for estimating statistical models (OLS, GLM, etc.), conducting tests, and data exploration. Core tool.",
    "category": "Core Libraries & Linear Models",
    "docs_url": "https://www.statsmodels.org/",
    "github_url": "https://github.com/statsmodels/statsmodels",
    "url": "https://github.com/statsmodels/statsmodels",
    "install": "pip install statsmodels",
    "tags": [
      "regression",
      "linear models"
    ],
    "best_for": "OLS regression, basic econometrics, data manipulation",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-pandas",
      "linear-algebra",
      "hypothesis-testing"
    ],
    "topic_tags": [
      "statistical-modeling",
      "regression-analysis",
      "econometrics",
      "hypothesis-testing",
      "python-library"
    ],
    "summary": "Statsmodels is Python's go-to library for classical statistical analysis and econometric modeling. It provides comprehensive implementations of regression models (OLS, logistic, Poisson), time series analysis, and statistical tests with detailed output similar to R or Stata. Essential for anyone doing rigorous statistical analysis in Python.",
    "use_cases": [
      "Running OLS regression to estimate treatment effects in an A/B test with proper statistical inference",
      "Fitting logistic regression models to predict user conversion and interpreting coefficients"
    ],
    "audience": [
      "Junior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "Python library for regression analysis",
      "statsmodels vs sklearn for statistical modeling",
      "how to run OLS regression in Python",
      "Python equivalent of R lm function"
    ]
  },
  {
    "name": "appelpy",
    "description": "Applied Econometrics Library bridging Stata-like syntax with Python. Built on statsmodels with convenient API.",
    "category": "Core Libraries & Linear Models",
    "docs_url": "https://appelpy.readthedocs.io/",
    "github_url": "https://github.com/mfarragher/appelpy",
    "url": "https://github.com/mfarragher/appelpy",
    "install": "pip install appelpy",
    "tags": [
      "regression",
      "linear models",
      "Stata"
    ],
    "best_for": "Stata-like econometrics workflow in Python",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-pandas",
      "basic-regression",
      "statsmodels"
    ],
    "topic_tags": [
      "regression",
      "stata-alternative",
      "econometrics",
      "linear-models",
      "python-package"
    ],
    "summary": "Appelpy is a Python package that provides Stata-like syntax for econometric analysis, built on top of statsmodels. It's designed for economists and data scientists transitioning from Stata to Python who want familiar commands and output formats. The library simplifies regression analysis with convenient methods for model specification, diagnostics, and results presentation.",
    "use_cases": [
      "Stata users switching to Python who want familiar regression syntax and output",
      "Running standard econometric models (OLS, fixed effects) with simplified commands"
    ],
    "audience": [
      "Junior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "Python package with Stata-like syntax for regression",
      "econometrics library that feels like Stata",
      "easy regression analysis in Python for economists",
      "appelpy vs statsmodels for linear models"
    ]
  },
  {
    "name": "collapse",
    "description": "High-performance data transformation package designed by an economist. Provides fast grouped operations, time series functions, and panel data tools with 10-100\u00d7 speedups over dplyr on large data.",
    "category": "Data Workflow",
    "docs_url": "https://sebkrantz.github.io/collapse/",
    "github_url": "https://github.com/SebKrantz/collapse",
    "url": "https://cran.r-project.org/package=collapse",
    "install": "install.packages(\"collapse\")",
    "tags": [
      "data-transformation",
      "high-performance",
      "panel-data",
      "time-series",
      "grouped-operations"
    ],
    "best_for": "High-performance data transformation optimized for economists\u201410-100\u00d7 faster than dplyr",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "R-dplyr",
      "panel-data-concepts"
    ],
    "topic_tags": [
      "data-transformation",
      "performance-optimization",
      "panel-data",
      "time-series",
      "grouped-operations"
    ],
    "summary": "High-performance data transformation package built specifically for economists working with large datasets. Offers dramatic speed improvements (10-100x) over standard tools like dplyr for grouped operations, time series transformations, and panel data manipulation. Designed to handle the scale and complexity typical in economic research and industry applications.",
    "use_cases": [
      "Processing multi-year firm-level datasets with millions of observations for panel regression analysis",
      "Real-time transformation of high-frequency trading data with complex time-based grouping operations"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "fast alternative to dplyr for large datasets",
      "high performance panel data transformation tools",
      "speed up grouped operations in R",
      "economist data processing package performance"
    ]
  },
  {
    "name": "data.table",
    "description": "Extension of data.frame providing fast aggregation of large data (100GB+), ordered joins, and memory-efficient operations. Uses reference semantics for in-place modification with concise syntax [:=, .SD, by=].",
    "category": "Data Workflow",
    "docs_url": "https://rdatatable.gitlab.io/data.table/",
    "github_url": "https://github.com/Rdatatable/data.table",
    "url": "https://cran.r-project.org/package=data.table",
    "install": "install.packages(\"data.table\")",
    "tags": [
      "data-manipulation",
      "fast",
      "large-data",
      "reference-semantics",
      "aggregation"
    ],
    "best_for": "Fast operations on large datasets (100GB+) with memory-efficient reference semantics",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "R-programming",
      "data-frame-operations",
      "SQL-joins"
    ],
    "topic_tags": [
      "data-manipulation",
      "R-package",
      "large-datasets",
      "memory-efficiency",
      "aggregation"
    ],
    "summary": "data.table is an R package that extends data.frame functionality for high-performance operations on large datasets (100GB+). It provides fast aggregation, ordered joins, and memory-efficient operations using reference semantics for in-place modifications. The package is essential for data scientists working with large-scale data processing in R environments.",
    "use_cases": [
      "Processing multi-gigabyte transaction logs for fraud detection with fast grouping and aggregation",
      "Joining multiple large customer datasets in memory without copying data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "fast R package for large dataset manipulation",
      "data.table vs pandas performance comparison",
      "memory efficient data processing in R",
      "how to handle 100GB datasets in R"
    ]
  },
  {
    "name": "haven",
    "description": "Import and export Stata, SPSS, and SAS data files preserving variable labels and value labels. Handles .dta, .sav, .sas7bdat, and .xpt formats with labelled vectors for metadata.",
    "category": "Data Workflow",
    "docs_url": "https://haven.tidyverse.org/",
    "github_url": "https://github.com/tidyverse/haven",
    "url": "https://cran.r-project.org/package=haven",
    "install": "install.packages(\"haven\")",
    "tags": [
      "Stata",
      "SPSS",
      "SAS",
      "data-import",
      "labelled-data"
    ],
    "best_for": "Reading and writing Stata, SPSS, and SAS files with preserved labels",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "R-basics",
      "data-frame-manipulation"
    ],
    "topic_tags": [
      "data-import",
      "statistical-software",
      "metadata-preservation",
      "file-formats"
    ],
    "summary": "The haven package provides seamless import and export functionality for proprietary statistical software formats including Stata, SPSS, and SAS files. It preserves valuable metadata like variable labels and value labels that are typically lost during data conversion. Essential for researchers working with datasets from multiple statistical platforms or transitioning between software ecosystems.",
    "use_cases": [
      "Converting legacy SPSS survey data to R while preserving question text and response labels for analysis",
      "Importing Stata panel datasets with documented variable definitions for replication studies"
    ],
    "audience": [
      "Junior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "how to read stata files in R",
      "import SPSS data with labels",
      "convert SAS dataset to R dataframe",
      "preserve variable labels when importing data"
    ]
  },
  {
    "name": "tidyverse",
    "description": "Meta-package installing core tidyverse packages: ggplot2 (visualization), dplyr (manipulation), tidyr (tidying), readr (import), purrr (functional programming), tibble (data frames), stringr (strings), and forcats (factors).",
    "category": "Data Workflow",
    "docs_url": "https://www.tidyverse.org/",
    "github_url": "https://github.com/tidyverse/tidyverse",
    "url": "https://cran.r-project.org/package=tidyverse",
    "install": "install.packages(\"tidyverse\")",
    "tags": [
      "tidyverse",
      "data-science",
      "dplyr",
      "ggplot2",
      "meta-package"
    ],
    "best_for": "Core tidyverse ecosystem for consistent data science workflows",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "r-basics",
      "data-frames"
    ],
    "topic_tags": [
      "r-programming",
      "data-manipulation",
      "data-visualization",
      "workflow-tools",
      "exploratory-analysis"
    ],
    "summary": "The tidyverse is R's most popular meta-package that installs a coherent collection of data science tools with consistent syntax and philosophy. It provides everything needed for a complete data workflow from import to visualization, making R code more readable and intuitive. Essential for anyone doing data analysis in R, from beginners learning their first data manipulation to experienced analysts building complex pipelines.",
    "use_cases": [
      "cleaning and analyzing customer transaction data for business insights",
      "creating publication-ready visualizations and summary statistics for research papers"
    ],
    "audience": [
      "Junior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "best R packages for data science workflow",
      "tidyverse vs base R for data analysis",
      "how to get started with R data manipulation",
      "comprehensive R package for visualization and data cleaning"
    ]
  },
  {
    "name": "AER",
    "description": "Companion package to 'Applied Econometrics with R' (Kleiber & Zeileis) plus datasets from Stock & Watson. Provides ivreg() for instrumental variables, tobit(), and econometric testing functions.",
    "category": "Datasets",
    "docs_url": "https://cran.r-project.org/web/packages/AER/AER.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=AER",
    "install": "install.packages(\"AER\")",
    "tags": [
      "datasets",
      "textbook",
      "instrumental-variables",
      "Stock-Watson",
      "Kleiber-Zeileis"
    ],
    "best_for": "Datasets and functions from 'Applied Econometrics with R' plus Stock & Watson data",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "R-programming",
      "linear-regression",
      "basic-econometrics"
    ],
    "topic_tags": [
      "instrumental-variables",
      "econometric-datasets",
      "R-package",
      "textbook-companion",
      "applied-econometrics"
    ],
    "summary": "The AER package is a comprehensive R companion to the popular 'Applied Econometrics with R' textbook, containing datasets and functions for econometric analysis. It provides easy access to classic datasets from Stock & Watson plus essential econometric functions like ivreg() for instrumental variables and tobit() for censored regression. This makes it ideal for students and practitioners learning foundational econometric methods with real data.",
    "use_cases": [
      "Working through econometrics textbook exercises with pre-loaded datasets and matching functions",
      "Teaching introductory econometrics course using standardized datasets that students can easily access"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "R package for econometrics textbook datasets",
      "how to run instrumental variables regression in R",
      "Stock Watson datasets R package",
      "AER package ivreg function tutorial"
    ]
  },
  {
    "name": "causaldata",
    "description": "Unified collection of datasets from three major causal inference textbooks: 'The Effect' (Huntington-Klein), 'Causal Inference: The Mixtape' (Cunningham), and 'Causal Inference: What If?' (Hern\u00e1n & Robins).",
    "category": "Datasets",
    "docs_url": "https://cran.r-project.org/web/packages/causaldata/causaldata.pdf",
    "github_url": "https://github.com/NickCH-K/causaldata",
    "url": "https://cran.r-project.org/package=causaldata",
    "install": "install.packages(\"causaldata\")",
    "tags": [
      "datasets",
      "causal-inference",
      "textbook",
      "The-Effect",
      "Mixtape"
    ],
    "best_for": "Datasets from The Effect, Causal Inference: The Mixtape, and What If? textbooks",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-statistics",
      "causal-inference-concepts"
    ],
    "topic_tags": [
      "causal-inference",
      "datasets",
      "textbook-resources",
      "econometrics",
      "research-data"
    ],
    "summary": "A unified R package containing datasets from three foundational causal inference textbooks, providing ready-to-use data for learning and practicing causal methods. Essential for students and practitioners working through these influential texts who need clean, standardized datasets. Eliminates the hassle of data preparation so users can focus on learning causal inference techniques.",
    "use_cases": [
      "Working through problem sets and examples from The Effect, Mixtape, or Causal Inference What If textbooks",
      "Teaching causal inference courses with standardized datasets that students can easily access"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "datasets for causal inference textbooks",
      "causal inference practice datasets",
      "The Effect book datasets download",
      "Mixtape textbook data examples"
    ]
  },
  {
    "name": "wooldridge",
    "description": "All 115 datasets from Wooldridge's 'Introductory Econometrics: A Modern Approach' (7th edition). Includes wage equations, crime data, housing prices, and classic econometrics teaching examples.",
    "category": "Datasets",
    "docs_url": "https://cran.r-project.org/web/packages/wooldridge/wooldridge.pdf",
    "github_url": "https://github.com/JustinMShea/wooldridge",
    "url": "https://cran.r-project.org/package=wooldridge",
    "install": "install.packages(\"wooldridge\")",
    "tags": [
      "datasets",
      "textbook",
      "teaching",
      "Wooldridge",
      "econometrics"
    ],
    "best_for": "115 datasets from Wooldridge's 'Introductory Econometrics' for teaching and examples",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "python-pandas",
      "R-basics",
      "linear-regression"
    ],
    "topic_tags": [
      "econometrics-datasets",
      "textbook-data",
      "teaching-materials",
      "regression-examples",
      "panel-data"
    ],
    "summary": "A comprehensive collection of 115 real-world datasets from Wooldridge's widely-used econometrics textbook, covering topics like labor economics, crime, housing, and education. These datasets are specifically curated for learning econometric methods and include classic examples used in thousands of classrooms worldwide. Perfect for practicing regression techniques, causal inference methods, and econometric modeling on clean, well-documented data.",
    "use_cases": [
      "Learning econometrics by replicating textbook examples and homework problems",
      "Teaching econometrics courses with standardized, well-documented datasets"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "Wooldridge econometrics datasets download",
      "classic econometrics teaching data examples",
      "wage equation regression practice datasets",
      "Introductory Econometrics textbook data"
    ]
  },
  {
    "name": "FactorAnalyzer",
    "description": "Specialized library for Exploratory (EFA) and Confirmatory (CFA) Factor Analysis with rotation options for interpretability.",
    "category": "Dimensionality Reduction",
    "docs_url": "https://factor-analyzer.readthedocs.io/en/latest/",
    "github_url": "https://github.com/EducationalTestingService/factor_analyzer",
    "url": "https://github.com/EducationalTestingService/factor_analyzer",
    "install": "pip install factor_analyzer",
    "tags": [
      "machine learning",
      "dimensionality"
    ],
    "best_for": "Feature extraction, PCA, high-dimensional data",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "linear-algebra",
      "principal-component-analysis"
    ],
    "topic_tags": [
      "factor-analysis",
      "exploratory-factor-analysis",
      "confirmatory-factor-analysis",
      "dimensionality-reduction",
      "survey-analysis"
    ],
    "summary": "FactorAnalyzer is a Python library that implements both Exploratory Factor Analysis (EFA) to discover underlying latent factors in data and Confirmatory Factor Analysis (CFA) to test hypothesized factor structures. It includes various rotation methods like varimax and promax to make factor loadings more interpretable. The package is particularly useful for psychometric analysis, survey research, and identifying latent constructs in high-dimensional data.",
    "use_cases": [
      "Analyzing customer survey responses to identify underlying satisfaction dimensions",
      "Validating psychological questionnaire structure by confirming hypothesized factor models"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "python library for factor analysis",
      "how to do confirmatory factor analysis in python",
      "EFA vs PCA implementation",
      "factor rotation methods python package"
    ]
  },
  {
    "name": "openTSNE",
    "description": "Optimized, parallel implementation of t-distributed Stochastic Neighbor Embedding (t-SNE) for large datasets.",
    "category": "Dimensionality Reduction",
    "docs_url": "https://opentsne.readthedocs.io/en/stable/",
    "github_url": "https://github.com/pavlin-policar/openTSNE",
    "url": "https://github.com/pavlin-policar/openTSNE",
    "install": "pip install opentsne",
    "tags": [
      "machine learning",
      "dimensionality"
    ],
    "best_for": "Feature extraction, PCA, high-dimensional data",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-scikit-learn",
      "matplotlib-plotting",
      "numpy-arrays"
    ],
    "topic_tags": [
      "t-sne",
      "dimensionality-reduction",
      "data-visualization",
      "clustering",
      "high-dimensional-data"
    ],
    "summary": "openTSNE is an optimized Python implementation of t-SNE that can handle large datasets through parallelization and memory efficiency improvements. Data scientists use it to visualize high-dimensional data by reducing it to 2D or 3D plots while preserving local neighborhood structure. It's particularly valuable when working with datasets too large for standard scikit-learn t-SNE implementations.",
    "use_cases": [
      "Visualizing customer segments from high-dimensional behavioral data with millions of users",
      "Creating 2D embeddings of product features or user preferences for exploratory data analysis"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "fast t-SNE implementation for large datasets",
      "parallel t-SNE python package",
      "visualize high dimensional data efficiently",
      "openTSNE vs scikit-learn t-SNE performance"
    ]
  },
  {
    "name": "umap-learn",
    "description": "Fast and scalable implementation of Uniform Manifold Approximation and Projection (UMAP) for non-linear reduction.",
    "category": "Dimensionality Reduction",
    "docs_url": "https://umap-learn.readthedocs.io/en/latest/",
    "github_url": "https://github.com/lmcinnes/umap",
    "url": "https://github.com/lmcinnes/umap",
    "install": "pip install umap-learn",
    "tags": [
      "machine learning",
      "dimensionality"
    ],
    "best_for": "Feature extraction, PCA, high-dimensional data",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-scikit-learn",
      "numpy-arrays",
      "manifold-learning"
    ],
    "topic_tags": [
      "umap",
      "dimensionality-reduction",
      "manifold-learning",
      "visualization",
      "embedding"
    ],
    "summary": "UMAP is a fast dimensionality reduction technique that preserves both local and global structure in high-dimensional data. It's particularly popular for creating 2D/3D visualizations of complex datasets and generating embeddings for downstream machine learning tasks. The umap-learn package provides an efficient Python implementation with scikit-learn compatibility.",
    "use_cases": [
      "Visualizing high-dimensional customer behavior data or product features in 2D scatter plots",
      "Creating lower-dimensional embeddings of text documents or genomic data for clustering analysis"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "fast dimensionality reduction python package",
      "UMAP implementation for data visualization",
      "alternative to t-SNE for embedding high dimensional data",
      "manifold learning library scikit-learn compatible"
    ]
  },
  {
    "name": "Biogeme",
    "description": "Maximum likelihood estimation of parametric models, with strong support for complex discrete choice models.",
    "category": "Discrete Choice Models",
    "docs_url": "https://biogeme.epfl.ch/index.html",
    "github_url": "https://github.com/michelbierlaire/biogeme",
    "url": "https://github.com/michelbierlaire/biogeme",
    "install": "pip install biogeme",
    "tags": [
      "discrete choice",
      "logit"
    ],
    "best_for": "Logit/probit models, consumer choice, demand estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "maximum-likelihood-estimation",
      "logistic-regression"
    ],
    "topic_tags": [
      "discrete-choice",
      "maximum-likelihood",
      "econometrics",
      "python-package",
      "logit-models"
    ],
    "summary": "Biogeme is a Python package for maximum likelihood estimation of parametric models, particularly specialized for discrete choice models like logit, probit, and nested logit. It's widely used by economists and data scientists for modeling consumer choice behavior, transportation decisions, and other scenarios where individuals select from discrete alternatives. The package provides flexible model specification and robust estimation capabilities for complex choice structures.",
    "use_cases": [
      "Modeling consumer preferences between product alternatives in e-commerce recommendation systems",
      "Analyzing transportation mode choice (car vs public transit vs bike) for urban planning decisions"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python package for discrete choice models",
      "biogeme logit model estimation",
      "maximum likelihood discrete choice python",
      "consumer choice modeling tools"
    ]
  },
  {
    "name": "PyBLP",
    "description": "Tools for estimating demand for differentiated products using the Berry-Levinsohn-Pakes (BLP) method.",
    "category": "Discrete Choice Models",
    "docs_url": "https://pyblp.readthedocs.io/",
    "github_url": "https://github.com/jeffgortmaker/pyblp",
    "url": "https://github.com/jeffgortmaker/pyblp",
    "install": "pip install pyblp",
    "tags": [
      "discrete choice",
      "logit"
    ],
    "best_for": "Logit/probit models, consumer choice, demand estimation",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "python-pandas",
      "instrumental-variables",
      "maximum-likelihood-estimation"
    ],
    "topic_tags": [
      "discrete-choice",
      "demand-estimation",
      "blp-method",
      "product-differentiation",
      "python-package"
    ],
    "summary": "PyBLP is a Python implementation of the Berry-Levinsohn-Pakes method for estimating consumer demand in markets with differentiated products. It's primarily used by economists and researchers studying industrial organization, antitrust, and market structure analysis. The package handles complex estimation problems involving endogeneity and unobserved product characteristics through instrumental variables and random coefficients.",
    "use_cases": [
      "Estimating price elasticity and market power in smartphone markets for antitrust analysis",
      "Analyzing consumer preferences for car features to predict demand for new vehicle models"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "Python package for BLP demand estimation",
      "how to estimate discrete choice models with endogeneity",
      "Berry Levinsohn Pakes implementation Python",
      "differentiated products demand estimation tools"
    ]
  },
  {
    "name": "PyLogit",
    "description": "Flexible implementation of conditional/multinomial logit models with utilities for data preparation.",
    "category": "Discrete Choice Models",
    "docs_url": null,
    "github_url": "https://github.com/timothyb0912/pylogit",
    "url": "https://github.com/timothyb0912/pylogit",
    "install": "pip install pylogit",
    "tags": [
      "discrete choice",
      "logit"
    ],
    "best_for": "Logit/probit models, consumer choice, demand estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "maximum-likelihood-estimation",
      "logistic-regression"
    ],
    "topic_tags": [
      "discrete-choice",
      "multinomial-logit",
      "conditional-logit",
      "choice-modeling",
      "python-package"
    ],
    "summary": "PyLogit is a Python package that implements conditional and multinomial logit models for analyzing discrete choice data. It provides flexible utilities for preparing choice datasets and estimating models where individuals select from multiple alternatives. The package is particularly useful for economists and data scientists working with consumer choice, transportation, or other decision-making data.",
    "use_cases": [
      "Modeling consumer product choices in market research studies",
      "Analyzing transportation mode selection for urban planning"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "python package for discrete choice models",
      "how to implement multinomial logit in python",
      "conditional logit model estimation tools",
      "discrete choice analysis python library"
    ]
  },
  {
    "name": "XLogit",
    "description": "Fast estimation of Multinomial Logit and Mixed Logit models, optimized for performance.",
    "category": "Discrete Choice Models",
    "docs_url": "https://xlogit.readthedocs.io/",
    "github_url": "https://github.com/arteagac/xlogit",
    "url": "https://github.com/arteagac/xlogit",
    "install": "pip install xlogit",
    "tags": [
      "discrete choice",
      "logit"
    ],
    "best_for": "Logit/probit models, consumer choice, demand estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "maximum-likelihood-estimation",
      "choice-modeling"
    ],
    "topic_tags": [
      "discrete-choice",
      "multinomial-logit",
      "mixed-logit",
      "python-package",
      "choice-modeling"
    ],
    "summary": "XLogit is a Python package for fast estimation of Multinomial Logit and Mixed Logit models with performance optimizations. It's primarily used by researchers and analysts working on discrete choice problems where individuals select from multiple alternatives. The package offers speed improvements over traditional estimation methods while maintaining statistical rigor.",
    "use_cases": [
      "Analyzing consumer product choice behavior from survey data",
      "Modeling transportation mode selection for urban planning studies"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "fast multinomial logit estimation python",
      "XLogit vs other choice modeling packages",
      "mixed logit model implementation python",
      "discrete choice analysis tools performance"
    ]
  },
  {
    "name": "torch-choice",
    "description": "PyTorch framework for flexible estimation of complex discrete choice models, leveraging GPU acceleration.",
    "category": "Discrete Choice Models",
    "docs_url": "https://gsbdbi.github.io/torch-choice/",
    "github_url": "https://github.com/gsbDBI/torch-choice",
    "url": "https://github.com/gsbDBI/torch-choice",
    "install": "pip install torch-choice",
    "tags": [
      "discrete choice",
      "logit"
    ],
    "best_for": "Logit/probit models, consumer choice, demand estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pytorch",
      "logistic-regression",
      "maximum-likelihood-estimation"
    ],
    "topic_tags": [
      "discrete-choice",
      "pytorch",
      "gpu-acceleration",
      "econometrics",
      "python-package"
    ],
    "summary": "PyTorch-based framework that enables fast, GPU-accelerated estimation of discrete choice models like multinomial logit, nested logit, and mixed logit. Popular among researchers and data scientists who need to model consumer choices, product adoption, or other categorical decision-making processes. Combines the flexibility of PyTorch with specialized tools for choice modeling.",
    "use_cases": [
      "modeling customer product selection behavior for e-commerce recommendation systems",
      "analyzing transportation mode choice for urban planning and policy evaluation"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "pytorch discrete choice modeling package",
      "GPU accelerated logit model estimation python",
      "torch-choice vs sklearn for choice models",
      "discrete choice modeling with neural networks pytorch"
    ]
  },
  {
    "name": "DoubleML",
    "description": "Implements the double/debiased ML framework (Chernozhukov et al.) for estimating causal parameters (ATE, LATE, POM) with ML nuisances.",
    "category": "Double/Debiased Machine Learning (DML)",
    "docs_url": "https://docs.doubleml.org/",
    "github_url": "https://github.com/DoubleML/doubleml-for-py",
    "url": "https://github.com/DoubleML/doubleml-for-py",
    "install": "pip install DoubleML",
    "tags": [
      "machine learning",
      "causal inference"
    ],
    "best_for": "High-dimensional controls, ML-based causal inference",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "scikit-learn",
      "causal-inference-fundamentals",
      "cross-validation"
    ],
    "topic_tags": [
      "double-machine-learning",
      "causal-inference",
      "treatment-effects",
      "python-package",
      "econometrics"
    ],
    "summary": "DoubleML is a Python package that implements the double/debiased machine learning framework for robust causal inference. It allows researchers to estimate treatment effects while using flexible machine learning models for nuisance parameters, avoiding bias from regularization. The package provides ready-to-use estimators for average treatment effects, local average treatment effects, and potential outcome means.",
    "use_cases": [
      "Estimating the causal effect of a product feature on user engagement while controlling for high-dimensional user characteristics",
      "Measuring the impact of a policy intervention using observational data with many confounding variables"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python package for double machine learning causal inference",
      "how to implement debiased ML for treatment effects",
      "DoubleML vs traditional causal inference methods",
      "estimate ATE with machine learning controls python"
    ]
  },
  {
    "name": "Doubly-Debiased-Lasso",
    "description": "High-dimensional inference under hidden confounding. Doubly debiased Lasso for valid inference.",
    "category": "Double/Debiased Machine Learning (DML)",
    "docs_url": "https://github.com/zijguo/Doubly-Debiased-Lasso",
    "github_url": "https://github.com/zijguo/Doubly-Debiased-Lasso",
    "url": "https://github.com/zijguo/Doubly-Debiased-Lasso",
    "install": "Install from GitHub",
    "tags": [
      "high-dimensional",
      "Lasso",
      "debiased"
    ],
    "best_for": "High-dim inference with confounding",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "lasso-regression",
      "causal-inference",
      "high-dimensional-statistics"
    ],
    "topic_tags": [
      "doubly-debiased",
      "lasso-regularization",
      "confounding-adjustment",
      "high-dimensional-inference",
      "causal-estimation"
    ],
    "summary": "Advanced method for conducting valid statistical inference in high-dimensional settings where traditional Lasso estimates are biased due to regularization and hidden confounding. Combines debiasing techniques to correct for both selection bias from Lasso penalization and confounding bias from unobserved variables. Enables researchers to obtain valid confidence intervals and p-values for treatment effects in complex observational data.",
    "use_cases": [
      "Estimating treatment effects in genomics studies with thousands of genetic markers and potential unmeasured confounders",
      "Analyzing digital marketing campaigns with high-dimensional user features where selection bias and confounding threaten causal conclusions"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "how to do causal inference with high dimensional data and hidden confounders",
      "doubly debiased lasso for treatment effect estimation",
      "valid inference lasso high dimensional confounding",
      "debiased machine learning hidden confounders package"
    ]
  },
  {
    "name": "EconML",
    "description": "Microsoft toolkit for estimating heterogeneous treatment effects using DML, causal forests, meta-learners, and orthogonal ML methods.",
    "category": "Double/Debiased Machine Learning (DML)",
    "docs_url": "https://econml.azurewebsites.net/",
    "github_url": "https://github.com/py-why/EconML",
    "url": "https://github.com/py-why/EconML",
    "install": "pip install econml",
    "tags": [
      "machine learning",
      "causal inference"
    ],
    "best_for": "High-dimensional controls, ML-based causal inference",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-scikit-learn",
      "linear-regression",
      "treatment-effect-estimation"
    ],
    "topic_tags": [
      "heterogeneous-treatment-effects",
      "causal-machine-learning",
      "python-package",
      "microsoft-research",
      "orthogonal-learning"
    ],
    "summary": "EconML is Microsoft's Python library for estimating heterogeneous treatment effects using advanced causal ML methods. It combines double/debiased machine learning, causal forests, and meta-learners to identify how treatment effects vary across different subgroups. The toolkit is designed for practitioners who need to go beyond average treatment effects to understand personalized causal impacts.",
    "use_cases": [
      "Estimating how price discounts affect different customer segments in e-commerce",
      "Measuring heterogeneous effects of job training programs across demographic groups"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python package for heterogeneous treatment effects",
      "EconML vs causal forests comparison",
      "how to estimate personalized treatment effects",
      "Microsoft causal inference toolkit"
    ]
  },
  {
    "name": "SynapseML",
    "description": "Microsoft's distributed ML library with native Double ML (DoubleMLEstimator) for heterogeneous treatment effects at scale.",
    "category": "Double/Debiased Machine Learning (DML)",
    "docs_url": "https://microsoft.github.io/SynapseML/",
    "github_url": "https://github.com/microsoft/SynapseML",
    "url": "https://github.com/microsoft/SynapseML",
    "install": "pip install synapseml",
    "tags": [
      "spark",
      "causal inference",
      "double ML",
      "distributed"
    ],
    "best_for": "Causal inference at 100M+ rows on Spark clusters",
    "language": "Spark",
    "difficulty": "intermediate",
    "prerequisites": [
      "apache-spark",
      "causal-inference-basics",
      "python-scikit-learn"
    ],
    "topic_tags": [
      "double-ml",
      "causal-inference",
      "distributed-computing",
      "treatment-effects",
      "spark"
    ],
    "summary": "SynapseML is Microsoft's distributed machine learning library that extends Spark with native support for Double ML estimation through DoubleMLEstimator. It enables scalable causal inference and heterogeneous treatment effect estimation on large datasets. The library is particularly valuable for tech economists who need to run causal analysis on big data infrastructure.",
    "use_cases": [
      "Estimating personalized treatment effects from A/B tests across millions of users in a streaming platform",
      "Measuring heterogeneous impacts of policy interventions using large-scale administrative datasets"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "distributed double ML implementation",
      "SynapseML causal inference tutorial",
      "scalable treatment effect estimation Spark",
      "Microsoft double ML library"
    ]
  },
  {
    "name": "pydoublelasso",
    "description": "Double\u2011post\u00a0Lasso estimator for high\u2011dimensional treatment effects (Belloni\u2011Chernozhukov\u2011Hansen\u202f2014).",
    "category": "Double/Debiased Machine Learning (DML)",
    "docs_url": "https://pypi.org/project/pydoublelasso/",
    "github_url": null,
    "url": "https://pypi.org/project/pydoublelasso/",
    "install": "pip install pydoublelasso",
    "tags": [
      "machine learning",
      "causal inference"
    ],
    "best_for": "High-dimensional controls, ML-based causal inference",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-scikit-learn",
      "lasso-regression",
      "causal-inference-fundamentals"
    ],
    "topic_tags": [
      "double-lasso",
      "high-dimensional-causal-inference",
      "treatment-effects",
      "regularization",
      "python-package"
    ],
    "summary": "PyDoubleLasso implements the Belloni-Chernozhukov-Hansen double-post LASSO method for estimating treatment effects when you have many control variables. It uses LASSO for variable selection in both treatment and outcome models, then applies post-LASSO estimation to get unbiased treatment effect estimates. Essential for causal inference in high-dimensional settings where traditional methods break down.",
    "use_cases": [
      "Estimating impact of marketing campaigns when you have hundreds of customer features and need to control for confounders",
      "Measuring effect of policy interventions using administrative data with many demographic and economic control variables"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "double lasso treatment effects python",
      "high dimensional causal inference package",
      "Belloni Chernozhukov Hansen implementation",
      "LASSO causal inference many controls"
    ]
  },
  {
    "name": "pyhtelasso",
    "description": "Debiased\u2011Lasso detector of heterogeneous treatment effects in randomized experiments.",
    "category": "Double/Debiased Machine Learning (DML)",
    "docs_url": "https://pypi.org/project/pyhtelasso/",
    "github_url": null,
    "url": "https://pypi.org/project/pyhtelasso/",
    "install": "pip install pyhtelasso",
    "tags": [
      "machine learning",
      "causal inference"
    ],
    "best_for": "High-dimensional controls, ML-based causal inference",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "python-scikit-learn",
      "causal-inference-fundamentals",
      "randomized-controlled-trials"
    ],
    "topic_tags": [
      "heterogeneous-treatment-effects",
      "debiased-lasso",
      "double-machine-learning",
      "causal-inference",
      "python-package"
    ],
    "summary": "A Python package implementing debiased Lasso methods to detect and estimate heterogeneous treatment effects in randomized experiments. It combines machine learning regularization techniques with causal inference to identify which subgroups respond differently to treatments. Particularly useful for researchers and data scientists working on personalized interventions or understanding treatment effect variation.",
    "use_cases": [
      "Analyzing A/B test results to identify which user segments respond differently to a new product feature",
      "Evaluating clinical trial data to determine which patient characteristics predict stronger treatment responses"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "debiased lasso heterogeneous treatment effects python",
      "detect subgroup effects randomized experiments",
      "pyhtelasso package tutorial",
      "double machine learning treatment heterogeneity"
    ]
  },
  {
    "name": "DeclareDesign",
    "description": "Ex ante experimental design declaration and diagnosis. Enables researchers to formally describe their research design, diagnose statistical properties via simulation, and improve designs before data collection.",
    "category": "Experimental Design",
    "docs_url": "https://declaredesign.org/",
    "github_url": "https://github.com/DeclareDesign/DeclareDesign",
    "url": "https://cran.r-project.org/package=DeclareDesign",
    "install": "install.packages(\"DeclareDesign\")",
    "tags": [
      "experimental-design",
      "pre-registration",
      "power-analysis",
      "simulation",
      "design-diagnosis"
    ],
    "best_for": "Ex ante experimental design declaration and diagnosis via simulation",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "randomized-experiments",
      "statistical-inference",
      "R-programming"
    ],
    "topic_tags": [
      "experimental-design",
      "power-analysis",
      "causal-inference",
      "simulation",
      "pre-registration"
    ],
    "summary": "DeclareDesign is an R package that helps researchers formally specify their experimental designs before collecting data. It enables simulation-based diagnosis of design properties like power, bias, and coverage to optimize study parameters. The package promotes transparent research by encouraging explicit declaration of assumptions and analysis plans.",
    "use_cases": [
      "A/B testing team wants to determine optimal sample sizes and randomization strategy before launching a new feature experiment",
      "Academic researcher needs to pre-register their field experiment design and demonstrate adequate statistical power for grant applications"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "how to calculate power analysis for A/B tests before running experiment",
      "tools for pre-registering experimental design and analysis plan",
      "simulate experimental outcomes to optimize study design",
      "R package for experimental design declaration and diagnosis"
    ]
  },
  {
    "name": "contextual",
    "description": "Multi-armed bandit algorithms including Thompson Sampling, UCB, and LinUCB. Directly applicable to adaptive A/B testing and recommendation optimization with simulation and evaluation tools.",
    "category": "Experimental Design",
    "docs_url": "https://nth-iteration-labs.github.io/contextual/",
    "github_url": "https://github.com/Nth-iteration-labs/contextual",
    "url": "https://cran.r-project.org/package=contextual",
    "install": "install.packages(\"contextual\")",
    "tags": [
      "bandits",
      "Thompson-sampling",
      "UCB",
      "adaptive-experiments",
      "A/B-testing"
    ],
    "best_for": "Multi-armed bandits for adaptive A/B testing with Thompson Sampling, UCB, LinUCB",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-numpy",
      "probability-distributions",
      "A/B-testing"
    ],
    "topic_tags": [
      "multi-armed-bandits",
      "adaptive-experimentation",
      "recommendation-systems",
      "online-optimization"
    ],
    "summary": "The contextual package implements multi-armed bandit algorithms for adaptive experimentation, allowing dynamic allocation of traffic to better-performing variants during A/B tests. It's essential for data scientists running experiments where you want to minimize regret while learning optimal strategies. Includes simulation tools to evaluate bandit performance before deployment.",
    "use_cases": [
      "Dynamically allocating website traffic between different homepage designs during an A/B test",
      "Optimizing personalized product recommendations by learning which recommendation algorithms work best for different user segments"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "multi armed bandit python package",
      "adaptive A/B testing Thompson sampling",
      "contextual bandits recommendation system",
      "bandit algorithms experimentation tools"
    ]
  },
  {
    "name": "fabricatr",
    "description": "Simulates realistic social science data for power analysis and design testing. Creates hierarchical data structures with correlated variables matching real-world patterns.",
    "category": "Experimental Design",
    "docs_url": "https://declaredesign.org/r/fabricatr/",
    "github_url": "https://github.com/DeclareDesign/fabricatr",
    "url": "https://cran.r-project.org/package=fabricatr",
    "install": "install.packages(\"fabricatr\")",
    "tags": [
      "data-simulation",
      "power-analysis",
      "hierarchical-data",
      "synthetic-data",
      "design-testing"
    ],
    "best_for": "Simulating realistic hierarchical data for experimental power analysis and design testing",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "R-programming",
      "experimental-design",
      "statistical-power"
    ],
    "topic_tags": [
      "data-simulation",
      "power-analysis",
      "experimental-design",
      "hierarchical-data",
      "A/B-testing"
    ],
    "summary": "fabricatr is an R package that generates realistic synthetic datasets with complex hierarchical structures and correlated variables. It's designed for researchers who need to test experimental designs and conduct power analyses before collecting real data. The package specializes in creating social science-style data that mimics real-world patterns and relationships.",
    "use_cases": [
      "Planning sample sizes for a multi-level A/B test across different user segments and geographic regions",
      "Testing whether your analysis pipeline can detect treatment effects in clustered randomized trials before launching"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "how to simulate hierarchical data for power analysis",
      "R package for creating synthetic experimental data",
      "fabricatr tutorial for A/B test simulation",
      "generate correlated variables for experimental design testing"
    ]
  },
  {
    "name": "randomizr",
    "description": "Proper randomization procedures for experiments with known assignment probabilities. Implements simple, complete, block, and cluster randomization with exact probability calculations for IPW estimation.",
    "category": "Experimental Design",
    "docs_url": "https://declaredesign.org/r/randomizr/",
    "github_url": "https://github.com/DeclareDesign/randomizr",
    "url": "https://cran.r-project.org/package=randomizr",
    "install": "install.packages(\"randomizr\")",
    "tags": [
      "randomization",
      "block-randomization",
      "cluster-randomization",
      "assignment-probability",
      "experiments"
    ],
    "best_for": "Proper experimental randomization with exact assignment probabilities for IPW",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-probability",
      "experimental-design-concepts",
      "R-programming"
    ],
    "topic_tags": [
      "randomization",
      "experimental-design",
      "causal-inference",
      "IPW-estimation",
      "R-package"
    ],
    "summary": "randomizr is an R package that implements proper randomization procedures for experiments, ensuring known assignment probabilities for valid statistical inference. It supports simple, complete, block, and cluster randomization designs with exact probability calculations. The package is essential for researchers who need to implement rigorous randomization schemes and calculate inverse probability weights for unbiased treatment effect estimation.",
    "use_cases": [
      "A/B testing platform needs to implement block randomization to ensure balanced treatment assignment across user segments",
      "Field experiment researcher running a clustered randomized trial in villages needs exact assignment probabilities for IPW estimation"
    ],
    "audience": [
      "Junior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "how to implement block randomization in R",
      "randomization package with assignment probabilities",
      "cluster randomization R package",
      "IPW estimation randomization probabilities"
    ]
  },
  {
    "name": "Nashpy",
    "description": "Computation of Nash equilibria for 2-player games. Support enumeration and Lemke-Howson algorithm.",
    "category": "Game Theory & Mechanism Design",
    "docs_url": "https://nashpy.readthedocs.io/",
    "github_url": "https://github.com/drvinceknight/Nashpy",
    "url": "https://github.com/drvinceknight/Nashpy",
    "install": "pip install nashpy",
    "tags": [
      "game theory",
      "Nash equilibrium"
    ],
    "best_for": "2-player Nash equilibrium computation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-numpy",
      "linear-algebra",
      "basic-optimization"
    ],
    "topic_tags": [
      "nash-equilibrium",
      "two-player-games",
      "algorithmic-game-theory",
      "python-package"
    ],
    "summary": "Nashpy is a Python library for computing Nash equilibria in 2-player games using enumeration and the Lemke-Howson algorithm. It's particularly useful for researchers and practitioners working on strategic interactions, auction design, and competitive analysis. The package provides clean implementations of classic game theory algorithms with a simple API.",
    "use_cases": [
      "Analyzing bidding strategies in two-sided marketplace auctions",
      "Modeling competitive pricing between two firms in market entry scenarios"
    ],
    "audience": [
      "Early-PhD",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "Python package for Nash equilibrium computation",
      "How to compute Nash equilibria in two player games",
      "Lemke Howson algorithm implementation",
      "Game theory Nash equilibrium solver"
    ]
  },
  {
    "name": "OpenSpiel",
    "description": "DeepMind's 70+ game environments with multi-agent RL algorithms including Alpha-Rank, Neural Fictitious Self-Play, and CFR variants.",
    "category": "Game Theory & Mechanism Design",
    "docs_url": "https://openspiel.readthedocs.io/",
    "github_url": "https://github.com/deepmind/open_spiel",
    "url": "https://github.com/deepmind/open_spiel",
    "install": "pip install open_spiel",
    "tags": [
      "game theory",
      "reinforcement learning",
      "multi-agent"
    ],
    "best_for": "Multi-agent RL and game-theoretic algorithms",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-programming",
      "reinforcement-learning-basics",
      "nash-equilibrium"
    ],
    "topic_tags": [
      "multi-agent-rl",
      "game-theory",
      "algorithmic-game-theory",
      "neural-fictitious-self-play",
      "counterfactual-regret-minimization"
    ],
    "summary": "OpenSpiel is DeepMind's comprehensive framework providing 70+ game environments for multi-agent reinforcement learning research. It includes implementations of advanced algorithms like Alpha-Rank, Neural Fictitious Self-Play, and CFR variants for studying strategic interactions. Researchers and practitioners use it to experiment with game-theoretic scenarios and develop multi-agent AI systems.",
    "use_cases": [
      "Training AI agents to compete in poker or other strategic games using counterfactual regret minimization",
      "Analyzing market competition dynamics by modeling firms as learning agents in auction environments"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "multi-agent reinforcement learning framework with game environments",
      "OpenSpiel tutorial for game theory research",
      "how to implement neural fictitious self-play in strategic games",
      "deepmind game theory package with CFR algorithms"
    ]
  },
  {
    "name": "fairpy",
    "description": "Fair division algorithms from academic papers. Implements cake-cutting and item allocation procedures.",
    "category": "Game Theory & Mechanism Design",
    "docs_url": "https://fairpy.readthedocs.io/",
    "github_url": "https://github.com/erelsgl/fairpy",
    "url": "https://github.com/erelsgl/fairpy",
    "install": "pip install fairpy",
    "tags": [
      "fair division",
      "allocation",
      "mechanism design"
    ],
    "best_for": "Fair division and cake-cutting algorithms",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-programming",
      "basic-game-theory",
      "algorithmic-thinking"
    ],
    "topic_tags": [
      "fair-division",
      "cake-cutting",
      "item-allocation",
      "mechanism-design",
      "python-package"
    ],
    "summary": "A Python package implementing academic fair division algorithms including cake-cutting and item allocation procedures. Provides ready-to-use implementations of classic and modern fairness mechanisms from game theory literature. Useful for researchers and practitioners needing to fairly allocate resources or analyze allocation mechanisms.",
    "use_cases": [
      "Dividing computational resources or budget fairly among team members or departments",
      "Research on auction mechanisms and fair allocation algorithms in economics or computer science"
    ],
    "audience": [
      "Early-PhD",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "fair division algorithms python implementation",
      "cake cutting algorithm library",
      "how to implement fair allocation mechanisms",
      "python package for mechanism design fairness"
    ]
  },
  {
    "name": "fairpyx",
    "description": "Course-seat allocation with capacity constraints. Practical fair division for university course assignment.",
    "category": "Game Theory & Mechanism Design",
    "docs_url": null,
    "github_url": "https://github.com/ariel-research/fairpyx",
    "url": "https://github.com/ariel-research/fairpyx",
    "install": "pip install fairpyx",
    "tags": [
      "fair division",
      "course allocation",
      "mechanism design"
    ],
    "best_for": "Course-seat allocation with constraints",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-programming",
      "linear-programming",
      "algorithmic-game-theory"
    ],
    "topic_tags": [
      "fair-division",
      "course-allocation",
      "mechanism-design",
      "capacity-constraints",
      "python-package"
    ],
    "summary": "Fairpyx is a Python package that implements fair division algorithms for allocating course seats with capacity constraints. It provides practical solutions for university course assignment problems using game theory and mechanism design principles. The package is particularly useful for educational institutions needing to fairly distribute limited course spots among students.",
    "use_cases": [
      "University registrar allocating popular course seats among students with different preferences",
      "Business school assigning MBA students to elective courses with enrollment caps"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "how to fairly allocate course seats with capacity limits",
      "python package for fair division course assignment",
      "mechanism design for university course allocation",
      "fairpyx tutorial for course seat assignment"
    ]
  },
  {
    "name": "pygambit",
    "description": "N-player extensive form games with Alan Turing Institute support. Computes Nash, perfect, and sequential equilibria.",
    "category": "Game Theory & Mechanism Design",
    "docs_url": "https://gambitproject.readthedocs.io/",
    "github_url": "https://github.com/gambitproject/gambit",
    "url": "https://github.com/gambitproject/gambit",
    "install": "pip install pygambit",
    "tags": [
      "game theory",
      "extensive form",
      "equilibrium"
    ],
    "best_for": "N-player extensive form game solving",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-basics",
      "nash-equilibrium",
      "game-theory-fundamentals"
    ],
    "topic_tags": [
      "game-theory",
      "extensive-form-games",
      "nash-equilibrium",
      "python-package",
      "mechanism-design"
    ],
    "summary": "Pygambit is a Python package for analyzing N-player extensive form games, with institutional support from the Alan Turing Institute. It provides computational tools for finding various equilibrium concepts including Nash, perfect, and sequential equilibria. Researchers and practitioners use it to model and solve complex strategic interactions with sequential decision-making.",
    "use_cases": [
      "Modeling auction mechanisms with multiple bidding rounds and computing optimal bidding strategies",
      "Analyzing multi-stage bargaining processes between firms in merger negotiations"
    ],
    "audience": [
      "Early-PhD",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "Python package for extensive form games",
      "How to compute sequential equilibrium in Python",
      "Nash equilibrium solver for multi-player games",
      "Game theory analysis tools Python"
    ]
  },
  {
    "name": "gamlss",
    "description": "Distributional regression where all parameters of a response distribution (location, scale, shape) can be modeled as functions of predictors, supporting 100+ distributions including highly skewed and kurtotic continuous and discrete distributions.",
    "category": "Generalized Additive Models",
    "docs_url": "https://www.gamlss.com/",
    "github_url": "https://github.com/gamlss-dev/gamlss",
    "url": "https://cran.r-project.org/package=gamlss",
    "install": "install.packages(\"gamlss\")",
    "tags": [
      "distributional-regression",
      "location-scale-shape",
      "flexible-distributions",
      "centile-estimation",
      "beyond-mean-modeling"
    ],
    "best_for": "Modeling non-normal responses where variance, skewness, or kurtosis depend on predictors, implementing Rigby & Stasinopoulos (2005)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "generalized-linear-models",
      "R-programming",
      "maximum-likelihood-estimation"
    ],
    "topic_tags": [
      "distributional-regression",
      "generalized-additive-models",
      "flexible-distributions",
      "centile-estimation",
      "R-package"
    ],
    "summary": "GAMLSS extends traditional regression by modeling all parameters of a response distribution (location, scale, shape) as functions of predictors, not just the mean. It supports over 100 distributions including highly skewed and heavy-tailed distributions, making it ideal for complex real-world data that violates standard regression assumptions. The package is particularly valuable for centile estimation and analyzing heteroskedastic data with non-normal distributions.",
    "use_cases": [
      "modeling-income-distributions-with-varying-skewness-across-demographics",
      "analyzing-medical-reference-curves-where-variance-changes-with-age"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "how to model both mean and variance in regression",
      "R package for non-normal response distributions",
      "distributional regression beyond GLMs",
      "modeling skewed data with heteroskedasticity"
    ]
  },
  {
    "name": "mgcv",
    "description": "The definitive GAM implementation providing generalized additive (mixed) models with automatic smoothness estimation via REML/GCV/ML, supporting thin plate splines, tensor products, multiple distributions, and scalable fitting for large datasets.",
    "category": "Generalized Additive Models",
    "docs_url": "https://cran.r-project.org/web/packages/mgcv/mgcv.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=mgcv",
    "install": "install.packages(\"mgcv\")",
    "tags": [
      "GAM",
      "splines",
      "smoothing",
      "penalized-regression",
      "mixed-models"
    ],
    "best_for": "Flexible nonparametric regression with automatic smoothing parameter selection, implementing Wood (2017)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "linear-regression",
      "R-programming",
      "smoothing-splines"
    ],
    "topic_tags": [
      "generalized-additive-models",
      "nonparametric-regression",
      "spline-smoothing",
      "mixed-effects",
      "R-package"
    ],
    "summary": "mgcv is R's premier package for fitting Generalized Additive Models (GAMs), enabling flexible nonparametric regression with automatic smoothness selection. It's widely used by data scientists and researchers who need to model complex nonlinear relationships while maintaining interpretability. The package handles everything from simple smooth curves to complex mixed effects models with multiple smoothing terms.",
    "use_cases": [
      "Modeling nonlinear relationships between customer engagement metrics and time/seasonality",
      "Analyzing dose-response curves in A/B tests where treatment effects vary smoothly across user segments"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for nonlinear regression with smooth curves",
      "how to fit GAM models in R with automatic smoothness",
      "mgcv vs other GAM implementations",
      "flexible regression for nonlinear relationships R"
    ]
  },
  {
    "name": "GeoLift",
    "description": "Meta's end-to-end synthetic control for geo experiments with multi-cell testing and power calculations.",
    "category": "Geo-Experiments & Lift Measurement",
    "docs_url": "https://facebookincubator.github.io/GeoLift/",
    "github_url": "https://github.com/facebookincubator/GeoLift",
    "url": "https://github.com/facebookincubator/GeoLift",
    "install": "pip install geolift",
    "tags": [
      "geo-experiments",
      "synthetic control",
      "incrementality"
    ],
    "best_for": "Meta's geo-level incrementality measurement",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "causal-inference",
      "python-programming",
      "a-b-testing"
    ],
    "topic_tags": [
      "geo-experiments",
      "synthetic-control",
      "causal-inference",
      "incrementality-testing",
      "python-package"
    ],
    "summary": "GeoLift is Meta's open-source Python package for running geo-level experiments using synthetic control methods. It enables data scientists to measure incrementality of marketing campaigns or product features across geographic regions with proper statistical power calculations. The package handles multi-cell testing scenarios and provides end-to-end workflow from experiment design to results analysis.",
    "use_cases": [
      "Measuring incremental lift from advertising campaigns across different cities or regions",
      "Testing impact of new product features rolled out to specific geographic markets"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "How to run geo experiments with synthetic control",
      "Meta GeoLift package for incrementality testing",
      "Geographic experiment design and analysis tools",
      "Synthetic control method for marketing lift measurement"
    ]
  },
  {
    "name": "matched_markets",
    "description": "Google's time-based regression with greedy search for optimal geo experiment groups.",
    "category": "Geo-Experiments & Lift Measurement",
    "docs_url": null,
    "github_url": "https://github.com/google/matched_markets",
    "url": "https://github.com/google/matched_markets",
    "install": "pip install matched-markets",
    "tags": [
      "geo-experiments",
      "market matching",
      "incrementality"
    ],
    "best_for": "Optimal geo experiment group selection",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "regression-analysis",
      "experimental-design",
      "python-pandas"
    ],
    "topic_tags": [
      "geo-experiments",
      "market-matching",
      "causal-inference",
      "google-package",
      "incrementality-testing"
    ],
    "summary": "Google's matched_markets package implements time-based regression with greedy search algorithms to find optimal control and treatment groups for geo experiments. It's designed for data scientists running incrementality tests who need to match geographic markets based on historical performance. The package automates the complex process of selecting statistically similar market pairs for reliable causal inference.",
    "use_cases": [
      "Testing impact of marketing campaigns across different cities by matching similar geographic markets",
      "Measuring incremental lift from product feature rollouts by selecting comparable regions as controls"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "how to match geographic markets for experiments",
      "Google geo experiment matching package",
      "time-based regression for market selection",
      "matched markets package tutorial"
    ]
  },
  {
    "name": "trimmed_match",
    "description": "Google's robust analysis for paired geo experiments using trimmed statistics. Handles outliers in geo-level data.",
    "category": "Geo-Experiments & Lift Measurement",
    "docs_url": null,
    "github_url": "https://github.com/google/trimmed_match",
    "url": "https://github.com/google/trimmed_match",
    "install": "pip install trimmed-match",
    "tags": [
      "geo-experiments",
      "robust statistics",
      "incrementality"
    ],
    "best_for": "Robust paired geo experiment analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "a-b-testing",
      "python-pandas",
      "outlier-detection"
    ],
    "topic_tags": [
      "geo-experiments",
      "robust-statistics",
      "incrementality",
      "trimmed-estimators",
      "paired-design"
    ],
    "summary": "Google's trimmed_match package provides robust statistical methods for analyzing paired geo experiments by using trimmed statistics to handle outliers in geographic data. It's designed for measuring incrementality in scenarios where traditional A/B testing isn't feasible due to network effects or spillovers. The package helps data scientists get reliable causal estimates even when geo-level metrics have extreme values.",
    "use_cases": [
      "Measuring the incremental impact of a new ad campaign across different cities while accounting for outlier markets",
      "Evaluating the effectiveness of a product feature rollout across geographic regions with varying baseline performance"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "how to handle outliers in geo experiments",
      "robust statistics for geographic A/B testing",
      "Google trimmed match package tutorial",
      "paired geo experiment analysis with outliers"
    ]
  },
  {
    "name": "Awesome Economics",
    "description": "Curated list of economics resources including datasets, software, courses, and blogs.",
    "category": "Inference & Reporting Tools",
    "docs_url": null,
    "github_url": "https://github.com/antontarasenko/awesome-economics",
    "url": "https://github.com/antontarasenko/awesome-economics",
    "install": "",
    "tags": [
      "curated list",
      "resources"
    ],
    "best_for": "Comprehensive economics resource directory",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-economics",
      "research-methodology"
    ],
    "topic_tags": [
      "curated-resources",
      "economics-datasets",
      "learning-materials",
      "research-tools"
    ],
    "summary": "A comprehensive curated collection of economics resources including datasets, software packages, online courses, and academic blogs. This repository serves as a starting point for economists and data scientists looking to explore economic research tools and methodologies. It provides organized access to foundational and specialized resources across various economics subfields.",
    "use_cases": [
      "Starting economics research and need to find relevant datasets and analytical tools",
      "Teaching an economics course and looking for comprehensive learning materials and software recommendations"
    ],
    "audience": [
      "Early-PhD",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "curated list of economics datasets and tools",
      "best resources for learning economics research methods",
      "comprehensive economics software and data sources",
      "where to find economics datasets and research tools"
    ]
  },
  {
    "name": "Computational Methods for practitioners",
    "description": "Open-source textbook by Richard Evans on computational methods for researchers using Python.",
    "category": "Inference & Reporting Tools",
    "docs_url": "https://opensourceecon.github.io/CompMethods/",
    "github_url": "https://github.com/OpenSourceEcon/CompMethods",
    "url": "https://opensourceecon.github.io/CompMethods/",
    "install": "",
    "tags": [
      "education",
      "computation",
      "textbook"
    ],
    "best_for": "Comprehensive computational economics course",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-basics",
      "linear-algebra",
      "calculus"
    ],
    "topic_tags": [
      "computational-methods",
      "python-programming",
      "textbook",
      "numerical-analysis",
      "open-source"
    ],
    "summary": "An open-source textbook by Richard Evans teaching computational methods for researchers using Python. It covers fundamental numerical techniques and programming approaches essential for quantitative research. The book provides practical implementation guidance for computational problems commonly encountered in economics and data science.",
    "use_cases": [
      "Learning how to implement optimization algorithms for economic modeling",
      "Building computational skills for dissertation research involving numerical methods"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "computational methods textbook python",
      "learn numerical methods for economics research",
      "open source book computational techniques",
      "python programming for quantitative researchers"
    ]
  },
  {
    "name": "Econ Project Templates",
    "description": "Cookiecutter templates for reproducible economics research projects. Standardized project structure.",
    "category": "Inference & Reporting Tools",
    "docs_url": "https://econ-project-templates.readthedocs.io/",
    "github_url": "https://github.com/OpenSourceEconomics/econ-project-templates",
    "url": "https://github.com/OpenSourceEconomics/econ-project-templates",
    "install": "",
    "tags": [
      "reproducibility",
      "templates",
      "workflow"
    ],
    "best_for": "Starting reproducible economics projects",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "command-line-basics",
      "git-version-control"
    ],
    "topic_tags": [
      "project-structure",
      "reproducible-research",
      "cookiecutter",
      "research-workflow",
      "standardization"
    ],
    "summary": "Cookiecutter templates that provide standardized folder structures and configuration files for economics research projects. These templates help researchers set up consistent, reproducible project environments with predefined directories for data, code, output, and documentation. They're particularly valuable for ensuring research reproducibility and streamlining collaboration across team members.",
    "use_cases": [
      "Starting a new empirical economics paper and needing organized folders for raw data, cleaned data, analysis scripts, and results",
      "Setting up a research project with multiple collaborators who need consistent file organization and naming conventions"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "How to organize economics research project files",
      "Cookiecutter templates for reproducible research",
      "Standard folder structure for empirical economics projects",
      "Project templates for economics data analysis"
    ]
  },
  {
    "name": "First Course in Causal Inference (Python)",
    "description": "Python implementation of Peng Ding's textbook 'A First Course in Causal Inference'. Educational resource with code examples.",
    "category": "Inference & Reporting Tools",
    "docs_url": "https://github.com/apoorvalal/ding_causalInference_python",
    "github_url": "https://github.com/apoorvalal/ding_causalInference_python",
    "url": "https://github.com/apoorvalal/ding_causalInference_python",
    "install": "",
    "tags": [
      "causal inference",
      "education",
      "textbook"
    ],
    "best_for": "Learning causal inference with Peng Ding's textbook",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-basics",
      "linear-regression",
      "randomized-experiments"
    ],
    "topic_tags": [
      "causal-inference",
      "educational-materials",
      "python-implementation",
      "textbook-companion",
      "daggers"
    ],
    "summary": "Python code companion to Peng Ding's causal inference textbook, providing hands-on implementation of key concepts like DAGs, potential outcomes, and identification strategies. Perfect for students and practitioners learning causal inference fundamentals through practical coding exercises. Bridges theory from the textbook with executable Python examples.",
    "use_cases": [
      "PhD student working through Ding's textbook wants to implement exercises in Python",
      "Data scientist transitioning from correlation to causation needs structured learning path with code"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "Python code for Peng Ding causal inference textbook",
      "how to learn causal inference with Python examples",
      "first course causal inference implementation",
      "beginner causal inference Python tutorial"
    ]
  },
  {
    "name": "Python Packages for Applied Economists",
    "description": "Curated collection of Python packages for applied researchers organized by functionality.",
    "category": "Inference & Reporting Tools",
    "docs_url": null,
    "github_url": "https://github.com/clibassi/python-packages-for-applied-economists",
    "url": "https://github.com/clibassi/python-packages-for-applied-economists",
    "install": "",
    "tags": [
      "curated list",
      "resources"
    ],
    "best_for": "Discovering econometrics packages by use case",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-pandas",
      "python-basics",
      "pip-package-management"
    ],
    "topic_tags": [
      "python-packages",
      "applied-economics",
      "econometrics",
      "package-discovery",
      "research-tools"
    ],
    "summary": "A curated collection of Python packages specifically selected for applied economists and researchers. This resource organizes packages by functionality to help researchers quickly find the right tools for their economic analysis needs. It serves as a comprehensive starting point for building a Python toolkit for empirical economic research.",
    "use_cases": [
      "Setting up Python environment for new economics research project",
      "Finding specialized packages for causal inference or econometric modeling"
    ],
    "audience": [
      "Junior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "best Python packages for economists",
      "Python tools for applied economics research",
      "econometrics packages Python curated list",
      "what Python libraries do economists use"
    ]
  },
  {
    "name": "clusterbootstraps",
    "description": "Wild cluster bootstrap and pairs cluster bootstrap implementations for clustered standard errors.",
    "category": "Inference & Reporting Tools",
    "docs_url": null,
    "github_url": "https://github.com/BingkunLin/clusterbootstraps",
    "url": "https://pypi.org/project/clusterbootstraps/",
    "install": "pip install clusterbootstraps",
    "tags": [
      "bootstrap",
      "clustered errors",
      "inference"
    ],
    "best_for": "Alternative cluster bootstrap implementations",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "regression-analysis",
      "clustered-standard-errors",
      "python-statsmodels"
    ],
    "topic_tags": [
      "bootstrap-methods",
      "clustered-standard-errors",
      "statistical-inference",
      "econometrics-tools"
    ],
    "summary": "Implementation of wild cluster bootstrap and pairs cluster bootstrap methods for computing robust standard errors when data has clustered structure. These bootstrap techniques provide alternatives to analytical clustered standard errors, especially useful when clusters are few or unbalanced. Commonly used in econometrics and causal inference when traditional asymptotic assumptions may not hold.",
    "use_cases": [
      "A/B testing with few treatment clusters where analytical standard errors may be unreliable",
      "Difference-in-differences analysis with small number of treated units requiring robust inference"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "wild cluster bootstrap python implementation",
      "bootstrap clustered standard errors few clusters",
      "pairs cluster bootstrap vs wild bootstrap",
      "robust inference small number clusters"
    ]
  },
  {
    "name": "maketables",
    "description": "Publication-ready regression tables for pyfixest, statsmodels, linearmodels. Outputs HTML (great-tables), LaTeX, Word.",
    "category": "Inference & Reporting Tools",
    "docs_url": null,
    "github_url": "https://github.com/py-econometrics/maketables",
    "url": "https://github.com/py-econometrics/maketables",
    "install": "pip install maketables",
    "tags": [
      "reporting",
      "tables",
      "visualization"
    ],
    "best_for": "Multi-format regression tables from pyfixest/statsmodels",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-pandas",
      "regression-analysis",
      "statsmodels-or-pyfixest"
    ],
    "topic_tags": [
      "regression-tables",
      "publication-ready",
      "latex-output",
      "html-tables",
      "statistical-reporting"
    ],
    "summary": "A Python package that creates publication-ready regression tables from popular econometrics libraries like pyfixest, statsmodels, and linearmodels. It outputs professional-looking tables in HTML, LaTeX, and Word formats, making it easy to include results in papers, reports, and presentations. Essential for anyone who needs to present regression results in a clean, standardized format.",
    "use_cases": [
      "Creating regression tables for academic papers that need LaTeX formatting",
      "Generating HTML tables for internal reports and stakeholder presentations"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "how to make publication ready regression tables in python",
      "convert pyfixest results to latex table",
      "python package for formatting regression output",
      "create professional regression tables from statsmodels"
    ]
  },
  {
    "name": "ShiftShareSE",
    "description": "Implements correct standard errors for Bartik/shift-share instrumental variables designs following Ad\u00e3o, Koles\u00e1r, and Morales (2019 QJE). Standard clustered SEs are typically incorrect for shift-share\u2014this package provides econometrically valid inference.",
    "category": "Instrumental Variables",
    "docs_url": "https://cran.r-project.org/web/packages/ShiftShareSE/ShiftShareSE.pdf",
    "github_url": "https://github.com/kolesarm/ShiftShareSE",
    "url": "https://cran.r-project.org/package=ShiftShareSE",
    "install": "install.packages(\"ShiftShareSE\")",
    "tags": [
      "shift-share",
      "Bartik",
      "instrumental-variables",
      "standard-errors",
      "regional-economics"
    ],
    "best_for": "Correct standard errors for Bartik/shift-share IV designs, implementing Ad\u00e3o, Koles\u00e1r & Morales (2019 QJE)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "instrumental-variables",
      "regression-clustering",
      "econometric-inference"
    ],
    "topic_tags": [
      "shift-share",
      "Bartik-instruments",
      "standard-errors",
      "regional-economics",
      "causal-inference"
    ],
    "summary": "ShiftShareSE implements econometrically correct standard errors for Bartik/shift-share instrumental variable designs, addressing the problem that standard clustered standard errors are typically invalid in these settings. The package follows the methodology from Ad\u00e3o, Koles\u00e1r, and Morales (2019 QJE) to provide valid statistical inference for shift-share research designs commonly used in regional economics and labor studies.",
    "use_cases": [
      "Estimating local labor market effects of trade shocks using industry composition as instruments",
      "Analyzing regional economic impacts of immigration using historical settlement patterns"
    ],
    "audience": [
      "Early-PhD",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "correct standard errors for Bartik instruments",
      "shift-share standard error correction",
      "Ad\u00e3o Koles\u00e1r Morales standard errors implementation",
      "how to fix clustered standard errors in shift-share designs"
    ]
  },
  {
    "name": "gmm",
    "description": "Generalized Method of Moments estimation implementing two-step GMM, iterated GMM, and continuous updated estimator (CUE) with HAC covariance matrices. Supports linear and nonlinear moment conditions.",
    "category": "Instrumental Variables",
    "docs_url": "https://cran.r-project.org/web/packages/gmm/gmm.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=gmm",
    "install": "install.packages(\"gmm\")",
    "tags": [
      "GMM",
      "method-of-moments",
      "HAC",
      "instrumental-variables",
      "CUE"
    ],
    "best_for": "Generalized Method of Moments estimation with two-step, iterated, and CUE estimators",
    "language": "R",
    "difficulty": "advanced",
    "prerequisites": [
      "instrumental-variables",
      "maximum-likelihood-estimation",
      "econometrics-theory"
    ],
    "topic_tags": [
      "generalized-method-of-moments",
      "instrumental-variables",
      "econometric-estimation",
      "endogeneity",
      "two-stage-least-squares"
    ],
    "summary": "GMM is a flexible econometric estimation framework that finds parameters by matching sample moments to theoretical moments from an economic model. It's particularly valuable when you have more moment conditions than parameters, allowing for overidentification tests and efficient estimation in the presence of endogeneity.",
    "use_cases": [
      "Estimating demand elasticities when prices are endogenous due to unobserved quality shocks",
      "Analyzing dynamic panel data models where lagged dependent variables create correlation with error terms"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "GMM estimation with instrumental variables",
      "two step GMM vs continuous updating estimator",
      "how to handle endogeneity with method of moments",
      "GMM overidentification tests python"
    ]
  },
  {
    "name": "ivmodel",
    "description": "Specialized package for weak instrument diagnostics implementing Anderson-Rubin tests, k-class estimators (LIML, Fuller), and sensitivity analysis following Jiang et al. (2015). Essential when instrument strength is questionable.",
    "category": "Instrumental Variables",
    "docs_url": "https://cran.r-project.org/web/packages/ivmodel/ivmodel.pdf",
    "github_url": "https://github.com/hyunseungkang/ivmodel",
    "url": "https://cran.r-project.org/package=ivmodel",
    "install": "install.packages(\"ivmodel\")",
    "tags": [
      "instrumental-variables",
      "weak-instruments",
      "Anderson-Rubin",
      "LIML",
      "sensitivity-analysis"
    ],
    "best_for": "Weak instrument diagnostics with Anderson-Rubin tests and k-class estimators (LIML, Fuller)",
    "language": "R",
    "difficulty": "advanced",
    "prerequisites": [
      "two-stage-least-squares",
      "instrumental-variables",
      "econometric-theory"
    ],
    "topic_tags": [
      "weak-instruments",
      "Anderson-Rubin",
      "LIML",
      "causal-inference",
      "econometrics"
    ],
    "summary": "The ivmodel package provides specialized diagnostics and estimation methods for instrumental variable analysis when instruments may be weak. It implements Anderson-Rubin confidence intervals, k-class estimators like LIML and Fuller, and sensitivity analysis tools that remain valid even with poor instrument strength. Essential for researchers who need robust inference when standard IV assumptions are violated.",
    "use_cases": [
      "Testing impact of education on wages using quarter-of-birth instruments with uncertain strength",
      "Evaluating policy effects with geographic variation instruments that may have limited predictive power"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "weak instrument diagnostics R package",
      "Anderson Rubin test implementation",
      "LIML estimator when instruments are weak",
      "sensitivity analysis for instrumental variables"
    ]
  },
  {
    "name": "ivreg",
    "description": "Modern implementation of two-stage least squares (2SLS) instrumental variables regression with comprehensive diagnostics including hat values, studentized residuals, and component-plus-residual plots. Successor to AER's ivreg() function with superior diagnostic tools.",
    "category": "Instrumental Variables",
    "docs_url": "https://zeileis.github.io/ivreg/",
    "github_url": "https://github.com/zeileis/ivreg",
    "url": "https://cran.r-project.org/package=ivreg",
    "install": "install.packages(\"ivreg\")",
    "tags": [
      "instrumental-variables",
      "2SLS",
      "IV-regression",
      "endogeneity",
      "diagnostics"
    ],
    "best_for": "Modern 2SLS instrumental variables regression with comprehensive diagnostic tools",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "ordinary-least-squares",
      "linear-regression",
      "econometric-identification"
    ],
    "topic_tags": [
      "instrumental-variables",
      "causal-inference",
      "endogeneity",
      "econometrics",
      "R-package"
    ],
    "summary": "Modern R package for two-stage least squares instrumental variables regression with enhanced diagnostic capabilities. Designed for economists and data scientists dealing with endogeneity problems in causal analysis. Provides comprehensive residual analysis and model validation tools beyond standard IV implementations.",
    "use_cases": [
      "Estimating price elasticity when price is correlated with unobserved demand factors",
      "Analyzing treatment effects when assignment is influenced by unobservable characteristics"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "R package for instrumental variables regression with diagnostics",
      "how to run 2SLS with residual plots in R",
      "best IV regression package for endogeneity testing",
      "ivreg vs AER package differences"
    ]
  },
  {
    "name": "momentfit",
    "description": "Modern S4-based implementation of Generalized Method of Moments supporting systems of equations, nonlinear moment conditions, and hypothesis testing. Successor to gmm package with object-oriented design.",
    "category": "Instrumental Variables",
    "docs_url": "https://cran.r-project.org/web/packages/momentfit/momentfit.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=momentfit",
    "install": "install.packages(\"momentfit\")",
    "tags": [
      "GMM",
      "S4-class",
      "systems-estimation",
      "moment-conditions",
      "hypothesis-testing"
    ],
    "best_for": "Modern object-oriented GMM estimation for systems of equations",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "linear-regression",
      "instrumental-variables",
      "R-programming"
    ],
    "topic_tags": [
      "generalized-method-moments",
      "causal-inference",
      "econometrics",
      "systems-estimation",
      "R-package"
    ],
    "summary": "Modern R package implementing Generalized Method of Moments (GMM) estimation with object-oriented S4 design. Supports complex econometric models including systems of equations and nonlinear moment conditions with robust hypothesis testing. Essential tool for causal inference when dealing with endogeneity and instrumental variables.",
    "use_cases": [
      "Estimating demand elasticity using price instruments when prices are endogenous",
      "Testing overidentifying restrictions in labor economics models with multiple instruments"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for GMM estimation with multiple equations",
      "how to implement generalized method of moments in R",
      "successor to gmm package with better object oriented design",
      "R tools for instrumental variables with nonlinear moment conditions"
    ]
  },
  {
    "name": "systemfit",
    "description": "Simultaneous systems estimation implementing Seemingly Unrelated Regression (SUR), two-stage least squares (2SLS), and three-stage least squares (3SLS). Critical for demand systems and structural macro models.",
    "category": "Instrumental Variables",
    "docs_url": "https://cran.r-project.org/web/packages/systemfit/systemfit.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=systemfit",
    "install": "install.packages(\"systemfit\")",
    "tags": [
      "SUR",
      "2SLS",
      "3SLS",
      "systems-estimation",
      "demand-systems"
    ],
    "best_for": "Simultaneous equation systems: SUR, 2SLS, and 3SLS estimation for demand systems and structural models",
    "language": "R",
    "difficulty": "advanced",
    "prerequisites": [
      "linear-algebra",
      "OLS-regression",
      "instrumental-variables"
    ],
    "topic_tags": [
      "systems-estimation",
      "structural-modeling",
      "demand-analysis",
      "econometric-packages",
      "simultaneous-equations"
    ],
    "summary": "R package for estimating simultaneous equation systems using advanced econometric methods like SUR, 2SLS, and 3SLS. Essential for structural economic models where multiple equations are interdependent, such as demand systems or macro models. Handles endogeneity and cross-equation correlations that single-equation methods cannot address.",
    "use_cases": [
      "Estimating consumer demand systems where prices and quantities are jointly determined",
      "Building structural macroeconomic models with simultaneous feedback between variables like investment and interest rates"
    ],
    "audience": [
      "Early-PhD",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "How to estimate simultaneous equation systems in R",
      "SUR vs 2SLS vs 3SLS comparison",
      "R package for demand system estimation",
      "systemfit tutorial for econometric analysis"
    ]
  },
  {
    "name": "py-econometrics `gmm`",
    "description": "Lightweight package for setting up and estimating custom GMM models based on user-defined moment conditions.",
    "category": "Instrumental Variables (IV) & GMM",
    "docs_url": "https://github.com/py-econometrics/gmm",
    "github_url": null,
    "url": "https://github.com/py-econometrics/gmm",
    "install": "pip install gmm",
    "tags": [
      "IV",
      "GMM"
    ],
    "best_for": "Endogeneity correction, 2SLS, moment estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "two-stage-least-squares",
      "numpy-linear-algebra"
    ],
    "topic_tags": [
      "gmm-estimation",
      "moment-conditions",
      "instrumental-variables",
      "custom-models",
      "python-package"
    ],
    "summary": "A lightweight Python package that allows economists to define custom moment conditions and estimate GMM models without being constrained to pre-built specifications. It's designed for researchers who need flexibility in their econometric modeling beyond standard IV approaches. The package handles the optimization and variance estimation while letting users focus on specifying their economic theory through moment conditions.",
    "use_cases": [
      "Estimating demand systems with multiple endogenous variables where standard 2SLS isn't sufficient",
      "Testing overidentifying restrictions in custom structural models with researcher-defined moment conditions"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "custom GMM estimation python package",
      "how to define moment conditions for GMM",
      "flexible GMM implementation beyond standard IV",
      "python library for custom econometric models"
    ]
  },
  {
    "name": "CausalMotifs",
    "description": "Meta's library for estimating heterogeneous spillover effects in A/B tests. Handles network interference.",
    "category": "Interference & Spillovers",
    "docs_url": "https://github.com/facebookresearch/CausalMotifs",
    "github_url": "https://github.com/facebookresearch/CausalMotifs",
    "url": "https://github.com/facebookresearch/CausalMotifs",
    "install": "pip install causal-motifs",
    "tags": [
      "network interference",
      "spillovers",
      "A/B testing"
    ],
    "best_for": "Spillover effects in social networks",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "randomized-controlled-trials",
      "graph-theory",
      "python-scipy"
    ],
    "topic_tags": [
      "network-interference",
      "spillover-effects",
      "experimental-design",
      "causal-inference",
      "python-package"
    ],
    "summary": "CausalMotifs is Meta's specialized library for measuring heterogeneous spillover effects in A/B tests when network interference violates standard experimental assumptions. It provides methods to estimate causal effects when treatment of one user affects outcomes of connected users. The package is designed for large-scale platform experiments where user interactions create complex interference patterns.",
    "use_cases": [
      "Social media platform testing feature rollouts where treated users' behavior affects their friends' engagement",
      "Marketplace experiments where seller incentives impact buyer experience through network effects"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "How to handle network interference in A/B tests",
      "Meta CausalMotifs spillover effects library",
      "Estimating heterogeneous treatment effects with user networks",
      "Python package for experimental interference patterns"
    ]
  },
  {
    "name": "spilled_t",
    "description": "Treatment and spillover effect estimation under network interference. Separates direct and indirect effects.",
    "category": "Interference & Spillovers",
    "docs_url": "https://github.com/mpleung/spilled_t",
    "github_url": "https://github.com/mpleung/spilled_t",
    "url": "https://github.com/mpleung/spilled_t",
    "install": "pip install spilled_t",
    "tags": [
      "network interference",
      "spillovers",
      "treatment effects"
    ],
    "best_for": "Separating direct and spillover effects",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "causal-inference",
      "network-analysis",
      "randomized-experiments"
    ],
    "topic_tags": [
      "network-interference",
      "spillover-effects",
      "causal-inference",
      "treatment-effects",
      "experimental-design"
    ],
    "summary": "spilled_t is a specialized package for estimating treatment effects when units are connected through networks, allowing interference between treated and untreated units. It separates direct effects (impact on treated units) from spillover effects (impact on connected untreated units). This is essential for experiments in social networks, marketplaces, or any setting where treatment of one unit can affect outcomes of connected units.",
    "use_cases": [
      "Measuring how advertising to some users affects purchasing behavior of their social network connections",
      "Evaluating educational interventions where treated students influence classroom peers through social learning"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "How to estimate spillover effects in network experiments",
      "Treatment effect estimation with network interference",
      "Separate direct and indirect effects in randomized trials",
      "Python package for network interference analysis"
    ]
  },
  {
    "name": "testinterference",
    "description": "Statistical tests for SUTVA violations and spillover hypotheses. Detects network interference in experiments.",
    "category": "Interference & Spillovers",
    "docs_url": "https://github.com/tkhdyanagi/testinterference",
    "github_url": "https://github.com/tkhdyanagi/testinterference",
    "url": "https://github.com/tkhdyanagi/testinterference",
    "install": "pip install testinterference",
    "tags": [
      "SUTVA",
      "spillovers",
      "hypothesis testing"
    ],
    "best_for": "Testing for spillover effects",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "randomized-experiments",
      "hypothesis-testing",
      "network-effects"
    ],
    "topic_tags": [
      "SUTVA-violations",
      "network-interference",
      "spillover-testing",
      "experimental-design",
      "causal-inference"
    ],
    "summary": "A statistical package for detecting violations of the Stable Unit Treatment Value Assumption (SUTVA) in randomized experiments. It provides hypothesis tests to identify when treatment effects spill over between units through network connections or geographic proximity. Essential for validating experimental assumptions in social networks, marketplaces, and spatial settings.",
    "use_cases": [
      "Testing whether a social media feature rollout affects both treated users and their untreated friends",
      "Detecting spillover effects in geo-experiments where treated cities influence nearby untreated cities"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "how to test for SUTVA violations in experiments",
      "detect network interference in randomized trials",
      "spillover effects hypothesis testing",
      "validate experimental assumptions network data"
    ]
  },
  {
    "name": "glmnet",
    "description": "Efficient procedures for fitting regularized generalized linear models via penalized maximum likelihood. Implements LASSO, ridge regression, and elastic net with extremely fast coordinate descent algorithms. Foundation for high-dimensional regression and causal ML.",
    "category": "Machine Learning",
    "docs_url": "https://glmnet.stanford.edu/",
    "github_url": null,
    "url": "https://cran.r-project.org/package=glmnet",
    "install": "install.packages(\"glmnet\")",
    "tags": [
      "LASSO",
      "ridge",
      "elastic-net",
      "regularization",
      "high-dimensional"
    ],
    "best_for": "LASSO, ridge, and elastic net regularization\u2014foundation for high-dimensional regression and causal ML",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "linear-regression",
      "python-scikit-learn",
      "cross-validation"
    ],
    "topic_tags": [
      "regularization",
      "feature-selection",
      "high-dimensional-regression",
      "coordinate-descent",
      "penalized-regression"
    ],
    "summary": "glmnet is the gold standard package for regularized regression, implementing LASSO, ridge, and elastic net with highly optimized algorithms. It's essential for high-dimensional problems where you have more features than observations or need automatic feature selection. Widely used in both traditional ML and modern causal inference methods.",
    "use_cases": [
      "Feature selection in high-dimensional datasets with thousands of variables",
      "Building sparse predictive models for A/B test analysis with many covariates"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "best package for LASSO regression in Python",
      "how to do feature selection with regularization",
      "glmnet vs scikit-learn for high dimensional data",
      "coordinate descent algorithm for elastic net"
    ]
  },
  {
    "name": "ranger",
    "description": "Fast implementation of random forests particularly suited for high-dimensional data. Provides survival forests, classification, and regression with efficient memory usage. Core backend for grf's causal forests.",
    "category": "Machine Learning",
    "docs_url": "https://cran.r-project.org/web/packages/ranger/ranger.pdf",
    "github_url": "https://github.com/imbs-hl/ranger",
    "url": "https://cran.r-project.org/package=ranger",
    "install": "install.packages(\"ranger\")",
    "tags": [
      "random-forests",
      "survival-forests",
      "high-dimensional",
      "fast",
      "causal-forests"
    ],
    "best_for": "Fast random forests for high-dimensional data\u2014backend for grf causal forests",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-scikit-learn",
      "R-programming",
      "ensemble-methods"
    ],
    "topic_tags": [
      "random-forests",
      "survival-analysis",
      "causal-inference",
      "high-dimensional-data",
      "R-package"
    ],
    "summary": "Ranger is a high-performance R package implementing random forests optimized for datasets with many features and large sample sizes. It supports classification, regression, and survival analysis with significantly faster training times and lower memory usage than standard implementations. The package serves as the computational backend for generalized random forests (grf) used in causal inference.",
    "use_cases": [
      "analyzing customer churn with thousands of behavioral features and censored observation times",
      "running causal forest experiments on high-dimensional user data to estimate heterogeneous treatment effects"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "fast random forest implementation for high dimensional data",
      "survival forests R package memory efficient",
      "ranger vs randomForest performance comparison",
      "causal forest backend implementation"
    ]
  },
  {
    "name": "tidymodels",
    "description": "Modern framework for modeling and machine learning using tidyverse principles. Meta-package including parsnip (model specification), recipes (preprocessing), workflows, tune (hyperparameter tuning), and yardstick (metrics). Successor to caret.",
    "category": "Machine Learning",
    "docs_url": "https://www.tidymodels.org/",
    "github_url": "https://github.com/tidymodels/tidymodels",
    "url": "https://cran.r-project.org/package=tidymodels",
    "install": "install.packages(\"tidymodels\")",
    "tags": [
      "machine-learning",
      "tidyverse",
      "modeling-framework",
      "hyperparameter-tuning",
      "preprocessing"
    ],
    "best_for": "Modern tidyverse-native ML framework with reproducible workflows\u2014successor to caret",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "r-programming",
      "dplyr-tidyverse",
      "basic-regression"
    ],
    "topic_tags": [
      "r-package",
      "ml-workflow",
      "model-tuning",
      "data-preprocessing",
      "predictive-modeling"
    ],
    "summary": "Tidymodels is a comprehensive R framework that streamlines the entire machine learning workflow using consistent tidyverse syntax. It provides unified interfaces for model specification, data preprocessing, hyperparameter tuning, and performance evaluation. The framework is designed to replace caret with a more modular and principled approach to predictive modeling.",
    "use_cases": [
      "Building and comparing multiple ML models for customer churn prediction with automated hyperparameter tuning",
      "Creating reproducible preprocessing pipelines for A/B test outcome prediction models"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "modern R machine learning framework",
      "tidyverse approach to ML workflows",
      "R alternative to Python scikit-learn",
      "hyperparameter tuning in R tidymodels"
    ]
  },
  {
    "name": "xgboost",
    "description": "Extreme Gradient Boosting implementing state-of-the-art gradient boosted decision trees. Highly efficient, scalable, and portable with interfaces to R, Python, and other languages. Essential for prediction in double ML workflows.",
    "category": "Machine Learning",
    "docs_url": "https://xgboost.readthedocs.io/",
    "github_url": "https://github.com/dmlc/xgboost",
    "url": "https://cran.r-project.org/package=xgboost",
    "install": "install.packages(\"xgboost\")",
    "tags": [
      "gradient-boosting",
      "XGBoost",
      "prediction",
      "machine-learning",
      "ensemble"
    ],
    "best_for": "State-of-the-art gradient boosting for prediction in causal ML and double ML workflows",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-scikit-learn",
      "gradient-descent",
      "decision-trees"
    ],
    "topic_tags": [
      "gradient-boosting",
      "ensemble-methods",
      "prediction",
      "double-ml",
      "causal-inference"
    ],
    "summary": "XGBoost is an optimized gradient boosting framework that builds ensembles of decision trees for superior predictive performance. It's the go-to choice for prediction tasks in causal inference workflows, particularly in double machine learning where you need high-quality nuisance parameter estimates. The package offers exceptional speed, memory efficiency, and handles missing values automatically.",
    "use_cases": [
      "Building first-stage prediction models in double ML for treatment effect estimation",
      "Creating high-performance predictors for A/B test variance reduction using CUPED"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "best gradient boosting package for double ML",
      "XGBoost vs random forest for causal inference",
      "how to use XGBoost in treatment effect estimation",
      "gradient boosting for nuisance parameter estimation"
    ]
  },
  {
    "name": "emmeans",
    "description": "Estimated Marginal Means (least-squares means) for factorial designs. Computes adjusted means and contrasts for balanced and unbalanced designs, with support for mixed models and Bayesian models.",
    "category": "Marginal Effects",
    "docs_url": "https://rvlenth.github.io/emmeans/",
    "github_url": "https://github.com/rvlenth/emmeans",
    "url": "https://cran.r-project.org/package=emmeans",
    "install": "install.packages(\"emmeans\")",
    "tags": [
      "marginal-means",
      "least-squares-means",
      "factorial-designs",
      "contrasts",
      "mixed-models"
    ],
    "best_for": "Estimated marginal means for factorial designs with interaction interpretation",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "linear-regression",
      "ANOVA",
      "R-programming"
    ],
    "topic_tags": [
      "marginal-effects",
      "experimental-design",
      "statistical-inference",
      "post-hoc-analysis",
      "R-package"
    ],
    "summary": "R package for computing estimated marginal means (EMMs) and contrasts from statistical models, especially useful for factorial experiments and complex designs. Provides adjusted means that account for covariates and unbalanced data, with built-in support for pairwise comparisons and custom contrasts. Essential tool for interpreting results from ANOVA, mixed models, and other factorial designs in experimental settings.",
    "use_cases": [
      "A/B testing with multiple treatment groups where you need to compare adjusted means while controlling for user demographics",
      "Agricultural experiment analyzing crop yields across different fertilizer types and irrigation levels with unbalanced sample sizes"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "how to compute adjusted means from factorial experiment in R",
      "emmeans package for post-hoc analysis after ANOVA",
      "marginal means comparison with unbalanced experimental design",
      "R package for pairwise contrasts in mixed models"
    ]
  },
  {
    "name": "marginaleffects",
    "description": "Modern standard for interpreting regression results\u2014up to 1000\u00d7 faster than margins. Computes marginal effects, predictions, contrasts, and slopes for 100+ model classes. Published in JSS 2024.",
    "category": "Marginal Effects",
    "docs_url": "https://marginaleffects.com/",
    "github_url": "https://github.com/vincentarelbundock/marginaleffects",
    "url": "https://cran.r-project.org/package=marginaleffects",
    "install": "install.packages(\"marginaleffects\")",
    "tags": [
      "marginal-effects",
      "predictions",
      "contrasts",
      "interpretation",
      "slopes"
    ],
    "best_for": "Modern marginal effects interpretation\u20141000\u00d7 faster than margins with 100+ model support, JSS 2024",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "regression-modeling",
      "python-statsmodels",
      "statistical-inference"
    ],
    "topic_tags": [
      "marginal-effects",
      "causal-inference",
      "regression-interpretation",
      "python-package",
      "econometrics"
    ],
    "summary": "A high-performance Python package for computing marginal effects, predictions, and contrasts from regression models. It provides a unified interface for interpreting results across 100+ model types, making it essential for economists and data scientists who need to understand how variables affect outcomes. The package is significantly faster than traditional tools and follows modern best practices for statistical interpretation.",
    "use_cases": [
      "Computing average marginal effects of policy variables in difference-in-differences studies",
      "Generating model predictions with confidence intervals for A/B test result interpretation"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "fast marginal effects python package",
      "how to interpret regression coefficients marginaleffects",
      "alternatives to stata margins command python",
      "compute predictions contrasts regression models python"
    ]
  },
  {
    "name": "Lifetimes",
    "description": "Analyze customer lifetime value (CLV) using probabilistic models (BG/NBD, Pareto/NBD) to predict purchases.",
    "category": "Marketing Mix Models (MMM) & Business Analytics",
    "docs_url": "https://lifetimes.readthedocs.io/en/latest/",
    "github_url": "https://github.com/CamDavidsonPilon/lifetimes",
    "url": "https://github.com/CamDavidsonPilon/lifetimes",
    "install": "pip install lifetimes",
    "tags": [
      "marketing",
      "analytics"
    ],
    "best_for": "Marketing ROI, media mix optimization, attribution",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "maximum-likelihood-estimation",
      "cohort-analysis"
    ],
    "topic_tags": [
      "customer-lifetime-value",
      "probabilistic-modeling",
      "retention-analysis",
      "marketing-analytics",
      "python-package"
    ],
    "summary": "Lifetimes is a Python package that implements probabilistic models like BG/NBD and Pareto/NBD to predict customer behavior and calculate lifetime value. It's widely used by data scientists at subscription companies, e-commerce platforms, and SaaS businesses to forecast customer purchases and retention. The package provides ready-to-use implementations of complex statistical models that would otherwise require significant mathematical expertise to build from scratch.",
    "use_cases": [
      "Predicting monthly recurring revenue and churn for a SaaS platform by modeling subscriber behavior patterns",
      "Optimizing marketing spend allocation by calculating CLV for different customer segments in e-commerce"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python package customer lifetime value",
      "BG/NBD model implementation",
      "how to predict customer churn probabilistic models",
      "lifetimes package tutorial CLV analysis"
    ]
  },
  {
    "name": "MaMiMo",
    "description": "Lightweight Python library focused specifically on Marketing Mix Modeling implementation.",
    "category": "Marketing Mix Models (MMM) & Business Analytics",
    "docs_url": null,
    "github_url": "https://github.com/Garve/mamimo",
    "url": "https://github.com/Garve/mamimo",
    "install": "pip install mamimo",
    "tags": [
      "marketing",
      "analytics"
    ],
    "best_for": "Marketing ROI, media mix optimization, attribution",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "linear-regression",
      "media-attribution-basics"
    ],
    "topic_tags": [
      "marketing-mix-modeling",
      "media-attribution",
      "python-package",
      "business-analytics",
      "advertising-optimization"
    ],
    "summary": "MaMiMo is a lightweight Python library designed specifically for implementing Marketing Mix Models to measure the effectiveness of different marketing channels. It provides tools for data scientists and marketing analysts to build MMM models that quantify the incremental impact of advertising spend across channels like TV, digital, and print. The library focuses on practical implementation with minimal overhead for teams looking to get started with marketing attribution analysis.",
    "use_cases": [
      "Measuring incremental lift from TV advertising campaigns versus digital channels for budget optimization",
      "Building attribution models to understand how different marketing touchpoints contribute to sales for quarterly planning"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for marketing mix modeling",
      "how to implement MMM in python",
      "marketing attribution analysis tools",
      "lightweight package for media mix optimization"
    ]
  },
  {
    "name": "PyMC Marketing",
    "description": "Collection of Bayesian marketing models built with PyMC, including MMM, CLV, and attribution.",
    "category": "Marketing Mix Models (MMM) & Business Analytics",
    "docs_url": "https://www.pymc-marketing.io/",
    "github_url": "https://github.com/pymc-labs/pymc-marketing",
    "url": "https://github.com/pymc-labs/pymc-marketing",
    "install": "pip install pymc-marketing",
    "tags": [
      "marketing",
      "analytics",
      "Bayesian"
    ],
    "best_for": "Marketing ROI, media mix optimization, attribution",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "bayesian-statistics",
      "pymc-basics"
    ],
    "topic_tags": [
      "marketing-mix-modeling",
      "customer-lifetime-value",
      "attribution-modeling",
      "bayesian-inference",
      "python-package"
    ],
    "summary": "PyMC Marketing is a Python package that implements Bayesian marketing models including Marketing Mix Models (MMM), Customer Lifetime Value (CLV), and attribution analysis. It provides pre-built statistical models for marketing analysts and data scientists to measure campaign effectiveness and customer value. The package leverages PyMC's probabilistic programming framework to handle uncertainty quantification in marketing measurements.",
    "use_cases": [
      "Measuring the incremental impact of different marketing channels (TV, digital, social) on sales revenue",
      "Estimating customer lifetime value and churn probability for subscription businesses"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "how to build marketing mix model in python",
      "bayesian customer lifetime value analysis",
      "marketing attribution modeling pymc",
      "python package for media mix modeling"
    ]
  },
  {
    "name": "mmm_stan",
    "description": "Python/STAN implementation of Bayesian Marketing Mix Models.",
    "category": "Marketing Mix Models (MMM) & Business Analytics",
    "docs_url": null,
    "github_url": "https://github.com/sibylhe/mmm_stan",
    "url": "https://github.com/sibylhe/mmm_stan",
    "install": "GitHub Repository",
    "tags": [
      "marketing",
      "analytics",
      "Bayesian"
    ],
    "best_for": "Marketing ROI, media mix optimization, attribution",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "bayesian-inference",
      "STAN-modeling"
    ],
    "topic_tags": [
      "marketing-mix-modeling",
      "bayesian-marketing",
      "attribution-analysis",
      "media-effectiveness",
      "python-package"
    ],
    "summary": "A Python implementation using STAN for Bayesian Marketing Mix Models that helps marketers measure the incremental impact of different marketing channels. This package provides tools for attribution analysis, media saturation curves, and adstock modeling to optimize marketing spend allocation. It's commonly used by data scientists in marketing analytics teams to quantify ROI across channels like TV, digital, and print advertising.",
    "use_cases": [
      "Measuring incremental lift from TV advertising campaigns while accounting for base sales and seasonality",
      "Optimizing budget allocation across digital channels (search, social, display) based on diminishing returns curves"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "How to implement Bayesian marketing mix models in Python",
      "STAN package for marketing attribution analysis",
      "Marketing mix modeling with adstock and saturation effects",
      "Python tools for measuring marketing channel effectiveness"
    ]
  },
  {
    "name": "ziln_cltv",
    "description": "Google's Zero-Inflated Lognormal loss for heavily-tailed LTV distributions. Outputs both predicted LTV and churn probability.",
    "category": "Marketing Mix Models (MMM) & Business Analytics",
    "docs_url": null,
    "github_url": "https://github.com/google/lifetime_value",
    "url": "https://github.com/google/lifetime_value",
    "install": "pip install lifetime-value",
    "tags": [
      "LTV",
      "customer analytics",
      "churn"
    ],
    "best_for": "Customer LTV with zero-inflated distributions",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-sklearn",
      "maximum-likelihood-estimation",
      "survival-analysis"
    ],
    "topic_tags": [
      "customer-lifetime-value",
      "zero-inflated-models",
      "churn-prediction",
      "python-package",
      "google-research"
    ],
    "summary": "Google's Zero-Inflated Lognormal loss function designed for customer lifetime value prediction in scenarios with high churn rates and heavily-tailed revenue distributions. The model simultaneously predicts both customer lifetime value and churn probability, making it ideal for businesses with significant customer dropout. Particularly useful for subscription services, gaming, and e-commerce where many customers generate zero long-term value.",
    "use_cases": [
      "Predicting customer lifetime value for a mobile game where many users churn immediately but some become high-value spenders",
      "Estimating subscription service CLV where a large portion of trial users never convert to paid plans"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "zero inflated lognormal customer lifetime value",
      "Google CLV model with churn probability",
      "heavily tailed LTV distribution modeling",
      "ziln loss function customer analytics"
    ]
  },
  {
    "name": "algmatch",
    "description": "Student-Project Allocation with lecturer preferences. Extends matching to three-sided markets.",
    "category": "Matching & Market Design",
    "docs_url": null,
    "github_url": null,
    "url": "https://pypi.org/project/algmatch/",
    "install": "pip install algmatch",
    "tags": [
      "matching",
      "market design",
      "allocation"
    ],
    "best_for": "Student-project-lecturer allocation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-programming",
      "linear-programming",
      "game-theory-basics"
    ],
    "topic_tags": [
      "three-sided-matching",
      "allocation-algorithms",
      "market-design",
      "optimization",
      "python-package"
    ],
    "summary": "Algmatch is a Python package that solves student-project allocation problems where both students and lecturers have preferences, extending traditional two-sided matching to three-sided markets. It's designed for academic institutions and organizations that need to optimally assign people to projects while considering multiple stakeholders' preferences. The package implements algorithms that find stable allocations in complex matching scenarios.",
    "use_cases": [
      "Assigning MBA students to consulting projects based on student preferences, company needs, and faculty supervisor availability",
      "Matching research assistants to lab projects considering student interests, PI preferences, and project requirements"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "three sided matching algorithm python",
      "student project allocation with preferences",
      "how to solve matching problems with lecturers and students",
      "market design allocation algorithms implementation"
    ]
  },
  {
    "name": "deep-opt-auctions",
    "description": "Neural network optimal auction design. Implements RegretNet, RochetNet for mechanism design.",
    "category": "Matching & Market Design",
    "docs_url": "https://github.com/saisrivatsan/deep-opt-auctions",
    "github_url": "https://github.com/saisrivatsan/deep-opt-auctions",
    "url": "https://github.com/saisrivatsan/deep-opt-auctions",
    "install": "Install from GitHub",
    "tags": [
      "auctions",
      "mechanism design",
      "deep learning"
    ],
    "best_for": "Neural network auction design",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "pytorch-neural-networks",
      "game-theory-mechanism-design",
      "gradient-descent-optimization"
    ],
    "topic_tags": [
      "neural-auctions",
      "regretnet",
      "rochetnet",
      "mechanism-design",
      "deep-learning"
    ],
    "summary": "Deep-opt-auctions implements neural network approaches to optimal auction design, featuring RegretNet and RochetNet architectures. These tools allow researchers and practitioners to design complex auction mechanisms that maximize revenue while satisfying incentive compatibility constraints. The package is particularly valuable for mechanism designers working on multi-item auctions or settings where analytical solutions are intractable.",
    "use_cases": [
      "Designing revenue-optimal auctions for online advertising platforms with multiple ad slots",
      "Creating procurement mechanisms for government contracts with complex bidder preferences"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "neural network auction design python",
      "RegretNet implementation mechanism design",
      "deep learning optimal auctions library",
      "RochetNet auction mechanism toolkit"
    ]
  },
  {
    "name": "kep_solver",
    "description": "Kidney exchange optimization with hierarchical objectives. Production-ready for kidney paired donation.",
    "category": "Matching & Market Design",
    "docs_url": "https://kep-solver.readthedocs.io/en/latest/",
    "github_url": "https://gitlab.com/wpettersson/kep_solver",
    "url": "https://pypi.org/project/kep_solver/",
    "install": "pip install kep-solver",
    "tags": [
      "matching",
      "market design",
      "kidney exchange"
    ],
    "best_for": "Kidney exchange program optimization",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "linear-programming",
      "graph-theory",
      "python-networkx"
    ],
    "topic_tags": [
      "kidney-exchange",
      "matching-algorithms",
      "optimization",
      "healthcare-economics",
      "production-package"
    ],
    "summary": "A specialized optimization package for kidney paired donation programs that handles complex matching constraints and hierarchical objectives. Designed for production use in real kidney exchange programs where patients with incompatible donors can swap kidneys. Implements state-of-the-art algorithms for finding optimal exchanges while respecting medical, geographic, and fairness constraints.",
    "use_cases": [
      "Implementing kidney exchange matching system for hospital network",
      "Research on market design mechanisms for organ allocation"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "kidney exchange optimization python",
      "matching algorithms for organ donation",
      "how to implement kidney paired donation system",
      "production ready kidney exchange solver"
    ]
  },
  {
    "name": "matching",
    "description": "Implements Stable Marriage, Hospital-Resident, Student-Allocation, and Stable Roommates using Gale-Shapley (JOSS paper).",
    "category": "Matching & Market Design",
    "docs_url": "https://daffidwilde.github.io/matching/",
    "github_url": "https://github.com/daffidwilde/matching",
    "url": "https://github.com/daffidwilde/matching",
    "install": "pip install matching",
    "tags": [
      "matching",
      "market design",
      "Gale-Shapley"
    ],
    "best_for": "Classic two-sided matching algorithms",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-basics",
      "graph-theory-concepts"
    ],
    "topic_tags": [
      "stable-matching",
      "market-design",
      "gale-shapley",
      "two-sided-markets",
      "python-package"
    ],
    "summary": "A Python package implementing classic matching algorithms including Stable Marriage and Hospital-Resident problems using the Gale-Shapley algorithm. These algorithms solve two-sided matching problems where participants have preferences and the goal is to find stable pairings. Useful for economists studying market design and data scientists working on assignment problems.",
    "use_cases": [
      "Modeling medical residency matching systems where doctors and hospitals rank each other",
      "Analyzing school choice mechanisms where students and schools have mutual preferences"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "gale shapley algorithm python implementation",
      "stable marriage problem solver package",
      "hospital resident matching algorithm code",
      "two sided matching market design tools"
    ]
  },
  {
    "name": "scarfmatch",
    "description": "Matching with couples using Scarf's algorithm. Essential for NRMP-style medical residency matching.",
    "category": "Matching & Market Design",
    "docs_url": null,
    "github_url": "https://github.com/dwtang/scarf",
    "url": "https://pypi.org/project/scarfmatch/",
    "install": "pip install scarfmatch",
    "tags": [
      "matching",
      "market design",
      "couples"
    ],
    "best_for": "Residency matching with couples",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-networkx",
      "game-theory",
      "linear-programming"
    ],
    "topic_tags": [
      "matching-algorithms",
      "two-sided-markets",
      "medical-residency",
      "couples-matching",
      "scarf-algorithm"
    ],
    "summary": "Implementation of Scarf's algorithm for matching problems involving couples, where partners have preferences over joint outcomes. Primarily used in medical residency matching (NRMP) where married couples need to be placed together. Handles the complex constraint that couple members must coordinate their matches while maintaining market stability.",
    "use_cases": [
      "Medical residency programs matching married doctor couples to nearby hospitals",
      "Academic job market matching dual-career academic couples to universities in same geographic area"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "couples matching algorithm medical residency",
      "how to implement Scarf algorithm for two-sided matching",
      "NRMP style matching with couples constraints",
      "python package for matching couples in markets"
    ]
  },
  {
    "name": "glmmTMB",
    "description": "Fit generalized linear mixed models with extensions including zero-inflation, hurdle models, heteroscedasticity, and autocorrelation using Template Model Builder (TMB) with automatic differentiation and Laplace approximation.",
    "category": "Mixed Effects",
    "docs_url": "https://glmmtmb.github.io/glmmTMB/",
    "github_url": "https://github.com/glmmTMB/glmmTMB",
    "url": "https://cran.r-project.org/package=glmmTMB",
    "install": "install.packages(\"glmmTMB\")",
    "tags": [
      "GLMM",
      "zero-inflation",
      "negative-binomial",
      "TMB",
      "overdispersion"
    ],
    "best_for": "Zero-inflated, overdispersed, or complex GLMMs beyond lme4 capabilities, implementing Brooks et al. (2017)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "generalized-linear-models",
      "mixed-effects-models",
      "R-programming"
    ],
    "topic_tags": [
      "mixed-effects",
      "zero-inflation",
      "overdispersion",
      "hierarchical-models",
      "bayesian-approximation"
    ],
    "summary": "glmmTMB is an R package for fitting generalized linear mixed models with advanced extensions like zero-inflation and overdispersion modeling. It uses Template Model Builder for fast automatic differentiation and Laplace approximation, making it particularly useful for complex hierarchical data with excess zeros or variance issues. The package is popular among researchers analyzing clustered or longitudinal data in ecology, economics, and social sciences.",
    "use_cases": [
      "Modeling customer purchase counts with store-level random effects and excess zeros",
      "Analyzing patient treatment outcomes across hospitals with overdispersed count data"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "zero inflated mixed effects models R",
      "glmmTMB vs lme4 overdispersion",
      "hierarchical negative binomial model",
      "mixed effects zero inflation package"
    ]
  },
  {
    "name": "lme4",
    "description": "Fit linear and generalized linear mixed-effects models using S4 classes with Eigen C++ library for efficient computation, supporting arbitrarily nested and crossed random effects structures for hierarchical and longitudinal data.",
    "category": "Mixed Effects",
    "docs_url": "https://cran.r-project.org/web/packages/lme4/vignettes/",
    "github_url": "https://github.com/lme4/lme4",
    "url": "https://cran.r-project.org/package=lme4",
    "install": "install.packages(\"lme4\")",
    "tags": [
      "linear-mixed-models",
      "GLMM",
      "random-effects",
      "hierarchical-models",
      "repeated-measures"
    ],
    "best_for": "Standard linear and generalized linear mixed-effects modeling with crossed/nested random effects, implementing Bates et al. (2015)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "linear-regression",
      "R-programming",
      "random-effects-concepts"
    ],
    "topic_tags": [
      "mixed-effects-models",
      "hierarchical-data",
      "longitudinal-analysis",
      "R-package",
      "random-effects"
    ],
    "summary": "lme4 is the standard R package for fitting linear and generalized linear mixed-effects models with complex random effect structures. It's widely used by researchers and data scientists working with hierarchical, grouped, or repeated measures data where observations are not independent. The package provides efficient computation through C++ optimization and supports nested and crossed random effects.",
    "use_cases": [
      "Analyzing student test scores clustered within schools and districts",
      "Modeling user behavior over time with random intercepts per user"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "R package for mixed effects models",
      "how to fit hierarchical linear models in R",
      "lme4 vs nlme for random effects",
      "analyzing clustered data with random effects R"
    ]
  },
  {
    "name": "lmerTest",
    "description": "Provides p-values for lme4 model fits via Satterthwaite's or Kenward-Roger degrees of freedom methods, with Type I/II/III ANOVA tables, model selection tools (step, drop1), and least-squares means calculations.",
    "category": "Mixed Effects",
    "docs_url": "https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf",
    "github_url": "https://github.com/runehaubo/lmerTestR",
    "url": "https://cran.r-project.org/package=lmerTest",
    "install": "install.packages(\"lmerTest\")",
    "tags": [
      "p-values",
      "Satterthwaite",
      "Kenward-Roger",
      "ANOVA",
      "hypothesis-testing"
    ],
    "best_for": "Getting p-values and formal hypothesis tests for lme4 linear mixed models, implementing Kuznetsova et al. (2017)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "lme4-package",
      "linear-mixed-models",
      "R-programming"
    ],
    "topic_tags": [
      "mixed-effects",
      "hypothesis-testing",
      "p-values",
      "ANOVA",
      "R-package"
    ],
    "summary": "lmerTest extends the lme4 package by adding statistical significance testing capabilities to linear mixed-effects models. It provides p-values using sophisticated degrees of freedom approximation methods and supports comprehensive model comparison workflows. Essential for researchers who need to perform hypothesis testing on hierarchical data structures.",
    "use_cases": [
      "Testing significance of treatment effects in randomized controlled trials with clustered data",
      "Evaluating feature importance in A/B tests with user-level random effects"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "how to get p-values from lme4 models",
      "lmerTest vs lme4 significance testing",
      "mixed effects ANOVA table in R",
      "Satterthwaite degrees of freedom mixed models"
    ]
  },
  {
    "name": "nlme",
    "description": "Fit Gaussian linear and nonlinear mixed-effects models with flexible correlation structures, variance functions for heteroscedasticity, and nested random effects. Ships with base R and offers more variance-covariance flexibility than lme4.",
    "category": "Mixed Effects",
    "docs_url": "https://cran.r-project.org/web/packages/nlme/nlme.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=nlme",
    "install": "install.packages(\"nlme\")",
    "tags": [
      "nonlinear-mixed-models",
      "autocorrelation",
      "heteroscedasticity",
      "repeated-measures",
      "longitudinal"
    ],
    "best_for": "Models requiring custom correlation structures, variance functions, or nonlinear mixed effects, implementing Pinheiro & Bates (2000)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "linear-regression",
      "R-programming",
      "panel-data"
    ],
    "topic_tags": [
      "mixed-effects-models",
      "longitudinal-analysis",
      "variance-modeling",
      "econometrics",
      "R-package"
    ],
    "summary": "nlme is R's built-in package for fitting linear and nonlinear mixed-effects models with sophisticated error structures. It excels at handling complex variance-covariance patterns, autocorrelated errors, and heteroscedasticity in longitudinal data. Popular among economists and researchers who need more flexible modeling of within-group correlations than standard approaches allow.",
    "use_cases": [
      "Modeling treatment effects in panel data where errors are correlated within individuals over time",
      "Analyzing firm performance data with heteroscedastic errors that vary by company size or industry"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "R package for mixed effects models with autocorrelated errors",
      "how to model heteroscedasticity in longitudinal data",
      "nlme vs lme4 for panel data analysis",
      "mixed effects regression with flexible variance structures"
    ]
  },
  {
    "name": "car",
    "description": "Functions accompanying 'An R Companion to Applied Regression.' Provides advanced regression diagnostics including variance inflation factors (VIF), Type II/III ANOVA, influence measures, linear hypothesis testing, power transformations (Box-Cox), and comprehensive diagnostic plots.",
    "category": "Model Diagnostics",
    "docs_url": "https://www.john-fox.ca/Companion/index.html",
    "github_url": null,
    "url": "https://cran.r-project.org/package=car",
    "install": "install.packages(\"car\")",
    "tags": [
      "regression-diagnostics",
      "VIF",
      "ANOVA",
      "hypothesis-testing",
      "influence-diagnostics"
    ],
    "best_for": "Classical regression diagnostics: VIF for multicollinearity, Type II/III ANOVA, linear hypothesis tests, from Fox & Weisberg (2019)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "linear-regression",
      "R-programming",
      "statistical-hypothesis-testing"
    ],
    "topic_tags": [
      "regression-diagnostics",
      "model-validation",
      "R-package",
      "ANOVA",
      "hypothesis-testing"
    ],
    "summary": "The car package is a comprehensive R toolkit for regression model diagnostics and validation, accompanying the popular 'Applied Regression' textbook. It provides essential diagnostic tools like variance inflation factors, influence measures, and hypothesis testing that are standard practice in econometric analysis. The package is widely used by researchers and analysts who need to validate regression assumptions and assess model quality.",
    "use_cases": [
      "Checking multicollinearity in wage regression models using VIF before publication",
      "Testing linear restrictions on coefficients in demand estimation models"
    ],
    "audience": [
      "Junior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "How to check multicollinearity in R regression",
      "VIF variance inflation factor R package",
      "R regression diagnostics car package",
      "Linear hypothesis testing R statistical significance"
    ]
  },
  {
    "name": "performance",
    "description": "Utilities for computing indices of model quality and goodness of fit, including R\u00b2, RMSE, ICC, AIC/BIC. Provides functions to check models for overdispersion, zero-inflation, multicollinearity (VIF), convergence, and singularity. Supports mixed effects and Bayesian models.",
    "category": "Model Diagnostics",
    "docs_url": "https://easystats.github.io/performance/",
    "github_url": "https://github.com/easystats/performance",
    "url": "https://cran.r-project.org/package=performance",
    "install": "install.packages(\"performance\")",
    "tags": [
      "model-diagnostics",
      "R-squared",
      "assumption-checking",
      "VIF",
      "goodness-of-fit"
    ],
    "best_for": "Comprehensive model quality assessment, especially the check_model() visual diagnostic panel, implementing L\u00fcdecke et al. (2021, JOSS)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "regression-modeling",
      "statistical-inference",
      "R-programming"
    ],
    "topic_tags": [
      "model-diagnostics",
      "goodness-of-fit",
      "mixed-effects",
      "bayesian-models",
      "multicollinearity"
    ],
    "summary": "The performance package provides comprehensive utilities for evaluating statistical models, including standard metrics like R\u00b2 and RMSE alongside diagnostic tests for common modeling issues. It supports both frequentist and Bayesian frameworks, with specialized functions for mixed effects models. Essential for practitioners who need to validate model assumptions and communicate model quality effectively.",
    "use_cases": [
      "Evaluating whether a customer churn prediction model meets performance thresholds before deployment",
      "Diagnosing multicollinearity and convergence issues in a hierarchical model analyzing treatment effects across multiple sites"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "how to check model assumptions in R",
      "calculate VIF multicollinearity R package",
      "evaluate mixed effects model performance",
      "R package for model diagnostics goodness of fit"
    ]
  },
  {
    "name": "see",
    "description": "Visualization toolbox for the easystats ecosystem built on ggplot2. Provides publication-ready plotting methods for model parameters, predictions, and performance diagnostics from all easystats packages via simple plot() calls.",
    "category": "Model Diagnostics",
    "docs_url": "https://easystats.github.io/see/",
    "github_url": "https://github.com/easystats/see",
    "url": "https://cran.r-project.org/package=see",
    "install": "install.packages(\"see\")",
    "tags": [
      "visualization",
      "ggplot2",
      "diagnostic-plots",
      "publication-ready",
      "easystats"
    ],
    "best_for": "Publication-ready visualizations of model diagnostics with a simple plot() interface, implementing L\u00fcdecke et al. (2021, JOSS)",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "R-programming",
      "ggplot2",
      "statistical-modeling"
    ],
    "topic_tags": [
      "model-diagnostics",
      "data-visualization",
      "statistical-plotting",
      "r-packages",
      "publication-graphics"
    ],
    "summary": "The 'see' package provides a unified visualization interface for the easystats ecosystem in R, automatically generating publication-ready diagnostic plots and model summaries. It simplifies the process of creating consistent, professional visualizations for statistical models by extending ggplot2 with specialized plotting methods. The package is designed for researchers and analysts who need quick, high-quality visual outputs from their statistical analyses.",
    "use_cases": [
      "Creating diagnostic plots for regression models to check assumptions and identify outliers",
      "Generating publication-ready visualizations of model parameters and confidence intervals for research papers"
    ],
    "audience": [
      "Junior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "R package for model diagnostic plots",
      "easystats visualization tools",
      "publication ready plots for statistical models",
      "ggplot2 extension for model diagnostics"
    ]
  },
  {
    "name": "CausalNLP",
    "description": "Causal inference for text data. Estimate treatment effects from unstructured text using NLP.",
    "category": "Natural Language Processing for Economics",
    "docs_url": null,
    "github_url": "https://github.com/amaiya/causalnlp",
    "url": "https://github.com/amaiya/causalnlp",
    "install": "pip install causalnlp",
    "tags": [
      "NLP",
      "causal inference",
      "text"
    ],
    "best_for": "Causal effects from text data",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "python-pandas",
      "scikit-learn",
      "propensity-score-matching"
    ],
    "topic_tags": [
      "causal-inference",
      "natural-language-processing",
      "treatment-effects",
      "text-analysis",
      "econometrics"
    ],
    "summary": "CausalNLP combines natural language processing with causal inference methods to estimate treatment effects from unstructured text data. It enables researchers to extract causal insights from textual information like reviews, social media posts, or documents where traditional structured data approaches fall short. The package is particularly valuable for economists and data scientists working with large-scale text datasets where experimental data is unavailable.",
    "use_cases": [
      "Measuring the causal effect of product features mentioned in customer reviews on purchase behavior",
      "Estimating treatment effects of policy announcements by analyzing news articles and social media sentiment"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "causal inference with text data python",
      "how to estimate treatment effects from reviews",
      "NLP package for causal analysis",
      "text-based causal inference methods"
    ]
  },
  {
    "name": "Gensim",
    "description": "Library focused on topic modeling (LDA, LSI) and document similarity analysis.",
    "category": "Natural Language Processing for Economics",
    "docs_url": "https://radimrehurek.com/gensim/",
    "github_url": "https://github.com/RaRe-Technologies/gensim",
    "url": "https://github.com/RaRe-Technologies/gensim",
    "install": "pip install gensim",
    "tags": [
      "NLP",
      "text analysis"
    ],
    "best_for": "Text analysis, sentiment analysis, document classification",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn",
      "numpy-arrays"
    ],
    "topic_tags": [
      "topic-modeling",
      "document-similarity",
      "text-mining",
      "unsupervised-learning",
      "NLP"
    ],
    "summary": "Gensim is a Python library specialized for topic modeling and document similarity analysis, implementing algorithms like Latent Dirichlet Allocation (LDA) and Latent Semantic Indexing (LSI). It's widely used by economists and data scientists to discover hidden thematic structures in large text corpora such as research papers, policy documents, or customer reviews. The library excels at memory-efficient processing of large document collections and provides tools for semantic similarity analysis.",
    "use_cases": [
      "Analyzing central bank meeting minutes to identify emerging policy themes and track their evolution over time",
      "Clustering academic economics papers by topic to identify research trends and measure knowledge spillovers between fields"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "topic modeling library for economics research",
      "how to find similar documents in large text corpus",
      "LDA implementation for analyzing policy documents",
      "python package for document clustering and similarity"
    ]
  },
  {
    "name": "Transformers",
    "description": "Access to thousands of pre-trained models for NLP tasks like text classification, summarization, embeddings, etc.",
    "category": "Natural Language Processing for Economics",
    "docs_url": "https://huggingface.co/transformers/",
    "github_url": "https://github.com/huggingface/transformers",
    "url": "https://github.com/huggingface/transformers",
    "install": "pip install transformers",
    "tags": [
      "NLP",
      "text analysis"
    ],
    "best_for": "Text analysis, sentiment analysis, document classification",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-basics",
      "pytorch-tensors",
      "neural-network-fundamentals"
    ],
    "topic_tags": [
      "transformers",
      "pre-trained-models",
      "text-classification",
      "language-models",
      "huggingface"
    ],
    "summary": "Transformers is a Python library providing access to thousands of pre-trained language models for various NLP tasks. It's widely used by data scientists and researchers to quickly implement state-of-the-art text analysis without training models from scratch. The library supports tasks like sentiment analysis, text summarization, and embedding generation with minimal code.",
    "use_cases": [
      "Analyzing customer feedback sentiment for product development decisions",
      "Extracting key insights from earnings call transcripts for investment research"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "How to classify text sentiment using pre-trained models",
      "Best Python library for NLP text analysis",
      "Hugging Face transformers tutorial for beginners",
      "Pre-trained models for economic text classification"
    ]
  },
  {
    "name": "spaCy",
    "description": "Industrial-strength NLP library for efficient text processing pipelines (NER, POS tagging, etc.).",
    "category": "Natural Language Processing for Economics",
    "docs_url": "https://spacy.io/",
    "github_url": "https://github.com/explosion/spaCy",
    "url": "https://github.com/explosion/spaCy",
    "install": "pip install spacy",
    "tags": [
      "NLP",
      "text analysis"
    ],
    "best_for": "Text analysis, sentiment analysis, document classification",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "regular-expressions",
      "basic-linguistics"
    ],
    "topic_tags": [
      "natural-language-processing",
      "text-mining",
      "named-entity-recognition",
      "python-library",
      "sentiment-analysis"
    ],
    "summary": "spaCy is a production-ready Python library for advanced natural language processing tasks including named entity recognition, part-of-speech tagging, and dependency parsing. It's designed for speed and efficiency in processing large volumes of text data. Tech economists use it to extract structured information from unstructured text sources like earnings calls, patent filings, and regulatory documents.",
    "use_cases": [
      "Extracting company names and financial metrics from SEC filings to build datasets for corporate finance research",
      "Processing job postings to identify skill requirements and wage premiums for labor economics analysis"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for named entity recognition in economics research",
      "how to extract companies and locations from earnings transcripts",
      "fast NLP preprocessing for economic text analysis",
      "spacy vs nltk for financial document processing"
    ]
  },
  {
    "name": "ggraph",
    "description": "Grammar of graphics for network data built on ggplot2. Provides layouts, geometries, and faceting specifically designed for network visualization with publication-quality output.",
    "category": "Network Analysis",
    "docs_url": "https://ggraph.data-imaginist.com/",
    "github_url": "https://github.com/thomasp85/ggraph",
    "url": "https://cran.r-project.org/package=ggraph",
    "install": "install.packages(\"ggraph\")",
    "tags": [
      "networks",
      "visualization",
      "ggplot2",
      "graph-layouts",
      "publication-ready"
    ],
    "best_for": "Publication-quality network visualization using ggplot2 grammar",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "R-ggplot2",
      "network-data-structures",
      "graph-theory-basics"
    ],
    "topic_tags": [
      "network-visualization",
      "ggplot2-extension",
      "graph-layouts",
      "social-networks",
      "R-package"
    ],
    "summary": "ggraph extends ggplot2's grammar of graphics to network data, enabling sophisticated network visualizations with consistent syntax. It provides specialized layouts, node and edge geometries, and faceting capabilities for creating publication-ready network plots. The package is particularly valuable for researchers and analysts who need to visualize complex relationships in social, biological, or technical networks.",
    "use_cases": [
      "Visualizing social media influence networks to identify key opinion leaders and information flow patterns",
      "Creating publication-ready plots of protein interaction networks showing different types of molecular relationships"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "How to visualize network data in R with ggplot2",
      "ggraph network visualization layouts",
      "Publication quality network plots R",
      "ggplot2 extension for graph data visualization"
    ]
  },
  {
    "name": "igraph",
    "description": "Comprehensive network analysis library with efficient algorithms for network creation, manipulation, and analysis. Provides centrality measures, community detection, graph visualization, and network statistics.",
    "category": "Network Analysis",
    "docs_url": "https://igraph.org/r/",
    "github_url": "https://github.com/igraph/rigraph",
    "url": "https://cran.r-project.org/package=igraph",
    "install": "install.packages(\"igraph\")",
    "tags": [
      "networks",
      "graph-algorithms",
      "centrality",
      "community-detection",
      "network-statistics"
    ],
    "best_for": "Comprehensive network analysis with efficient algorithms for centrality, community detection, and visualization",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-basics",
      "graph-theory-fundamentals",
      "numpy-arrays"
    ],
    "topic_tags": [
      "network-analysis",
      "graph-algorithms",
      "social-networks",
      "community-detection",
      "centrality-measures"
    ],
    "summary": "igraph is a powerful network analysis library that provides comprehensive tools for creating, analyzing, and visualizing complex networks. It's widely used by data scientists and researchers for studying social networks, biological systems, and infrastructure networks. The library offers efficient implementations of graph algorithms, centrality measures, and community detection methods.",
    "use_cases": [
      "Analyzing social media networks to identify influential users and community structures",
      "Studying protein interaction networks in computational biology to understand cellular processes"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for network analysis and graph algorithms",
      "how to detect communities in social networks",
      "centrality measures for network analysis in python",
      "igraph vs networkx for graph analysis"
    ]
  },
  {
    "name": "sna",
    "description": "Social network analysis tools including network visualization, centrality measures, and statistical models for network data. Part of the statnet suite for network regression and exponential random graph models.",
    "category": "Network Analysis",
    "docs_url": "https://cran.r-project.org/web/packages/sna/sna.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=sna",
    "install": "install.packages(\"sna\")",
    "tags": [
      "social-networks",
      "network-regression",
      "statnet",
      "ERGM",
      "centrality"
    ],
    "best_for": "Social network analysis and network regression as part of the statnet suite",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "R-programming",
      "graph-theory",
      "statistical-modeling"
    ],
    "topic_tags": [
      "social-networks",
      "network-analysis",
      "ERGM",
      "centrality-measures",
      "R-package"
    ],
    "summary": "The sna package provides comprehensive tools for analyzing social networks in R, including network visualization, centrality calculations, and statistical modeling. It's part of the statnet suite and enables researchers to fit exponential random graph models (ERGMs) to understand network formation processes. The package is widely used in sociology, economics, and organizational research for studying relationships and influence patterns.",
    "use_cases": [
      "Analyzing employee collaboration networks to identify key influencers and communication bottlenecks in organizations",
      "Modeling friendship networks in schools to understand how social ties form and influence academic outcomes"
    ],
    "audience": [
      "Early-PhD",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for social network analysis",
      "how to calculate centrality measures in networks",
      "ERGM exponential random graph models R",
      "statnet suite network regression tools"
    ]
  },
  {
    "name": "tidygraph",
    "description": "Tidy data interface for network/graph data. Extends dplyr verbs to work with nodes and edges, enabling pipe-friendly network manipulation that integrates seamlessly with ggraph for visualization.",
    "category": "Network Analysis",
    "docs_url": "https://tidygraph.data-imaginist.com/",
    "github_url": "https://github.com/thomasp85/tidygraph",
    "url": "https://cran.r-project.org/package=tidygraph",
    "install": "install.packages(\"tidygraph\")",
    "tags": [
      "networks",
      "tidyverse",
      "graph-manipulation",
      "dplyr",
      "pipes"
    ],
    "best_for": "Tidy manipulation of network data with dplyr-style verbs for nodes and edges",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "r-dplyr",
      "ggplot2",
      "basic-graph-theory"
    ],
    "topic_tags": [
      "network-analysis",
      "tidyverse",
      "graph-manipulation",
      "data-wrangling",
      "r-package"
    ],
    "summary": "tidygraph brings the familiar dplyr grammar to network analysis, making it easy to manipulate nodes and edges using pipes and standard tidyverse verbs. It's designed for R users who want to analyze networks without learning specialized graph packages from scratch. The package integrates seamlessly with ggraph for creating publication-ready network visualizations.",
    "use_cases": [
      "Analyzing social network data to identify influential users and community structures",
      "Studying organizational hierarchies and communication patterns in corporate datasets"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "tidyverse approach to network analysis in R",
      "dplyr verbs for graph data manipulation",
      "pipe-friendly network analysis R package",
      "integrate network analysis with ggplot workflow"
    ]
  },
  {
    "name": "Faer",
    "description": "State-of-the-art linear algebra for Rust with Cholesky, QR, SVD decompositions and multithreaded solvers for large systems.",
    "category": "Numerical Optimization & Computational Tools",
    "docs_url": "https://docs.rs/faer",
    "github_url": "https://github.com/sarah-quinones/faer-rs",
    "url": "https://crates.io/crates/faer",
    "install": "cargo add faer",
    "tags": [
      "rust",
      "linear algebra",
      "matrix",
      "performance"
    ],
    "best_for": "High-performance matrix decompositions for custom estimators",
    "language": "Rust",
    "difficulty": "intermediate",
    "prerequisites": [
      "rust-programming",
      "linear-algebra-fundamentals",
      "matrix-decomposition"
    ],
    "topic_tags": [
      "rust-linear-algebra",
      "matrix-decomposition",
      "high-performance-computing",
      "numerical-methods",
      "optimization-solvers"
    ],
    "summary": "Faer is a high-performance linear algebra library for Rust that provides state-of-the-art implementations of matrix decompositions like Cholesky, QR, and SVD. It features multithreaded solvers optimized for large-scale systems, making it ideal for computationally intensive applications requiring maximum performance. The library is designed for Rust developers who need fast, memory-safe linear algebra operations without sacrificing speed.",
    "use_cases": [
      "Building high-frequency trading systems that require ultra-fast matrix operations for portfolio optimization",
      "Developing machine learning inference engines in Rust that need efficient linear algebra for real-time predictions"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "rust linear algebra library high performance",
      "fast matrix decomposition rust cholesky QR SVD",
      "multithreaded linear algebra solver rust",
      "rust alternative to numpy scipy linear algebra"
    ]
  },
  {
    "name": "JAX",
    "description": "High-performance numerical computing with autograd and XLA compilation on CPU/GPU/TPU.",
    "category": "Numerical Optimization & Computational Tools",
    "docs_url": "https://jax.readthedocs.io/",
    "github_url": "https://github.com/google/jax",
    "url": "https://github.com/google/jax",
    "install": "pip install jax",
    "tags": [
      "optimization",
      "computation"
    ],
    "best_for": "Solving optimization problems, numerical methods",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-numpy",
      "automatic-differentiation",
      "linear-algebra"
    ],
    "topic_tags": [
      "automatic-differentiation",
      "neural-networks",
      "gpu-computing",
      "scientific-computing",
      "jit-compilation"
    ],
    "summary": "JAX is a NumPy-compatible library for high-performance machine learning research that combines automatic differentiation with XLA compilation for CPUs, GPUs, and TPUs. It enables functional programming patterns for neural networks and scientific computing with just-in-time compilation for speed. Popular in research settings for implementing custom models and optimization algorithms that need to scale across accelerators.",
    "use_cases": [
      "Training custom neural network architectures with automatic gradient computation",
      "Implementing scientific computing simulations that need GPU acceleration"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "fast NumPy alternative for GPU computing",
      "automatic differentiation library for machine learning",
      "JAX vs PyTorch for research",
      "JIT compilation for scientific computing Python"
    ]
  },
  {
    "name": "Nalgebra",
    "description": "General-purpose linear algebra library for Rust with dense and sparse matrices, widely used in graphics and physics.",
    "category": "Numerical Optimization & Computational Tools",
    "docs_url": "https://www.nalgebra.org/",
    "github_url": "https://github.com/dimforge/nalgebra",
    "url": "https://crates.io/crates/nalgebra",
    "install": "cargo add nalgebra",
    "tags": [
      "rust",
      "linear algebra",
      "matrix",
      "sparse"
    ],
    "best_for": "General-purpose linear algebra in Rust",
    "language": "Rust",
    "difficulty": "intermediate",
    "prerequisites": [
      "rust-programming",
      "matrix-operations",
      "linear-algebra-fundamentals"
    ],
    "topic_tags": [
      "linear-algebra",
      "rust-libraries",
      "matrix-computation",
      "numerical-methods",
      "sparse-matrices"
    ],
    "summary": "Nalgebra is a comprehensive linear algebra library for Rust that provides efficient implementations of dense and sparse matrix operations. It's particularly popular in graphics programming, physics simulations, and scientific computing applications where performance and memory safety are critical. The library offers a clean API for vector spaces, transformations, and decompositions commonly needed in computational work.",
    "use_cases": [
      "Building 3D graphics engines requiring fast matrix transformations and geometric operations",
      "Implementing physics simulations with large sparse systems for finite element analysis"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "rust linear algebra library for matrix operations",
      "nalgebra vs numpy performance comparison",
      "sparse matrix computation in rust",
      "best rust library for scientific computing linear algebra"
    ]
  },
  {
    "name": "Ndarray",
    "description": "N-dimensional array library for Rust\u2014the NumPy equivalent with slicing, broadcasting, and BLAS/LAPACK integration.",
    "category": "Numerical Optimization & Computational Tools",
    "docs_url": "https://docs.rs/ndarray",
    "github_url": "https://github.com/rust-ndarray/ndarray",
    "url": "https://crates.io/crates/ndarray",
    "install": "cargo add ndarray",
    "tags": [
      "rust",
      "arrays",
      "numpy",
      "scientific computing"
    ],
    "best_for": "NumPy-style N-dimensional arrays in Rust",
    "language": "Rust",
    "difficulty": "intermediate",
    "prerequisites": [
      "rust-programming",
      "numpy-arrays",
      "linear-algebra-basics"
    ],
    "topic_tags": [
      "rust",
      "numerical-computing",
      "arrays",
      "scientific-computing",
      "performance"
    ],
    "summary": "Ndarray is Rust's equivalent to NumPy, providing efficient n-dimensional arrays with slicing, broadcasting, and mathematical operations. It enables high-performance scientific computing in Rust with BLAS/LAPACK integration for linear algebra operations. This library is essential for data scientists and researchers who need NumPy-like functionality but want Rust's memory safety and performance guarantees.",
    "use_cases": [
      "Building high-performance machine learning algorithms in Rust that require matrix operations and array manipulations",
      "Developing numerical simulation systems where memory safety and speed are critical requirements"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "numpy equivalent for rust",
      "rust scientific computing arrays",
      "high performance matrix operations rust",
      "ndarray vs numpy rust"
    ]
  },
  {
    "name": "PyTorch",
    "description": "Popular deep learning framework with flexible automatic differentiation.",
    "category": "Numerical Optimization & Computational Tools",
    "docs_url": "https://pytorch.org/",
    "github_url": "https://github.com/pytorch/pytorch",
    "url": "https://github.com/pytorch/pytorch",
    "install": "(See PyTorch website)",
    "tags": [
      "optimization",
      "computation",
      "machine learning"
    ],
    "best_for": "Solving optimization problems, numerical methods",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-programming",
      "linear-algebra",
      "numpy-arrays"
    ],
    "topic_tags": [
      "deep-learning",
      "neural-networks",
      "automatic-differentiation",
      "gpu-computing",
      "python-framework"
    ],
    "summary": "PyTorch is a flexible deep learning framework that provides automatic differentiation and dynamic computation graphs. It's widely used by researchers and practitioners for building and training neural networks, from simple models to complex architectures. The framework offers intuitive Python APIs and strong GPU acceleration for efficient model development.",
    "use_cases": [
      "Building and training recommendation systems for user behavior prediction",
      "Developing computer vision models for image classification or object detection"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What deep learning framework should I use for my first neural network project",
      "PyTorch vs TensorFlow for recommendation systems",
      "How to implement gradient descent with automatic differentiation",
      "Best Python library for training deep learning models"
    ]
  },
  {
    "name": "jaxonometrics",
    "description": "JAX-ecosystem implementations of standard econometrics routines for GPU computation.",
    "category": "Numerical Optimization & Computational Tools",
    "docs_url": null,
    "github_url": "https://github.com/py-econometrics/jaxonometrics",
    "url": "https://github.com/py-econometrics/jaxonometrics",
    "install": "GitHub Repository",
    "tags": [
      "optimization",
      "JAX",
      "GPU"
    ],
    "best_for": "GPU-accelerated econometrics with JAX",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "econometrics-fundamentals",
      "python-numpy",
      "linear-regression"
    ],
    "topic_tags": [
      "econometrics",
      "gpu-computing",
      "jax",
      "numerical-methods",
      "computational-economics"
    ],
    "summary": "JAX-based econometrics package that accelerates standard econometric computations using GPU parallelization. Provides drop-in replacements for common econometric estimators with automatic differentiation and vectorization capabilities. Particularly useful for researchers running large-scale economic analyses or Monte Carlo simulations.",
    "use_cases": [
      "Running IV regression on millions of observations with GPU acceleration",
      "Performing bootstrap inference for difference-in-differences with thousands of replications"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "GPU econometrics package Python",
      "JAX implementation econometric methods",
      "fast econometrics computation GPU",
      "accelerated regression analysis JAX"
    ]
  },
  {
    "name": "torchonometrics",
    "description": "Econometrics implementations in PyTorch. Leverages autodiff and GPU acceleration for econometric methods.",
    "category": "Numerical Optimization & Computational Tools",
    "docs_url": null,
    "github_url": "https://github.com/apoorvalal/torchonometrics",
    "url": "https://github.com/apoorvalal/torchonometrics",
    "install": "GitHub Repository",
    "tags": [
      "optimization",
      "computation",
      "PyTorch"
    ],
    "best_for": "GPU-accelerated econometrics with PyTorch autodiff",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "pytorch-tensors",
      "linear-regression",
      "maximum-likelihood-estimation"
    ],
    "topic_tags": [
      "econometrics",
      "pytorch",
      "gpu-acceleration",
      "autodiff",
      "causal-inference"
    ],
    "summary": "Torchonometrics brings classical econometric methods into the PyTorch ecosystem, enabling GPU acceleration and automatic differentiation for econometric estimation. It's particularly valuable for researchers and practitioners who need to scale econometric models to large datasets or integrate them with deep learning workflows. The package maintains econometric rigor while leveraging modern computational tools.",
    "use_cases": [
      "Estimating instrumental variables models on million-row datasets with GPU acceleration",
      "Building hybrid models that combine econometric identification strategies with neural network components"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "econometrics in pytorch",
      "gpu accelerated causal inference",
      "pytorch econometric models",
      "autodiff for econometrics"
    ]
  },
  {
    "name": "Argmin",
    "description": "Numerical optimization framework for Rust with Newton, BFGS, L-BFGS, trust region, and derivative-free methods for MLE/GMM.",
    "category": "Optimization",
    "docs_url": "https://docs.rs/argmin",
    "github_url": "https://github.com/argmin-rs/argmin",
    "url": "https://crates.io/crates/argmin",
    "install": "cargo add argmin",
    "tags": [
      "rust",
      "optimization",
      "BFGS",
      "MLE",
      "GMM"
    ],
    "best_for": "Maximum Likelihood and GMM estimation in Rust",
    "language": "Rust",
    "difficulty": "intermediate",
    "prerequisites": [
      "rust-programming",
      "numerical-optimization",
      "maximum-likelihood-estimation"
    ],
    "topic_tags": [
      "numerical-optimization",
      "rust-library",
      "BFGS",
      "trust-region",
      "maximum-likelihood"
    ],
    "summary": "Argmin is a comprehensive numerical optimization framework for Rust that implements classical algorithms like BFGS, L-BFGS, Newton methods, and trust region approaches. It's designed for researchers and engineers who need efficient optimization routines for maximum likelihood estimation and generalized method of moments in Rust applications. The library provides both gradient-based and derivative-free optimization methods with a unified interface.",
    "use_cases": [
      "Estimating parameters in econometric models using maximum likelihood estimation in Rust applications",
      "Optimizing objective functions in high-performance computational economics research where Rust's speed is critical"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "Rust optimization library for MLE",
      "BFGS implementation in Rust",
      "numerical optimization framework Rust",
      "Rust library for maximum likelihood estimation"
    ]
  },
  {
    "name": "cvxpy",
    "description": "Domain-specific language for convex optimization problems. Write math as code \u2014 the standard for convex problems.",
    "category": "Optimization",
    "docs_url": "https://www.cvxpy.org/",
    "github_url": "https://github.com/cvxpy/cvxpy",
    "url": "https://www.cvxpy.org/",
    "install": "pip install cvxpy",
    "tags": [
      "convex optimization",
      "linear programming",
      "quadratic programming"
    ],
    "best_for": "Convex optimization with intuitive syntax",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-numpy",
      "linear-algebra",
      "mathematical-optimization"
    ],
    "topic_tags": [
      "convex-optimization",
      "linear-programming",
      "quadratic-programming",
      "mathematical-modeling",
      "python-package"
    ],
    "summary": "CVXPY is a Python library that lets you write convex optimization problems using intuitive mathematical syntax that closely mirrors how you'd write them on paper. It's the go-to tool for data scientists and researchers who need to solve portfolio optimization, resource allocation, or machine learning problems with constraints. The library automatically transforms your problem formulation into the appropriate solver format and handles the computational details.",
    "use_cases": [
      "Portfolio optimization with risk constraints and transaction costs",
      "Machine learning model training with regularization and fairness constraints"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python convex optimization library",
      "how to solve linear programming in python",
      "cvxpy vs scipy optimize",
      "portfolio optimization python package"
    ]
  },
  {
    "name": "gurobipy",
    "description": "Python interface for Gurobi, the best-in-class commercial solver. LP, QP, MIP, and MIQP.",
    "category": "Optimization",
    "docs_url": "https://www.gurobi.com/documentation/",
    "github_url": null,
    "url": "https://www.gurobi.com/",
    "install": "pip install gurobipy",
    "tags": [
      "optimization",
      "solver",
      "MIP",
      "commercial"
    ],
    "best_for": "Best-in-class solver \u2014 free for academics",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "linear-programming",
      "python-optimization",
      "mixed-integer-programming"
    ],
    "topic_tags": [
      "commercial-solver",
      "linear-programming",
      "mixed-integer-programming",
      "operations-research",
      "python-optimization"
    ],
    "summary": "Gurobipy is the Python interface for Gurobi Optimizer, a premium commercial solver for mathematical optimization problems. It's widely regarded as the fastest and most reliable solver for linear programming (LP), quadratic programming (QP), and mixed-integer programming (MIP) problems. Tech economists and data scientists use it for complex optimization tasks where performance and solution quality are critical.",
    "use_cases": [
      "Solving large-scale supply chain optimization problems with thousands of constraints and variables",
      "Implementing auction mechanisms and market design algorithms that require exact optimal solutions"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "best commercial optimization solver for Python",
      "how to solve mixed integer programming problems in Python",
      "Gurobi vs open source solvers for large optimization problems",
      "Python package for linear programming with commercial license"
    ]
  },
  {
    "name": "ortools",
    "description": "Google's operations research toolkit. Constraint programming, routing, linear/integer programming, and scheduling.",
    "category": "Optimization",
    "docs_url": "https://developers.google.com/optimization",
    "github_url": "https://github.com/google/or-tools",
    "url": "https://developers.google.com/optimization",
    "install": "pip install ortools",
    "tags": [
      "OR",
      "routing",
      "scheduling",
      "constraint programming"
    ],
    "best_for": "Production-ready combinatorial optimization",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-programming",
      "linear-algebra",
      "mathematical-optimization"
    ],
    "topic_tags": [
      "operations-research",
      "constraint-programming",
      "vehicle-routing",
      "linear-programming",
      "scheduling-optimization"
    ],
    "summary": "Google's OR-Tools is a comprehensive optimization library that provides solvers for constraint programming, routing problems, and linear/integer programming. It's widely used by data scientists and operations researchers to solve complex scheduling, routing, and resource allocation problems. The toolkit offers both high-level APIs and low-level solver access for various optimization challenges.",
    "use_cases": [
      "Optimizing delivery routes for logistics companies with constraints like vehicle capacity and time windows",
      "Scheduling employees or resources across shifts while satisfying coverage requirements and work regulations"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "Google OR-Tools constraint programming tutorial",
      "vehicle routing problem solver Python",
      "employee scheduling optimization library",
      "linear programming solver comparison OR-Tools"
    ]
  },
  {
    "name": "scipy.optimize",
    "description": "Optimization algorithms built into SciPy. Minimization, root finding, curve fitting, and linear programming.",
    "category": "Optimization",
    "docs_url": "https://docs.scipy.org/doc/scipy/reference/optimize.html",
    "github_url": "https://github.com/scipy/scipy",
    "url": "https://docs.scipy.org/doc/scipy/reference/optimize.html",
    "install": "pip install scipy",
    "tags": [
      "optimization",
      "minimization",
      "root finding"
    ],
    "best_for": "General-purpose optimization \u2014 start here for basics",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-numpy",
      "calculus-derivatives",
      "linear-algebra"
    ],
    "topic_tags": [
      "optimization",
      "scipy",
      "numerical-methods",
      "curve-fitting",
      "linear-programming"
    ],
    "summary": "SciPy's optimization module provides a comprehensive suite of algorithms for finding optimal solutions to mathematical problems. It's widely used by data scientists and researchers for parameter estimation, model fitting, and solving constrained optimization problems. The package offers user-friendly interfaces to powerful numerical methods without requiring deep mathematical expertise.",
    "use_cases": [
      "Fitting machine learning model hyperparameters using grid search or gradient-based methods",
      "Estimating parameters for econometric models by minimizing loss functions"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python optimization library for parameter fitting",
      "scipy minimize function tutorial",
      "how to do curve fitting in python",
      "linear programming solver python"
    ]
  },
  {
    "name": "FixedEffectModel",
    "description": "Panel data modeling with IV tests (weak IV, over-identification, endogeneity) and 2-step GMM estimation.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": null,
    "github_url": "https://github.com/ksecology/FixedEffectModel",
    "url": "https://github.com/ksecology/FixedEffectModel",
    "install": "pip install FixedEffectModel",
    "tags": [
      "panel data",
      "fixed effects",
      "IV"
    ],
    "best_for": "Panel regression with comprehensive IV diagnostics",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "linear-regression",
      "instrumental-variables"
    ],
    "topic_tags": [
      "panel-data",
      "fixed-effects",
      "instrumental-variables",
      "gmm-estimation",
      "causal-inference"
    ],
    "summary": "A Python package for panel data analysis with fixed effects modeling and comprehensive IV diagnostics. Provides econometric tools for handling endogeneity through instrumental variables and two-step GMM estimation. Essential for economists and data scientists working with longitudinal data where unobserved heterogeneity is a concern.",
    "use_cases": [
      "Analyzing the effect of policy changes on firm outcomes using company panel data with time-invariant unobserved characteristics",
      "Estimating causal effects of education on wages using individual longitudinal data with endogenous schooling decisions"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "panel data fixed effects python package",
      "IV tests weak instruments overidentification",
      "GMM estimation panel data econometrics",
      "fixed effects model endogeneity testing"
    ]
  },
  {
    "name": "FixedEffectModelPyHDFE",
    "description": "Solves linear models with high-dimensional fixed effects, supporting robust variance calculation and IV.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://pypi.org/project/FixedEffectModelPyHDFE/",
    "github_url": null,
    "url": "https://pypi.org/project/FixedEffectModelPyHDFE/",
    "install": "pip install FixedEffectModelPyHDFE",
    "tags": [
      "panel data",
      "fixed effects"
    ],
    "best_for": "Longitudinal analysis, controlling for unobserved heterogeneity",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "linear-regression",
      "econometric-theory"
    ],
    "topic_tags": [
      "fixed-effects",
      "panel-data",
      "high-dimensional",
      "instrumental-variables",
      "econometrics"
    ],
    "summary": "FixedEffectModelPyHDFE is a Python implementation for estimating linear models with high-dimensional fixed effects, commonly used in econometric analysis. It handles large categorical variables efficiently and supports robust standard errors and instrumental variable estimation. This package is essential for economists and data scientists working with panel data where controlling for unobserved heterogeneity is critical.",
    "use_cases": [
      "Estimating causal effects in worker-firm matched data with both worker and firm fixed effects",
      "Analyzing product pricing across markets while controlling for time-varying product and market characteristics"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "Python package for high dimensional fixed effects regression",
      "How to estimate two-way fixed effects model in Python",
      "FixedEffectModelPyHDFE vs Stata reghdfe equivalent",
      "Panel data regression with firm and time fixed effects Python"
    ]
  },
  {
    "name": "Linearmodels",
    "description": "Estimation of fixed, random, pooled OLS models for panel data. Also Fama-MacBeth and between/first-difference estimators.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://bashtage.github.io/linearmodels/",
    "github_url": "https://github.com/bashtage/linearmodels",
    "url": "https://github.com/bashtage/linearmodels",
    "install": "pip install linearmodels",
    "tags": [
      "panel data",
      "fixed effects"
    ],
    "best_for": "Longitudinal analysis, controlling for unobserved heterogeneity",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "linear-regression",
      "econometrics-basics"
    ],
    "topic_tags": [
      "panel-data",
      "fixed-effects",
      "fama-macbeth",
      "econometrics",
      "causal-inference"
    ],
    "summary": "Linearmodels is a Python package for econometric analysis of panel data, offering comprehensive tools for fixed effects, random effects, and pooled OLS estimation. It includes specialized estimators like Fama-MacBeth for finance applications and between/first-difference methods for causal inference. The package is designed for economists and data scientists working with longitudinal data who need robust statistical methods beyond basic regression.",
    "use_cases": [
      "Analyzing employee productivity across companies over time while controlling for unobserved firm characteristics",
      "Estimating price elasticity effects in A/B tests across different user segments and time periods"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "python package for panel data fixed effects",
      "how to run fama macbeth regression python",
      "panel data econometrics library python",
      "fixed effects estimation longitudinal data"
    ]
  },
  {
    "name": "PyFixest",
    "description": "Fast estimation of linear models with multiple high-dimensional fixed effects (like R's `fixest`). Supports OLS, IV, Poisson, robust/cluster SEs.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://github.com/py-econometrics/pyfixest",
    "github_url": null,
    "url": "https://github.com/py-econometrics/pyfixest",
    "install": "pip install pyfixest",
    "tags": [
      "panel data",
      "fixed effects"
    ],
    "best_for": "Longitudinal analysis, controlling for unobserved heterogeneity",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "linear-regression",
      "econometric-modeling"
    ],
    "topic_tags": [
      "fixed-effects",
      "panel-data",
      "econometrics",
      "causal-inference",
      "python-package"
    ],
    "summary": "PyFixest is a Python package for fast estimation of linear models with high-dimensional fixed effects, porting R's popular fixest functionality. It enables economists and data scientists to efficiently run OLS, IV, and Poisson regressions with multiple fixed effects and robust standard errors. The package is particularly valuable for causal inference work requiring control for unobserved heterogeneity across entities or time periods.",
    "use_cases": [
      "Estimating treatment effects in difference-in-differences studies with firm and time fixed effects",
      "Running wage regressions controlling for worker and employer fixed effects in matched employer-employee data"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "python package for fixed effects regression",
      "PyFixest vs R fixest equivalent",
      "how to estimate panel data models in python",
      "high dimensional fixed effects python"
    ]
  },
  {
    "name": "alpaca",
    "description": "Fits generalized linear models (Poisson, negative binomial, logit, probit, Gamma) with high-dimensional k-way fixed effects. Partials out factors during log-likelihood optimization and provides robust/multi-way clustered standard errors, fixed effects recovery, and analytical bias corrections for binary choice models.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://cran.r-project.org/web/packages/alpaca/vignettes/howto.html",
    "github_url": "https://github.com/amrei-stammann/alpaca",
    "url": "https://cran.r-project.org/package=alpaca",
    "install": "install.packages(\"alpaca\")",
    "tags": [
      "glm",
      "fixed-effects",
      "poisson-regression",
      "negative-binomial",
      "gravity-models"
    ],
    "best_for": "Nonlinear panel models (Poisson, logit, probit, negative binomial) with multiple high-dimensional fixed effects, especially structural gravity models for international trade",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "generalized-linear-models",
      "panel-data-methods",
      "econometric-clustering"
    ],
    "topic_tags": [
      "fixed-effects",
      "high-dimensional-econometrics",
      "gravity-models",
      "bias-correction",
      "clustered-standard-errors"
    ],
    "summary": "Alpaca is a specialized package for fitting generalized linear models with high-dimensional fixed effects, particularly useful for gravity models and panel data with many categorical variables. It efficiently handles computational challenges by partialing out fixed effects during optimization and provides robust inference tools. The package is designed for econometric applications requiring both flexible modeling and reliable standard error estimation.",
    "use_cases": [
      "Estimating gravity models for international trade with country-pair and time fixed effects",
      "Analyzing firm-level productivity with multi-way fixed effects for industry, location, and year"
    ],
    "audience": [
      "Early-PhD",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "high dimensional fixed effects GLM package",
      "gravity model estimation with fixed effects",
      "Poisson regression with multiple fixed effects",
      "bias correction for logit models with fixed effects"
    ]
  },
  {
    "name": "bife",
    "description": "Estimates fixed effects binary choice models (logit and probit) with potentially many individual fixed effects using a pseudo-demeaning algorithm. Addresses the incidental parameters problem through analytical bias correction based on Fern\u00e1ndez-Val (2009) and computes average partial effects.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://cran.r-project.org/web/packages/bife/vignettes/howto.html",
    "github_url": "https://github.com/amrei-stammann/bife",
    "url": "https://cran.r-project.org/package=bife",
    "install": "install.packages(\"bife\")",
    "tags": [
      "binary-choice",
      "fixed-effects",
      "logit-probit",
      "bias-correction",
      "panel-data"
    ],
    "best_for": "Fast estimation of fixed effects logit/probit models on large panel data with analytical bias correction for the incidental parameters problem",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "panel-data-methods",
      "logistic-regression",
      "R-programming"
    ],
    "topic_tags": [
      "binary-choice-models",
      "fixed-effects-estimation",
      "bias-correction",
      "panel-econometrics"
    ],
    "summary": "The bife package estimates binary choice models (logit/probit) with individual fixed effects using a pseudo-demeaning algorithm that handles many fixed effects efficiently. It addresses the incidental parameters problem through analytical bias correction and computes average partial effects. This is essential for researchers working with panel data where individual heterogeneity needs to be controlled for in binary outcome models.",
    "use_cases": [
      "Analyzing employee promotion decisions across firms while controlling for unobserved worker characteristics",
      "Studying customer purchase behavior over time while accounting for individual preferences and habits"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "fixed effects logit panel data R",
      "binary choice model with individual fixed effects",
      "how to handle incidental parameters problem logit",
      "bife package bias correction binary outcomes"
    ]
  },
  {
    "name": "duckreg",
    "description": "Out-of-core regression (OLS/IV) for very large datasets using DuckDB aggregation. Handles data that doesn't fit in memory.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://github.com/py-econometrics/duckreg",
    "github_url": null,
    "url": "https://github.com/py-econometrics/duckreg",
    "install": "pip install duckreg",
    "tags": [
      "panel data",
      "fixed effects"
    ],
    "best_for": "Longitudinal analysis, controlling for unobserved heterogeneity",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "SQL-queries",
      "linear-regression"
    ],
    "topic_tags": [
      "out-of-core-computing",
      "large-datasets",
      "regression-analysis",
      "duckdb",
      "memory-optimization"
    ],
    "summary": "Duckreg enables regression analysis on datasets too large to fit in memory by leveraging DuckDB's efficient aggregation capabilities. It supports both OLS and instrumental variables estimation through out-of-core computation, making big data econometrics accessible without requiring distributed computing infrastructure. The package is particularly valuable for researchers working with administrative datasets or large-scale observational data.",
    "use_cases": [
      "Analyzing multi-year administrative datasets with millions of observations for causal inference studies",
      "Running fixed effects regressions on large transaction or user behavior datasets that exceed RAM capacity"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "regression on datasets too big for memory",
      "out of core OLS estimation python",
      "how to run fixed effects on large datasets",
      "duckdb regression analysis large data"
    ]
  },
  {
    "name": "fixest",
    "description": "Fast and comprehensive package for estimating econometric models with multiple high-dimensional fixed effects, including OLS, GLM, Poisson, and negative binomial models. Features native support for clustered standard errors (up to four-way), instrumental variables, and modern difference-in-differences estimators including Sun-Abraham for staggered treatments.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://lrberge.github.io/fixest/",
    "github_url": "https://github.com/lrberge/fixest",
    "url": "https://cran.r-project.org/package=fixest",
    "install": "install.packages(\"fixest\")",
    "tags": [
      "fixed-effects",
      "panel-data",
      "clustered-standard-errors",
      "difference-in-differences",
      "instrumental-variables"
    ],
    "best_for": "Fast, production-ready estimation of linear/GLM models with multiple high-dimensional fixed effects and publication-quality regression tables via etable()",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "regression-analysis",
      "panel-data-concepts",
      "difference-in-differences"
    ],
    "topic_tags": [
      "fixed-effects",
      "panel-data",
      "clustered-standard-errors",
      "difference-in-differences",
      "R-package"
    ],
    "summary": "fixest is a high-performance R package for estimating econometric models with multiple high-dimensional fixed effects, supporting OLS, GLM, Poisson, and negative binomial models. It provides native support for clustered standard errors, instrumental variables, and modern difference-in-differences estimators including Sun-Abraham for staggered treatments. The package is optimized for speed and handles large datasets efficiently while offering comprehensive econometric functionality.",
    "use_cases": [
      "Analyzing the impact of policy changes across different states and time periods using difference-in-differences with staggered treatment timing",
      "Estimating demand elasticity from transaction data with customer and product fixed effects while clustering standard errors at the market level"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "fast R package for fixed effects regression",
      "how to handle staggered difference in differences",
      "R package for clustered standard errors",
      "fixest vs felm performance comparison"
    ]
  },
  {
    "name": "lfe",
    "description": "Efficiently estimates linear models with multiple high-dimensional fixed effects using the Method of Alternating Projections. Designed for datasets with factors having thousands of levels (hundreds of thousands of dummy variables), with full support for 2SLS instrumental variables and multi-way clustered standard errors.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://cran.r-project.org/web/packages/lfe/lfe.pdf",
    "github_url": "https://github.com/r-econometrics/lfe",
    "url": "https://cran.r-project.org/package=lfe",
    "install": "install.packages(\"lfe\")",
    "tags": [
      "high-dimensional-fe",
      "worker-firm",
      "memory-efficient",
      "instrumental-variables",
      "clustered-se"
    ],
    "best_for": "AKM-style wage decompositions and matched employer-employee data with hundreds of thousands of worker/firm fixed effects",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "linear-regression",
      "R-programming",
      "panel-data-structure"
    ],
    "topic_tags": [
      "fixed-effects",
      "high-dimensional-data",
      "panel-econometrics",
      "R-package",
      "instrumental-variables"
    ],
    "summary": "The lfe package provides memory-efficient estimation of linear models with multiple high-dimensional fixed effects using alternating projections. It's designed for econometric analysis with datasets containing factors with thousands of levels, such as worker-firm matched data or multi-dimensional panel studies. The package supports instrumental variables and multi-way clustered standard errors for robust inference.",
    "use_cases": [
      "analyzing wage effects in worker-firm matched datasets with thousands of companies and workers",
      "estimating treatment effects in multi-dimensional panel data with firm, time, and industry fixed effects"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "R package for high dimensional fixed effects",
      "how to estimate worker firm fixed effects efficiently",
      "lfe package tutorial panel data",
      "multi-way clustered standard errors R"
    ]
  },
  {
    "name": "panelhetero",
    "description": "Heterogeneity analysis across units in panel data. Detects and characterizes unit-level variation.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://github.com/tkhdyanagi/panelhetero",
    "github_url": "https://github.com/tkhdyanagi/panelhetero",
    "url": "https://github.com/tkhdyanagi/panelhetero",
    "install": "pip install panelhetero",
    "tags": [
      "panel data",
      "heterogeneity",
      "unit effects"
    ],
    "best_for": "Unit heterogeneity in panels",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "fixed-effects-regression",
      "panel-data-structure"
    ],
    "topic_tags": [
      "panel-data",
      "heterogeneity-analysis",
      "unit-effects",
      "econometrics",
      "python-package"
    ],
    "summary": "A Python package for analyzing heterogeneity across units in panel datasets, helping researchers detect and characterize unit-level variation beyond standard fixed effects. It provides tools to identify subgroups of units with similar behavior patterns and quantify the extent of heterogeneous treatment effects or parameter variation across panel units.",
    "use_cases": [
      "Analyzing whether a marketing campaign has different effects across geographic regions using sales panel data",
      "Detecting heterogeneous responses to policy changes across firms in longitudinal business datasets"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "how to detect heterogeneity in panel data",
      "python package for unit-level variation analysis",
      "analyzing different effects across panel units",
      "heterogeneous treatment effects in longitudinal data"
    ]
  },
  {
    "name": "panelr",
    "description": "Automates within-between (hybrid) model specification for panel/longitudinal data, combining fixed effects robustness to time-invariant confounding with random effects ability to estimate time-invariant coefficients. Uses lme4 for multilevel estimation with optional Bayesian (brms) and GEE (geepack) backends.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://panelr.jacob-long.com/",
    "github_url": "https://github.com/jacob-long/panelr",
    "url": "https://cran.r-project.org/package=panelr",
    "install": "install.packages(\"panelr\")",
    "tags": [
      "hybrid-models",
      "within-between",
      "panel-data",
      "longitudinal-analysis",
      "bell-jones"
    ],
    "best_for": "Researchers needing fixed effects-equivalent estimates while retaining time-invariant predictors and random slopes",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "panel-data-concepts",
      "lme4-package",
      "fixed-effects-models"
    ],
    "topic_tags": [
      "hybrid-models",
      "within-between",
      "panel-data",
      "longitudinal-analysis",
      "multilevel-modeling"
    ],
    "summary": "panelr automates the specification of within-between (hybrid) models for panel data analysis, combining the benefits of fixed effects and random effects approaches. It handles the complex data transformations needed to estimate both time-varying and time-invariant effects simultaneously. The package supports multiple estimation backends including lme4, brms, and geepack for different modeling needs.",
    "use_cases": [
      "Analyzing employee productivity over time while controlling for unobserved worker characteristics and estimating effects of education",
      "Studying firm performance across quarters while accounting for industry fixed effects and measuring impact of CEO characteristics"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "within between model R package",
      "hybrid panel data models automation",
      "estimate time invariant effects with fixed effects",
      "panelr vs plm for panel data"
    ]
  },
  {
    "name": "plm",
    "description": "Comprehensive econometrics package for linear panel models providing fixed effects (within), random effects, between, first-difference, Hausman-Taylor, and nested random effects estimators. Includes GMM, FGLS, and extensive diagnostic tests for serial correlation, cross-sectional dependence, and panel unit roots.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://cran.r-project.org/web/packages/plm/vignettes/",
    "github_url": "https://github.com/ycroissant/plm",
    "url": "https://cran.r-project.org/package=plm",
    "install": "install.packages(\"plm\")",
    "tags": [
      "panel-data",
      "econometrics",
      "fixed-effects",
      "random-effects",
      "hausman-test"
    ],
    "best_for": "Comprehensive panel data analysis requiring within/between/random effects estimation, Hausman tests, and extensive diagnostic testing",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "linear-regression",
      "hypothesis-testing"
    ],
    "topic_tags": [
      "panel-data",
      "fixed-effects",
      "econometrics",
      "causal-inference",
      "python-package"
    ],
    "summary": "The plm package is a comprehensive Python library for estimating linear panel data models with multiple econometric estimators. It provides tools for fixed effects, random effects, and advanced methods like Hausman-Taylor and GMM estimation. Essential for economists and data scientists working with longitudinal data who need to control for unobserved heterogeneity and perform rigorous causal inference.",
    "use_cases": [
      "Analyzing the effect of policy changes across states over time while controlling for state-specific unobserved factors",
      "Estimating the impact of company characteristics on firm performance using multi-year corporate panel data"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "python package for panel data fixed effects",
      "how to run Hausman test for panel data",
      "plm vs statsmodels for longitudinal analysis",
      "panel data econometrics library python"
    ]
  },
  {
    "name": "pydynpd",
    "description": "Estimation of dynamic panel data models using Arellano-Bond (Difference GMM) and Blundell-Bond (System GMM). Includes Windmeijer correction & tests.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://doi.org/10.21105/joss.04416",
    "github_url": "https://github.com/dazhwu/pydynpd",
    "url": "https://github.com/dazhwu/pydynpd",
    "install": "pip install pydynpd",
    "tags": [
      "panel data",
      "fixed effects"
    ],
    "best_for": "Longitudinal analysis, controlling for unobserved heterogeneity",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "python-pandas",
      "panel-data-econometrics",
      "instrumental-variables"
    ],
    "topic_tags": [
      "dynamic-panel-data",
      "GMM-estimation",
      "arellano-bond",
      "econometric-methods",
      "python-package"
    ],
    "summary": "A Python package for estimating dynamic panel data models using advanced econometric methods including Arellano-Bond (Difference GMM) and Blundell-Bond (System GMM) estimators. It addresses endogeneity issues in panel data where lagged dependent variables are included as regressors. The package includes the Windmeijer finite-sample correction and diagnostic tests for model specification.",
    "use_cases": [
      "Estimating firm productivity dynamics where current performance depends on past performance and unobserved firm characteristics",
      "Analyzing how past government spending affects current economic growth across countries while controlling for country-specific factors"
    ],
    "audience": [
      "Early-PhD",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python package dynamic panel data GMM",
      "arellano bond estimation python",
      "how to estimate dynamic panel models with endogeneity",
      "blundell bond system GMM implementation"
    ]
  },
  {
    "name": "WebPower",
    "description": "Comprehensive collection of tools for basic and advanced statistical power analysis including correlation, t-test, ANOVA, regression, mediation analysis, structural equation modeling (SEM), and multilevel models. Features both R package and web interface.",
    "category": "Power Analysis",
    "docs_url": "https://webpower.psychstat.org/",
    "github_url": "https://github.com/johnnyzhz/WebPower",
    "url": "https://cran.r-project.org/package=WebPower",
    "install": "install.packages(\"WebPower\")",
    "tags": [
      "power-analysis",
      "SEM",
      "mediation",
      "multilevel-models",
      "cluster-randomized-trials"
    ],
    "best_for": "Advanced power analysis for SEM, mediation, and cluster randomized trials, implementing Zhang & Yuan (2018)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "statistical-hypothesis-testing",
      "R-programming",
      "experimental-design"
    ],
    "topic_tags": [
      "power-analysis",
      "experimental-design",
      "SEM",
      "mediation-analysis",
      "statistical-software"
    ],
    "summary": "WebPower is a comprehensive R package and web application for statistical power analysis across various experimental designs and statistical models. It enables researchers to calculate sample sizes, detect effect sizes, and plan studies for everything from basic t-tests to complex structural equation models and multilevel designs. The tool is essential for proper experimental planning and ensuring studies have adequate statistical power to detect meaningful effects.",
    "use_cases": [
      "Planning sample size for a randomized controlled trial testing the effect of a new product feature on user engagement",
      "Determining statistical power for a mediation analysis examining how workplace training affects performance through employee motivation"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "How to calculate sample size for A/B test power analysis",
      "Statistical power analysis for mediation models",
      "Power analysis tools for multilevel experimental designs",
      "R package for SEM power calculations"
    ]
  },
  {
    "name": "pwr",
    "description": "Provides basic power calculations using effect sizes and notation from Cohen (1988). Supports t-tests, chi-squared tests, one-way ANOVA, correlation tests, proportion tests, and general linear models with analytical (closed-form) solutions.",
    "category": "Power Analysis",
    "docs_url": "https://cran.r-project.org/web/packages/pwr/pwr.pdf",
    "github_url": "https://github.com/heliosdrm/pwr",
    "url": "https://cran.r-project.org/package=pwr",
    "install": "install.packages(\"pwr\")",
    "tags": [
      "power-analysis",
      "sample-size",
      "effect-size",
      "Cohen-d",
      "t-test"
    ],
    "best_for": "Basic power calculations for standard statistical tests following Cohen's conventions from Cohen (1988)",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "hypothesis-testing",
      "t-tests",
      "basic-statistics"
    ],
    "topic_tags": [
      "power-analysis",
      "sample-size",
      "effect-size",
      "experimental-design",
      "Cohen-d"
    ],
    "summary": "The pwr package provides essential power calculations for common statistical tests using Cohen's standardized effect sizes. It helps researchers determine appropriate sample sizes before running experiments and assess the power of completed studies. The package covers fundamental tests like t-tests, chi-squared, ANOVA, and correlation analysis with easy-to-use analytical solutions.",
    "use_cases": [
      "Planning sample size for an A/B test to detect a meaningful conversion rate difference",
      "Determining if a completed experiment had sufficient power to detect small effect sizes"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "how to calculate sample size for t-test",
      "power analysis package R",
      "Cohen's d effect size calculator",
      "determine sample size for A/B test statistical power"
    ]
  },
  {
    "name": "simr",
    "description": "Calculates power for generalized linear mixed models (GLMMs) using Monte Carlo simulation. Designed to work with lme4 models; supports LMMs and GLMMs with crossed random effects, non-normal responses, and complex variance structures where analytical solutions are unavailable.",
    "category": "Power Analysis",
    "docs_url": "https://cran.r-project.org/web/packages/simr/vignettes/fromscratch.html",
    "github_url": "https://github.com/pitakakariki/simr",
    "url": "https://cran.r-project.org/package=simr",
    "install": "install.packages(\"simr\")",
    "tags": [
      "power-analysis",
      "mixed-models",
      "simulation",
      "lme4",
      "GLMM"
    ],
    "best_for": "Power analysis for hierarchical/multilevel models via simulation when analytical solutions don't exist, implementing Green & MacLeod (2016, MEE)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "linear-mixed-models",
      "R-lme4",
      "hypothesis-testing"
    ],
    "topic_tags": [
      "power-analysis",
      "mixed-models",
      "monte-carlo-simulation",
      "experimental-design",
      "R-package"
    ],
    "summary": "simr is an R package that calculates statistical power for generalized linear mixed models through Monte Carlo simulation. It's particularly valuable for researchers designing experiments with complex hierarchical data structures where traditional power calculation methods fall short. The package integrates seamlessly with lme4 models and handles non-normal responses and crossed random effects.",
    "use_cases": [
      "Planning sample sizes for A/B tests with user clustering and multiple treatment levels",
      "Determining power for educational interventions with students nested within schools and teachers"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "power analysis for mixed effects models",
      "sample size calculation for hierarchical data",
      "Monte Carlo power simulation R package",
      "lme4 power analysis tools"
    ]
  },
  {
    "name": "ADOpy",
    "description": "Bayesian Adaptive Design Optimization (ADO) for tuning experiments in real-time, with models for psychometric tasks.",
    "category": "Power Simulation & Design of Experiments",
    "docs_url": "https://adopy.readthedocs.io/en/latest/",
    "github_url": "https://github.com/adopy/adopy",
    "url": "https://github.com/adopy/adopy",
    "install": "pip install adopy",
    "tags": [
      "power analysis",
      "experiments",
      "Bayesian"
    ],
    "best_for": "Sample size calculation, experimental design, power analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "bayesian-inference",
      "python-scipy",
      "experimental-design"
    ],
    "topic_tags": [
      "adaptive-design",
      "bayesian-optimization",
      "psychometrics",
      "real-time-experiments"
    ],
    "summary": "ADOpy is a Python package that implements Bayesian Adaptive Design Optimization for running experiments that adapt in real-time based on incoming data. It's particularly useful for psychometric experiments where you want to optimize stimulus selection to maximize information gain. The package includes pre-built models for common psychological tasks and allows researchers to design more efficient experiments.",
    "use_cases": [
      "Optimizing A/B test allocation in real-time based on early results to minimize sample size needed",
      "Running psychophysical experiments that adaptively select stimuli to efficiently estimate perceptual thresholds"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "adaptive experiment design python",
      "bayesian optimization for A/B tests",
      "real-time experiment tuning",
      "psychometric adaptive testing tools"
    ]
  },
  {
    "name": "Adaptive",
    "description": "Parallel active learning library for adaptive function sampling/evaluation, with live plotting for monitoring.",
    "category": "Power Simulation & Design of Experiments",
    "docs_url": "https://adaptive.readthedocs.io/en/latest/",
    "github_url": "https://github.com/python-adaptive/adaptive",
    "url": "https://github.com/python-adaptive/adaptive",
    "install": "pip install adaptive",
    "tags": [
      "power analysis",
      "experiments"
    ],
    "best_for": "Sample size calculation, experimental design, power analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-scipy",
      "bayesian-optimization",
      "parallel-computing"
    ],
    "topic_tags": [
      "adaptive-sampling",
      "active-learning",
      "function-evaluation",
      "parallel-optimization",
      "experimental-design"
    ],
    "summary": "Adaptive is a Python library for intelligent sampling and evaluation of functions using active learning techniques. It enables parallel computation to efficiently explore parameter spaces by adaptively choosing which points to evaluate next. The library includes live plotting capabilities to monitor the learning progress in real-time.",
    "use_cases": [
      "Hyperparameter tuning for machine learning models where function evaluations are expensive",
      "Optimizing simulation parameters in A/B testing frameworks with costly experimental runs"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "parallel active learning python library",
      "adaptive function sampling optimization",
      "smart parameter space exploration tools",
      "efficient hyperparameter optimization with live monitoring"
    ]
  },
  {
    "name": "Ambrosia",
    "description": "End-to-end A/B testing from MobileTeleSystems with PySpark support. Covers experiment design, multi-group splitting, matching, and inference.",
    "category": "Power Simulation & Design of Experiments",
    "docs_url": null,
    "github_url": "https://github.com/MobileTeleSystems/Ambrosia",
    "url": "https://github.com/MobileTeleSystems/Ambrosia",
    "install": "pip install ambrosia",
    "tags": [
      "A/B testing",
      "experimentation",
      "Spark"
    ],
    "best_for": "End-to-end A/B testing with PySpark",
    "language": "Spark",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "pyspark",
      "hypothesis-testing"
    ],
    "topic_tags": [
      "a-b-testing",
      "experimental-design",
      "pyspark",
      "causal-inference",
      "python-package"
    ],
    "summary": "Ambrosia is a comprehensive A/B testing framework from MobileTeleSystems that handles the full experimental workflow from design to analysis using PySpark for scalability. It provides tools for multi-group randomization, matching methods, and statistical inference on large datasets. The package is particularly valuable for data scientists working with big data experimentation pipelines in production environments.",
    "use_cases": [
      "Running large-scale product feature tests across millions of mobile users with proper randomization and statistical analysis",
      "Designing and analyzing multi-arm experiments for marketing campaigns with matched treatment and control groups"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "PySpark A/B testing framework",
      "end-to-end experimentation pipeline Python",
      "multi-group randomization with matching",
      "scalable A/B testing package Spark"
    ]
  },
  {
    "name": "DoEgen",
    "description": "Automates generation and optimization of designs, especially for mixed factor-level experiments; computes efficiency metrics.",
    "category": "Power Simulation & Design of Experiments",
    "docs_url": null,
    "github_url": "https://github.com/sebhaan/DoEgen",
    "url": "https://github.com/sebhaan/DoEgen",
    "install": "pip install DoEgen",
    "tags": [
      "power analysis",
      "experiments"
    ],
    "best_for": "Sample size calculation, experimental design, power analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "experimental-design-theory",
      "python-scipy",
      "statistical-power-analysis"
    ],
    "topic_tags": [
      "experimental-design",
      "design-optimization",
      "mixed-factors",
      "power-analysis",
      "automation"
    ],
    "summary": "DoEgen is a package that automatically generates and optimizes experimental designs, particularly for complex experiments with mixed factor levels (continuous and categorical variables). It computes key efficiency metrics like D-optimality and power to help researchers create statistically robust experimental setups. The tool is especially valuable for practitioners who need to balance multiple constraints while maximizing statistical power.",
    "use_cases": [
      "Optimizing A/B test designs with both continuous variables (price, timeout) and categorical factors (UI themes, algorithms)",
      "Planning manufacturing experiments with mixed quantitative process parameters and qualitative material choices"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "automated experimental design with mixed factors",
      "optimize experiment design efficiency metrics",
      "DoEgen mixed factor level experiments",
      "experimental design automation python package"
    ]
  },
  {
    "name": "pyDOE2",
    "description": "Implements classical Design of Experiments: factorial (full/fractional), response surface (Box-Behnken, CCD), Latin Hypercube.",
    "category": "Power Simulation & Design of Experiments",
    "docs_url": "https://pythonhosted.org/pyDOE2/",
    "github_url": "https://github.com/clicumu/pyDOE2",
    "url": "https://github.com/clicumu/pyDOE2",
    "install": "pip install pyDOE2",
    "tags": [
      "power analysis",
      "experiments"
    ],
    "best_for": "Sample size calculation, experimental design, power analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "linear-regression",
      "factorial-analysis"
    ],
    "topic_tags": [
      "design-of-experiments",
      "factorial-design",
      "latin-hypercube",
      "response-surface",
      "experimental-design"
    ],
    "summary": "pyDOE2 is a Python package that implements classical Design of Experiments methods for efficiently planning experiments and parameter studies. It provides tools for creating factorial designs, response surface methodologies, and space-filling designs like Latin Hypercube sampling. Data scientists and researchers use it to systematically explore parameter spaces with minimal experimental runs while maximizing information gain.",
    "use_cases": [
      "Planning A/B tests with multiple factors to understand interaction effects between UI changes, pricing, and user segments",
      "Optimizing machine learning hyperparameters using response surface methodology to find optimal configurations with fewer training runs"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "python package for design of experiments",
      "how to create factorial designs in python",
      "latin hypercube sampling implementation",
      "response surface methodology python library"
    ]
  },
  {
    "name": "tea-tasting",
    "description": "Calculate A/B test statistics directly within data warehouses (BigQuery, ClickHouse, Snowflake, Spark) via Ibis interface. Supports CUPED/CUPAC.",
    "category": "Power Simulation & Design of Experiments",
    "docs_url": "https://tea-tasting.e10v.me/",
    "github_url": "https://github.com/e10v/tea-tasting",
    "url": "https://github.com/e10v/tea-tasting",
    "install": "pip install tea-tasting",
    "tags": [
      "A/B testing",
      "experimentation",
      "data warehouses"
    ],
    "best_for": "In-warehouse A/B test analysis with variance reduction",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "SQL-queries",
      "python-pandas",
      "hypothesis-testing"
    ],
    "topic_tags": [
      "A-B-testing",
      "data-warehouses",
      "experimentation-platform",
      "CUPED",
      "statistical-inference"
    ],
    "summary": "Tea-tasting enables data scientists to run A/B test statistical analysis directly in cloud data warehouses without moving data locally. It provides a unified Python interface via Ibis to calculate test statistics across BigQuery, Snowflake, ClickHouse, and Spark, with built-in support for variance reduction techniques like CUPED and CUPAC.",
    "use_cases": [
      "Running experiment analysis on petabyte-scale user behavior data stored in BigQuery without data export",
      "Implementing CUPED variance reduction for subscription conversion tests using historical user metrics as covariates"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "How to run A/B tests in BigQuery without downloading data",
      "Python package for warehouse-native experimentation analysis",
      "CUPED implementation for cloud data warehouses",
      "Statistical testing directly in Snowflake or ClickHouse"
    ]
  },
  {
    "name": "CausalImpact",
    "description": "Python port of Google's R package for estimating causal effects of interventions on time series using Bayesian structural time-series models.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": "https://google.github.io/CausalImpact/CausalImpact/CausalImpact.html",
    "github_url": "https://github.com/tcassou/causal_impact",
    "url": "https://github.com/tcassou/causal_impact",
    "install": "pip install causalimpact",
    "tags": [
      "DiD",
      "synthetic control",
      "RDD",
      "Bayesian"
    ],
    "best_for": "Policy evaluation, natural experiments, quasi-experiments",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "bayesian-statistics",
      "time-series-analysis",
      "python-pandas"
    ],
    "topic_tags": [
      "causal-inference",
      "time-series",
      "bayesian-methods",
      "intervention-analysis",
      "python-package"
    ],
    "summary": "CausalImpact is a Python implementation of Google's Bayesian structural time-series package for measuring the causal effect of interventions on time series data. It's widely used by data scientists and researchers to evaluate the impact of marketing campaigns, policy changes, or product launches by comparing observed outcomes to counterfactual predictions. The package automatically handles seasonality, trends, and external covariates while providing uncertainty quantification.",
    "use_cases": [
      "Measuring the impact of a new feature launch on user engagement metrics",
      "Evaluating the effectiveness of a marketing campaign on sales revenue"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "How to measure causal impact of intervention on time series",
      "Python package for Bayesian causal inference time series",
      "Google CausalImpact Python implementation",
      "Evaluate marketing campaign effectiveness time series analysis"
    ]
  },
  {
    "name": "Differences",
    "description": "Implements modern difference-in-differences methods for staggered adoption designs (e.g., Callaway & Sant'Anna).",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": "https://bernardodionisi.github.io/differences/",
    "github_url": "https://github.com/bernardodionisi/differences",
    "url": "https://github.com/bernardodionisi/differences",
    "install": "pip install differences",
    "tags": [
      "DiD",
      "synthetic control",
      "RDD"
    ],
    "best_for": "Policy evaluation, natural experiments, quasi-experiments",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "difference-in-differences",
      "panel-data-analysis",
      "python-statsmodels"
    ],
    "topic_tags": [
      "difference-in-differences",
      "staggered-adoption",
      "causal-inference",
      "program-evaluation",
      "python-package"
    ],
    "summary": "A Python package implementing state-of-the-art difference-in-differences estimators for staggered adoption designs, including Callaway & Sant'Anna's methods. Addresses common issues with two-way fixed effects models when treatment timing varies across units. Essential for practitioners working with modern DiD techniques in policy evaluation and A/B testing contexts.",
    "use_cases": [
      "Evaluating the impact of a policy rollout where different states adopt at different times",
      "Measuring the effect of a product feature launched across different markets on different dates"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "Callaway Sant'Anna difference in differences python",
      "staggered adoption DiD implementation",
      "modern difference in differences package",
      "two way fixed effects bias solution"
    ]
  },
  {
    "name": "SyntheticControlMethods",
    "description": "Implementation of synthetic control methods for comparative case studies when panel data is available.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": null,
    "github_url": "https://github.com/OscarEngelbrektson/SyntheticControlMethods",
    "url": "https://github.com/OscarEngelbrektson/SyntheticControlMethods",
    "install": "pip install SyntheticControlMethods",
    "tags": [
      "DiD",
      "synthetic control",
      "RDD"
    ],
    "best_for": "Policy evaluation, natural experiments, quasi-experiments",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "difference-in-differences",
      "python-pandas",
      "linear-regression"
    ],
    "topic_tags": [
      "synthetic-control",
      "causal-inference",
      "panel-data",
      "comparative-case-studies"
    ],
    "summary": "SyntheticControlMethods implements synthetic control methodology for causal inference when you have one treated unit and multiple control units over time. It creates a synthetic version of the treated unit using weighted combinations of control units to estimate counterfactual outcomes. This is particularly useful for policy evaluation and comparative case studies where traditional randomized experiments aren't feasible.",
    "use_cases": [
      "Evaluating the impact of a new product launch in one market by comparing against a synthetic control made from similar untreated markets",
      "Measuring the effect of a policy change in one state/country by constructing a synthetic version from other unaffected regions"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "synthetic control methods python implementation",
      "how to evaluate policy impact with synthetic control",
      "difference between synthetic control and difference in differences",
      "synthetic control package for causal inference"
    ]
  },
  {
    "name": "TFP CausalImpact",
    "description": "TensorFlow Probability port of Google's CausalImpact. Bayesian structural time-series for intervention effects.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": "https://github.com/google/tfp-causalimpact",
    "github_url": "https://github.com/google/tfp-causalimpact",
    "url": "https://github.com/google/tfp-causalimpact",
    "install": "pip install tfcausalimpact",
    "tags": [
      "causal impact",
      "time series",
      "Bayesian"
    ],
    "best_for": "TensorFlow-based causal impact analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-tensorflow",
      "time-series-analysis",
      "bayesian-inference"
    ],
    "topic_tags": [
      "causal-impact",
      "bayesian-time-series",
      "intervention-analysis",
      "structural-time-series",
      "tensorflow-probability"
    ],
    "summary": "TensorFlow Probability implementation of Google's CausalImpact package for measuring causal effects of interventions in time series data. Uses Bayesian structural time-series models to estimate what would have happened in the absence of an intervention. Particularly useful for A/B testing scenarios where randomization isn't feasible.",
    "use_cases": [
      "Measuring the impact of a marketing campaign on website traffic using control markets",
      "Evaluating the effect of a product feature launch on user engagement metrics"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "tensorflow causal impact package",
      "bayesian time series intervention analysis",
      "causalimpact python tensorflow implementation",
      "measuring treatment effects time series data"
    ]
  },
  {
    "name": "csdid",
    "description": "Python adaptation of the R `did` package. Implements multi-period DiD with staggered treatment timing (Callaway & Sant\u2019Anna).",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": null,
    "github_url": "https://github.com/d2cml-ai/csdid",
    "url": "https://github.com/d2cml-ai/csdid",
    "install": "pip install csdid",
    "tags": [
      "DiD",
      "synthetic control",
      "RDD"
    ],
    "best_for": "Policy evaluation, natural experiments, quasi-experiments",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "difference-in-differences",
      "causal-inference"
    ],
    "topic_tags": [
      "difference-in-differences",
      "staggered-treatment",
      "causal-inference",
      "python-package",
      "callaway-santanna"
    ],
    "summary": "Python implementation of the Callaway & Sant'Anna difference-in-differences estimator for settings with staggered treatment adoption across multiple time periods. This package handles the complexities of varying treatment timing and provides robust causal effect estimates. Essential tool for economists and data scientists conducting policy evaluation with panel data.",
    "use_cases": [
      "Evaluating the impact of minimum wage increases rolled out across different states at different times",
      "Measuring the effect of a product feature launched to different user cohorts in staggered phases"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python package for staggered difference in differences",
      "callaway sant'anna did implementation python",
      "multi-period difference in differences with varying treatment timing",
      "how to handle staggered treatment adoption in causal inference"
    ]
  },
  {
    "name": "didet",
    "description": "DiD with general treatment patterns. Handles effective treatment timing beyond simple staggered adoption.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": "https://github.com/tkhdyanagi/didet",
    "github_url": "https://github.com/tkhdyanagi/didet",
    "url": "https://github.com/tkhdyanagi/didet",
    "install": "pip install didet",
    "tags": [
      "DiD",
      "treatment timing",
      "causal inference"
    ],
    "best_for": "DiD with general treatment patterns",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "difference-in-differences",
      "panel-data-analysis",
      "python-pandas"
    ],
    "topic_tags": [
      "difference-in-differences",
      "treatment-timing",
      "causal-inference",
      "python-package",
      "panel-data"
    ],
    "summary": "DiDet is a Python package for difference-in-differences analysis that handles complex treatment timing patterns beyond the standard staggered adoption setup. It allows researchers to analyze scenarios with varying treatment intensities, multiple treatment periods, and non-monotonic treatment patterns. The package is particularly useful for tech economists studying interventions with nuanced rollout strategies.",
    "use_cases": [
      "Analyzing the impact of a feature rollout that was turned on and off multiple times across different user segments",
      "Evaluating policy interventions where treatment intensity varied over time and across regions"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "DiD with complex treatment timing python",
      "difference in differences non-staggered adoption",
      "how to handle varying treatment intensity DiD",
      "python package flexible treatment patterns causal inference"
    ]
  },
  {
    "name": "didhetero",
    "description": "Doubly robust estimation for group-time conditional average treatment effects. UCB for heterogeneous DiD.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": "https://github.com/tkhdyanagi/didhetero",
    "github_url": "https://github.com/tkhdyanagi/didhetero",
    "url": "https://github.com/tkhdyanagi/didhetero",
    "install": "pip install didhetero",
    "tags": [
      "DiD",
      "heterogeneous effects",
      "doubly robust"
    ],
    "best_for": "Heterogeneous treatment effects in DiD",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "difference-in-differences",
      "causal-inference",
      "doubly-robust-estimation"
    ],
    "topic_tags": [
      "heterogeneous-effects",
      "difference-in-differences",
      "causal-inference",
      "treatment-heterogeneity",
      "doubly-robust"
    ],
    "summary": "This package implements doubly robust estimation methods for identifying heterogeneous treatment effects in difference-in-differences designs. It allows researchers to estimate group-time conditional average treatment effects when treatment impacts vary across different subgroups or time periods. The methods provide robust inference by combining outcome regression and propensity score weighting approaches.",
    "use_cases": [
      "Evaluating how a policy rollout affects different demographic groups differently over time",
      "Measuring heterogeneous impacts of a product feature launch across user segments and time periods"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "heterogeneous difference in differences R package",
      "doubly robust DiD with varying treatment effects",
      "group-time average treatment effects estimation",
      "conditional ATET difference in differences"
    ]
  },
  {
    "name": "mlsynth",
    "description": "Implements advanced synthetic control methods: forward DiD, cluster SC, factor models, and proximal SC. Designed for single-treated-unit settings.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": "https://mlsynth.readthedocs.io/en/latest/",
    "github_url": "https://github.com/jaredjgreathouse/mlsynth",
    "url": "https://github.com/jaredjgreathouse/mlsynth",
    "install": "pip install mlsynth",
    "tags": [
      "DiD",
      "synthetic control",
      "RDD"
    ],
    "best_for": "Policy evaluation, natural experiments, quasi-experiments",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "difference-in-differences",
      "python-pandas",
      "causal-inference-methods"
    ],
    "topic_tags": [
      "synthetic-control",
      "difference-in-differences",
      "causal-inference",
      "policy-evaluation",
      "python-package"
    ],
    "summary": "Advanced Python package implementing cutting-edge synthetic control methods including forward DiD, cluster synthetic control, factor models, and proximal synthetic control. Designed specifically for rigorous causal inference in single-treated-unit scenarios where traditional methods may fail. Used by researchers and data scientists conducting policy evaluations and natural experiments.",
    "use_cases": [
      "Evaluating the impact of a new policy implemented in one state/city using other regions as synthetic controls",
      "Analyzing the causal effect of a product launch in a single market by constructing synthetic comparison units"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "synthetic control methods python package",
      "forward difference in differences implementation",
      "single treated unit causal inference tools",
      "advanced synthetic control cluster methods"
    ]
  },
  {
    "name": "pycinc",
    "description": "Changes\u2011in\u2011Changes (CiC) estimator for distributional treatment effects (Athey\u00a0&\u00a0Imbens\u202f2006).",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": "https://pypi.org/project/pycinc/",
    "github_url": null,
    "url": "https://pypi.org/project/pycinc/",
    "install": "pip install pycinc",
    "tags": [
      "DiD",
      "synthetic control",
      "RDD",
      "causal inference"
    ],
    "best_for": "Policy evaluation, natural experiments, quasi-experiments",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "difference-in-differences",
      "quantile-regression",
      "panel-data-analysis"
    ],
    "topic_tags": [
      "changes-in-changes",
      "distributional-effects",
      "causal-inference",
      "treatment-effects",
      "python-package"
    ],
    "summary": "PycinC implements the Changes-in-Changes estimator from Athey & Imbens (2006) for measuring treatment effects across the entire outcome distribution, not just the mean. This method extends difference-in-differences to account for distributional changes and is particularly useful when treatment effects vary across quantiles. Researchers use it to understand heterogeneous treatment impacts and policy effects on inequality.",
    "use_cases": [
      "Evaluating how minimum wage increases affect wage distributions at different percentiles",
      "Analyzing distributional impacts of education interventions on test score distributions across student ability levels"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "changes in changes estimator python implementation",
      "distributional treatment effects athey imbens",
      "quantile difference in differences python",
      "heterogeneous treatment effects distribution analysis"
    ]
  },
  {
    "name": "pyleebounds",
    "description": "Lee\u00a0(2009) sample\u2011selection bounds for treatment effects; trims treated distribution to match selection rates.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": "https://pypi.org/project/pyleebounds/",
    "github_url": null,
    "url": "https://pypi.org/project/pyleebounds/",
    "install": "pip install pyleebounds",
    "tags": [
      "DiD",
      "synthetic control",
      "RDD",
      "causal inference"
    ],
    "best_for": "Policy evaluation, natural experiments, quasi-experiments",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "difference-in-differences",
      "propensity-score-matching",
      "python-scipy"
    ],
    "topic_tags": [
      "sample-selection-bias",
      "treatment-effects",
      "causal-inference",
      "lee-bounds",
      "python-package"
    ],
    "summary": "PyLeeBounds implements Lee (2009) bounds for estimating treatment effects when treatment assignment affects sample selection. The method addresses selection bias by trimming the treated group distribution to match control group selection rates, providing bounds on the true treatment effect. It's essential for researchers dealing with endogenous sample selection in randomized or quasi-experimental settings.",
    "use_cases": [
      "Evaluating job training programs where treatment affects labor force participation rates",
      "Measuring educational intervention effects when treatment influences school dropout decisions"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "Lee bounds sample selection bias treatment effects",
      "how to handle endogenous sample selection in RCT",
      "python package for Lee 2009 bounds estimation",
      "treatment effect bounds when selection is affected"
    ]
  },
  {
    "name": "pysyncon",
    "description": "Synthetic control method implementation compatible with R's Synth and augsynth packages.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": null,
    "github_url": "https://github.com/sdfordham/pysyncon",
    "url": "https://github.com/sdfordham/pysyncon",
    "install": "pip install pysyncon",
    "tags": [
      "synthetic control",
      "causal inference",
      "panel data"
    ],
    "best_for": "R Synth-compatible synthetic control in Python",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "difference-in-differences",
      "panel-data-analysis"
    ],
    "topic_tags": [
      "synthetic-control",
      "causal-inference",
      "panel-data",
      "treatment-effects",
      "python-package"
    ],
    "summary": "Python implementation of the synthetic control method for causal inference with panel data, providing R Synth package compatibility. Used by researchers and data scientists to estimate treatment effects when randomized experiments aren't feasible. Constructs synthetic control units from donor pool to compare against treated units.",
    "use_cases": [
      "Evaluating impact of policy intervention on a specific region using other regions as controls",
      "Measuring effect of product launch in test market by creating synthetic control from similar markets"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "synthetic control method python implementation",
      "pysyncon vs R synth package comparison",
      "causal inference panel data python tools",
      "how to implement synthetic control analysis"
    ]
  },
  {
    "name": "rdd",
    "description": "Toolkit for sharp RDD analysis, including bandwidth calculation and estimation, integrating with pandas.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": null,
    "github_url": "https://github.com/evan-magnusson/rdd",
    "url": "https://github.com/evan-magnusson/rdd",
    "install": "pip install rdd",
    "tags": [
      "DiD",
      "synthetic control",
      "RDD"
    ],
    "best_for": "Policy evaluation, natural experiments, quasi-experiments",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "regression-discontinuity-design",
      "causal-inference"
    ],
    "topic_tags": [
      "regression-discontinuity",
      "causal-inference",
      "program-evaluation",
      "bandwidth-selection",
      "python-package"
    ],
    "summary": "A Python toolkit specifically designed for sharp regression discontinuity design (RDD) analysis with integrated pandas support. It provides automated bandwidth calculation and estimation methods for identifying causal effects at treatment thresholds. The package streamlines the technical implementation of RDD while maintaining statistical rigor.",
    "use_cases": [
      "Evaluating the impact of a scholarship program on student outcomes using GPA cutoffs",
      "Measuring the effect of credit score thresholds on loan approval and subsequent financial behavior"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "python package for regression discontinuity design",
      "RDD bandwidth selection implementation",
      "sharp RDD analysis with pandas",
      "regression discontinuity toolkit python"
    ]
  },
  {
    "name": "rdrobust",
    "description": "Comprehensive tools for Regression Discontinuity Designs (RDD), including optimal bandwidth selection, estimation, inference.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": "https://pypi.org/project/rdrobust/",
    "github_url": "https://github.com/rdpackages/rdrobust",
    "url": "https://github.com/rdpackages/rdrobust",
    "install": "pip install rdrobust",
    "tags": [
      "DiD",
      "synthetic control",
      "RDD"
    ],
    "best_for": "Policy evaluation, natural experiments, quasi-experiments",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "regression-analysis",
      "causal-inference-fundamentals",
      "R-programming"
    ],
    "topic_tags": [
      "regression-discontinuity",
      "causal-inference",
      "program-evaluation",
      "R-package",
      "treatment-effects"
    ],
    "summary": "rdrobust is an R package that implements state-of-the-art methods for regression discontinuity designs, providing tools for bandwidth selection, robust estimation, and statistical inference. It's widely used by economists and data scientists to estimate causal effects when treatment assignment has a discontinuous cutoff rule. The package handles both sharp and fuzzy RDD designs with modern bias-correction and inference procedures.",
    "use_cases": [
      "Evaluating the impact of a scholarship program that awards aid to students scoring above a specific test threshold",
      "Measuring the effect of a policy change that applies to companies above a certain size cutoff"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "R package for regression discontinuity analysis",
      "how to implement RDD with optimal bandwidth selection",
      "rdrobust vs manual regression discontinuity",
      "regression discontinuity tools for causal inference"
    ]
  },
  {
    "name": "synthlearners",
    "description": "Fast synthetic control estimators for panel data problems. Optimized ATT estimation with multiple SC algorithms.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": null,
    "github_url": "https://github.com/apoorvalal/synthlearners",
    "url": "https://github.com/apoorvalal/synthlearners",
    "install": "pip install synthlearners",
    "tags": [
      "synthetic control",
      "causal inference",
      "panel data"
    ],
    "best_for": "Optimized synthetic control with multiple algorithm options",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "difference-in-differences",
      "causal-inference-fundamentals"
    ],
    "topic_tags": [
      "synthetic-control",
      "causal-inference",
      "panel-data",
      "treatment-effects",
      "python-package"
    ],
    "summary": "synthlearners is a Python package providing optimized implementations of synthetic control methods for causal inference in panel data settings. It offers multiple algorithms for estimating average treatment effects (ATT) when units are treated at different times. The package is designed for speed and ease of use, making synthetic control methods accessible for practitioners working with observational data.",
    "use_cases": [
      "Evaluating the impact of a policy change on treated states/regions compared to untreated control units",
      "Measuring the causal effect of a product launch or marketing campaign across different markets using historical panel data"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "synthetic control python package for causal inference",
      "how to implement synthetic control method in python",
      "ATT estimation with synthetic control algorithms",
      "python library for panel data causal inference"
    ]
  },
  {
    "name": "pyqreg",
    "description": "Fast quantile regression solver using interior point methods, supporting robust and clustered standard errors.",
    "category": "Quantile Regression & Distributional Methods",
    "docs_url": "https://github.com/mozjay0619/pyqreg",
    "github_url": null,
    "url": "https://github.com/mozjay0619/pyqreg",
    "install": "pip install pyqreg",
    "tags": [
      "quantile",
      "regression"
    ],
    "best_for": "Heterogeneous effects, distributional analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "linear-regression",
      "scipy-optimization"
    ],
    "topic_tags": [
      "quantile-regression",
      "robust-statistics",
      "interior-point",
      "clustered-errors",
      "distributional-analysis"
    ],
    "summary": "pyqreg is a Python package for fast quantile regression using interior point optimization methods. It provides robust alternatives to ordinary least squares by modeling conditional quantiles of the outcome distribution, with support for clustered and robust standard errors. The package is particularly useful for economists and data scientists analyzing heterogeneous treatment effects or non-normal error distributions.",
    "use_cases": [
      "Analyzing wage gaps across income distribution quantiles where effects vary by earnings level",
      "Estimating treatment effects on health outcomes when impacts differ across patient risk profiles"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "fast quantile regression python package",
      "how to estimate quantile regression with clustered standard errors",
      "pyqreg vs statsmodels quantile regression",
      "interior point methods for quantile regression implementation"
    ]
  },
  {
    "name": "pyrifreg",
    "description": "Recentered Influence\u2011Function (RIF) regression for unconditional quantile & distributional effects (Firpo\u202fet\u202fal.,\u202f2008).",
    "category": "Quantile Regression & Distributional Methods",
    "docs_url": "https://github.com/vyasenov/pyrifreg",
    "github_url": null,
    "url": "https://github.com/vyasenov/pyrifreg",
    "install": "pip install pyrifreg",
    "tags": [
      "quantile",
      "regression"
    ],
    "best_for": "Heterogeneous effects, distributional analysis",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "quantile-regression",
      "causal-inference",
      "python-scipy"
    ],
    "topic_tags": [
      "quantile-regression",
      "distributional-effects",
      "unconditional-quantile",
      "rif-regression",
      "causal-analysis"
    ],
    "summary": "PyRIFReg implements Recentered Influence Function (RIF) regression to estimate unconditional quantile effects and distributional impacts beyond the mean. This advanced econometric method allows researchers to understand how treatments or policies affect different parts of the outcome distribution, not just average effects. Particularly valuable for policy evaluation where distributional consequences matter as much as average impacts.",
    "use_cases": [
      "Analyzing how minimum wage increases affect different parts of the wage distribution rather than just average wages",
      "Evaluating educational interventions' impact on test score inequality by examining effects at various quantiles"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "RIF regression python implementation",
      "unconditional quantile effects estimation",
      "distributional impact analysis tools",
      "Firpo Fortin Lemieux method python"
    ]
  },
  {
    "name": "quantile-forest",
    "description": "Scikit-learn compatible implementation of Quantile Regression Forests for non-parametric estimation.",
    "category": "Quantile Regression & Distributional Methods",
    "docs_url": "https://zillow.github.io/quantile-forest/",
    "github_url": "https://github.com/zillow/quantile-forest",
    "url": "https://github.com/zillow/quantile-forest",
    "install": "pip install quantile-forest",
    "tags": [
      "quantile",
      "regression"
    ],
    "best_for": "Heterogeneous effects, distributional analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "scikit-learn",
      "random-forests",
      "quantile-regression"
    ],
    "topic_tags": [
      "quantile-regression",
      "random-forests",
      "uncertainty-quantification",
      "scikit-learn",
      "python-package"
    ],
    "summary": "Quantile-forest is a scikit-learn compatible Python package that implements Quantile Regression Forests, extending random forests to estimate conditional quantiles instead of just point predictions. It enables non-parametric estimation of prediction intervals and full conditional distributions without distributional assumptions. The package is particularly useful for uncertainty quantification and risk assessment in machine learning applications.",
    "use_cases": [
      "Estimating confidence intervals for house price predictions to understand pricing uncertainty",
      "Risk management in financial forecasting by modeling tail quantiles of portfolio returns"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "quantile regression forest python implementation",
      "scikit-learn prediction intervals",
      "non-parametric uncertainty quantification random forests",
      "how to get confidence intervals from random forest predictions"
    ]
  },
  {
    "name": "broom",
    "description": "Converts messy output from 100+ statistical model types into consistent tidy tibbles using three verbs: tidy() for coefficient-level statistics, glance() for model-level summaries (R\u00b2, AIC), and augment() for fitted values and residuals.",
    "category": "Regression Output",
    "docs_url": "https://broom.tidymodels.org/",
    "github_url": "https://github.com/tidymodels/broom",
    "url": "https://cran.r-project.org/package=broom",
    "install": "install.packages(\"broom\")",
    "tags": [
      "tidy-data",
      "tidymodels",
      "statistical-models",
      "tidyverse",
      "modeling"
    ],
    "best_for": "Converting R statistical model output into consistent tidy data frames for analysis pipelines, based on Wickham (2014, JSS)",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "R-programming",
      "linear-regression",
      "tidyverse-dplyr"
    ],
    "topic_tags": [
      "model-output",
      "data-cleaning",
      "statistical-reporting",
      "tidyverse",
      "regression-analysis"
    ],
    "summary": "The broom package standardizes the messy, inconsistent output from R's statistical models into clean, analysis-ready data frames. It's essential for data scientists who need to extract coefficients, model statistics, or predictions from models in a consistent format. The package works with over 100 model types, making it a go-to tool for anyone doing statistical modeling in R.",
    "use_cases": [
      "Extracting regression coefficients from multiple models to create comparison tables",
      "Getting fitted values and residuals from various model types for diagnostic plotting"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "How to clean up R model output",
      "Extract coefficients from lm glm models R",
      "Tidy statistical model results R",
      "Get fitted values residuals from models R"
    ]
  },
  {
    "name": "gt",
    "description": "Build display tables from tabular data using a cohesive grammar of table parts (header, stub, body, footer). Enables progressive construction of publication-quality tables with extensive formatting, footnotes, and cell styling. Outputs to HTML, LaTeX, and RTF.",
    "category": "Regression Output",
    "docs_url": "https://gt.rstudio.com/",
    "github_url": "https://github.com/rstudio/gt",
    "url": "https://cran.r-project.org/package=gt",
    "install": "install.packages(\"gt\")",
    "tags": [
      "grammar-of-tables",
      "display-tables",
      "HTML-tables",
      "Posit",
      "formatting"
    ],
    "best_for": "Publication-ready display tables with precise formatting control and multiple output formats",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "R-basics",
      "data-frames"
    ],
    "topic_tags": [
      "table-formatting",
      "publication-ready",
      "HTML-output",
      "data-presentation",
      "R-visualization"
    ],
    "summary": "The gt package provides a grammar of tables approach for creating publication-quality display tables in R. It allows progressive table construction with extensive formatting options, footnotes, and styling capabilities. Outputs can be generated for HTML, LaTeX, and RTF formats, making it ideal for reports, presentations, and academic publications.",
    "use_cases": [
      "Creating formatted regression output tables for academic papers with proper styling and footnotes",
      "Building executive summary tables with custom formatting for business reports and dashboards"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for creating formatted tables",
      "how to make publication ready tables in R",
      "gt package tutorial for beginners",
      "best R package for HTML table formatting"
    ]
  },
  {
    "name": "gtsummary",
    "description": "Creates publication-ready analytical and summary tables (Table 1 demographics, regression results, survival analyses) with one line of code. Auto-detects variable types, calculates appropriate statistics, and formats regression models with reference rows and appropriate headers.",
    "category": "Regression Output",
    "docs_url": "https://www.danieldsjoberg.com/gtsummary/",
    "github_url": "https://github.com/ddsjoberg/gtsummary",
    "url": "https://cran.r-project.org/package=gtsummary",
    "install": "install.packages(\"gtsummary\")",
    "tags": [
      "summary-tables",
      "Table1",
      "clinical-tables",
      "regression-tables",
      "reproducible-research"
    ],
    "best_for": "Table 1 demographics and regression summary tables for medical/scientific publications, implementing Sjoberg et al. (2021, R Journal)",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "R-programming",
      "regression-analysis",
      "data-manipulation"
    ],
    "topic_tags": [
      "summary-tables",
      "regression-output",
      "publication-ready",
      "clinical-research",
      "reproducible-research"
    ],
    "summary": "An R package that automatically generates publication-ready summary tables and regression output with minimal code. Popular in clinical research and academic settings for creating standardized Table 1 demographics and formatted regression results. Handles variable type detection and statistical formatting automatically.",
    "use_cases": [
      "Creating Table 1 demographics for a clinical trial publication",
      "Formatting logistic regression results for an A/B test report"
    ],
    "audience": [
      "Junior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "how to make publication ready tables in R",
      "automatic Table 1 generator",
      "format regression output for papers",
      "R package for summary statistics tables"
    ]
  },
  {
    "name": "modelsummary",
    "description": "Creates publication-quality tables summarizing multiple statistical models side-by-side, plus coefficient plots, data summaries, and correlation matrices. Supports 100+ model types via broom/parameters with output to HTML, LaTeX, Word, PDF, PNG, and Excel.",
    "category": "Regression Output",
    "docs_url": "https://modelsummary.com/",
    "github_url": "https://github.com/vincentarelbundock/modelsummary",
    "url": "https://cran.r-project.org/package=modelsummary",
    "install": "install.packages(\"modelsummary\")",
    "tags": [
      "regression-tables",
      "model-summary",
      "coefficient-plots",
      "publication-tables",
      "tidyverse"
    ],
    "best_for": "Modern, flexible regression tables with extensive customization\u2014the successor to stargazer, implementing Arel-Bundock (2022, JSS)",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "r-programming",
      "linear-regression",
      "ggplot2"
    ],
    "topic_tags": [
      "regression-tables",
      "model-comparison",
      "publication-ready",
      "statistical-output",
      "r-package"
    ],
    "summary": "An R package that creates professional-looking tables comparing multiple statistical models side-by-side, perfect for academic papers and reports. It automatically formats regression coefficients, standard errors, and model statistics across 100+ model types. The package also generates coefficient plots and exports to multiple formats including LaTeX, HTML, and Word.",
    "use_cases": [
      "PhD student comparing different regression specifications in their dissertation chapter",
      "Data scientist presenting A/B test results with multiple model variations to stakeholders"
    ],
    "audience": [
      "Junior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "how to create publication quality regression tables in R",
      "compare multiple models side by side table",
      "R package for formatting regression output",
      "export regression results to LaTeX Word Excel"
    ]
  },
  {
    "name": "stargazer",
    "description": "Produces well-formatted LaTeX, HTML/CSS, and ASCII regression tables with multiple models side-by-side, plus summary statistics tables. Widely used in economics with journal-specific formatting styles (AER, QJE, ASR).",
    "category": "Regression Output",
    "docs_url": "https://cran.r-project.org/web/packages/stargazer/vignettes/stargazer.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=stargazer",
    "install": "install.packages(\"stargazer\")",
    "tags": [
      "LaTeX-tables",
      "regression-output",
      "academic-publishing",
      "economics",
      "HTML-tables"
    ],
    "best_for": "Quick, publication-ready LaTeX tables for economics journals with classic formatting",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "R-programming",
      "linear-regression",
      "LaTeX-basics"
    ],
    "topic_tags": [
      "regression-tables",
      "academic-publishing",
      "LaTeX-output",
      "R-packages",
      "statistical-reporting"
    ],
    "summary": "stargazer is an R package that creates publication-ready regression tables and summary statistics in LaTeX, HTML, and ASCII formats. It's the go-to tool for economics researchers who need to present multiple regression models side-by-side with professional formatting. The package supports journal-specific styles and handles complex table layouts automatically.",
    "use_cases": [
      "Creating regression tables for economics papers with multiple model specifications to show robustness checks",
      "Generating formatted summary statistics tables for dataset descriptions in academic publications"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "how to make regression tables in R for papers",
      "stargazer package tutorial economics",
      "create LaTeX tables from R regression output",
      "format multiple regression models side by side"
    ]
  },
  {
    "name": "texreg",
    "description": "Converts coefficients, standard errors, significance stars, and fit statistics from statistical models into LaTeX, HTML, Word, or console output. Highly extensible with support for custom model types and confidence intervals.",
    "category": "Regression Output",
    "docs_url": "https://cran.r-project.org/web/packages/texreg/vignettes/texreg.pdf",
    "github_url": "https://github.com/leifeld/texreg",
    "url": "https://cran.r-project.org/package=texreg",
    "install": "install.packages(\"texreg\")",
    "tags": [
      "LaTeX-tables",
      "HTML-tables",
      "model-comparison",
      "Word-export",
      "extensible"
    ],
    "best_for": "Highly extensible regression tables with easy custom model type extensions, implementing Leifeld (2013, JSS)",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "R-regression-models",
      "LaTeX-basics"
    ],
    "topic_tags": [
      "regression-tables",
      "LaTeX-output",
      "model-comparison",
      "academic-publishing",
      "R-package"
    ],
    "summary": "R package that automatically formats regression model outputs into publication-ready tables for LaTeX, HTML, Word, or console display. Essential tool for academics and data scientists who need to present multiple models side-by-side with proper formatting. Saves hours of manual table creation and ensures consistent, professional presentation of statistical results.",
    "use_cases": [
      "Creating comparison tables of multiple regression models for academic paper submission",
      "Generating HTML tables of A/B test results for stakeholder presentation"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "R package to export regression tables to LaTeX",
      "how to create publication ready regression tables",
      "compare multiple models in formatted table R",
      "texreg package tutorial regression output"
    ]
  },
  {
    "name": "here",
    "description": "Simple path construction from project root. Uses heuristics to find project root (RStudio, .git, .here) enabling portable paths that work across different machines and working directories.",
    "category": "Reproducibility",
    "docs_url": "https://here.r-lib.org/",
    "github_url": "https://github.com/r-lib/here",
    "url": "https://cran.r-project.org/package=here",
    "install": "install.packages(\"here\")",
    "tags": [
      "paths",
      "project-management",
      "reproducibility",
      "portability",
      "working-directory"
    ],
    "best_for": "Portable file paths from project root for reproducible scripts",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "R-basics",
      "file-systems"
    ],
    "topic_tags": [
      "file-paths",
      "project-structure",
      "R-package",
      "reproducible-research"
    ],
    "summary": "The 'here' package provides a simple solution for constructing file paths relative to your project root in R. It automatically detects project boundaries using common markers like .git folders or RStudio project files, eliminating hardcoded paths that break when code is shared or moved. Essential for creating reproducible analyses that work seamlessly across different machines and environments.",
    "use_cases": [
      "Loading data files in R scripts that need to work for collaborators with different folder structures",
      "Building reproducible research projects where analysis scripts reference data and output folders consistently"
    ],
    "audience": [
      "Junior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "R package for relative file paths",
      "how to make R scripts portable across machines",
      "here package R reproducible paths",
      "fix working directory issues R projects"
    ]
  },
  {
    "name": "renv",
    "description": "Project-local R dependency management. Creates reproducible environments by recording package versions in a lockfile, isolating project libraries, and enabling version restore.",
    "category": "Reproducibility",
    "docs_url": "https://rstudio.github.io/renv/",
    "github_url": "https://github.com/rstudio/renv",
    "url": "https://cran.r-project.org/package=renv",
    "install": "install.packages(\"renv\")",
    "tags": [
      "reproducibility",
      "package-management",
      "dependency-isolation",
      "lockfile",
      "environments"
    ],
    "best_for": "Project-local package management for reproducible R environments",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "R-programming",
      "package-installation",
      "version-control"
    ],
    "topic_tags": [
      "dependency-management",
      "reproducible-research",
      "R-environment",
      "project-isolation"
    ],
    "summary": "renv is an R package that creates isolated, reproducible project environments by managing package dependencies locally. It records exact package versions in a lockfile and allows teams to restore identical environments across different machines. Essential for ensuring research and analysis projects remain reproducible over time.",
    "use_cases": [
      "Sharing R analysis projects with collaborators while ensuring everyone uses identical package versions",
      "Maintaining multiple R projects with different package version requirements without conflicts"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "How to make R projects reproducible across different computers",
      "R package version management for data science projects",
      "renv vs packrat for R dependency management",
      "isolate R package versions by project"
    ]
  },
  {
    "name": "rmarkdown",
    "description": "Dynamic documents combining R code with Markdown text. Generates reproducible reports in HTML, PDF, Word, and slides. Foundation for literate programming and reproducible research in R.",
    "category": "Reproducibility",
    "docs_url": "https://rmarkdown.rstudio.com/",
    "github_url": "https://github.com/rstudio/rmarkdown",
    "url": "https://cran.r-project.org/package=rmarkdown",
    "install": "install.packages(\"rmarkdown\")",
    "tags": [
      "literate-programming",
      "reproducible-research",
      "dynamic-documents",
      "reporting",
      "Markdown"
    ],
    "best_for": "Literate programming and reproducible reports combining R code with Markdown",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "R-programming",
      "markdown-syntax"
    ],
    "topic_tags": [
      "reproducible-research",
      "report-generation",
      "literate-programming",
      "data-visualization",
      "scientific-writing"
    ],
    "summary": "R Markdown is a framework that combines R code with plain text to create dynamic, reproducible documents. It allows analysts to embed code, results, and visualizations directly into reports that can be exported to multiple formats. This makes it essential for creating transparent, updatable analysis reports in academic and industry settings.",
    "use_cases": [
      "Creating automated quarterly business reports that update with new data",
      "Writing reproducible research papers with embedded statistical analysis and plots"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "how to create reproducible reports in R",
      "R markdown tutorial for data analysis",
      "best tool for combining R code and documentation",
      "generate PDF reports from R analysis"
    ]
  },
  {
    "name": "targets",
    "description": "Make-like pipeline toolkit for R. Declares dependencies between pipeline steps, skips up-to-date targets, and supports parallel execution. Standard for reproducible research workflows.",
    "category": "Reproducibility",
    "docs_url": "https://docs.ropensci.org/targets/",
    "github_url": "https://github.com/ropensci/targets",
    "url": "https://cran.r-project.org/package=targets",
    "install": "install.packages(\"targets\")",
    "tags": [
      "pipelines",
      "reproducibility",
      "make",
      "dependency-tracking",
      "parallel"
    ],
    "best_for": "Make-like reproducible pipelines with automatic dependency tracking and parallel execution",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "R-programming",
      "command-line-basics",
      "data-analysis-workflows"
    ],
    "topic_tags": [
      "workflow-management",
      "reproducible-research",
      "pipeline-automation",
      "R-package",
      "dependency-tracking"
    ],
    "summary": "targets is R's premier pipeline management system that automatically tracks dependencies between analysis steps and only re-runs outdated components. It's become the gold standard for reproducible research workflows in R, replacing manual script orchestration with intelligent automation. Data scientists and researchers use it to build robust, scalable analysis pipelines that save time and prevent errors.",
    "use_cases": [
      "Managing a multi-step machine learning pipeline where data preprocessing, model training, and evaluation depend on each other",
      "Coordinating a research project with multiple data sources, statistical analyses, and report generation that need to update when underlying data changes"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "R pipeline management tool",
      "how to automate R analysis workflows",
      "targets package tutorial",
      "reproducible research workflow R"
    ]
  },
  {
    "name": "clubSandwich",
    "description": "Provides cluster-robust variance estimators with small-sample corrections, including bias-reduced linearization (BRL/CR2). Includes functions for hypothesis testing with Satterthwaite degrees of freedom and Hotelling's T\u00b2 approximation\u2014essential when the number of clusters is small.",
    "category": "Robust Standard Errors",
    "docs_url": "https://jepusto.github.io/clubSandwich/",
    "github_url": "https://github.com/jepusto/clubSandwich",
    "url": "https://cran.r-project.org/package=clubSandwich",
    "install": "install.packages(\"clubSandwich\")",
    "tags": [
      "cluster-robust",
      "small-sample-corrections",
      "bias-reduced-linearization",
      "fixed-effects",
      "meta-analysis"
    ],
    "best_for": "Cluster-robust inference when the number of clusters is small, especially in panel data and meta-analysis, implementing Pustejovsky & Tipton (2018, JBES)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "linear-regression",
      "fixed-effects-models",
      "clustered-data"
    ],
    "topic_tags": [
      "cluster-robust-standard-errors",
      "small-sample-corrections",
      "econometrics",
      "causal-inference",
      "r-package"
    ],
    "summary": "clubSandwich is an R package that provides cluster-robust variance estimators with small-sample corrections, particularly useful when you have few clusters. It implements advanced methods like bias-reduced linearization (CR2) and provides proper hypothesis testing procedures when standard asymptotic assumptions break down due to small cluster counts.",
    "use_cases": [
      "A/B testing analysis with only 8-12 treatment markets where standard cluster-robust SEs are unreliable",
      "Policy evaluation study with state-level treatment but only 20 states in the dataset"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "cluster robust standard errors with few clusters",
      "small sample correction for clustered data R",
      "bias reduced linearization clubSandwich",
      "CR2 variance estimator implementation"
    ]
  },
  {
    "name": "lmtest",
    "description": "Collection of tests for diagnostic checking in linear regression models. Provides the essential coeftest() function for testing coefficients with alternative variance-covariance matrices (pairs with sandwich), plus Breusch-Pagan, Durbin-Watson, and RESET tests.",
    "category": "Robust Standard Errors",
    "docs_url": "https://cran.r-project.org/web/packages/lmtest/vignettes/lmtest-intro.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=lmtest",
    "install": "install.packages(\"lmtest\")",
    "tags": [
      "regression-diagnostics",
      "heteroskedasticity-test",
      "Breusch-Pagan",
      "Durbin-Watson",
      "serial-correlation"
    ],
    "best_for": "Testing coefficient significance with robust SEs and diagnostic tests for regression assumptions, implementing Zeileis & Hothorn (2002)",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "linear-regression",
      "R-programming",
      "sandwich-package"
    ],
    "topic_tags": [
      "regression-diagnostics",
      "robust-standard-errors",
      "heteroskedasticity",
      "econometrics",
      "R-package"
    ],
    "summary": "Essential R package for testing assumptions in linear regression models. Provides coeftest() for robust standard errors and diagnostic tests like Breusch-Pagan for heteroskedasticity and Durbin-Watson for serial correlation. Standard tool for econometric analysis and ensuring regression validity.",
    "use_cases": [
      "Testing whether your regression residuals have constant variance before trusting coefficient estimates",
      "Checking for serial correlation in time series regressions to validate model assumptions"
    ],
    "audience": [
      "Junior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "how to test for heteroskedasticity in R",
      "coeftest function robust standard errors",
      "Breusch Pagan test R package",
      "regression diagnostic tests econometrics"
    ]
  },
  {
    "name": "sandwich",
    "description": "Object-oriented software for model-robust covariance matrix estimators including heteroscedasticity-consistent (HC0-HC5), heteroscedasticity- and autocorrelation-consistent (HAC/Newey-West), clustered, panel, and bootstrap covariances. Works with lm, glm, fixest, survival models, and many others.",
    "category": "Robust Standard Errors",
    "docs_url": "https://sandwich.R-Forge.R-project.org/",
    "github_url": null,
    "url": "https://cran.r-project.org/package=sandwich",
    "install": "install.packages(\"sandwich\")",
    "tags": [
      "robust-standard-errors",
      "heteroskedasticity-consistent",
      "HAC-covariance",
      "cluster-robust",
      "Newey-West"
    ],
    "best_for": "Computing robust standard errors for cross-sectional, time series, clustered, or panel data, implementing Zeileis (2004, 2006, 2020, JSS)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "linear-regression",
      "R-programming",
      "statistical-inference"
    ],
    "topic_tags": [
      "robust-standard-errors",
      "heteroskedasticity",
      "clustered-standard-errors",
      "econometrics",
      "R-package"
    ],
    "summary": "The sandwich package provides robust covariance matrix estimators for regression models in R, addressing violations of standard assumptions like heteroskedasticity and autocorrelation. It's essential for econometric analysis where you need reliable standard errors and confidence intervals despite model misspecification. The package integrates seamlessly with popular modeling functions and offers various robust estimators including HC, HAC, and clustered variants.",
    "use_cases": [
      "Analyzing panel data with firm-level clustering to account for within-firm correlation in financial performance regressions",
      "Correcting for heteroskedasticity in cross-sectional wage equations where variance increases with education level"
    ],
    "audience": [
      "Early-PhD",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "How to get robust standard errors in R regression",
      "sandwich package heteroskedasticity consistent standard errors",
      "clustered standard errors R implementation",
      "Newey West HAC covariance matrix R"
    ]
  },
  {
    "name": "(PySAL Core)",
    "description": "The broader PySAL ecosystem contains many tools for spatial data handling, weights, visualization, and analysis.",
    "category": "Spatial Econometrics",
    "docs_url": "https://pysal.org/",
    "github_url": "https://github.com/pysal/pysal",
    "url": "https://github.com/pysal/pysal",
    "install": "pip install pysal",
    "tags": [
      "spatial",
      "geography"
    ],
    "best_for": "Geographic data, spatial autocorrelation, regional analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "geopandas",
      "spatial-autocorrelation"
    ],
    "topic_tags": [
      "spatial-analysis",
      "econometrics",
      "python-package",
      "geographic-data",
      "spatial-weights"
    ],
    "summary": "PySAL is Python's comprehensive spatial analysis library providing tools for exploratory spatial data analysis, spatial econometrics, and geographic modeling. It offers functionality for creating spatial weights matrices, testing for spatial autocorrelation, and running spatial regression models. The library is widely used by economists, geographers, and data scientists working with location-based data.",
    "use_cases": [
      "Analyzing housing price spillovers across neighborhoods using spatial lag models",
      "Testing for geographic clustering in economic outcomes like unemployment rates"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "python library for spatial econometrics",
      "how to analyze geographic spillover effects",
      "PySAL spatial weights matrix tutorial",
      "spatial autocorrelation testing in python"
    ]
  },
  {
    "name": "Apache Sedona",
    "description": "Distributed spatial analytics engine (formerly GeoSpark) with spatial SQL, K-NN joins, and range queries for spatial econometrics.",
    "category": "Spatial Econometrics",
    "docs_url": "https://sedona.apache.org/",
    "github_url": "https://github.com/apache/sedona",
    "url": "https://github.com/apache/sedona",
    "install": "pip install apache-sedona",
    "tags": [
      "spark",
      "spatial",
      "GIS",
      "distributed"
    ],
    "best_for": "Constructing spatial weight matrices and distance-based instruments at scale",
    "language": "Spark",
    "difficulty": "intermediate",
    "prerequisites": [
      "apache-spark",
      "SQL-queries",
      "spatial-data-formats"
    ],
    "topic_tags": [
      "distributed-computing",
      "spatial-analysis",
      "geospatial-econometrics",
      "big-data",
      "spark-ecosystem"
    ],
    "summary": "Apache Sedona is a distributed spatial analytics engine built on Apache Spark that enables large-scale geospatial analysis through spatial SQL operations. It provides efficient spatial joins, range queries, and K-nearest neighbor operations for processing massive spatial datasets. Tech economists use it to analyze location-based economic phenomena like market concentration, urban development patterns, and regional economic spillovers.",
    "use_cases": [
      "Analyzing ride-sharing market competition by calculating spatial proximity between drivers and demand hotspots across metropolitan areas",
      "Measuring economic spillover effects by performing spatial joins between business locations and demographic data for regional policy research"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "distributed spatial analysis spark",
      "large scale GIS data processing",
      "spatial econometrics big data tools",
      "geospatial joins spark ecosystem"
    ]
  },
  {
    "name": "PySAL (spreg)",
    "description": "The spatial regression `spreg` module of PySAL. Implements spatial lag, error, IV models, and diagnostics.",
    "category": "Spatial Econometrics",
    "docs_url": "https://pysal.org/spreg/",
    "github_url": "https://github.com/pysal/spreg",
    "url": "https://github.com/pysal/spreg",
    "install": "pip install spreg",
    "tags": [
      "spatial",
      "geography"
    ],
    "best_for": "Geographic data, spatial autocorrelation, regional analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "linear-regression",
      "geospatial-data"
    ],
    "topic_tags": [
      "spatial-regression",
      "econometrics",
      "geospatial-analysis",
      "python-package",
      "spatial-autocorrelation"
    ],
    "summary": "PySAL's spreg module provides Python implementations of spatial regression models that account for geographic relationships in data. It handles spatial lag models (where nearby observations influence each other), spatial error models (where errors are spatially correlated), and instrumental variable approaches. Essential for economists and data scientists analyzing location-based phenomena like housing prices, regional economic development, or disease spread.",
    "use_cases": [
      "Analyzing how housing prices in one neighborhood affect prices in adjacent areas",
      "Modeling regional unemployment rates while accounting for spillover effects between neighboring counties"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "python package for spatial regression models",
      "how to handle spatial autocorrelation in econometrics",
      "PySAL spatial lag model implementation",
      "spatial error model python library"
    ]
  },
  {
    "name": "sf",
    "description": "The modern standard for spatial vector data in R, implementing Simple Features access (ISO 19125). Represents spatial data as data frames with geometry list-columns, enabling seamless tidyverse integration. Interfaces with GDAL (I/O), GEOS (geometry operations), PROJ (projections), and s2 (spherical geometry).",
    "category": "Spatial Econometrics",
    "docs_url": "https://r-spatial.github.io/sf/",
    "github_url": "https://github.com/r-spatial/sf",
    "url": "https://cran.r-project.org/package=sf",
    "install": "install.packages(\"sf\")",
    "tags": [
      "simple-features",
      "spatial-data",
      "vector-data",
      "tidyverse",
      "GDAL-GEOS-PROJ"
    ],
    "best_for": "Reading, writing, manipulating, and visualizing spatial vector data; foundation for all spatial workflows, implementing Pebesma (2018)",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "R-data-frames",
      "basic-GIS-concepts",
      "tidyverse-dplyr"
    ],
    "topic_tags": [
      "spatial-analysis",
      "GIS-data",
      "R-package",
      "vector-geometry",
      "geospatial"
    ],
    "summary": "The sf package is the modern R standard for working with spatial vector data like points, lines, and polygons. It integrates seamlessly with tidyverse workflows by storing spatial geometries as special columns in regular data frames. This makes spatial data manipulation as intuitive as working with regular tabular data while providing access to powerful geospatial operations.",
    "use_cases": [
      "Analyzing retail store locations and customer demographics within geographic boundaries",
      "Processing transportation networks and calculating route distances for logistics optimization"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for spatial data analysis",
      "how to work with shapefiles in R",
      "modern R spatial data tidyverse",
      "sf package tutorial geospatial analysis"
    ]
  },
  {
    "name": "spatialreg",
    "description": "Comprehensive package for spatial regression model estimation, split from spdep in 2019. Provides maximum likelihood, two-stage least squares, and GMM estimation for spatial lag (SAR), spatial error (SEM), and combined (SARAR/SAC) models, plus Spatial Durbin and SLX variants with impact calculations.",
    "category": "Spatial Econometrics",
    "docs_url": "https://r-spatial.github.io/spatialreg/",
    "github_url": "https://github.com/r-spatial/spatialreg",
    "url": "https://cran.r-project.org/package=spatialreg",
    "install": "install.packages(\"spatialreg\")",
    "tags": [
      "spatial-regression",
      "maximum-likelihood",
      "spatial-lag",
      "spatial-error",
      "GMM"
    ],
    "best_for": "Estimating cross-sectional spatial regression models (SAR, SEM, SAC, SDM) with maximum likelihood or GMM, implementing Bivand & Piras (2015)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "spatial-autocorrelation",
      "linear-regression",
      "R-programming"
    ],
    "topic_tags": [
      "spatial-regression",
      "spatial-econometrics",
      "maximum-likelihood",
      "spatial-autocorrelation",
      "R-package"
    ],
    "summary": "The spatialreg package provides comprehensive tools for estimating spatial regression models in R, handling spatial dependencies in cross-sectional data. It offers multiple estimation methods including maximum likelihood and GMM for spatial lag, spatial error, and combined models. Essential for economists and data scientists working with geographically clustered data where standard regression assumptions fail.",
    "use_cases": [
      "analyzing house prices with neighborhood spillover effects",
      "studying regional economic growth with spatial interdependencies"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "spatial regression R package",
      "how to handle spatial autocorrelation in regression",
      "spatialreg vs spdep differences",
      "spatial lag model estimation R"
    ]
  },
  {
    "name": "spdep",
    "description": "The foundational R package for spatial weights matrix creation and spatial autocorrelation testing. Provides functions for creating spatial weights from polygon contiguities and point patterns, computing global statistics (Moran's I, Geary's C), local indicators (LISA), and Lagrange multiplier tests.",
    "category": "Spatial Econometrics",
    "docs_url": "https://r-spatial.github.io/spdep/",
    "github_url": "https://github.com/r-spatial/spdep",
    "url": "https://cran.r-project.org/package=spdep",
    "install": "install.packages(\"spdep\")",
    "tags": [
      "spatial-weights",
      "autocorrelation",
      "morans-i",
      "neighborhood-analysis",
      "spatial-statistics"
    ],
    "best_for": "Creating spatial weights matrices and testing for spatial autocorrelation in cross-sectional data, implementing Bivand & Wong (2018)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "R-programming",
      "spatial-data-structures",
      "regression-analysis"
    ],
    "topic_tags": [
      "spatial-weights",
      "spatial-autocorrelation",
      "morans-i",
      "neighborhood-analysis",
      "R-package"
    ],
    "summary": "The foundational R package for spatial econometrics that enables creation of spatial weights matrices and testing for spatial autocorrelation. Provides essential functions for defining neighborhood structures, computing Moran's I and Geary's C statistics, and running diagnostic tests for spatial dependence. Essential toolkit for any spatial analysis workflow in economics or regional science.",
    "use_cases": [
      "Testing whether house prices show spatial clustering patterns across neighborhoods",
      "Analyzing regional unemployment spillovers and economic contagion effects"
    ],
    "audience": [
      "Early-PhD",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "how to create spatial weights matrix in R",
      "testing spatial autocorrelation Moran's I",
      "R package for neighborhood analysis",
      "spatial dependence diagnostic tests"
    ]
  },
  {
    "name": "splm",
    "description": "Maximum likelihood and GMM estimation for spatial panel data models. Implements fixed and random effects specifications with spatial lag and/or spatial error components, including the Kapoor-Kelejian-Prucha (2007) GM estimator. Provides diagnostic tests for spatial autocorrelation in panel settings.",
    "category": "Spatial Econometrics",
    "docs_url": "https://cran.r-project.org/web/packages/splm/splm.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=splm",
    "install": "install.packages(\"splm\")",
    "tags": [
      "spatial-panel",
      "panel-data",
      "fixed-effects",
      "random-effects",
      "GMM"
    ],
    "best_for": "Estimating spatial econometric models with panel (longitudinal) data structures, implementing Millo & Piras (2012)",
    "language": "R",
    "difficulty": "advanced",
    "prerequisites": [
      "panel-data-methods",
      "spatial-econometrics",
      "maximum-likelihood-estimation"
    ],
    "topic_tags": [
      "spatial-panel",
      "econometric-methods",
      "maximum-likelihood",
      "GMM-estimation",
      "spatial-autocorrelation"
    ],
    "summary": "The splm package provides maximum likelihood and GMM estimation methods specifically designed for spatial panel data models. It handles complex econometric specifications including fixed and random effects with spatial lag and error components, implementing advanced estimators like Kapoor-Kelejian-Prucha. Essential for researchers analyzing panel data where observations exhibit spatial dependence across geographic units over time.",
    "use_cases": [
      "Analyzing regional economic growth patterns across states/countries over multiple years with spatial spillover effects",
      "Estimating housing price dynamics across metropolitan areas accounting for neighborhood spatial correlation in panel data"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "spatial panel data estimation in R",
      "Kapoor Kelejian Prucha estimator implementation",
      "GMM estimation for spatial econometrics",
      "spatial autocorrelation tests panel data"
    ]
  },
  {
    "name": "Awesome Quant",
    "description": "Curated list of quantitative finance libraries and resources (many statistical/TS tools overlap with econometrics).",
    "category": "Standard Errors, Bootstrapping & Reporting",
    "docs_url": "https://wilsonfreitas.github.io/awesome-quant/",
    "github_url": null,
    "url": "https://wilsonfreitas.github.io/awesome-quant/",
    "install": "",
    "tags": [
      "bootstrap",
      "standard errors"
    ],
    "best_for": "Robust inference, clustered SEs, result presentation",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-basics",
      "statistical-inference",
      "pandas-dataframes"
    ],
    "topic_tags": [
      "quantitative-finance",
      "time-series",
      "statistical-libraries",
      "bootstrap",
      "econometrics"
    ],
    "summary": "A comprehensive collection of Python, R, and other libraries for quantitative finance, including tools for time series analysis, statistical modeling, and bootstrapping methods. Many of these libraries provide robust standard error calculations and resampling techniques that are directly applicable to econometric analysis. This curated list serves as a discovery tool for finding specialized packages for financial econometrics and statistical inference.",
    "use_cases": [
      "Finding Python libraries for calculating robust standard errors in financial time series models",
      "Discovering R packages for bootstrap methods when analyzing market microstructure data"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python libraries for quantitative finance bootstrap methods",
      "curated list of econometrics packages for finance",
      "best tools for financial time series standard errors",
      "quantitative finance libraries with statistical inference"
    ]
  },
  {
    "name": "Beyond Jupyter (TransferLab)",
    "description": "Teaches software design principles for ML\u2014modularity, abstraction, and reproducibility\u2014going beyond ad hoc Jupyter workflows. Focus on maintainable, production-quality ML code.",
    "category": "Standard Errors, Bootstrapping & Reporting",
    "docs_url": "https://transferlab.ai/trainings/beyond-jupyter/",
    "github_url": null,
    "url": "https://transferlab.ai/trainings/beyond-jupyter/",
    "install": "",
    "tags": [
      "bootstrap",
      "standard errors"
    ],
    "best_for": "Robust inference, clustered SEs, result presentation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "jupyter-notebooks",
      "python-scikit-learn",
      "git-version-control"
    ],
    "topic_tags": [
      "software-engineering",
      "ml-production",
      "code-organization",
      "reproducible-research",
      "python-packaging"
    ],
    "summary": "A comprehensive guide to transforming messy Jupyter notebook workflows into well-structured, maintainable ML codebases. Covers essential software engineering practices like modular design, abstraction layers, and automated testing for machine learning projects. Essential for data scientists transitioning from exploratory analysis to production-ready ML systems.",
    "use_cases": [
      "Refactoring a prototype ML model from notebooks into a deployable package",
      "Setting up reproducible experiment tracking and model versioning for a research team"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "How to organize ML code beyond Jupyter notebooks",
      "Best practices for production machine learning code",
      "Software engineering principles for data science",
      "How to make ML experiments reproducible and maintainable"
    ]
  },
  {
    "name": "Causal Inference for the Brave and True",
    "description": "Modern introduction to causal inference methods (DiD, IV, RDD, Synth, ML-based) with Python code examples.",
    "category": "Standard Errors, Bootstrapping & Reporting",
    "docs_url": "https://matheusfacure.github.io/python-causality-handbook/",
    "github_url": null,
    "url": "https://matheusfacure.github.io/python-causality-handbook/",
    "install": "",
    "tags": [
      "bootstrap",
      "standard errors"
    ],
    "best_for": "Robust inference, clustered SEs, result presentation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "linear-regression",
      "hypothesis-testing"
    ],
    "topic_tags": [
      "causal-inference",
      "difference-in-differences",
      "instrumental-variables",
      "regression-discontinuity",
      "python-implementation"
    ],
    "summary": "A comprehensive guide to modern causal inference methods including Difference-in-Differences, Instrumental Variables, Regression Discontinuity Design, and Synthetic Controls with hands-on Python implementations. Designed for practitioners who want to move beyond correlational analysis to establish causal relationships in observational data. Combines theoretical foundations with practical coding examples for real-world applications.",
    "use_cases": [
      "Evaluating the causal impact of a product feature launch on user engagement using natural experiments",
      "Measuring the effectiveness of a marketing campaign while controlling for selection bias and confounding factors"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "causal inference tutorial with Python examples",
      "how to implement difference in differences analysis",
      "instrumental variables regression discontinuity guide",
      "causal analysis methods for observational data"
    ]
  },
  {
    "name": "Coding for Economists",
    "description": "Practical guide by A. Turrell on using Python for modern econometric research, data analysis, and workflows.",
    "category": "Standard Errors, Bootstrapping & Reporting",
    "docs_url": "https://aeturrell.github.io/coding-for-economists/",
    "github_url": null,
    "url": "https://aeturrell.github.io/coding-for-economists/",
    "install": "",
    "tags": [
      "bootstrap",
      "standard errors"
    ],
    "best_for": "Robust inference, clustered SEs, result presentation",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-basics",
      "econometric-fundamentals"
    ],
    "topic_tags": [
      "python-econometrics",
      "bootstrap-methods",
      "standard-errors",
      "econometric-workflows",
      "data-analysis"
    ],
    "summary": "A comprehensive guide by Arthur Turrell that teaches economists how to use Python for econometric analysis, focusing on practical implementation of standard error calculations and bootstrap methods. The resource bridges the gap between theoretical econometrics and modern programming practices, making it ideal for researchers transitioning from Stata or R to Python workflows.",
    "use_cases": [
      "Implementing robust standard errors in Python for regression analysis when transitioning from Stata",
      "Setting up bootstrap confidence intervals for non-standard econometric estimators in research projects"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "How to calculate robust standard errors in Python for economics",
      "Bootstrap methods implementation guide for economists",
      "Python econometrics tutorial for beginners",
      "Coding workflows for econometric research in Python"
    ]
  },
  {
    "name": "Deep Learning Specialization (Coursera)",
    "description": "Intermediate 5-course series by Andrew Ng covering deep neural networks, CNNs, RNNs, transformers, and real-world DL applications using TensorFlow.",
    "category": "Standard Errors, Bootstrapping & Reporting",
    "docs_url": "https://www.coursera.org/specializations/deep-learning",
    "github_url": null,
    "url": "https://www.coursera.org/specializations/deep-learning",
    "install": "",
    "tags": [
      "bootstrap",
      "standard errors",
      "machine learning"
    ],
    "best_for": "Robust inference, clustered SEs, result presentation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-programming",
      "linear-algebra",
      "calculus"
    ],
    "topic_tags": [
      "deep-learning",
      "neural-networks",
      "tensorflow",
      "computer-vision",
      "nlp"
    ],
    "summary": "Comprehensive 5-course specialization by Andrew Ng covering foundational and advanced deep learning concepts including CNNs, RNNs, and transformers. Provides hands-on experience with TensorFlow for building real-world deep learning applications. Ideal for practitioners transitioning from traditional ML to deep learning methods.",
    "use_cases": [
      "Building image classification models for product recommendation systems",
      "Implementing sequence-to-sequence models for natural language processing tasks"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "Andrew Ng deep learning course TensorFlow",
      "best deep learning specialization for data scientists",
      "CNN RNN transformer course structured learning",
      "intermediate deep learning with practical applications"
    ]
  },
  {
    "name": "Machine Learning Specialization (Coursera)",
    "description": "Beginner-friendly 3-course series by Andrew Ng covering core ML methods (regression, classification, clustering, trees, NN) with hands-on projects.",
    "category": "Standard Errors, Bootstrapping & Reporting",
    "docs_url": "https://www.coursera.org/specializations/machine-learning-introduction/",
    "github_url": null,
    "url": "https://www.coursera.org/specializations/machine-learning-introduction/",
    "install": "",
    "tags": [
      "bootstrap",
      "standard errors"
    ],
    "best_for": "Robust inference, clustered SEs, result presentation",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-basics",
      "linear-algebra"
    ],
    "topic_tags": [
      "machine-learning",
      "supervised-learning",
      "neural-networks",
      "coursera",
      "andrew-ng"
    ],
    "summary": "Andrew Ng's comprehensive 3-course Machine Learning Specialization on Coursera provides a beginner-friendly introduction to core ML concepts including regression, classification, clustering, and neural networks. The program combines theoretical foundations with hands-on Python projects using popular libraries. Ideal for those starting their ML journey who want structured learning with practical implementation experience.",
    "use_cases": [
      "Building first predictive model for business metrics",
      "Transitioning from traditional analytics to machine learning methods"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "Andrew Ng machine learning course",
      "beginner ML specialization with hands-on projects",
      "coursera machine learning certification",
      "intro to neural networks and classification"
    ]
  },
  {
    "name": "Python for Econometrics",
    "description": "Comprehensive intro notes by Kevin Sheppard covering Python basics, core libraries, and econometrics applications.",
    "category": "Standard Errors, Bootstrapping & Reporting",
    "docs_url": "https://www.kevinsheppard.com/files/teaching/python/notes/python_introduction_2023.pdf",
    "github_url": null,
    "url": "https://www.kevinsheppard.com/files/teaching/python/notes/python_introduction_2023.pdf",
    "install": "",
    "tags": [
      "bootstrap",
      "standard errors"
    ],
    "best_for": "Robust inference, clustered SEs, result presentation",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-python-syntax",
      "linear-regression"
    ],
    "topic_tags": [
      "python-econometrics",
      "bootstrap-methods",
      "standard-errors",
      "intro-tutorial",
      "kevin-sheppard"
    ],
    "summary": "Kevin Sheppard's comprehensive introduction to using Python for econometric analysis, covering essential libraries like NumPy, Pandas, and statsmodels. The notes bridge basic Python programming with econometric applications, focusing on practical implementation of standard error calculations and bootstrapping techniques. Ideal for economists transitioning from R/Stata to Python or data scientists learning econometric methods.",
    "use_cases": [
      "Learning to implement robust standard errors and clustered standard errors in Python instead of Stata",
      "Building bootstrap confidence intervals for econometric models using Python libraries"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "python econometrics tutorial beginner",
      "how to calculate robust standard errors python",
      "bootstrap methods python economics",
      "kevin sheppard python econometrics notes"
    ]
  },
  {
    "name": "QuantEcon Lectures",
    "description": "High-quality lecture series on quantitative economic modeling, computational tools, and economics using Python/Julia.",
    "category": "Standard Errors, Bootstrapping & Reporting",
    "docs_url": "https://quantecon.org/lectures/",
    "github_url": null,
    "url": "https://quantecon.org/lectures/",
    "install": "",
    "tags": [
      "bootstrap",
      "standard errors"
    ],
    "best_for": "Robust inference, clustered SEs, result presentation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-programming",
      "basic-statistics",
      "linear-regression"
    ],
    "topic_tags": [
      "bootstrap",
      "standard-errors",
      "quantitative-economics",
      "computational-methods",
      "python-lectures"
    ],
    "summary": "Comprehensive lecture series covering quantitative economic modeling with focus on computational methods for statistical inference. Teaches bootstrap methods and standard error calculation techniques using Python and Julia for economic analysis. Designed for economists and data scientists who need rigorous statistical foundations for empirical work.",
    "use_cases": [
      "Calculating confidence intervals for regression coefficients in economic models",
      "Implementing bootstrap procedures to assess uncertainty in policy impact estimates"
    ],
    "audience": [
      "Early-PhD",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "how to calculate bootstrap standard errors in economics",
      "quantitative economics lectures python",
      "bootstrap methods for economic modeling",
      "standard error computation tutorial economics"
    ]
  },
  {
    "name": "SciPy Bootstrap",
    "description": "(`scipy.stats.bootstrap`) Computes bootstrap confidence intervals for various statistics using percentile, BCa methods.",
    "category": "Standard Errors, Bootstrapping & Reporting",
    "docs_url": "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html",
    "github_url": "https://github.com/scipy/scipy",
    "url": "https://github.com/scipy/scipy",
    "install": "pip install scipy",
    "tags": [
      "bootstrap",
      "standard errors"
    ],
    "best_for": "Robust inference, clustered SEs, result presentation",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-scipy",
      "numpy-arrays",
      "confidence-intervals"
    ],
    "topic_tags": [
      "bootstrap",
      "confidence-intervals",
      "statistical-inference",
      "scipy",
      "uncertainty-quantification"
    ],
    "summary": "SciPy's bootstrap function provides a simple interface for computing bootstrap confidence intervals using resampling methods. It supports multiple confidence interval types including percentile and bias-corrected accelerated (BCa) methods. This is essential for quantifying uncertainty when analytical standard errors are difficult to derive.",
    "use_cases": [
      "Computing confidence intervals for complex statistics like median or correlation coefficients where analytical formulas are unavailable",
      "Estimating uncertainty in A/B test metrics when sample distributions are non-normal or when dealing with ratio metrics"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "how to compute bootstrap confidence intervals in python",
      "scipy bootstrap function tutorial",
      "bootstrap standard errors for median",
      "BCa bootstrap confidence intervals scipy"
    ]
  },
  {
    "name": "Stargazer",
    "description": "Python port of R's stargazer for creating publication-quality regression tables (HTML, LaTeX) from `statsmodels` & `linearmodels` results.",
    "category": "Standard Errors, Bootstrapping & Reporting",
    "docs_url": null,
    "github_url": "https://github.com/StatsReporting/stargazer",
    "url": "https://github.com/StatsReporting/stargazer",
    "install": "pip install stargazer",
    "tags": [
      "bootstrap",
      "standard errors"
    ],
    "best_for": "Robust inference, clustered SEs, result presentation",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-statsmodels",
      "regression-analysis",
      "latex-basics"
    ],
    "topic_tags": [
      "regression-tables",
      "publication-formatting",
      "statsmodels",
      "latex-output",
      "research-reporting"
    ],
    "summary": "Stargazer is a Python package that creates professional, publication-ready regression tables from statsmodels and linearmodels results. It formats output for HTML and LaTeX, making it easy to include statistical results in academic papers and reports. The package is particularly valuable for researchers who need clean, standardized table formatting without manual work.",
    "use_cases": [
      "Creating formatted regression tables for academic paper submission with multiple model comparisons",
      "Generating HTML tables of econometric results for internal research reports and presentations"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "python package for regression tables like R stargazer",
      "how to format statsmodels output for latex",
      "publication quality regression tables python",
      "convert statsmodels results to formatted table"
    ]
  },
  {
    "name": "The Missing Semester of Your CS Education (MIT)",
    "description": "Teaches essential developer tools often skipped in formal education\u2014command line, Git, Vim, scripting, debugging, etc.",
    "category": "Standard Errors, Bootstrapping & Reporting",
    "docs_url": "https://missing.csail.mit.edu/",
    "github_url": null,
    "url": "https://missing.csail.mit.edu/",
    "install": "",
    "tags": [
      "bootstrap",
      "standard errors"
    ],
    "best_for": "Robust inference, clustered SEs, result presentation",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-programming",
      "command-line-basics"
    ],
    "topic_tags": [
      "developer-tools",
      "command-line",
      "git",
      "vim",
      "debugging"
    ],
    "summary": "MIT's comprehensive course covering essential developer tools and workflows that are typically missing from computer science curricula. Teaches practical skills like shell scripting, version control, text editors, and debugging that are crucial for day-to-day programming work. Perfect foundation for anyone entering tech roles who needs to master the development environment.",
    "use_cases": [
      "New data scientist needs to learn Git for collaboration and version control of analysis code",
      "PhD student wants to improve productivity with command-line tools and automated workflows for research"
    ],
    "audience": [
      "Junior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "essential developer tools for data scientists",
      "learn command line and git basics",
      "missing programming skills not taught in school",
      "developer workflow tools tutorial"
    ]
  },
  {
    "name": "wildboottest",
    "description": "Fast implementation of various wild cluster bootstrap algorithms (WCR, WCU) for robust inference, especially with few clusters.",
    "category": "Standard Errors, Bootstrapping & Reporting",
    "docs_url": "https://py-econometrics.github.io/wildboottest/",
    "github_url": "https://github.com/py-econometrics/wildboottest",
    "url": "https://github.com/py-econometrics/wildboottest",
    "install": "pip install wildboottest",
    "tags": [
      "bootstrap",
      "standard errors"
    ],
    "best_for": "Robust inference, clustered SEs, result presentation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "clustered-standard-errors",
      "bootstrap-methods",
      "python-regression"
    ],
    "topic_tags": [
      "wild-bootstrap",
      "clustered-inference",
      "robust-standard-errors",
      "few-clusters",
      "python-package"
    ],
    "summary": "A Python package implementing fast wild cluster bootstrap algorithms for robust statistical inference when you have few clusters. It's particularly useful for economists and data scientists who need reliable confidence intervals and p-values in clustered data settings where traditional asymptotic methods fail. The package offers both restricted (WCR) and unrestricted (WCU) wild bootstrap variants with computational optimizations.",
    "use_cases": [
      "A/B testing with few treatment groups where you need robust confidence intervals for treatment effects",
      "Evaluating policy interventions across states/regions when you have limited geographic clusters"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "wild bootstrap for few clusters python",
      "robust standard errors clustered data bootstrap",
      "wildboottest package implementation guide",
      "bootstrap inference few clusters econometrics"
    ]
  },
  {
    "name": "FilterPy",
    "description": "Focuses on Kalman filters (standard, EKF, UKF) and smoothers with a clear, pedagogical implementation style.",
    "category": "State Space & Volatility Models",
    "docs_url": "https://filterpy.readthedocs.io/en/latest/",
    "github_url": "https://github.com/rlabbe/filterpy",
    "url": "https://github.com/rlabbe/filterpy",
    "install": "pip install filterpy",
    "tags": [
      "volatility",
      "state space"
    ],
    "best_for": "GARCH, stochastic volatility, Kalman filtering",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-numpy",
      "linear-algebra",
      "bayesian-inference"
    ],
    "topic_tags": [
      "kalman-filter",
      "time-series",
      "state-estimation",
      "python-package",
      "signal-processing"
    ],
    "summary": "FilterPy is a Python library that provides clear, educational implementations of Kalman filters and related state estimation algorithms. It's designed with a pedagogical approach, making complex filtering concepts accessible through well-documented code and examples. The library covers standard Kalman filters, Extended Kalman Filters (EKF), Unscented Kalman Filters (UKF), and various smoothing algorithms.",
    "use_cases": [
      "tracking user engagement states over time in product analytics",
      "estimating hidden volatility regimes in financial time series data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python kalman filter implementation tutorial",
      "how to estimate hidden states in time series",
      "filterpy vs other state space modeling libraries",
      "kalman filter for volatility modeling python"
    ]
  },
  {
    "name": "Metran",
    "description": "Specialized package for estimating Dynamic Factor Models (DFM) using state-space methods and Kalman filtering.",
    "category": "State Space & Volatility Models",
    "docs_url": null,
    "github_url": "https://github.com/pastas/metran",
    "url": "https://github.com/pastas/metran",
    "install": "pip install metran",
    "tags": [
      "volatility",
      "state space"
    ],
    "best_for": "GARCH, stochastic volatility, Kalman filtering",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "kalman-filtering",
      "time-series-analysis",
      "maximum-likelihood-estimation"
    ],
    "topic_tags": [
      "dynamic-factor-models",
      "kalman-filtering",
      "state-space-models",
      "time-series",
      "econometrics"
    ],
    "summary": "Metran is a specialized package for estimating Dynamic Factor Models (DFM) that capture common trends across multiple time series using state-space representation and Kalman filtering. It's primarily used by econometricians and quantitative researchers working with high-dimensional time series data. The package enables decomposition of observed variables into common factors and idiosyncratic components for forecasting and dimensionality reduction.",
    "use_cases": [
      "Nowcasting GDP using mixed-frequency economic indicators",
      "Risk factor modeling in finance with multiple asset return series"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "dynamic factor model python package",
      "kalman filter state space econometrics",
      "DFM estimation mixed frequency data",
      "nowcasting with dynamic factors"
    ]
  },
  {
    "name": "PyKalman",
    "description": "Implements Kalman filter, smoother, and EM algorithm for parameter estimation, including support for missing values and UKF.",
    "category": "State Space & Volatility Models",
    "docs_url": "https://pypi.org/project/pykalman/",
    "github_url": "https://github.com/pykalman/pykalman",
    "url": "https://github.com/pykalman/pykalman",
    "install": "pip install pykalman",
    "tags": [
      "volatility",
      "state space"
    ],
    "best_for": "GARCH, stochastic volatility, Kalman filtering",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-numpy",
      "linear-algebra",
      "time-series-analysis"
    ],
    "topic_tags": [
      "kalman-filter",
      "state-space-models",
      "time-series",
      "parameter-estimation",
      "python-package"
    ],
    "summary": "PyKalman is a Python implementation of Kalman filtering algorithms for state space modeling and time series analysis. It provides tools for filtering, smoothing, and parameter estimation using the EM algorithm, with robust handling of missing data. The package is particularly useful for economists and data scientists working with dynamic models and noisy time series data.",
    "use_cases": [
      "Tracking dynamic pricing models with noisy market data and missing observations",
      "Estimating hidden economic indicators like consumer sentiment from observable market variables"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python kalman filter implementation missing data",
      "state space model parameter estimation EM algorithm",
      "kalman smoother python package economics",
      "unscented kalman filter python library"
    ]
  },
  {
    "name": "PyMC Statespace",
    "description": "(See Bayesian) Bayesian state-space modeling using PyMC, integrating Kalman filtering within MCMC for parameter estimation.",
    "category": "State Space & Volatility Models",
    "docs_url": "https://pymc-statespace.readthedocs.io/en/latest/",
    "github_url": "https://github.com/pymc-devs/pymc-statespace",
    "url": "https://github.com/pymc-devs/pymc-statespace",
    "install": "pip install pymc-statespace",
    "tags": [
      "volatility",
      "state space",
      "Bayesian"
    ],
    "best_for": "GARCH, stochastic volatility, Kalman filtering",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "PyMC-basics",
      "Kalman-filtering",
      "MCMC-methods"
    ],
    "topic_tags": [
      "state-space-models",
      "Bayesian-inference",
      "time-series",
      "volatility-modeling",
      "MCMC"
    ],
    "summary": "PyMC Statespace combines Bayesian inference with state-space modeling by integrating Kalman filtering within MCMC parameter estimation. It enables researchers to model latent states and time-varying parameters with full uncertainty quantification. This package is particularly valuable for economists and data scientists working with dynamic systems where traditional frequentist approaches fall short.",
    "use_cases": [
      "Modeling time-varying volatility in financial returns with parameter uncertainty",
      "Estimating latent economic states like business cycle phases with Bayesian credible intervals"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "Bayesian state space modeling in Python",
      "PyMC Kalman filter MCMC",
      "time varying parameter estimation Bayesian",
      "volatility modeling with uncertainty quantification"
    ]
  },
  {
    "name": "stochvol",
    "description": "Efficient Bayesian estimation of stochastic volatility (SV) models using MCMC.",
    "category": "State Space & Volatility Models",
    "docs_url": "https://stochvol.readthedocs.io/en/latest/",
    "github_url": "https://github.com/rektory/stochvol",
    "url": "https://github.com/rektory/stochvol",
    "install": "pip install stochvol",
    "tags": [
      "volatility",
      "state space",
      "Bayesian"
    ],
    "best_for": "GARCH, stochastic volatility, Kalman filtering",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "MCMC-methods",
      "bayesian-inference",
      "time-series-analysis"
    ],
    "topic_tags": [
      "stochastic-volatility",
      "bayesian-estimation",
      "financial-econometrics",
      "time-series",
      "MCMC"
    ],
    "summary": "The stochvol package provides efficient Bayesian estimation of stochastic volatility models using Markov Chain Monte Carlo methods. It's designed for researchers and practitioners working with financial time series who need to model time-varying volatility patterns. The package offers fast implementation of various SV model specifications with comprehensive diagnostic tools.",
    "use_cases": [
      "Modeling volatility clustering in stock returns for risk management applications",
      "Estimating time-varying volatility parameters for option pricing and derivative valuation"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "bayesian stochastic volatility estimation R",
      "MCMC volatility modeling package",
      "stochastic volatility models implementation",
      "time varying volatility bayesian analysis"
    ]
  },
  {
    "name": "HypoRS",
    "description": "Hypothesis testing library for Rust with T-tests, Z-tests, ANOVA, Chi-square, designed to work seamlessly with Polars DataFrames.",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": "https://lib.rs/crates/hypors",
    "github_url": "https://github.com/astronights/hypors",
    "url": "https://crates.io/crates/hypors",
    "install": "cargo add hypors",
    "tags": [
      "rust",
      "hypothesis testing",
      "t-test",
      "ANOVA",
      "polars"
    ],
    "best_for": "Statistical hypothesis testing with Polars integration",
    "language": "Rust",
    "difficulty": "intermediate",
    "prerequisites": [
      "rust-programming",
      "statistical-hypothesis-testing",
      "polars-dataframes"
    ],
    "topic_tags": [
      "hypothesis-testing",
      "rust-statistics",
      "statistical-inference",
      "polars-integration",
      "t-tests"
    ],
    "summary": "HypoRS is a comprehensive hypothesis testing library for Rust that implements classical statistical tests including T-tests, Z-tests, ANOVA, and Chi-square tests. It's designed with seamless Polars DataFrame integration, making it ideal for data scientists working in the Rust ecosystem. The library provides a modern, performant alternative to Python's scipy.stats for statistical inference tasks.",
    "use_cases": [
      "A/B testing analysis on user engagement metrics stored in Polars DataFrames",
      "Quality control testing in manufacturing data pipelines built with Rust"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "rust library for statistical hypothesis testing",
      "how to do t-tests in rust with polars",
      "statistical inference library rust dataframes",
      "rust alternative to scipy stats hypothesis testing"
    ]
  },
  {
    "name": "Pingouin",
    "description": "User-friendly interface for common statistical tests (ANOVA, ANCOVA, t-tests, correlations, chi\u00b2, reliability) built on pandas & scipy.",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": "https://pingouin-stats.org/",
    "github_url": "https://github.com/raphaelvallat/pingouin",
    "url": "https://github.com/raphaelvallat/pingouin",
    "install": "pip install pingouin",
    "tags": [
      "inference",
      "hypothesis testing"
    ],
    "best_for": "Hypothesis tests, confidence intervals, multiple testing",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-pandas",
      "scipy-stats",
      "statistical-hypothesis-testing"
    ],
    "topic_tags": [
      "statistical-testing",
      "python-package",
      "anova",
      "correlation-analysis",
      "t-tests"
    ],
    "summary": "Pingouin is a beginner-friendly Python package that simplifies running common statistical tests like ANOVA, t-tests, and correlations with pandas-style syntax. It provides clean, interpretable outputs for hypothesis testing without requiring deep statistical programming knowledge. Perfect for data scientists who need reliable statistical inference tools with minimal setup complexity.",
    "use_cases": [
      "A/B testing conversion rates between different website variants using t-tests and effect size calculations",
      "Analyzing survey data to test correlations between user satisfaction scores and product features"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "easy to use python package for statistical tests",
      "beginner friendly ANOVA in python",
      "simple correlation analysis python library",
      "pandas compatible statistical testing package"
    ]
  },
  {
    "name": "PyWhy-Stats",
    "description": "Part of the PyWhy ecosystem providing statistical methods specifically for causal applications, including various independence tests and power-divergence methods.",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": "https://pywhy-stats.readthedocs.io/",
    "github_url": "https://github.com/py-why/pywhy-stats",
    "url": "https://github.com/py-why/pywhy-stats",
    "install": "pip install pywhy-stats",
    "tags": [
      "inference",
      "hypothesis testing"
    ],
    "best_for": "Hypothesis tests, confidence intervals, multiple testing",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scipy-stats",
      "causal-inference-basics"
    ],
    "topic_tags": [
      "independence-tests",
      "causal-inference",
      "statistical-testing",
      "python-package",
      "hypothesis-testing"
    ],
    "summary": "PyWhy-Stats is a specialized statistical package within the PyWhy ecosystem that provides independence tests and statistical methods tailored for causal inference applications. It offers implementations of various hypothesis tests and power-divergence methods specifically designed to support causal analysis workflows. Data scientists and researchers use it to test assumptions and validate statistical relationships in causal modeling pipelines.",
    "use_cases": [
      "Testing conditional independence assumptions before running a causal inference model",
      "Validating instrumental variable assumptions by testing independence between instruments and confounders"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "independence tests for causal inference python",
      "PyWhy statistical testing package",
      "how to test causal assumptions statistically",
      "python library for causal inference hypothesis testing"
    ]
  },
  {
    "name": "Scipy.stats",
    "description": "Foundational module within SciPy for a wide range of statistical functions, distributions, and hypothesis tests (t-tests, ANOVA, chi\u00b2, KS, etc.).",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": "https://docs.scipy.org/doc/scipy/reference/stats.html",
    "github_url": "https://github.com/scipy/scipy",
    "url": "https://github.com/scipy/scipy",
    "install": "pip install scipy",
    "tags": [
      "inference",
      "hypothesis testing"
    ],
    "best_for": "Hypothesis tests, confidence intervals, multiple testing",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-basics",
      "numpy-arrays",
      "descriptive-statistics"
    ],
    "topic_tags": [
      "statistical-testing",
      "probability-distributions",
      "python-statistics",
      "hypothesis-testing",
      "significance-testing"
    ],
    "summary": "Scipy.stats is the go-to Python module for statistical analysis, providing implementations of dozens of probability distributions and essential hypothesis tests. It's the foundational toolkit for data scientists and researchers who need to perform statistical inference, from basic t-tests to complex goodness-of-fit tests. The module handles both the computational heavy-lifting and provides intuitive interfaces for common statistical workflows.",
    "use_cases": [
      "A/B testing analyst comparing conversion rates between two app versions using t-tests or chi-square tests",
      "Research scientist validating that their experimental data follows a normal distribution before applying parametric methods"
    ],
    "audience": [
      "Junior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "python statistical tests package",
      "how to do t-test in python",
      "scipy stats tutorial for beginners",
      "python hypothesis testing library"
    ]
  },
  {
    "name": "Statrs",
    "description": "Comprehensive statistical distributions for Rust (Normal, T, Gamma, etc.) with PDF, CDF, quantile functions\u2014the scipy.stats equivalent.",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": "https://docs.rs/statrs",
    "github_url": "https://github.com/statrs-dev/statrs",
    "url": "https://crates.io/crates/statrs",
    "install": "cargo add statrs",
    "tags": [
      "rust",
      "statistics",
      "distributions",
      "probability"
    ],
    "best_for": "Probability distributions and basic statistics in Rust",
    "language": "Rust",
    "difficulty": "intermediate",
    "prerequisites": [
      "rust-programming",
      "probability-distributions",
      "scipy-stats"
    ],
    "topic_tags": [
      "statistical-distributions",
      "rust-statistics",
      "probability-functions",
      "quantile-methods"
    ],
    "summary": "Statrs is a Rust library providing comprehensive statistical distributions with PDF, CDF, and quantile functions, serving as the Rust equivalent to Python's scipy.stats. It's designed for data scientists and researchers who need high-performance statistical computations in Rust applications. The library supports major distributions like Normal, T, Gamma, and others with reliable numerical implementations.",
    "use_cases": [
      "Building high-performance statistical applications in Rust that require probability calculations",
      "Implementing Monte Carlo simulations or statistical models where Python's performance is insufficient"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "rust equivalent of scipy stats distributions",
      "statistical distributions library for rust",
      "how to compute pdf cdf in rust",
      "rust probability distributions package"
    ]
  },
  {
    "name": "expectation",
    "description": "E-values and game-theoretic probability for sequential testing. Enables early signal detection with proper error control.",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": null,
    "github_url": "https://github.com/jakorostami/expectation",
    "url": "https://pypi.org/project/expectation/",
    "install": "pip install expectation",
    "tags": [
      "sequential testing",
      "e-values",
      "hypothesis testing"
    ],
    "best_for": "E-value based sequential hypothesis testing",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-scipy",
      "hypothesis-testing",
      "sequential-analysis"
    ],
    "topic_tags": [
      "e-values",
      "sequential-testing",
      "game-theoretic-probability",
      "early-stopping",
      "statistical-inference"
    ],
    "summary": "E-values provide a game-theoretic approach to hypothesis testing that enables continuous monitoring and early stopping with guaranteed error control. This package implements sequential testing methods that can detect signals as soon as they emerge, without the multiple testing penalties of traditional p-values. It's particularly valuable for A/B testing and clinical trials where early decision-making is critical.",
    "use_cases": [
      "A/B testing with early stopping when treatment effect is detected",
      "Clinical trial monitoring for safety signals with continuous data collection"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "sequential testing with early stopping control",
      "e-values vs p-values for A/B testing",
      "game theoretic probability python package",
      "continuous monitoring hypothesis testing methods"
    ]
  },
  {
    "name": "gcimpute",
    "description": "Gaussian copula imputation for mixed variable types with streaming capability (Journal of Statistical Software 2024).",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": null,
    "github_url": "https://github.com/udellgroup/gcimpute",
    "url": "https://github.com/udellgroup/gcimpute",
    "install": "pip install gcimpute",
    "tags": [
      "missing data",
      "imputation"
    ],
    "best_for": "Mixed-type missing data imputation with copulas",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "missing-data-theory",
      "copula-models"
    ],
    "topic_tags": [
      "gaussian-copula",
      "missing-data",
      "imputation",
      "mixed-data-types",
      "streaming-data"
    ],
    "summary": "A Python package implementing Gaussian copula-based imputation for datasets with mixed variable types (continuous, categorical, ordinal). It handles missing data by modeling the dependence structure between variables using copulas, with streaming capability for large datasets that don't fit in memory.",
    "use_cases": [
      "Imputing missing values in customer datasets with mixed demographics and behavioral features",
      "Preprocessing large streaming datasets with missing entries for real-time ML pipelines"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "gaussian copula imputation mixed data types",
      "streaming missing data imputation python",
      "copula based imputation package",
      "mixed variable imputation large datasets"
    ]
  },
  {
    "name": "hypothetical",
    "description": "Library focused on hypothesis testing: ANOVA/MANOVA, t-tests, chi-square, Fisher's exact, nonparametric tests (Mann-Whitney, Kruskal-Wallis, etc.).",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": null,
    "github_url": "https://github.com/aschleg/hypothetical",
    "url": "https://github.com/aschleg/hypothetical",
    "install": "pip install hypothetical",
    "tags": [
      "inference",
      "hypothesis testing"
    ],
    "best_for": "Hypothesis tests, confidence intervals, multiple testing",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-pandas",
      "descriptive-statistics",
      "p-values"
    ],
    "topic_tags": [
      "hypothesis-testing",
      "statistical-tests",
      "anova",
      "t-tests",
      "nonparametric"
    ],
    "summary": "A Python library providing comprehensive hypothesis testing tools including parametric tests (t-tests, ANOVA) and nonparametric alternatives (Mann-Whitney, Kruskal-Wallis). Essential for data scientists and researchers who need to validate statistical claims and compare groups in their analyses. Offers both classical and robust testing methods with clear result interpretation.",
    "use_cases": [
      "Testing whether a new feature significantly improves user engagement metrics across different user segments",
      "Comparing treatment effectiveness across multiple groups in a clinical trial or A/B test experiment"
    ],
    "audience": [
      "Junior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "python library for t-tests and ANOVA",
      "how to do hypothesis testing in python",
      "statistical significance testing tools",
      "nonparametric tests python package"
    ]
  },
  {
    "name": "lifelines",
    "description": "Comprehensive library for survival analysis: Kaplan-Meier, Nelson-Aalen, Cox regression, AFT models, handling censored data.",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": "https://lifelines.readthedocs.io/en/latest/",
    "github_url": "https://github.com/CamDavidsonPilon/lifelines",
    "url": "https://github.com/CamDavidsonPilon/lifelines",
    "install": "pip install lifelines",
    "tags": [
      "inference",
      "hypothesis testing"
    ],
    "best_for": "Hypothesis tests, confidence intervals, multiple testing",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scipy-stats",
      "regression-analysis"
    ],
    "topic_tags": [
      "survival-analysis",
      "kaplan-meier",
      "cox-regression",
      "censored-data",
      "python-library"
    ],
    "summary": "Lifelines is a Python library for survival analysis that handles time-to-event data with censoring. It provides implementations of key survival models including Kaplan-Meier curves, Cox proportional hazards, and accelerated failure time models. Data scientists and researchers use it to analyze customer churn, equipment failure, clinical trials, and other duration-based phenomena.",
    "use_cases": [
      "Analyzing customer subscription lifetimes and churn patterns for retention modeling",
      "Studying user engagement duration and feature adoption survival curves in product analytics"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for survival analysis",
      "how to do kaplan meier analysis",
      "cox regression python implementation",
      "analyzing time to event data with censoring"
    ]
  },
  {
    "name": "miceforest",
    "description": "LightGBM-accelerated multiple imputation by chained equations. Fast MICE for large datasets.",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": "https://miceforest.readthedocs.io/",
    "github_url": "https://github.com/AnotherSamWilson/miceforest",
    "url": "https://github.com/AnotherSamWilson/miceforest",
    "install": "pip install miceforest",
    "tags": [
      "missing data",
      "imputation",
      "machine learning"
    ],
    "best_for": "Fast MICE imputation with LightGBM",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "missing-data-mechanisms",
      "gradient-boosting"
    ],
    "topic_tags": [
      "multiple-imputation",
      "missing-data",
      "lightgbm",
      "mice",
      "data-preprocessing"
    ],
    "summary": "miceforest is a Python package that implements Multiple Imputation by Chained Equations (MICE) using LightGBM as the underlying model. It provides a fast, scalable solution for handling missing data in large datasets by leveraging gradient boosting instead of traditional linear models. The package is particularly useful for data scientists working with complex datasets where missing data patterns are non-linear.",
    "use_cases": [
      "Imputing missing values in customer datasets with mixed data types before running machine learning models",
      "Handling missing survey responses in social science research with complex interaction patterns"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "fast multiple imputation python large datasets",
      "lightgbm mice implementation missing data",
      "how to handle missing values with gradient boosting",
      "miceforest vs sklearn iterative imputer performance"
    ]
  },
  {
    "name": "savvi",
    "description": "Safe Anytime Valid Inference using e-processes and confidence sequences (Ramdas et al. 2023). Valid inference at any stopping time.",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": null,
    "github_url": "https://github.com/assuncaolfi/savvi",
    "url": "https://pypi.org/project/savvi/",
    "install": "pip install savvi",
    "tags": [
      "sequential testing",
      "A/B testing",
      "anytime valid"
    ],
    "best_for": "Always-valid sequential inference for experiments",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "hypothesis-testing",
      "sequential-analysis",
      "martingale-theory"
    ],
    "topic_tags": [
      "sequential-testing",
      "anytime-valid-inference",
      "e-processes",
      "confidence-sequences",
      "optional-stopping"
    ],
    "summary": "SAVVI implements e-processes and confidence sequences for statistically valid inference at any stopping time, solving the optional stopping problem in sequential testing. It enables researchers to peek at results and stop experiments early while maintaining statistical validity. Particularly valuable for A/B testing and adaptive trial designs where traditional p-values break down under sequential monitoring.",
    "use_cases": [
      "Running A/B tests where you need to monitor results continuously and stop early if significant effects are detected",
      "Clinical trials or expensive experiments where you want valid statistical inference even when stopping rules depend on observed data"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "anytime valid inference python package",
      "sequential A/B testing without multiple testing corrections",
      "e-processes confidence sequences implementation",
      "optional stopping problem solution for experiments"
    ]
  },
  {
    "name": "Dolo",
    "description": "Framework for describing and solving economic models (DSGE, OLG, etc.) using a declarative YAML-based format.",
    "category": "Structural Econometrics & Estimation",
    "docs_url": "https://dolo.readthedocs.io/en/latest/",
    "github_url": "https://github.com/EconForge/dolo",
    "url": "https://github.com/EconForge/dolo",
    "install": "pip install dolo",
    "tags": [
      "structural",
      "estimation"
    ],
    "best_for": "Structural models, GMM estimation, BLP-style demand",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "python-programming",
      "dynamic-programming",
      "macroeconomic-theory"
    ],
    "topic_tags": [
      "dsge-models",
      "economic-modeling",
      "yaml-configuration",
      "dynamic-programming",
      "macroeconomics"
    ],
    "summary": "Dolo is a Python framework that allows economists to specify and solve complex dynamic economic models using human-readable YAML configuration files. It handles DSGE models, overlapping generations models, and other structural economic frameworks with built-in solution algorithms. The declarative approach separates model specification from solution methods, making economic models more reproducible and easier to modify.",
    "use_cases": [
      "Building and solving DSGE models for monetary policy analysis without writing low-level numerical code",
      "Comparing solution methods across different overlapping generations models using consistent YAML specifications"
    ],
    "audience": [
      "Early-PhD",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "How to solve DSGE models in Python",
      "YAML-based economic modeling framework",
      "Tools for structural macroeconomic models",
      "Dolo vs Dynare for economic modeling"
    ]
  },
  {
    "name": "Greeners",
    "description": "Comprehensive Rust econometrics library with OLS, IV, panel data estimators, fixed effects, DiD, and heteroskedasticity-robust standard errors (HC0-HC3).",
    "category": "Structural Econometrics & Estimation",
    "docs_url": "https://docs.rs/greeners",
    "github_url": "https://github.com/greeners-rs/greeners",
    "url": "https://crates.io/crates/greeners",
    "install": "cargo add greeners",
    "tags": [
      "rust",
      "econometrics",
      "IV",
      "panel data",
      "robust SE"
    ],
    "best_for": "Academic econometrics in Rust: IV, DiD, robust SEs",
    "language": "Rust",
    "difficulty": "intermediate",
    "prerequisites": [
      "rust-programming",
      "linear-regression",
      "instrumental-variables"
    ],
    "topic_tags": [
      "rust-econometrics",
      "panel-data",
      "difference-in-differences",
      "robust-standard-errors",
      "instrumental-variables"
    ],
    "summary": "Greeners is a comprehensive Rust econometrics library offering core estimation methods including OLS, instrumental variables, and panel data models. It provides robust standard error corrections and implements difference-in-differences estimators with fixed effects capabilities. The library is designed for economists and data scientists who need high-performance econometric analysis in Rust environments.",
    "use_cases": [
      "Estimating causal effects of policy interventions using difference-in-differences with panel data",
      "Running instrumental variables regressions with heteroskedasticity-robust standard errors for program evaluation"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "rust econometrics library with IV estimation",
      "difference in differences implementation in rust",
      "panel data fixed effects rust package",
      "robust standard errors econometrics rust"
    ]
  },
  {
    "name": "HARK",
    "description": "Toolkit for solving, simulating, and estimating models with heterogeneous agents (e.g., consumption-saving).",
    "category": "Structural Econometrics & Estimation",
    "docs_url": "https://hark.readthedocs.io/en/latest/",
    "github_url": "https://github.com/econ-ark/HARK",
    "url": "https://github.com/econ-ark/HARK",
    "install": "pip install econ-ark",
    "tags": [
      "structural",
      "estimation"
    ],
    "best_for": "Structural models, GMM estimation, BLP-style demand",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "dynamic-programming",
      "maximum-likelihood-estimation",
      "numerical-optimization"
    ],
    "topic_tags": [
      "heterogeneous-agents",
      "consumption-saving",
      "structural-models",
      "lifecycle-models",
      "behavioral-economics"
    ],
    "summary": "HARK is a Python toolkit for building and estimating structural economic models with heterogeneous agents, particularly focused on consumption-saving problems. It provides pre-built model classes and numerical methods for solving dynamic optimization problems commonly found in macroeconomics and household finance research. Researchers use it to estimate parameters, run policy simulations, and test theoretical predictions against empirical data.",
    "use_cases": [
      "Estimating how different types of households respond to changes in interest rates or income uncertainty",
      "Simulating the distributional effects of tax policy changes on consumption and savings behavior"
    ],
    "audience": [
      "Early-PhD",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "heterogeneous agent models python",
      "structural estimation consumption saving",
      "HARK toolkit economic modeling",
      "dynamic programming household behavior"
    ]
  },
  {
    "name": "QuantEcon.py",
    "description": "Core library for quantitative economics: dynamic programming, Markov chains, game theory, numerical methods.",
    "category": "Structural Econometrics & Estimation",
    "docs_url": "https://quantecon.org/python-lectures/",
    "github_url": "https://github.com/QuantEcon/QuantEcon.py",
    "url": "https://github.com/QuantEcon/QuantEcon.py",
    "install": "pip install quantecon",
    "tags": [
      "structural",
      "estimation"
    ],
    "best_for": "Structural models, GMM estimation, BLP-style demand",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-numpy",
      "linear-algebra",
      "dynamic-programming"
    ],
    "topic_tags": [
      "dynamic-programming",
      "markov-chains",
      "game-theory",
      "numerical-methods",
      "python-library"
    ],
    "summary": "QuantEcon.py is a comprehensive Python library providing computational tools for quantitative economics research and analysis. It implements core methods including dynamic programming solvers, Markov chain analysis, game theory algorithms, and numerical optimization routines. The library is widely used by economists and data scientists for structural modeling, policy analysis, and theoretical research.",
    "use_cases": [
      "Solving dynamic economic models like optimal consumption-savings problems with value function iteration",
      "Analyzing market competition using Nash equilibrium computation and evolutionary game dynamics"
    ],
    "audience": [
      "Early-PhD",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "Python library for dynamic programming in economics",
      "How to solve Markov decision processes for economic models",
      "QuantEcon package for game theory and numerical methods",
      "Tools for structural econometric modeling in Python"
    ]
  },
  {
    "name": "dcegm",
    "description": "JAX-compatible DC-EGM algorithm for discrete-continuous dynamic programming (Iskhakov et al. 2017).",
    "category": "Structural Econometrics & Estimation",
    "docs_url": null,
    "github_url": "https://github.com/OpenSourceEconomics/dcegm",
    "url": "https://github.com/OpenSourceEconomics/dcegm",
    "install": "pip install dcegm",
    "tags": [
      "structural",
      "dynamic programming",
      "JAX"
    ],
    "best_for": "Discrete-continuous choice models with EGM",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "dynamic-programming",
      "JAX",
      "structural-models"
    ],
    "topic_tags": [
      "discrete-continuous-choice",
      "endogenous-grid-method",
      "computational-economics",
      "JAX-implementation",
      "dynamic-programming"
    ],
    "summary": "JAX implementation of the DC-EGM (Discrete-Continuous Endogenous Grid Method) algorithm for solving dynamic programming problems with both discrete and continuous choices. This method efficiently handles complex structural models where agents make simultaneous decisions across different choice dimensions. Essential for researchers working on lifecycle models, consumption-saving problems, or any structural model combining discrete and continuous optimization.",
    "use_cases": [
      "Estimating lifecycle consumption-saving models with discrete labor supply choices",
      "Solving dynamic investment problems with both portfolio allocation and participation decisions"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "JAX implementation discrete continuous dynamic programming",
      "DC-EGM algorithm Python package",
      "endogenous grid method discrete continuous choice",
      "structural model solver JAX compatible"
    ]
  },
  {
    "name": "econpizza",
    "description": "Solve nonlinear heterogeneous agent models (HANK) with perfect foresight. Efficient perturbation and projection methods.",
    "category": "Structural Econometrics & Estimation",
    "docs_url": "https://econpizza.readthedocs.io/",
    "github_url": "https://github.com/gboehl/econpizza",
    "url": "https://github.com/gboehl/econpizza",
    "install": "pip install econpizza",
    "tags": [
      "structural",
      "DSGE",
      "HANK"
    ],
    "best_for": "Nonlinear HANK models with aggregate shocks",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "python-programming",
      "dynamic-programming",
      "DSGE-modeling"
    ],
    "topic_tags": [
      "heterogeneous-agents",
      "DSGE",
      "macroeconomics",
      "nonlinear-models",
      "python-package"
    ],
    "summary": "Econpizza is a Python package for solving complex heterogeneous agent New Keynesian (HANK) models with nonlinear dynamics and perfect foresight. It implements efficient perturbation and projection methods for macroeconomic models where agents have different characteristics. Primarily used by macroeconomists and central bank researchers working on monetary policy analysis.",
    "use_cases": [
      "Analyzing monetary policy transmission through heterogeneous household responses to interest rate changes",
      "Studying fiscal policy effects when agents have different income levels and borrowing constraints"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "Python package for heterogeneous agent models",
      "How to solve HANK models with perfect foresight",
      "Nonlinear DSGE model solver Python",
      "Perturbation methods for heterogeneous agent models"
    ]
  },
  {
    "name": "gEconpy",
    "description": "DSGE modeling tools inspired by R's gEcon. Automatic first-order condition derivation with Dynare export.",
    "category": "Structural Econometrics & Estimation",
    "docs_url": null,
    "github_url": "https://github.com/jessegrabowski/gEconpy",
    "url": "https://github.com/jessegrabowski/gEconpy",
    "install": "pip install gEconpy",
    "tags": [
      "structural",
      "DSGE",
      "estimation"
    ],
    "best_for": "Symbolic DSGE derivation with Dynare compatibility",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "python-sympy",
      "DSGE-modeling",
      "maximum-likelihood-estimation"
    ],
    "topic_tags": [
      "DSGE",
      "structural-modeling",
      "macroeconomics",
      "dynare",
      "python-package"
    ],
    "summary": "gEconpy is a Python package for building and estimating Dynamic Stochastic General Equilibrium (DSGE) models, inspired by R's gEcon package. It automatically derives first-order conditions from economic models and exports them to Dynare for numerical solution. The tool is primarily used by macroeconomists and researchers working on structural economic models.",
    "use_cases": [
      "Building a New Keynesian DSGE model to analyze monetary policy transmission mechanisms",
      "Estimating parameters of a real business cycle model using Bayesian methods via Dynare integration"
    ],
    "audience": [
      "Early-PhD",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "Python package for DSGE modeling",
      "automatic first order conditions derivation DSGE",
      "gEcon equivalent for Python",
      "Dynare integration Python DSGE tools"
    ]
  },
  {
    "name": "gegravity",
    "description": "General equilibrium structural gravity modeling for trade policy analysis. Only Python package for Anderson-van Wincoop GE gravity.",
    "category": "Structural Econometrics & Estimation",
    "docs_url": null,
    "github_url": "https://github.com/peter-herman/gegravity",
    "url": "https://pypi.org/project/gegravity/",
    "install": "pip install gegravity",
    "tags": [
      "trade",
      "gravity models",
      "structural"
    ],
    "best_for": "GE structural gravity for trade policy",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "trade-economics",
      "structural-econometrics",
      "python-pandas"
    ],
    "topic_tags": [
      "gravity-models",
      "trade-policy",
      "general-equilibrium",
      "structural-econometrics",
      "python-package"
    ],
    "summary": "gegravity is a specialized Python package for structural gravity modeling in international trade, implementing the Anderson-van Wincoop general equilibrium framework. It's designed for trade economists and policy analysts who need to estimate how trade costs, tariffs, and other policies affect bilateral trade flows. The package is unique as the only Python implementation of GE gravity models, making advanced trade policy analysis more accessible.",
    "use_cases": [
      "Estimating the trade effects of Brexit or other trade policy changes using structural gravity models",
      "Analyzing how transportation infrastructure investments affect bilateral trade patterns through reduced trade costs"
    ],
    "audience": [
      "Early-PhD",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "Anderson van Wincoop gravity model Python",
      "structural gravity modeling trade policy",
      "general equilibrium trade analysis package",
      "Python gravity model international trade"
    ]
  },
  {
    "name": "pydsge",
    "description": "DSGE model simulation, filtering, and Bayesian estimation. Handles occasionally binding constraints.",
    "category": "Structural Econometrics & Estimation",
    "docs_url": null,
    "github_url": "https://github.com/gboehl/pydsge",
    "url": "https://github.com/gboehl/pydsge",
    "install": "pip install pydsge",
    "tags": [
      "structural",
      "DSGE",
      "Bayesian"
    ],
    "best_for": "DSGE estimation with occasionally binding constraints",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "python-scipy",
      "bayesian-mcmc",
      "macroeconomic-theory"
    ],
    "topic_tags": [
      "DSGE-models",
      "bayesian-estimation",
      "macroeconomic-modeling",
      "structural-econometrics",
      "kalman-filtering"
    ],
    "summary": "Python package for Dynamic Stochastic General Equilibrium (DSGE) model simulation, filtering, and Bayesian estimation with support for occasionally binding constraints like zero lower bound. Primarily used by macroeconomists and central bank researchers for structural macroeconomic modeling and policy analysis. Enables full Bayesian estimation workflow from model specification to posterior inference.",
    "use_cases": [
      "Central bank economist estimating a New Keynesian DSGE model with zero lower bound constraints for monetary policy analysis",
      "Academic researcher conducting Bayesian estimation of medium-scale DSGE model to study business cycle dynamics"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "Python package for DSGE model estimation",
      "Bayesian estimation of macroeconomic models",
      "DSGE modeling with zero lower bound",
      "structural macroeconomic model simulation Python"
    ]
  },
  {
    "name": "pynare",
    "description": "Python wrapper/interface to Dynare for DSGE model solving. Bridge between Python workflows and Dynare.",
    "category": "Structural Econometrics & Estimation",
    "docs_url": null,
    "github_url": "https://github.com/gboehl/pynare",
    "url": "https://github.com/gboehl/pynare",
    "install": "pip install pynare",
    "tags": [
      "structural",
      "DSGE",
      "Dynare"
    ],
    "best_for": "Running Dynare models from Python",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-basics",
      "DSGE-modeling",
      "Dynare-syntax"
    ],
    "topic_tags": [
      "DSGE",
      "macroeconomic-modeling",
      "python-wrapper",
      "structural-estimation",
      "dynare"
    ],
    "summary": "Pynare is a Python wrapper that provides an interface to Dynare, allowing economists to solve Dynamic Stochastic General Equilibrium (DSGE) models within Python workflows. It bridges the gap between Python's data science ecosystem and Dynare's specialized DSGE modeling capabilities. This tool is particularly useful for researchers who want to integrate macroeconomic modeling with Python-based analysis pipelines.",
    "use_cases": [
      "Running DSGE model simulations and parameter estimation within Jupyter notebooks alongside other economic analysis",
      "Building automated pipelines that combine DSGE model results with Python data processing and visualization tools"
    ],
    "audience": [
      "Early-PhD",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python wrapper for Dynare DSGE models",
      "how to run Dynare from Python",
      "DSGE modeling in Python",
      "integrate Dynare with Python workflow"
    ]
  },
  {
    "name": "respy",
    "description": "Simulation and estimation of finite-horizon dynamic discrete choice (DDC) models (e.g., labor/education choice).",
    "category": "Structural Econometrics & Estimation",
    "docs_url": "https://respy.readthedocs.io/en/latest/",
    "github_url": "https://github.com/OpenSourceEconomics/respy",
    "url": "https://github.com/OpenSourceEconomics/respy",
    "install": "pip install respy",
    "tags": [
      "structural",
      "estimation"
    ],
    "best_for": "Structural models, GMM estimation, BLP-style demand",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "maximum-likelihood-estimation",
      "dynamic-programming",
      "python-numpy"
    ],
    "topic_tags": [
      "dynamic-discrete-choice",
      "structural-models",
      "labor-economics",
      "simulation",
      "estimation"
    ],
    "summary": "respy is a Python package for simulating and estimating finite-horizon dynamic discrete choice models, commonly used in labor and education economics. It allows researchers to model sequential decision-making processes where agents make discrete choices (like work vs. education) over time. The package handles the computational complexity of solving and estimating these structural models using methods like maximum likelihood.",
    "use_cases": [
      "Modeling career decisions where individuals choose between working, attending school, or staying home over their lifetime",
      "Estimating the returns to education by modeling how people decide when to enter/exit schooling based on expected future earnings"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "dynamic discrete choice model python",
      "structural labor economics estimation package",
      "simulate education career decisions model",
      "respy dynamic programming estimation"
    ]
  },
  {
    "name": "upper-envelope",
    "description": "Fast upper envelope scan for discrete-continuous dynamic programming. JAX and numba implementations.",
    "category": "Structural Econometrics & Estimation",
    "docs_url": null,
    "github_url": "https://github.com/OpenSourceEconomics/upper-envelope",
    "url": "https://github.com/OpenSourceEconomics/upper-envelope",
    "install": "pip install upper-envelope",
    "tags": [
      "structural",
      "dynamic programming",
      "optimization"
    ],
    "best_for": "Fast upper envelope computation for DC-EGM",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "dynamic-programming",
      "JAX-optimization",
      "structural-models"
    ],
    "topic_tags": [
      "upper-envelope",
      "dynamic-programming",
      "structural-estimation",
      "JAX",
      "numba"
    ],
    "summary": "Fast upper envelope scan implementation for discrete-continuous dynamic programming problems in structural econometrics. Provides optimized JAX and numba backends for computing upper envelopes efficiently in complex optimization routines. Essential for researchers solving high-dimensional structural models where computational speed is critical.",
    "use_cases": [
      "Estimating dynamic labor supply models with discrete job choices and continuous hours decisions",
      "Solving firm investment problems with discrete capacity expansion and continuous investment levels"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "fast upper envelope algorithm dynamic programming",
      "JAX implementation structural model optimization",
      "discrete continuous dynamic programming solver",
      "upper envelope scan numba JAX performance"
    ]
  },
  {
    "name": "OpenMx",
    "description": "Extended SEM software with programmatic model specification via paths (RAM) or matrix algebra, supporting mixture distributions, item factor analysis, state space models, and behavior genetics twin studies.",
    "category": "Structural Equation Modeling",
    "docs_url": "https://openmx.ssri.psu.edu/",
    "github_url": "https://github.com/OpenMx/OpenMx",
    "url": "https://cran.r-project.org/package=OpenMx",
    "install": "install.packages(\"OpenMx\")",
    "tags": [
      "SEM",
      "matrix-algebra",
      "twin-studies",
      "behavior-genetics",
      "IFA"
    ],
    "best_for": "Complex/advanced SEM, behavior genetics, and researchers needing maximum specification flexibility, implementing Neale et al. (2016)",
    "language": "R",
    "difficulty": "advanced",
    "prerequisites": [
      "structural-equation-modeling",
      "matrix-algebra",
      "R-programming"
    ],
    "topic_tags": [
      "SEM",
      "latent-variables",
      "behavior-genetics",
      "twin-studies",
      "matrix-specification"
    ],
    "summary": "OpenMx is an advanced R package for structural equation modeling that allows programmatic model specification through path diagrams or matrix algebra. It extends beyond basic SEM to support complex models including mixture distributions, item factor analysis, state space models, and specialized behavior genetics applications like twin studies.",
    "use_cases": [
      "Modeling genetic and environmental influences on traits using twin study data",
      "Building custom latent variable models with non-standard distributions or constraints"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "R package for twin study analysis behavior genetics",
      "structural equation modeling with matrix specification",
      "OpenMx vs lavaan for complex SEM models",
      "how to model genetic influences twin data R"
    ]
  },
  {
    "name": "blavaan",
    "description": "Bayesian latent variable analysis extending lavaan with MCMC estimation via Stan or JAGS, supporting Bayesian CFA, SEM, growth models, and model comparison with WAIC, LOO, and Bayes factors.",
    "category": "Structural Equation Modeling",
    "docs_url": "https://ecmerkle.github.io/blavaan/",
    "github_url": "https://github.com/ecmerkle/blavaan",
    "url": "https://cran.r-project.org/package=blavaan",
    "install": "install.packages(\"blavaan\")",
    "tags": [
      "Bayesian-SEM",
      "Stan",
      "JAGS",
      "MCMC",
      "latent-variables"
    ],
    "best_for": "Bayesian inference for SEM models using familiar lavaan syntax, implementing Merkle & Rosseel (2018)",
    "language": "R",
    "difficulty": "advanced",
    "prerequisites": [
      "lavaan-SEM",
      "Stan-MCMC",
      "Bayesian-statistics"
    ],
    "topic_tags": [
      "Bayesian-SEM",
      "latent-variable-modeling",
      "MCMC-estimation",
      "structural-equation-modeling",
      "Stan"
    ],
    "summary": "blavaan extends the popular lavaan package to perform Bayesian structural equation modeling using MCMC estimation via Stan or JAGS. It enables researchers to incorporate prior knowledge, quantify uncertainty through posterior distributions, and perform robust model comparison using Bayesian information criteria. This package is essential for psychometricians and social scientists who need principled uncertainty quantification in their latent variable models.",
    "use_cases": [
      "Testing measurement invariance across groups while incorporating prior knowledge about factor loadings",
      "Building confirmatory factor analysis models with informative priors when sample sizes are small"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "Bayesian structural equation modeling in R",
      "how to do Bayesian CFA with Stan",
      "lavaan Bayesian extension MCMC",
      "Bayesian latent variable analysis package"
    ]
  },
  {
    "name": "lavaan",
    "description": "Free, open-source latent variable analysis providing commercial-quality functionality for path analysis, confirmatory factor analysis, structural equation modeling, and growth curve models with intuitive model syntax.",
    "category": "Structural Equation Modeling",
    "docs_url": "https://lavaan.ugent.be/",
    "github_url": "https://github.com/yrosseel/lavaan",
    "url": "https://cran.r-project.org/package=lavaan",
    "install": "install.packages(\"lavaan\")",
    "tags": [
      "SEM",
      "CFA",
      "path-analysis",
      "latent-variables",
      "psychometrics"
    ],
    "best_for": "General-purpose structural equation modeling with accessible syntax for researchers, implementing Rosseel (2012)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "R-programming",
      "linear-regression",
      "factor-analysis"
    ],
    "topic_tags": [
      "structural-equation-modeling",
      "confirmatory-factor-analysis",
      "latent-variables",
      "psychometrics",
      "R-package"
    ],
    "summary": "lavaan is an R package for structural equation modeling that provides an intuitive syntax for specifying complex latent variable models. It's widely used by researchers in psychology, marketing, and social sciences for testing theoretical frameworks and measurement models. The package offers comprehensive functionality for path analysis, confirmatory factor analysis, and growth curve modeling with robust statistical output.",
    "use_cases": [
      "Testing whether customer satisfaction mediates the relationship between product quality and purchase intention",
      "Validating a multi-factor survey instrument by confirming the underlying latent construct structure"
    ],
    "audience": [
      "Early-PhD",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for structural equation modeling",
      "how to do confirmatory factor analysis in R",
      "lavaan SEM tutorial",
      "latent variable modeling R package"
    ]
  },
  {
    "name": "SDV (Synthetic Data Vault)",
    "description": "Comprehensive library for generating synthetic tabular, relational, and time series data using various models.",
    "category": "Synthetic Data Generation",
    "docs_url": "https://sdv.dev/",
    "github_url": "https://github.com/sdv-dev/SDV",
    "url": "https://github.com/sdv-dev/SDV",
    "install": "pip install sdv",
    "tags": [
      "synthetic data",
      "simulation"
    ],
    "best_for": "Privacy-preserving data, simulation, augmentation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn",
      "statistical-distributions"
    ],
    "topic_tags": [
      "synthetic-data",
      "privacy-preserving",
      "tabular-data",
      "data-simulation",
      "generative-models"
    ],
    "summary": "SDV is a comprehensive Python library for generating synthetic data that maintains statistical properties of original datasets. It supports tabular, relational, and time series data using various generative models including GANs and statistical approaches. Data scientists and researchers use it for privacy-preserving analytics, testing pipelines, and data augmentation.",
    "use_cases": [
      "Creating synthetic customer data for model testing when production data contains PII",
      "Generating additional training samples for machine learning models with limited data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "How to generate synthetic data that preserves privacy",
      "Python library for creating fake tabular data",
      "SDV synthetic data vault tutorial",
      "Generate realistic synthetic datasets for testing"
    ]
  },
  {
    "name": "Synthpop",
    "description": "Port of the R package for generating synthetic populations based on sample survey data.",
    "category": "Synthetic Data Generation",
    "docs_url": null,
    "github_url": "https://github.com/alan-turing-institute/synthpop",
    "url": "https://github.com/alan-turing-institute/synthpop",
    "install": "pip install synthpop",
    "tags": [
      "synthetic data",
      "simulation"
    ],
    "best_for": "Privacy-preserving data, simulation, augmentation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "survey-sampling",
      "data-privacy-techniques"
    ],
    "topic_tags": [
      "synthetic-data",
      "survey-simulation",
      "privacy-preserving",
      "population-modeling",
      "r-package-port"
    ],
    "summary": "Synthpop is a Python port of the popular R package that generates synthetic datasets mimicking the statistical properties of original survey data while preserving privacy. It's widely used by researchers and data scientists who need realistic fake data for testing, sharing, or publication. The package implements multiple synthesis methods to create populations that maintain the relationships and distributions found in sensitive source data.",
    "use_cases": [
      "Creating shareable datasets for academic research when original survey data contains PII",
      "Generating realistic test data for validating analysis pipelines before accessing production survey data"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "how to generate synthetic survey data in python",
      "synthpop package for creating fake population data",
      "privacy preserving synthetic data generation tools",
      "python alternative to R synthpop package"
    ]
  },
  {
    "name": "quanteda",
    "description": "Comprehensive framework for quantitative text analysis. Provides fast text preprocessing, document-feature matrices, dictionary analysis, and integration with topic models. Standard for political science text analysis.",
    "category": "Text Analysis",
    "docs_url": "https://quanteda.io/",
    "github_url": "https://github.com/quanteda/quanteda",
    "url": "https://cran.r-project.org/package=quanteda",
    "install": "install.packages(\"quanteda\")",
    "tags": [
      "text-analysis",
      "NLP",
      "document-term-matrix",
      "text-preprocessing",
      "political-science"
    ],
    "best_for": "Comprehensive quantitative text analysis with fast preprocessing and document-feature matrices",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "R-programming",
      "document-term-matrices",
      "text-tokenization"
    ],
    "topic_tags": [
      "text-analysis",
      "R-package",
      "document-feature-matrix",
      "political-text-analysis",
      "NLP-preprocessing"
    ],
    "summary": "Quanteda is R's leading package for quantitative text analysis, offering efficient text preprocessing, document-feature matrix creation, and dictionary-based analysis. Widely adopted in political science and social science research for analyzing large text corpora. Provides seamless integration with topic modeling and other advanced text mining techniques.",
    "use_cases": [
      "analyzing political speeches and manifestos to identify policy positions across parties",
      "processing social media data to measure public sentiment toward policy changes"
    ],
    "audience": [
      "Early-PhD",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for text analysis political science",
      "how to create document term matrix in R",
      "quanteda vs tidytext for NLP preprocessing",
      "best R tools for analyzing political text data"
    ]
  },
  {
    "name": "stm",
    "description": "Structural Topic Models incorporating document-level metadata as covariates affecting topic prevalence and content. Enables studying how topics vary across groups or time with uncertainty quantification.",
    "category": "Text Analysis",
    "docs_url": "https://www.structuraltopicmodel.com/",
    "github_url": "https://github.com/bstewart/stm",
    "url": "https://cran.r-project.org/package=stm",
    "install": "install.packages(\"stm\")",
    "tags": [
      "topic-models",
      "text-analysis",
      "covariates",
      "LDA",
      "document-metadata"
    ],
    "best_for": "Structural topic models with document metadata affecting topic prevalence and content",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "topic-modeling-LDA",
      "R-programming",
      "bayesian-inference"
    ],
    "topic_tags": [
      "structural-topic-models",
      "document-covariates",
      "text-mining",
      "bayesian-nlp",
      "metadata-analysis"
    ],
    "summary": "STM extends traditional topic modeling by incorporating document-level metadata (like author, time, or group) as covariates that can influence both topic prevalence and content. Unlike standard LDA, it allows researchers to study how topics systematically vary across different conditions while providing proper uncertainty quantification. Popular among social scientists and applied researchers analyzing textual data with rich metadata.",
    "use_cases": [
      "Analyzing how political speech topics change over time periods or across party affiliations",
      "Studying how product review themes vary by customer demographics or product categories"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "topic modeling with covariates R package",
      "how to include metadata in topic models",
      "structural topic models vs LDA comparison",
      "analyzing text data with document-level variables"
    ]
  },
  {
    "name": "text2vec",
    "description": "Efficient text vectorization with word embeddings (GloVe), topic models (LDA), and document similarity. Memory-efficient streaming API for large corpora with C++ backend.",
    "category": "Text Analysis",
    "docs_url": "https://text2vec.org/",
    "github_url": "https://github.com/dselivanov/text2vec",
    "url": "https://cran.r-project.org/package=text2vec",
    "install": "install.packages(\"text2vec\")",
    "tags": [
      "word-embeddings",
      "GloVe",
      "text-vectorization",
      "LDA",
      "document-similarity"
    ],
    "best_for": "Efficient word embeddings (GloVe) and text vectorization for large corpora",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-programming",
      "natural-language-processing",
      "matrix-operations"
    ],
    "topic_tags": [
      "text-vectorization",
      "word-embeddings",
      "topic-modeling",
      "document-similarity",
      "R-package"
    ],
    "summary": "text2vec is an R package for efficient text analysis providing word embeddings (GloVe), topic modeling (LDA), and document similarity calculations. It features a memory-efficient streaming API with C++ backend for processing large text corpora. Popular among data scientists for feature engineering and text preprocessing in machine learning pipelines.",
    "use_cases": [
      "Building recommendation systems by computing document similarity between user profiles and product descriptions",
      "Creating word embeddings for sentiment analysis or text classification models in production"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for word embeddings and text vectorization",
      "how to compute document similarity with GloVe embeddings",
      "memory efficient text processing for large corpora",
      "LDA topic modeling implementation in R"
    ]
  },
  {
    "name": "tidytext",
    "description": "Tidy data principles for text mining. Converts text to tidy format (one-token-per-row), enabling analysis with dplyr, ggplot2, and other tidyverse tools. Accompanies the book 'Text Mining with R'.",
    "category": "Text Analysis",
    "docs_url": "https://juliasilge.github.io/tidytext/",
    "github_url": "https://github.com/juliasilge/tidytext",
    "url": "https://cran.r-project.org/package=tidytext",
    "install": "install.packages(\"tidytext\")",
    "tags": [
      "text-mining",
      "tidyverse",
      "tokenization",
      "sentiment-analysis",
      "NLP"
    ],
    "best_for": "Tidy text mining with dplyr and ggplot2 integration\u2014accompanies 'Text Mining with R'",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "R-dplyr",
      "basic-text-preprocessing",
      "ggplot2"
    ],
    "topic_tags": [
      "text-mining",
      "tidyverse",
      "tokenization",
      "sentiment-analysis",
      "R-package"
    ],
    "summary": "tidytext is an R package that applies tidy data principles to text analysis by converting text into one-token-per-row format. It integrates seamlessly with tidyverse tools like dplyr and ggplot2, making text mining accessible to users familiar with tidy data workflows. The package is designed around the companion book 'Text Mining with R' and provides intuitive functions for tokenization, sentiment analysis, and text visualization.",
    "use_cases": [
      "Analyzing customer reviews or survey responses to extract sentiment and key themes",
      "Processing social media posts or news articles to identify trending topics and perform word frequency analysis"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "How to do text mining in R with tidyverse",
      "R package for sentiment analysis with dplyr",
      "Convert text to tidy format for analysis",
      "Text mining with R book package"
    ]
  },
  {
    "name": "ARCH",
    "description": "Specialized library for modeling and forecasting conditional volatility using ARCH, GARCH, EGARCH, and related models.",
    "category": "Time Series Econometrics",
    "docs_url": "https://arch.readthedocs.io/",
    "github_url": "https://github.com/bashtage/arch",
    "url": "https://github.com/bashtage/arch",
    "install": "pip install arch",
    "tags": [
      "time series",
      "econometrics"
    ],
    "best_for": "ARIMA, cointegration, VAR models",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "time-series-analysis",
      "maximum-likelihood-estimation"
    ],
    "topic_tags": [
      "volatility-modeling",
      "GARCH",
      "financial-econometrics",
      "heteroskedasticity",
      "python-package"
    ],
    "summary": "ARCH is a Python library for modeling time-varying volatility in financial and economic time series using ARCH, GARCH, and related models. It's widely used by quantitative analysts and researchers to forecast conditional variance and analyze volatility clustering. The package provides easy-to-use implementations of various volatility models with diagnostic tools and forecasting capabilities.",
    "use_cases": [
      "Modeling and forecasting stock return volatility for risk management",
      "Analyzing volatility clustering in cryptocurrency prices for trading strategies"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "GARCH model implementation python",
      "volatility forecasting time series",
      "conditional heteroskedasticity modeling",
      "ARCH GARCH python package"
    ]
  },
  {
    "name": "KFAS",
    "description": "State space modeling framework for exponential family time series with computationally efficient Kalman filtering, smoothing, forecasting, and simulation. Supports observations from Gaussian, Poisson, binomial, negative binomial, and gamma distributions.",
    "category": "Time Series Econometrics",
    "docs_url": "https://cran.r-project.org/web/packages/KFAS/KFAS.pdf",
    "github_url": "https://github.com/helske/KFAS",
    "url": "https://cran.r-project.org/package=KFAS",
    "install": "install.packages(\"KFAS\")",
    "tags": [
      "state-space",
      "kalman-filter",
      "time-series",
      "forecasting",
      "exponential-family"
    ],
    "best_for": "Multivariate time series modeling with non-Gaussian observations (e.g., count data with Poisson), implementing Helske (2017)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "kalman-filters",
      "maximum-likelihood-estimation",
      "R-programming"
    ],
    "topic_tags": [
      "state-space-models",
      "kalman-filtering",
      "time-series-forecasting",
      "exponential-family",
      "R-package"
    ],
    "summary": "KFAS is an R package for state space modeling with exponential family distributions, providing efficient Kalman filter implementations for non-Gaussian time series. It enables researchers and practitioners to handle complex time series with count data, binary outcomes, or other non-normal distributions while maintaining computational efficiency. The package is particularly valuable for econometric modeling where traditional Gaussian assumptions are violated.",
    "use_cases": [
      "modeling-weekly-retail-sales-with-poisson-distributed-counts",
      "forecasting-binary-market-events-with-time-varying-probabilities"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "state space models for non-gaussian time series R",
      "kalman filter package poisson binomial data",
      "KFAS R package exponential family forecasting",
      "non-gaussian state space modeling econometrics"
    ]
  },
  {
    "name": "Kats",
    "description": "Broad toolkit for time series analysis, including multivariate analysis, detection (outliers, change points, trends), feature extraction.",
    "category": "Time Series Econometrics",
    "docs_url": "https://facebookresearch.github.io/Kats/",
    "github_url": "https://github.com/facebookresearch/Kats",
    "url": "https://github.com/facebookresearch/Kats",
    "install": "pip install kats",
    "tags": [
      "time series",
      "econometrics"
    ],
    "best_for": "ARIMA, cointegration, VAR models",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "statsmodels",
      "time-series-basics"
    ],
    "topic_tags": [
      "time-series-analysis",
      "anomaly-detection",
      "changepoint-detection",
      "forecasting",
      "multivariate-analysis"
    ],
    "summary": "Kats is a comprehensive Python toolkit developed by Facebook for time series analysis and forecasting. It provides unified APIs for detection tasks like outlier identification, changepoint analysis, and trend detection, along with feature extraction capabilities. The package is designed for practitioners who need robust time series tools beyond basic forecasting.",
    "use_cases": [
      "Detecting anomalies in user engagement metrics for A/B test validity",
      "Identifying structural breaks in revenue time series during product launches"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "Python package for time series anomaly detection",
      "How to detect changepoints in multivariate time series",
      "Kats vs Prophet for time series analysis",
      "Time series feature extraction tools Python"
    ]
  },
  {
    "name": "LocalProjections",
    "description": "Community implementations of Jord\u00e0 (2005) Local Projections for estimating impulse responses without VAR assumptions.",
    "category": "Time Series Econometrics",
    "docs_url": null,
    "github_url": "https://github.com/elenev/localprojections",
    "url": "https://github.com/elenev/localprojections",
    "install": "Install from source",
    "tags": [
      "time series",
      "econometrics"
    ],
    "best_for": "ARIMA, cointegration, VAR models",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "time-series-analysis",
      "linear-regression",
      "impulse-response-functions"
    ],
    "topic_tags": [
      "local-projections",
      "impulse-response",
      "time-series",
      "causal-inference",
      "econometrics"
    ],
    "summary": "Local Projections is a method for estimating impulse response functions that avoids the restrictive assumptions of Vector Autoregressions (VARs). It directly estimates the response of variables to shocks at different horizons using separate regressions, making it more robust to model misspecification. Popular in macroeconomics and finance for analyzing policy effects and market dynamics.",
    "use_cases": [
      "Estimating how GDP responds to monetary policy shocks over multiple quarters",
      "Analyzing stock market reactions to earnings announcements across different time horizons"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "local projections vs VAR comparison",
      "how to estimate impulse responses without VAR assumptions",
      "Jord\u00e0 local projections implementation",
      "time series causal inference methods"
    ]
  },
  {
    "name": "TS-Flint",
    "description": "Two Sigma's time-series library for Spark with optimized temporal joins, as-of joins, and distributed OLS for high-frequency data.",
    "category": "Time Series Econometrics",
    "docs_url": "https://ts-flint.readthedocs.io/",
    "github_url": "https://github.com/twosigma/flint",
    "url": "https://github.com/twosigma/flint",
    "install": "pip install ts-flint",
    "tags": [
      "spark",
      "time series",
      "temporal joins",
      "fintech"
    ],
    "best_for": "High-frequency financial data with inexact timestamp matching",
    "language": "Spark",
    "difficulty": "intermediate",
    "prerequisites": [
      "apache-spark",
      "python-pandas",
      "SQL-joins"
    ],
    "topic_tags": [
      "time-series",
      "spark-distributed",
      "temporal-joins",
      "financial-data",
      "econometrics"
    ],
    "summary": "TS-Flint is Two Sigma's open-source time series library built on Apache Spark for handling large-scale temporal data. It provides optimized temporal joins, as-of joins, and distributed statistical methods like OLS regression specifically designed for high-frequency financial and economic data. The library enables efficient analysis of time series data that's too large for single-machine processing.",
    "use_cases": [
      "Analyzing high-frequency trading data by joining price feeds with news events using as-of joins to avoid look-ahead bias",
      "Running distributed OLS regressions on large panels of stock returns with market factors across multiple time periods"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "spark library for time series analysis",
      "temporal joins spark high frequency data",
      "distributed OLS regression time series",
      "two sigma flint financial data processing"
    ]
  },
  {
    "name": "dlm",
    "description": "Maximum likelihood and Bayesian analysis of Normal linear state space models (Dynamic Linear Models). Features numerically stable SVD-based algorithms for Kalman filtering and smoothing, plus tools for MCMC-based Bayesian inference including forward filtering backward sampling (FFBS).",
    "category": "Time Series Econometrics",
    "docs_url": "https://cran.r-project.org/web/packages/dlm/vignettes/dlm.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=dlm",
    "install": "install.packages(\"dlm\")",
    "tags": [
      "state-space",
      "kalman-filter",
      "Bayesian",
      "time-series",
      "dynamic-linear-models"
    ],
    "best_for": "Bayesian analysis of linear Gaussian state space models with MCMC methods (Gibbs sampling), implementing Petris (2010)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "linear-algebra",
      "maximum-likelihood-estimation",
      "time-series-basics"
    ],
    "topic_tags": [
      "state-space-models",
      "kalman-filtering",
      "bayesian-inference",
      "time-series-econometrics",
      "r-package"
    ],
    "summary": "The dlm package provides robust implementations of Dynamic Linear Models for time series analysis with unobserved state variables. It offers both classical maximum likelihood estimation via numerically stable Kalman filtering and Bayesian MCMC methods including forward filtering backward sampling. Economists and data scientists use it for modeling time-varying parameters, forecasting with uncertainty quantification, and analyzing structural breaks in economic data.",
    "use_cases": [
      "modeling-time-varying-coefficients-in-regression",
      "forecasting-macroeconomic-variables-with-uncertainty"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "R package for Kalman filtering and state space models",
      "how to implement dynamic linear models in R",
      "Bayesian time series analysis with state space models",
      "forward filtering backward sampling R implementation"
    ]
  },
  {
    "name": "dynlm",
    "description": "Provides an interface for fitting dynamic linear regression models with extended formula syntax. Supports convenient lag operators L(), differencing d(), trend(), season(), and harmonic components while preserving time series attributes.",
    "category": "Time Series Econometrics",
    "docs_url": "https://cran.r-project.org/web/packages/dynlm/dynlm.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=dynlm",
    "install": "install.packages(\"dynlm\")",
    "tags": [
      "dynamic-regression",
      "lag-operator",
      "time-series-regression",
      "distributed-lags",
      "formula-syntax"
    ],
    "best_for": "Time series regression with easy specification of lags, differences, and seasonal patterns using formula syntax",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "linear-regression",
      "time-series-analysis",
      "R-programming"
    ],
    "topic_tags": [
      "dynamic-regression",
      "lag-operators",
      "time-series-modeling",
      "econometric-packages",
      "R-package"
    ],
    "summary": "dynlm is an R package that simplifies fitting dynamic linear regression models by extending standard formula syntax with lag operators, differencing, and seasonal components. It's particularly useful for econometricians and data scientists working with time series data who need to model relationships with lagged variables. The package maintains time series properties while providing intuitive syntax for complex temporal modeling.",
    "use_cases": [
      "modeling-sales-with-lagged-advertising-effects",
      "estimating-distributed-lag-models-for-policy-impact-analysis"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "R package for dynamic regression with lags",
      "how to model lagged variables in time series regression",
      "distributed lag models in R",
      "econometric regression with lag operators"
    ]
  },
  {
    "name": "mFilter",
    "description": "Implements time series filters for extracting trend and cyclical components. Includes Hodrick-Prescott, Baxter-King, Christiano-Fitzgerald, Butterworth, and trigonometric regression filters commonly used in macroeconomics and business cycle analysis.",
    "category": "Time Series Econometrics",
    "docs_url": "https://cran.r-project.org/web/packages/mFilter/mFilter.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=mFilter",
    "install": "install.packages(\"mFilter\")",
    "tags": [
      "HP-filter",
      "Baxter-King",
      "trend-extraction",
      "business-cycles",
      "detrending"
    ],
    "best_for": "Decomposing time series into trend and cyclical components for business cycle analysis",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "time-series-analysis",
      "python-pandas",
      "macroeconomic-indicators"
    ],
    "topic_tags": [
      "time-series-filtering",
      "business-cycle-analysis",
      "trend-extraction",
      "macroeconometrics",
      "detrending-methods"
    ],
    "summary": "mFilter is a Python package that implements classic econometric filters for decomposing time series into trend and cyclical components. It provides ready-to-use implementations of Hodrick-Prescott, Baxter-King, and other filters commonly used in macroeconomic research and business cycle analysis. Essential for economists and data scientists working with economic time series data.",
    "use_cases": [
      "Extracting business cycle fluctuations from GDP data to study economic recessions and expansions",
      "Detrending financial time series to isolate short-term volatility from long-term growth patterns"
    ],
    "audience": [
      "Early-PhD",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "How to implement Hodrick-Prescott filter in Python",
      "Business cycle analysis tools for economic data",
      "Time series detrending methods for macroeconomics",
      "Extract trend and cyclical components from GDP data"
    ]
  },
  {
    "name": "strucchange",
    "description": "Testing, monitoring, and dating structural changes in linear regression models. Implements the generalized fluctuation test framework (CUSUM, MOSUM, recursive estimates) and F-test framework (Chow test, supF, aveF, expF) with breakpoint estimation and confidence intervals.",
    "category": "Time Series Econometrics",
    "docs_url": "https://cran.r-project.org/web/packages/strucchange/vignettes/strucchange-intro.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=strucchange",
    "install": "install.packages(\"strucchange\")",
    "tags": [
      "structural-break",
      "CUSUM",
      "Chow-test",
      "breakpoints",
      "parameter-stability"
    ],
    "best_for": "Detecting and dating parameter instability and structural breaks in regression relationships, implementing Zeileis et al. (2002)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "linear-regression",
      "time-series-analysis",
      "R-programming"
    ],
    "topic_tags": [
      "structural-breaks",
      "econometric-testing",
      "time-series",
      "regression-diagnostics",
      "R-package"
    ],
    "summary": "The strucchange package provides comprehensive tools for detecting when coefficients in linear regression models change over time. It implements multiple testing frameworks including CUSUM tests and Chow tests to identify structural breaks and estimate breakpoint dates with confidence intervals. Essential for economists and data scientists analyzing policy changes, market shifts, or other regime changes in time series data.",
    "use_cases": [
      "Testing whether a marketing campaign caused a structural break in customer conversion rates",
      "Identifying when economic policy changes affected the relationship between unemployment and inflation"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "How to test for structural breaks in R",
      "CUSUM test for parameter stability",
      "Chow test implementation R package",
      "Detecting breakpoints in time series regression"
    ]
  },
  {
    "name": "tsDyn",
    "description": "Implements nonlinear autoregressive time series models including threshold AR (TAR/SETAR), smooth transition AR (STAR, LSTAR), and multivariate extensions (TVAR, TVECM). Enables regime-switching dynamics analysis with parametric and non-parametric approaches.",
    "category": "Time Series Econometrics",
    "docs_url": "https://github.com/MatthieuStigler/tsDyn/wiki",
    "github_url": "https://github.com/MatthieuStigler/tsDyn",
    "url": "https://cran.r-project.org/package=tsDyn",
    "install": "install.packages(\"tsDyn\")",
    "tags": [
      "nonlinear",
      "SETAR",
      "LSTAR",
      "threshold-VAR",
      "regime-switching"
    ],
    "best_for": "Modeling regime-switching dynamics and threshold cointegration in univariate and multivariate series",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "time-series-analysis",
      "autoregressive-models",
      "R-programming"
    ],
    "topic_tags": [
      "nonlinear-time-series",
      "regime-switching",
      "threshold-models",
      "econometrics",
      "R-package"
    ],
    "summary": "tsDyn is an R package for fitting nonlinear autoregressive time series models that can switch between different regimes based on threshold values or smooth transitions. It's particularly valuable for economists and data scientists analyzing financial markets, macroeconomic indicators, or any time series where relationships change over time. The package supports both univariate models (TAR, SETAR, STAR) and multivariate extensions (TVAR, TVECM) for complex economic systems.",
    "use_cases": [
      "Modeling stock market volatility that behaves differently during bull vs bear markets",
      "Analyzing central bank policy transmission effects that vary across economic cycles"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "nonlinear time series models R package",
      "threshold autoregressive models implementation",
      "regime switching time series analysis",
      "SETAR LSTAR models R"
    ]
  },
  {
    "name": "urca",
    "description": "Implements unit root and cointegration tests commonly used in applied econometric analysis. Includes Augmented Dickey-Fuller, Phillips-Perron, KPSS, Elliott-Rothenberg-Stock, and Zivot-Andrews tests, plus Johansen's cointegration procedure for multivariate series.",
    "category": "Time Series Econometrics",
    "docs_url": "https://cran.r-project.org/web/packages/urca/urca.pdf",
    "github_url": "https://github.com/bpfaff/urca",
    "url": "https://cran.r-project.org/package=urca",
    "install": "install.packages(\"urca\")",
    "tags": [
      "unit-root",
      "cointegration",
      "ADF-test",
      "KPSS",
      "Johansen"
    ],
    "best_for": "Testing stationarity and finding cointegrating relationships in non-stationary time series, implementing Pfaff (2008)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "time-series-analysis",
      "linear-regression",
      "R-programming"
    ],
    "topic_tags": [
      "unit-root-testing",
      "cointegration",
      "time-series",
      "stationarity",
      "econometrics"
    ],
    "summary": "The urca package provides essential statistical tests for analyzing time series stationarity and long-run relationships between variables. It implements standard econometric tests like ADF and KPSS for unit roots, plus Johansen's method for testing cointegration between multiple series. These tools are fundamental for proper time series modeling and avoiding spurious regression results.",
    "use_cases": [
      "Testing whether stock prices or economic indicators are stationary before building forecasting models",
      "Analyzing long-run equilibrium relationships between related financial variables like interest rates and exchange rates"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "How to test for unit roots in time series data",
      "Cointegration testing in R",
      "ADF test vs KPSS test for stationarity",
      "Johansen cointegration test implementation"
    ]
  },
  {
    "name": "vars",
    "description": "Comprehensive package for Vector Autoregression (VAR), Structural VAR (SVAR), and Structural Vector Error Correction (SVEC) models. Provides estimation, lag selection, diagnostic testing, forecasting, Granger causality analysis, impulse response functions, and forecast error variance decomposition.",
    "category": "Time Series Econometrics",
    "docs_url": "https://cran.r-project.org/web/packages/vars/vars.pdf",
    "github_url": "https://github.com/bpfaff/vars",
    "url": "https://cran.r-project.org/package=vars",
    "install": "install.packages(\"vars\")",
    "tags": [
      "VAR",
      "SVAR",
      "impulse-response",
      "Granger-causality",
      "FEVD"
    ],
    "best_for": "Multivariate time series analysis with impulse response functions and variance decomposition, implementing Pfaff (2008)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "multivariate-statistics",
      "R-programming",
      "time-series-analysis"
    ],
    "topic_tags": [
      "VAR-models",
      "impulse-response",
      "Granger-causality",
      "structural-econometrics",
      "R-package"
    ],
    "summary": "The vars package is R's comprehensive toolkit for Vector Autoregression analysis, enabling economists to model relationships between multiple time series variables simultaneously. It provides end-to-end functionality from model estimation and diagnostic testing to advanced techniques like impulse response functions and forecast error variance decomposition. Essential for researchers studying macroeconomic dynamics, policy transmission mechanisms, and causal relationships in multivariate time series data.",
    "use_cases": [
      "Analyzing how monetary policy shocks propagate through macroeconomic variables like GDP, inflation, and unemployment",
      "Studying dynamic relationships between tech company metrics like user growth, revenue, and engagement over time"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "VAR model R package",
      "impulse response functions econometrics",
      "Granger causality testing multivariate time series",
      "structural VAR analysis tools"
    ]
  },
  {
    "name": "Augurs",
    "description": "Time series forecasting and analysis for Rust with ETS, MSTL decomposition, seasonality detection, outlier detection, and Prophet-style models.",
    "category": "Time Series Forecasting",
    "docs_url": "https://docs.augu.rs/",
    "github_url": "https://github.com/grafana/augurs",
    "url": "https://crates.io/crates/augurs",
    "install": "cargo add augurs",
    "tags": [
      "rust",
      "time series",
      "forecasting",
      "ETS",
      "MSTL"
    ],
    "best_for": "Time series forecasting and structural analysis in Rust",
    "language": "Rust",
    "difficulty": "intermediate",
    "prerequisites": [
      "rust-programming",
      "time-series-basics",
      "statistical-forecasting"
    ],
    "topic_tags": [
      "time-series-forecasting",
      "rust-programming",
      "seasonality-detection",
      "outlier-detection",
      "exponential-smoothing"
    ],
    "summary": "Augurs is a Rust library for time series forecasting that implements classical methods like ETS (Exponential Smoothing) and MSTL decomposition alongside modern approaches like Prophet-style models. It provides a high-performance toolkit for seasonality detection, outlier identification, and production-ready forecasting workflows. The library is designed for developers who need fast, reliable time series analysis in Rust environments.",
    "use_cases": [
      "Building high-performance demand forecasting systems for e-commerce inventory management",
      "Implementing real-time anomaly detection in IoT sensor data streams"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "rust time series forecasting library",
      "ETS exponential smoothing rust implementation",
      "seasonality detection rust package",
      "time series outlier detection rust"
    ]
  },
  {
    "name": "MLForecast",
    "description": "Scalable time series forecasting using machine learning models (e.g., LightGBM, XGBoost) as regressors.",
    "category": "Time Series Forecasting",
    "docs_url": "https://nixtla.github.io/mlforecast/",
    "github_url": "https://github.com/Nixtla/mlforecast",
    "url": "https://github.com/Nixtla/mlforecast",
    "install": "pip install mlforecast",
    "tags": [
      "forecasting",
      "time series",
      "machine learning"
    ],
    "best_for": "Prediction, demand forecasting, trend analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn",
      "lightgbm-xgboost"
    ],
    "topic_tags": [
      "time-series",
      "machine-learning",
      "forecasting",
      "scalable-ml",
      "python-package"
    ],
    "summary": "MLForecast is a Python package that applies machine learning models like LightGBM and XGBoost to time series forecasting problems. It provides a scalable framework for converting time series data into supervised learning problems with engineered features. The package is designed for practitioners who want to leverage gradient boosting methods for forecasting at scale.",
    "use_cases": [
      "Forecasting daily sales across thousands of retail SKUs using historical patterns and external features",
      "Predicting server resource usage across multiple data centers using machine learning regressors"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "time series forecasting with lightgbm",
      "scalable forecasting python package",
      "machine learning for time series prediction",
      "MLForecast vs traditional forecasting methods"
    ]
  },
  {
    "name": "NeuralForecast",
    "description": "Deep learning models (N-BEATS, N-HiTS, Transformers, RNNs) for time series forecasting, built on PyTorch Lightning.",
    "category": "Time Series Forecasting",
    "docs_url": "https://nixtla.github.io/neuralforecast/",
    "github_url": "https://github.com/Nixtla/neuralforecast",
    "url": "https://github.com/Nixtla/neuralforecast",
    "install": "pip install neuralforecast",
    "tags": [
      "forecasting",
      "time series",
      "machine learning"
    ],
    "best_for": "Prediction, demand forecasting, trend analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pytorch",
      "time-series-analysis",
      "neural-networks"
    ],
    "topic_tags": [
      "neural-forecasting",
      "deep-learning",
      "time-series",
      "pytorch-lightning"
    ],
    "summary": "NeuralForecast is a PyTorch Lightning-based library that implements state-of-the-art deep learning models for time series forecasting, including N-BEATS, N-HiTS, and Transformer architectures. It's designed for data scientists and researchers who need to apply advanced neural network methods to forecasting problems. The package provides production-ready implementations with built-in training workflows and model evaluation capabilities.",
    "use_cases": [
      "Forecasting daily sales for retail chains with complex seasonal patterns",
      "Predicting server resource usage for capacity planning in cloud infrastructure"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "deep learning time series forecasting python",
      "N-BEATS implementation pytorch",
      "neural network models for forecasting",
      "transformer time series prediction library"
    ]
  },
  {
    "name": "Prophet",
    "description": "Forecasting procedure for time series with strong seasonality and trend components, developed by Facebook.",
    "category": "Time Series Forecasting",
    "docs_url": "https://facebook.github.io/prophet/",
    "github_url": "https://github.com/facebook/prophet",
    "url": "https://github.com/facebook/prophet",
    "install": "pip install prophet",
    "tags": [
      "forecasting",
      "time series"
    ],
    "best_for": "Prediction, demand forecasting, trend analysis",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-pandas",
      "time-series-concepts",
      "basic-regression"
    ],
    "topic_tags": [
      "time-series-forecasting",
      "seasonality-modeling",
      "trend-analysis",
      "facebook-prophet",
      "business-forecasting"
    ],
    "summary": "Prophet is an automated forecasting tool designed for business time series with daily observations that display patterns on different time scales. It handles seasonality, holidays, and trend changes automatically with minimal parameter tuning. The package is particularly effective for forecasting problems where you have historical data with strong seasonal effects.",
    "use_cases": [
      "Forecasting daily active users or revenue for a tech product with weekly and yearly seasonality",
      "Predicting demand for e-commerce items accounting for holidays and promotional events"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "How to forecast time series with seasonality in Python",
      "Facebook Prophet tutorial for business forecasting",
      "Best tool for automated time series forecasting",
      "Prophet vs ARIMA for seasonal data"
    ]
  },
  {
    "name": "StatsForecast",
    "description": "Fast, scalable implementations of popular statistical forecasting models (ETS, ARIMA, Theta, etc.) optimized for performance.",
    "category": "Time Series Forecasting",
    "docs_url": "https://nixtla.github.io/statsforecast/",
    "github_url": "https://github.com/Nixtla/statsforecast",
    "url": "https://github.com/Nixtla/statsforecast",
    "install": "pip install statsforecast",
    "tags": [
      "forecasting",
      "time series"
    ],
    "best_for": "Prediction, demand forecasting, trend analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "time-series-analysis",
      "statistical-models"
    ],
    "topic_tags": [
      "statistical-forecasting",
      "time-series",
      "arima",
      "exponential-smoothing",
      "python-package"
    ],
    "summary": "StatsForecast is a high-performance Python package that implements classical statistical forecasting methods like ARIMA, ETS, and Theta models with optimized speed and scalability. It's designed for practitioners who need reliable, fast implementations of established forecasting techniques rather than cutting-edge ML approaches. The package is particularly useful for production forecasting workflows where interpretability and computational efficiency matter.",
    "use_cases": [
      "Forecasting daily sales across thousands of SKUs in retail",
      "Predicting server resource usage for capacity planning"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "fast ARIMA implementation python",
      "statistical forecasting package performance",
      "ETS exponential smoothing python library",
      "classical time series forecasting tools"
    ]
  },
  {
    "name": "fable",
    "description": "A tidyverse-native forecasting framework providing ETS, ARIMA, and other models for tidy time series (tsibble objects). Enables fitting multiple models across many time series simultaneously with a consistent formula-based interface.",
    "category": "Time Series Forecasting",
    "docs_url": "https://fable.tidyverts.org/",
    "github_url": "https://github.com/tidyverts/fable",
    "url": "https://cran.r-project.org/package=fable",
    "install": "install.packages(\"fable\")",
    "tags": [
      "time-series",
      "tidyverse",
      "ARIMA",
      "ETS",
      "tsibble"
    ],
    "best_for": "Tidy forecasting workflows handling many related time series with tidyverse-consistent syntax",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "R-dplyr",
      "time-series-decomposition",
      "tsibble-objects"
    ],
    "topic_tags": [
      "forecasting",
      "tidyverse",
      "time-series",
      "R-package",
      "ARIMA"
    ],
    "summary": "fable is an R package that brings forecasting models like ARIMA and ETS into the tidyverse ecosystem, working seamlessly with tsibble time series objects. It allows analysts to fit multiple forecasting models across hundreds of time series simultaneously using familiar dplyr-style syntax. The package is particularly valuable for scaling forecasting workflows while maintaining code readability and reproducibility.",
    "use_cases": [
      "Forecasting demand across thousands of product SKUs in retail analytics",
      "Generating revenue predictions for multiple business units or geographic regions"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "tidyverse forecasting package R",
      "how to forecast multiple time series with dplyr",
      "fable vs forecast package comparison",
      "ARIMA modeling with tsibble objects"
    ]
  },
  {
    "name": "forecast",
    "description": "The foundational R package for univariate time series forecasting. Provides methods for exponential smoothing via state space models (ETS), automatic ARIMA modeling with auto.arima(), TBATS for complex seasonality, and comprehensive model evaluation tools.",
    "category": "Time Series Forecasting",
    "docs_url": "https://pkg.robjhyndman.com/forecast/",
    "github_url": "https://github.com/robjhyndman/forecast",
    "url": "https://cran.r-project.org/package=forecast",
    "install": "install.packages(\"forecast\")",
    "tags": [
      "time-series",
      "ARIMA",
      "exponential-smoothing",
      "ETS",
      "auto.arima"
    ],
    "best_for": "Classical statistical forecasting for univariate time series with automatic model selection, implementing Hyndman & Khandakar (2008)",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-R",
      "univariate-time-series",
      "descriptive-statistics"
    ],
    "topic_tags": [
      "time-series",
      "ARIMA",
      "exponential-smoothing",
      "forecasting",
      "R-package"
    ],
    "summary": "The forecast package is R's go-to tool for univariate time series forecasting, offering automated model selection and forecasting methods. It's widely used by analysts and researchers for its user-friendly interface to complex forecasting techniques like ARIMA and exponential smoothing. The package excels at handling seasonal patterns and provides comprehensive diagnostic tools for model evaluation.",
    "use_cases": [
      "forecasting monthly sales revenue for business planning",
      "predicting daily website traffic for capacity planning"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for automatic ARIMA modeling",
      "how to forecast time series with seasonality in R",
      "best R tools for exponential smoothing",
      "auto.arima function tutorial"
    ]
  },
  {
    "name": "pmdarima",
    "description": "ARIMA modeling with automatic parameter selection (auto-ARIMA), similar to R's `forecast::auto.arima`.",
    "category": "Time Series Forecasting",
    "docs_url": "https://alkaline-ml.com/pmdarima/",
    "github_url": "https://github.com/alkaline-ml/pmdarima",
    "url": "https://github.com/alkaline-ml/pmdarima",
    "install": "pip install pmdarima",
    "tags": [
      "forecasting",
      "time series"
    ],
    "best_for": "Prediction, demand forecasting, trend analysis",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-pandas",
      "time-series-basics",
      "statsmodels"
    ],
    "topic_tags": [
      "auto-arima",
      "time-series-forecasting",
      "parameter-tuning",
      "python-package"
    ],
    "summary": "pmdarima is a Python package that automatically selects optimal ARIMA model parameters for time series forecasting, eliminating manual hyperparameter tuning. It provides a user-friendly interface similar to R's auto.arima function, making ARIMA modeling accessible to practitioners without deep statistical expertise. The package handles data preprocessing, model selection, and forecast generation in a streamlined workflow.",
    "use_cases": [
      "Forecasting monthly sales revenue with automatic model selection",
      "Predicting daily website traffic without manual ARIMA parameter tuning"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "automatic ARIMA parameter selection python",
      "pmdarima vs statsmodels ARIMA",
      "auto arima forecasting package python",
      "best python library for ARIMA time series"
    ]
  },
  {
    "name": "prophet",
    "description": "Automatic forecasting procedure based on an additive decomposable model with non-linear trends, yearly/weekly/daily seasonality, and holiday effects. Robust to missing data, trend shifts, and outliers; designed for business time series with strong seasonal patterns.",
    "category": "Time Series Forecasting",
    "docs_url": "https://facebook.github.io/prophet/",
    "github_url": "https://github.com/facebook/prophet",
    "url": "https://cran.r-project.org/package=prophet",
    "install": "install.packages(\"prophet\")",
    "tags": [
      "time-series",
      "Facebook",
      "decomposable-model",
      "seasonality",
      "holidays"
    ],
    "best_for": "Business time series forecasting with multiple seasonalities, holiday effects, and automated tunable forecasts, implementing Taylor & Letham (2018)",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "python-pandas",
      "time-series-basics",
      "matplotlib"
    ],
    "topic_tags": [
      "time-series-forecasting",
      "business-analytics",
      "seasonality",
      "trend-analysis",
      "automated-forecasting"
    ],
    "summary": "Prophet is Facebook's open-source forecasting tool that automatically generates predictions for business time series data with minimal configuration. It handles seasonal patterns, holidays, and missing data out-of-the-box, making it accessible to analysts without deep forecasting expertise. The tool excels at business metrics like daily active users, revenue, or inventory demand that show strong seasonal patterns.",
    "use_cases": [
      "Forecasting daily website traffic for capacity planning",
      "Predicting monthly sales revenue accounting for seasonal trends and holidays"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "easy time series forecasting tool for business data",
      "how to forecast sales with seasonality and holidays",
      "Facebook prophet vs traditional forecasting methods",
      "automated forecasting for daily active users"
    ]
  },
  {
    "name": "sktime",
    "description": "Unified framework for various time series tasks, including forecasting with classical, ML, and deep learning models.",
    "category": "Time Series Forecasting",
    "docs_url": "https://www.sktime.net/en/latest/",
    "github_url": "https://github.com/sktime/sktime",
    "url": "https://github.com/sktime/sktime",
    "install": "pip install sktime",
    "tags": [
      "forecasting",
      "time series",
      "machine learning"
    ],
    "best_for": "Prediction, demand forecasting, trend analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn",
      "time-series-decomposition"
    ],
    "topic_tags": [
      "time-series-forecasting",
      "scikit-learn-compatible",
      "python-package",
      "machine-learning",
      "classical-forecasting"
    ],
    "summary": "sktime is a scikit-learn compatible Python package that provides a unified interface for time series forecasting, classification, and regression. It combines classical statistical methods (ARIMA, exponential smoothing) with modern machine learning and deep learning approaches in a single framework. The package is designed to make time series analysis more accessible while maintaining the flexibility needed for advanced modeling.",
    "use_cases": [
      "Forecasting product demand across multiple SKUs with different seasonal patterns",
      "Predicting server resource usage to optimize cloud infrastructure scaling"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python package for time series forecasting",
      "scikit-learn compatible time series library",
      "unified framework for classical and ML forecasting",
      "sktime vs prophet vs statsmodels for forecasting"
    ]
  },
  {
    "name": "CatBoost",
    "description": "Gradient boosting library excelling with categorical features (minimal preprocessing needed). Robust against overfitting.",
    "category": "Tree & Ensemble Methods for Prediction",
    "docs_url": "https://catboost.ai/docs/",
    "github_url": "https://github.com/catboost/catboost",
    "url": "https://github.com/catboost/catboost",
    "install": "pip install catboost",
    "tags": [
      "machine learning",
      "prediction"
    ],
    "best_for": "Random forests, gradient boosting, prediction tasks",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-scikit-learn",
      "gradient-boosting",
      "cross-validation"
    ],
    "topic_tags": [
      "gradient-boosting",
      "categorical-features",
      "ensemble-methods",
      "overfitting-prevention"
    ],
    "summary": "CatBoost is a gradient boosting library that handles categorical features natively without requiring extensive preprocessing like one-hot encoding. It's particularly popular among data scientists for its robustness against overfitting and competitive performance in machine learning competitions and production systems.",
    "use_cases": [
      "E-commerce recommendation systems with mixed categorical and numerical user/product features",
      "Customer churn prediction using demographic categories and behavioral metrics"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "gradient boosting library for categorical data",
      "CatBoost vs XGBoost for mixed data types",
      "best ensemble method for categorical features",
      "overfitting resistant boosting algorithm"
    ]
  },
  {
    "name": "LightGBM",
    "description": "Fast, distributed gradient boosting (also supports RF). Known for speed, low memory usage, and handling large datasets.",
    "category": "Tree & Ensemble Methods for Prediction",
    "docs_url": "https://lightgbm.readthedocs.io/",
    "github_url": "https://github.com/microsoft/LightGBM",
    "url": "https://github.com/microsoft/LightGBM",
    "install": "pip install lightgbm",
    "tags": [
      "machine learning",
      "prediction"
    ],
    "best_for": "Random forests, gradient boosting, prediction tasks",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-scikit-learn",
      "gradient-boosting",
      "pandas-dataframes"
    ],
    "topic_tags": [
      "gradient-boosting",
      "ensemble-methods",
      "high-performance-ml",
      "large-datasets",
      "python-package"
    ],
    "summary": "LightGBM is a high-performance gradient boosting framework that excels at handling large datasets with minimal memory usage. It's widely adopted by data scientists and ML engineers for its speed and efficiency compared to other boosting methods like XGBoost. The package supports both gradient boosting and random forests, making it versatile for various prediction tasks.",
    "use_cases": [
      "Predicting user conversion rates on large e-commerce datasets with millions of records",
      "Building real-time recommendation systems that need fast inference on high-dimensional feature sets"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "fast gradient boosting for large datasets",
      "LightGBM vs XGBoost performance comparison",
      "memory efficient machine learning python",
      "best gradient boosting library for production"
    ]
  },
  {
    "name": "Linfa",
    "description": "Rust ML toolkit inspired by scikit-learn with GLMs, clustering (K-Means), PCA, SVM, and regularization (Lasso/Ridge).",
    "category": "Tree & Ensemble Methods for Prediction",
    "docs_url": "https://rust-ml.github.io/linfa/",
    "github_url": "https://github.com/rust-ml/linfa",
    "url": "https://crates.io/crates/linfa",
    "install": "cargo add linfa",
    "tags": [
      "rust",
      "machine learning",
      "clustering",
      "PCA",
      "SVM"
    ],
    "best_for": "scikit-learn style ML in Rust",
    "language": "Rust",
    "difficulty": "intermediate",
    "prerequisites": [
      "rust-programming",
      "scikit-learn",
      "linear-regression"
    ],
    "topic_tags": [
      "rust-ml",
      "scikit-learn-alternative",
      "systems-programming",
      "performance-optimization",
      "ml-toolkit"
    ],
    "summary": "Linfa is a Rust-based machine learning toolkit that provides scikit-learn-inspired APIs for common ML algorithms including GLMs, clustering, and dimensionality reduction. It offers memory-safe, high-performance implementations suitable for production systems where speed and reliability are critical. The library is ideal for developers building ML pipelines in Rust or those seeking alternatives to Python-based ML stacks.",
    "use_cases": [
      "Building high-performance ML microservices in Rust where Python's GIL creates bottlenecks",
      "Implementing ML algorithms in embedded systems or edge computing environments requiring memory safety"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "rust machine learning library like scikit-learn",
      "high performance ML toolkit for systems programming",
      "linfa rust clustering PCA implementation",
      "scikit-learn alternative for production systems"
    ]
  },
  {
    "name": "NGBoost",
    "description": "Extends gradient boosting to probabilistic prediction, providing uncertainty estimates alongside point predictions. Built on scikit-learn.",
    "category": "Tree & Ensemble Methods for Prediction",
    "docs_url": "https://stanfordmlgroup.github.io/ngboost/",
    "github_url": "https://github.com/stanfordmlgroup/ngboost",
    "url": "https://github.com/stanfordmlgroup/ngboost",
    "install": "pip install ngboost",
    "tags": [
      "machine learning",
      "prediction"
    ],
    "best_for": "Random forests, gradient boosting, prediction tasks",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "scikit-learn",
      "gradient-boosting",
      "probability-distributions"
    ],
    "topic_tags": [
      "uncertainty-quantification",
      "probabilistic-prediction",
      "gradient-boosting",
      "ensemble-methods"
    ],
    "summary": "NGBoost extends traditional gradient boosting to output full probability distributions instead of just point predictions, enabling uncertainty quantification in machine learning models. It's particularly valuable for data scientists who need to understand prediction confidence and risk assessment. Built on scikit-learn, it maintains familiar APIs while adding probabilistic capabilities.",
    "use_cases": [
      "Predicting customer lifetime value with confidence intervals for business planning",
      "Medical diagnosis predictions where uncertainty bounds are critical for clinical decisions"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "gradient boosting with uncertainty estimates",
      "probabilistic prediction python package",
      "how to get confidence intervals from tree models",
      "NGBoost vs regular gradient boosting uncertainty"
    ]
  },
  {
    "name": "Scikit-learn Ens.",
    "description": "(`RandomForestClassifier`/`Regressor`) Widely-used, versatile implementation of Random Forests. Easy API and parallel processing support.",
    "category": "Tree & Ensemble Methods for Prediction",
    "docs_url": "https://scikit-learn.org/stable/modules/ensemble.html#random-forests",
    "github_url": "https://github.com/scikit-learn/scikit-learn",
    "url": "https://github.com/scikit-learn/scikit-learn",
    "install": "pip install scikit-learn",
    "tags": [
      "machine learning",
      "prediction"
    ],
    "best_for": "Random forests, gradient boosting, prediction tasks",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-pandas",
      "scikit-learn-basics",
      "train-test-split"
    ],
    "topic_tags": [
      "random-forest",
      "ensemble-methods",
      "classification",
      "regression",
      "feature-importance"
    ],
    "summary": "Scikit-learn's Random Forest implementation provides an easy-to-use interface for building ensemble models that combine multiple decision trees. It's one of the most popular machine learning algorithms for both classification and regression tasks, offering built-in feature importance scores and robust performance across diverse datasets. The implementation includes automatic parallelization and handles mixed data types well.",
    "use_cases": [
      "Predicting customer churn using mixed categorical and numerical features",
      "Estimating house prices with feature importance ranking for real estate analysis"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "random forest python implementation",
      "scikit learn ensemble methods",
      "how to use RandomForestClassifier",
      "best random forest package for beginners"
    ]
  },
  {
    "name": "SmartCore",
    "description": "Rust ML library with regression, classification, clustering, matrix decomposition (SVD, PCA), and model selection tools.",
    "category": "Tree & Ensemble Methods for Prediction",
    "docs_url": "https://docs.rs/smartcore",
    "github_url": "https://github.com/smartcorelib/smartcore",
    "url": "https://crates.io/crates/smartcore",
    "install": "cargo add smartcore",
    "tags": [
      "rust",
      "machine learning",
      "regression",
      "classification"
    ],
    "best_for": "Comprehensive ML algorithms in pure Rust",
    "language": "Rust",
    "difficulty": "intermediate",
    "prerequisites": [
      "rust-programming",
      "linear-algebra",
      "supervised-learning"
    ],
    "topic_tags": [
      "rust-ml",
      "systems-programming",
      "performance-optimization",
      "cross-platform-ml",
      "native-compilation"
    ],
    "summary": "SmartCore is a comprehensive machine learning library built in Rust, offering high-performance implementations of regression, classification, clustering, and dimensionality reduction algorithms. It's designed for developers who need fast, memory-safe ML computations with native performance. The library provides a full suite of model selection and evaluation tools alongside core algorithms.",
    "use_cases": [
      "Building high-performance ML pipelines in production systems where speed and memory safety are critical",
      "Developing cross-platform ML applications that need to compile to native binaries without runtime dependencies"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "rust machine learning library with PCA and clustering",
      "fast ML library alternative to scikit-learn in rust",
      "high performance regression and classification rust",
      "memory safe ML algorithms for production systems"
    ]
  },
  {
    "name": "XGBoost",
    "description": "High-performance, optimized gradient boosting library (also supports RF). Known for speed, efficiency, and winning competitions.",
    "category": "Tree & Ensemble Methods for Prediction",
    "docs_url": "https://xgboost.readthedocs.io/",
    "github_url": "https://github.com/dmlc/xgboost",
    "url": "https://github.com/dmlc/xgboost",
    "install": "pip install xgboost",
    "tags": [
      "machine learning",
      "prediction"
    ],
    "best_for": "Random forests, gradient boosting, prediction tasks",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-sklearn",
      "decision-trees",
      "cross-validation"
    ],
    "topic_tags": [
      "gradient-boosting",
      "ensemble-methods",
      "competition-ml",
      "tree-models",
      "feature-importance"
    ],
    "summary": "XGBoost is a highly optimized gradient boosting framework that combines multiple weak learners (typically decision trees) to create powerful predictive models. It's widely adopted in industry and competitive machine learning for its exceptional performance, built-in regularization, and efficient handling of missing values. The library provides both regression and classification capabilities with extensive hyperparameter tuning options.",
    "use_cases": [
      "Predicting customer churn with mixed categorical and numerical features",
      "Building recommendation system ranking models for e-commerce platforms"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "best gradient boosting library for tabular data",
      "XGBoost vs random forest performance comparison",
      "how to tune XGBoost hyperparameters",
      "ensemble methods for structured prediction problems"
    ]
  },
  {
    "name": "cuML (RAPIDS)",
    "description": "GPU-accelerated implementation of Random Forests for significant speedups on large datasets. Scikit-learn compatible API.",
    "category": "Tree & Ensemble Methods for Prediction",
    "docs_url": "https://docs.rapids.ai/api/cuml/stable/",
    "github_url": "https://github.com/rapidsai/cuml",
    "url": "https://github.com/rapidsai/cuml",
    "install": "conda install ... (See RAPIDS docs)",
    "tags": [
      "machine learning",
      "prediction"
    ],
    "best_for": "Random forests, gradient boosting, prediction tasks",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "scikit-learn",
      "python-pandas",
      "CUDA-programming"
    ],
    "topic_tags": [
      "random-forest",
      "GPU-acceleration",
      "scikit-learn-compatible",
      "large-scale-ml",
      "RAPIDS"
    ],
    "summary": "cuML is NVIDIA's GPU-accelerated machine learning library that provides scikit-learn compatible implementations of popular algorithms like Random Forests. It delivers significant performance speedups on large datasets by leveraging GPU parallelization while maintaining familiar APIs. Data scientists can drop it in as a replacement for scikit-learn models when working with datasets that benefit from GPU acceleration.",
    "use_cases": [
      "Training Random Forest models on millions of e-commerce transactions for fraud detection",
      "Running large-scale feature selection experiments on genomics datasets with thousands of features"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "GPU accelerated random forest python",
      "scikit-learn compatible GPU machine learning",
      "RAPIDS cuML vs scikit-learn performance",
      "how to speed up random forest training large datasets"
    ]
  },
  {
    "name": "CausalLift",
    "description": "Uplift modeling for observational (non-RCT) data using inverse probability weighting.",
    "category": "Uplift Modeling",
    "docs_url": "https://causallift.readthedocs.io/",
    "github_url": "https://github.com/Minyus/causallift",
    "url": "https://github.com/Minyus/causallift",
    "install": "pip install causallift",
    "tags": [
      "uplift modeling",
      "observational data",
      "IPW"
    ],
    "best_for": "Uplift from observational data with IPW",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "propensity-score-matching",
      "python-scikit-learn",
      "randomized-controlled-trials"
    ],
    "topic_tags": [
      "uplift-modeling",
      "causal-inference",
      "treatment-effects",
      "observational-data",
      "python-package"
    ],
    "summary": "CausalLift is a Python package for uplift modeling that estimates treatment effects from observational data using inverse probability weighting. It helps data scientists identify which individuals are most likely to respond positively to treatments when randomized experiments aren't feasible. The package is particularly useful for marketing campaigns and policy interventions where you need to estimate causal effects from non-experimental data.",
    "use_cases": [
      "Estimating which customers would increase purchases if targeted with a marketing campaign using historical transaction data",
      "Evaluating the effectiveness of a new feature rollout on user engagement when A/B testing wasn't initially implemented"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "uplift modeling python package observational data",
      "how to estimate treatment effects without randomized experiment",
      "inverse probability weighting for causal inference",
      "CausalLift tutorial for marketing campaigns"
    ]
  },
  {
    "name": "UpliftML",
    "description": "Booking.com's enterprise uplift modeling via PySpark and H2O. Six meta-learners plus Uplift Random Forest with ROI-constrained optimization.",
    "category": "Uplift Modeling",
    "docs_url": null,
    "github_url": "https://github.com/bookingcom/upliftml",
    "url": "https://github.com/bookingcom/upliftml",
    "install": "pip install upliftml",
    "tags": [
      "uplift modeling",
      "treatment effects",
      "marketing"
    ],
    "best_for": "Enterprise-scale uplift with ROI optimization",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "pyspark-ml",
      "causal-inference",
      "scikit-learn"
    ],
    "topic_tags": [
      "uplift-modeling",
      "treatment-effects",
      "marketing-optimization",
      "causal-ml",
      "pyspark"
    ],
    "summary": "UpliftML is Booking.com's production-ready uplift modeling library that combines six meta-learners with Uplift Random Forest for measuring treatment effects. Built on PySpark and H2O for enterprise scale, it includes ROI-constrained optimization to maximize business impact. Data scientists use it to optimize marketing campaigns and personalization strategies by identifying which customers will respond positively to treatments.",
    "use_cases": [
      "Optimizing email marketing campaigns by identifying customers most likely to convert when contacted",
      "Personalizing discount offers by finding users who need incentives versus those who would purchase anyway"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "uplift modeling library for marketing campaigns",
      "pyspark causal inference treatment effects",
      "booking.com uplift ml implementation",
      "roi constrained uplift optimization tools"
    ]
  },
  {
    "name": "pylift",
    "description": "Wayfair's uplift modeling wrapping sklearn for speed with rigorous Qini curve evaluation.",
    "category": "Uplift Modeling",
    "docs_url": "https://pylift.readthedocs.io/",
    "github_url": "https://github.com/wayfair/pylift",
    "url": "https://github.com/wayfair/pylift",
    "install": "pip install pylift",
    "tags": [
      "uplift modeling",
      "treatment effects",
      "marketing"
    ],
    "best_for": "Fast uplift with Qini curve evaluation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-sklearn",
      "causal-inference",
      "A-B-testing"
    ],
    "topic_tags": [
      "uplift-modeling",
      "treatment-effects",
      "marketing-attribution",
      "qini-curves",
      "causal-ml"
    ],
    "summary": "pylift is Wayfair's Python package that extends scikit-learn for uplift modeling, enabling practitioners to identify which customers will respond positively to treatments. It provides fast implementations with rigorous evaluation through Qini curves, making it practical for marketing and product experimentation at scale.",
    "use_cases": [
      "Identifying which customers to target with promotional campaigns based on predicted treatment response",
      "Optimizing product recommendation strategies by modeling individual-level causal effects"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "uplift modeling python package",
      "how to measure treatment effects in marketing campaigns",
      "qini curve evaluation for causal inference",
      "wayfair pylift sklearn treatment effects"
    ]
  },
  {
    "name": "cowplot",
    "description": "Publication-ready ggplot2 themes and plot arrangement utilities. Provides clean themes, plot annotations, and functions for combining plots with shared axes.",
    "category": "Visualization",
    "docs_url": "https://wilkelab.org/cowplot/",
    "github_url": "https://github.com/wilkelab/cowplot",
    "url": "https://cran.r-project.org/package=cowplot",
    "install": "install.packages(\"cowplot\")",
    "tags": [
      "ggplot2",
      "themes",
      "publication-ready",
      "plot-arrangement",
      "annotations"
    ],
    "best_for": "Publication-ready ggplot2 themes and multi-plot arrangements with annotations",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "R-ggplot2",
      "data-visualization",
      "R-programming"
    ],
    "topic_tags": [
      "data-visualization",
      "publication-graphics",
      "plot-themes",
      "R-package"
    ],
    "summary": "Cowplot is an R package that extends ggplot2 with publication-ready themes and utilities for combining multiple plots. It provides clean, minimalist themes and functions to arrange plots with shared legends and aligned axes. Popular among researchers and analysts who need to create professional figures for papers, reports, and presentations.",
    "use_cases": [
      "Creating multi-panel figures for academic papers with consistent formatting",
      "Building dashboard-style layouts combining different chart types with shared legends"
    ],
    "audience": [
      "Junior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "how to make publication ready plots in R",
      "ggplot2 themes for academic papers",
      "combine multiple ggplot2 plots into one figure",
      "cowplot tutorial for clean data visualizations"
    ]
  },
  {
    "name": "patchwork",
    "description": "Compose multiple ggplot2 plots into publication-ready multi-panel figures. Uses intuitive operators (+, |, /) for arrangement with automatic alignment and shared legends.",
    "category": "Visualization",
    "docs_url": "https://patchwork.data-imaginist.com/",
    "github_url": "https://github.com/thomasp85/patchwork",
    "url": "https://cran.r-project.org/package=patchwork",
    "install": "install.packages(\"patchwork\")",
    "tags": [
      "ggplot2",
      "multi-panel",
      "figure-composition",
      "visualization",
      "publication-ready"
    ],
    "best_for": "Composing multi-panel ggplot2 figures with intuitive + and | operators",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "ggplot2-basics",
      "R-programming"
    ],
    "topic_tags": [
      "data-visualization",
      "publication-figures",
      "multi-panel-plots",
      "ggplot2-extension"
    ],
    "summary": "Patchwork is an R package that simplifies combining multiple ggplot2 plots into cohesive multi-panel figures using intuitive operators. It automatically handles plot alignment, shared legends, and spacing, making it easy to create publication-quality composite visualizations. The package is essential for researchers and analysts who need to present multiple related plots in a single figure.",
    "use_cases": [
      "Creating multi-panel figures for academic papers showing results across different conditions or datasets",
      "Building dashboards with multiple related visualizations that need consistent alignment and shared aesthetics"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "how to combine multiple ggplot2 plots into one figure",
      "R package for multi-panel publication figures",
      "patchwork ggplot2 plot composition",
      "create subplot layouts in R ggplot2"
    ]
  }
]
