[
  {
    "name": "Ax (Meta Adaptive Experimentation)",
    "description": "Meta's open-source platform for adaptive experimentation. Bayesian optimization, multi-objective optimization, and automated experiment design. Built on BoTorch for AI-assisted experimentation.",
    "category": "Adaptive Experimentation & Bandits",
    "docs_url": "https://ax.dev/docs/why-ax",
    "github_url": "https://github.com/facebook/Ax",
    "url": "https://ax.dev",
    "install": "pip install ax-platform",
    "tags": [
      "adaptive experimentation",
      "Bayesian optimization",
      "multi-objective"
    ],
    "best_for": "Adaptive experimentation, Bayesian optimization, automated experiment design",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "null"
    ],
    "topic_tags": [
      "bayesian",
      "optimization",
      "experimentation"
    ],
    "summary": "Ax is Meta's open-source platform designed for adaptive experimentation, utilizing Bayesian optimization and automated experiment design. It is primarily used by data scientists and researchers looking to enhance their experimentation processes with AI-assisted methods.",
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for adaptive experimentation",
      "how to perform Bayesian optimization in python",
      "automated experiment design in python",
      "multi-objective optimization with Ax",
      "Meta Ax package documentation",
      "Ax platform for experimentation examples"
    ],
    "use_cases": [
      "Optimizing marketing strategies through A/B testing",
      "Designing experiments for product feature testing"
    ],
    "primary_use_cases": [
      "A/B test analysis",
      "multi-objective optimization"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "BoTorch"
    ],
    "related_packages": [
      "Optuna",
      "Hyperopt"
    ],
    "maintenance_status": "active",
    "model_score": 0.031
  },
  {
    "name": "causal-learn",
    "description": "Comprehensive Python package serving as Python translation and extension of Java-based Tetrad toolkit for causal discovery algorithms.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": "https://causal-learn.readthedocs.io/",
    "github_url": "https://github.com/py-why/causal-learn",
    "url": "https://github.com/py-why/causal-learn",
    "install": "pip install causal-learn",
    "tags": [
      "causal inference",
      "graphs"
    ],
    "best_for": "Learning causal structure from data, DAG estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "graphs"
    ],
    "summary": "Causal-learn is a comprehensive Python package that serves as a translation and extension of the Java-based Tetrad toolkit for causal discovery algorithms. It is used by researchers and practitioners in the field of causal inference to analyze and model causal relationships.",
    "use_cases": [
      "Analyzing causal relationships in datasets",
      "Developing causal models for research"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for causal discovery",
      "how to perform causal inference in python",
      "causal graphs in python",
      "Tetrad toolkit in python",
      "causal analysis with python",
      "causal-learn package usage"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "Tetrad",
      "DoWhy"
    ],
    "maintenance_status": "active",
    "model_score": 0.0288
  },
  {
    "name": "spaCy",
    "description": "Industrial-strength NLP library for efficient text processing pipelines (NER, POS tagging, etc.).",
    "category": "Natural Language Processing for Economics",
    "docs_url": "https://spacy.io/",
    "github_url": "https://github.com/explosion/spaCy",
    "url": "https://github.com/explosion/spaCy",
    "install": "pip install spacy",
    "tags": [
      "NLP",
      "text analysis"
    ],
    "best_for": "Text analysis, sentiment analysis, document classification",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "spaCy is an industrial-strength NLP library designed for efficient text processing pipelines, including tasks such as Named Entity Recognition (NER) and Part-of-Speech (POS) tagging. It is widely used by data scientists and researchers in various fields, including economics, for natural language processing tasks.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for NLP",
      "how to do text analysis in python",
      "spaCy tutorial",
      "using spaCy for NER",
      "spaCy POS tagging example",
      "best NLP libraries in python"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "NLTK",
      "Transformers"
    ],
    "maintenance_status": "active",
    "model_score": 0.0227
  },
  {
    "name": "Bambi",
    "description": "High-level interface for building Bayesian GLMMs, built on top of PyMC. Uses formula syntax similar to R's `lme4`.",
    "category": "Bayesian Econometrics",
    "docs_url": "https://bambinos.github.io/bambi/",
    "github_url": "https://github.com/bambinos/bambi",
    "url": "https://github.com/bambinos/bambi",
    "install": "pip install bambi",
    "tags": [
      "Bayesian",
      "inference"
    ],
    "best_for": "Uncertainty quantification, prior-informed inference, probabilistic modeling",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "bayesian"
    ],
    "summary": "Bambi provides a high-level interface for building Bayesian Generalized Linear Mixed Models (GLMMs) using a formula syntax similar to R's `lme4`. It is designed for users who want to perform Bayesian inference in a user-friendly manner.",
    "use_cases": [
      "Modeling hierarchical data",
      "Analyzing longitudinal data"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for Bayesian GLMMs",
      "how to build Bayesian models in python",
      "Bambi package for Bayesian inference",
      "Bayesian econometrics in python",
      "using PyMC for GLMMs",
      "high-level Bayesian modeling in python"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "PyMC"
    ],
    "maintenance_status": "active",
    "model_score": 0.0217
  },
  {
    "name": "Ananke",
    "description": "Causal inference using graphical models (DAGs), including identification theory and effect estimation.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": "https://ananke.readthedocs.io/",
    "github_url": "https://gitlab.com/causal/ananke",
    "url": "https://gitlab.com/causal/ananke",
    "install": "pip install ananke-causal",
    "tags": [
      "causal inference",
      "graphs"
    ],
    "best_for": "Learning causal structure from data, DAG estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "graphs"
    ],
    "summary": "Ananke is a Python package designed for causal inference using graphical models, specifically directed acyclic graphs (DAGs). It is useful for researchers and data scientists interested in identification theory and effect estimation.",
    "use_cases": [
      "Estimating causal effects in observational studies",
      "Conducting A/B tests using graphical models"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to estimate effects using DAGs in python",
      "causal discovery tools in python",
      "graphical models for data analysis python",
      "python package for identification theory",
      "using Ananke for A/B testing"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "DoWhy",
      "CausalInference"
    ],
    "maintenance_status": "active",
    "model_score": 0.018
  },
  {
    "name": "EconML",
    "description": "Microsoft toolkit for estimating heterogeneous treatment effects using DML, causal forests, meta-learners, and orthogonal ML methods.",
    "category": "Double/Debiased Machine Learning (DML)",
    "docs_url": "https://econml.azurewebsites.net/",
    "github_url": "https://github.com/py-why/EconML",
    "url": "https://github.com/py-why/EconML",
    "install": "pip install econml",
    "tags": [
      "machine learning",
      "causal inference"
    ],
    "best_for": "High-dimensional controls, ML-based causal inference",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "EconML is a Microsoft toolkit designed for estimating heterogeneous treatment effects using advanced machine learning techniques such as DML, causal forests, and meta-learners. It is primarily used by data scientists and researchers in the fields of economics and social sciences.",
    "use_cases": [
      "Estimating treatment effects in clinical trials",
      "Analyzing A/B test results"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for estimating treatment effects",
      "how to use causal forests in python",
      "EconML tutorial",
      "DML methods in python",
      "causal inference toolkit python",
      "meta-learners for treatment effects python"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "EconML"
    ],
    "maintenance_status": "active",
    "model_score": 0.0148
  },
  {
    "name": "EconML",
    "description": "Microsoft's package for ML-based causal inference. Double ML, causal forests, instrumental variables, and dynamic treatment regimes. Strong theoretical foundations.",
    "category": "Healthcare Economics & Health-Tech",
    "docs_url": "https://econml.azurewebsites.net/",
    "github_url": "https://github.com/py-why/EconML",
    "url": "https://econml.azurewebsites.net/",
    "install": "pip install econml",
    "tags": [
      "causal inference",
      "double ML",
      "causal forests"
    ],
    "best_for": "Rigorous ML-based causal inference with econometric foundations",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "EconML is a Python package developed by Microsoft for machine learning-based causal inference. It is designed for researchers and practitioners in healthcare economics and health-tech who need to analyze treatment effects and causal relationships.",
    "use_cases": [
      "Estimating causal effects in healthcare studies",
      "Analyzing treatment effects in clinical trials"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to use double ML in python",
      "causal forests implementation in python",
      "instrumental variables in python",
      "dynamic treatment regimes in python",
      "EconML documentation",
      "EconML examples"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "DoWhy",
      "CausalML"
    ],
    "maintenance_status": "active",
    "model_score": 0.0148
  },
  {
    "name": "EconML",
    "description": "Microsoft's library for heterogeneous treatment effects with Double ML, Causal Forests, and DRLearner",
    "category": "Causal Inference",
    "docs_url": "https://econml.azurewebsites.net/",
    "github_url": "https://github.com/py-why/econml",
    "url": "https://econml.azurewebsites.net/",
    "install": "pip install econml",
    "tags": [
      "Double ML",
      "Causal Forest",
      "CATE",
      "Microsoft"
    ],
    "best_for": "State-of-the-art heterogeneous treatment effect estimation for targeting",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "EconML is a library developed by Microsoft for estimating heterogeneous treatment effects using methods like Double ML, Causal Forests, and DRLearner. It is primarily used by data scientists and researchers interested in causal inference.",
    "use_cases": [
      "Estimating treatment effects in clinical trials",
      "Evaluating marketing strategies through A/B testing"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for heterogeneous treatment effects",
      "how to use Double ML in python",
      "Causal Forests implementation in python",
      "DRLearner tutorial",
      "EconML documentation",
      "Microsoft causal inference library"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "causalml",
      "doWhy"
    ],
    "maintenance_status": "active",
    "model_score": 0.0148
  },
  {
    "name": "EconML",
    "description": "Microsoft's library for heterogeneous treatment effect estimation. Implements DML, causal forests, and instrumental variable methods with sklearn-compatible API.",
    "category": "MarTech & Customer Analytics",
    "docs_url": "https://econml.azurewebsites.net/",
    "github_url": "https://github.com/microsoft/EconML",
    "url": "https://github.com/microsoft/EconML",
    "install": "pip install econml",
    "tags": [
      "causal-inference",
      "treatment-effects",
      "DML",
      "econometrics"
    ],
    "best_for": "Rigorous heterogeneous treatment effect estimation with econometric foundations",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "econometrics"
    ],
    "summary": "EconML is a library developed by Microsoft for estimating heterogeneous treatment effects using methods like DML, causal forests, and instrumental variable techniques. It is designed for data scientists and researchers interested in causal inference and treatment effect analysis.",
    "use_cases": [
      "Estimating treatment effects in clinical trials",
      "Analyzing marketing campaign impacts",
      "Evaluating policy interventions"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to estimate treatment effects in python",
      "EconML documentation",
      "causal forests in python",
      "DML implementation in python",
      "instrumental variable methods python"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "treatment effect estimation"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "DoWhy",
      "CausalML"
    ],
    "maintenance_status": "active",
    "framework_compatibility": [
      "scikit-learn"
    ],
    "model_score": 0.0148
  },
  {
    "name": "BayesianBandits",
    "description": "Lightweight microframework for Bayesian bandits (Thompson Sampling) with support for contextual/restless/delayed rewards.",
    "category": "Adaptive Experimentation & Bandits",
    "docs_url": "https://bayesianbandits.readthedocs.io/en/latest/",
    "github_url": "https://github.com/bayesianbandits/bayesianbandits",
    "url": "https://github.com/bayesianbandits/bayesianbandits",
    "install": "pip install bayesianbandits",
    "tags": [
      "A/B testing",
      "experimentation",
      "Bayesian"
    ],
    "best_for": "Online A/B testing, multi-armed bandits, adaptive allocation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "bayesian",
      "experimentation"
    ],
    "summary": "BayesianBandits is a lightweight microframework designed for implementing Bayesian bandits using Thompson Sampling. It is suitable for users interested in adaptive experimentation and can handle contextual, restless, and delayed rewards.",
    "use_cases": [
      "Optimizing marketing strategies through A/B testing",
      "Personalizing content delivery based on user behavior"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for Bayesian bandits",
      "how to implement Thompson Sampling in python",
      "A/B testing framework in python",
      "contextual bandits python library",
      "delayed rewards in Bayesian bandits",
      "lightweight microframework for experimentation"
    ],
    "primary_use_cases": [
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "PyMC3",
      "TensorFlow Probability"
    ],
    "maintenance_status": "active",
    "model_score": 0.0123
  },
  {
    "name": "abracadabra",
    "description": "Sequential testing with always-valid inference. Supports continuous monitoring of A/B tests.",
    "category": "Adaptive Experimentation & Bandits",
    "docs_url": "https://pypi.org/project/abracadabra/",
    "github_url": "https://github.com/quizlet/abracadabra",
    "url": "https://pypi.org/project/abracadabra/",
    "install": "pip install abracadabra",
    "tags": [
      "sequential testing",
      "A/B testing",
      "always-valid"
    ],
    "best_for": "Continuous experiment monitoring",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "adaptive-experimentation"
    ],
    "summary": "Abracadabra is a Python package designed for sequential testing with always-valid inference, allowing for continuous monitoring of A/B tests. It is useful for data scientists and researchers involved in experimental design and analysis.",
    "use_cases": [
      "Monitoring A/B tests in real-time",
      "Conducting sequential analysis for marketing experiments"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for sequential testing",
      "how to do A/B testing in python",
      "continuous monitoring of A/B tests",
      "always-valid inference in experiments"
    ],
    "primary_use_cases": [
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0112
  },
  {
    "name": "bunching",
    "description": "Implements Kleven-Waseem style bunching estimation for kink and notch designs. Calculates parametric elasticities from bunching at tax thresholds with publication-ready output.",
    "category": "Bunching Estimation",
    "docs_url": "https://cran.r-project.org/web/packages/bunching/bunching.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=bunching",
    "install": "install.packages(\"bunching\")",
    "tags": [
      "bunching",
      "kink-design",
      "notch-design",
      "tax-research",
      "elasticity"
    ],
    "best_for": "Kleven-Waseem bunching estimation at kinks and notches for tax research",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "bunching",
      "tax-research",
      "elasticity"
    ],
    "summary": "The 'bunching' package implements Kleven-Waseem style bunching estimation for kink and notch designs. It is used by researchers and analysts in tax policy to calculate parametric elasticities from bunching at tax thresholds.",
    "use_cases": [
      "Estimating tax elasticities at specific income thresholds",
      "Analyzing the effects of tax policy changes on taxpayer behavior"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for bunching estimation",
      "how to calculate elasticities in R",
      "Kleven-Waseem bunching method in R",
      "R tax threshold analysis package",
      "bunching analysis for tax research",
      "notch design estimation in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0109
  },
  {
    "name": "PyXAB",
    "description": "Library for advanced bandit problems: X-armed bandits (continuous/structured action spaces) and online optimization.",
    "category": "Adaptive Experimentation & Bandits",
    "docs_url": "https://pyxab.readthedocs.io/en/latest/",
    "github_url": "https://github.com/WilliamLwj/PyXAB",
    "url": "https://github.com/WilliamLwj/PyXAB",
    "install": "pip install pyxab",
    "tags": [
      "A/B testing",
      "experimentation"
    ],
    "best_for": "Online A/B testing, multi-armed bandits, adaptive allocation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "PyXAB is a library designed for tackling advanced bandit problems, specifically focusing on X-armed bandits with continuous and structured action spaces. It is useful for researchers and practitioners involved in online optimization and experimentation.",
    "use_cases": [],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for X-armed bandits",
      "how to perform online optimization in python",
      "advanced bandit problems python library",
      "A/B testing with structured action spaces",
      "experimentation library for python",
      "bandit algorithms in python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0104
  },
  {
    "name": "bsts",
    "description": "Bayesian Structural Time Series providing the foundation for CausalImpact. Supports spike-and-slab variable selection, multiple state components (trend, seasonality, regression), and non-Gaussian outcomes. Developed at Google.",
    "category": "Bayesian Causal Inference",
    "docs_url": "https://cran.r-project.org/web/packages/bsts/bsts.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=bsts",
    "install": "install.packages(\"bsts\")",
    "tags": [
      "Bayesian",
      "structural-time-series",
      "spike-and-slab",
      "state-space",
      "Google"
    ],
    "best_for": "Bayesian structural time series with spike-and-slab selection\u2014foundation for CausalImpact",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "time-series",
      "bayesian"
    ],
    "summary": "The bsts package provides a framework for Bayesian Structural Time Series modeling, which is foundational for causal impact analysis. It is particularly useful for users interested in understanding the effects of interventions over time.",
    "use_cases": [
      "Analyzing the impact of marketing campaigns on sales",
      "Forecasting future trends based on historical data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "bayesian structural time series R",
      "how to perform causal impact analysis in R",
      "R package for spike-and-slab variable selection",
      "time series modeling with bsts",
      "bayesian analysis for time series",
      "structural time series in R"
    ],
    "primary_use_cases": [
      "causal impact analysis",
      "time series forecasting"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0102
  },
  {
    "name": "ContextualBandits",
    "description": "Implements a wide range of contextual bandit algorithms (linear, tree-based, neural) and off-policy evaluation methods.",
    "category": "Adaptive Experimentation & Bandits",
    "docs_url": "https://contextual-bandits.readthedocs.io/",
    "github_url": "https://github.com/david-cortes/contextualbandits",
    "url": "https://github.com/david-cortes/contextualbandits",
    "install": "pip install contextualbandits",
    "tags": [
      "A/B testing",
      "experimentation",
      "machine learning"
    ],
    "best_for": "Online A/B testing, multi-armed bandits, adaptive allocation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "machine learning"
    ],
    "summary": "ContextualBandits implements a variety of contextual bandit algorithms and off-policy evaluation methods. It is useful for researchers and practitioners in the fields of machine learning and experimentation.",
    "use_cases": [
      "personalized recommendations",
      "dynamic pricing strategies"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for contextual bandits",
      "how to implement A/B testing in python",
      "contextual bandit algorithms in python",
      "off-policy evaluation methods python",
      "experimentation tools in python",
      "machine learning library for bandits"
    ],
    "primary_use_cases": [
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0102
  },
  {
    "name": "MABWiser",
    "description": "Production-ready, scikit-learn style library for contextual & stochastic bandits with parallelism and simulation tools.",
    "category": "Adaptive Experimentation & Bandits",
    "docs_url": "https://fidelity.github.io/mabwiser/",
    "github_url": "https://github.com/fidelity/mabwiser",
    "url": "https://github.com/fidelity/mabwiser",
    "install": "pip install mabwiser",
    "tags": [
      "A/B testing",
      "experimentation"
    ],
    "best_for": "Online A/B testing, multi-armed bandits, adaptive allocation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "adaptive experimentation",
      "bandit algorithms"
    ],
    "summary": "MABWiser is a production-ready library designed for contextual and stochastic bandits, offering tools for parallelism and simulation. It is suitable for data scientists and researchers involved in experimentation and A/B testing.",
    "use_cases": [
      "Running A/B tests with contextual bandits",
      "Simulating bandit algorithms for research"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for contextual bandits",
      "how to perform A/B testing in python",
      "stochastic bandits library python",
      "parallelism in experimentation python",
      "MABWiser documentation",
      "scikit-learn style bandits library"
    ],
    "primary_use_cases": [
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "scikit-learn",
      "bandit",
      "PyTorch-Bandits"
    ],
    "maintenance_status": "active",
    "model_score": 0.0102
  },
  {
    "name": "Open Bandit Pipeline (OBP)",
    "description": "Framework for **offline evaluation (OPE)** of bandit policies using logged data. Implements IPS, DR, DM estimators.",
    "category": "Adaptive Experimentation & Bandits",
    "docs_url": "https://zr-obp.readthedocs.io/en/latest/",
    "github_url": "https://github.com/st-tech/zr-obp",
    "url": "https://github.com/st-tech/zr-obp",
    "install": "pip install obp",
    "tags": [
      "A/B testing",
      "experimentation"
    ],
    "best_for": "Online A/B testing, multi-armed bandits, adaptive allocation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "bandit-algorithms"
    ],
    "summary": "Open Bandit Pipeline (OBP) is a framework designed for offline evaluation of bandit policies using logged data. It is particularly useful for researchers and practitioners in the field of adaptive experimentation who need to assess the performance of various bandit algorithms.",
    "use_cases": [
      "Evaluating the effectiveness of different bandit algorithms",
      "Analyzing logged data from previous experiments"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for offline evaluation of bandit policies",
      "how to implement IPS estimator in python",
      "bandit algorithms evaluation in python",
      "offline policy evaluation with logged data",
      "using Open Bandit Pipeline for A/B testing",
      "how to analyze bandit policies in python"
    ],
    "primary_use_cases": [
      "offline evaluation of bandit policies",
      "implementing IPS, DR, DM estimators"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0096
  },
  {
    "name": "SMPyBandits",
    "description": "Comprehensive research framework for single/multi-player MAB algorithms (stochastic, adversarial, contextual).",
    "category": "Adaptive Experimentation & Bandits",
    "docs_url": "https://smpybandits.github.io/",
    "github_url": "https://github.com/SMPyBandits/SMPyBandits",
    "url": "https://github.com/SMPyBandits/SMPyBandits",
    "install": "pip install SMPyBandits",
    "tags": [
      "A/B testing",
      "experimentation"
    ],
    "best_for": "Online A/B testing, multi-armed bandits, adaptive allocation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "SMPyBandits is a comprehensive research framework designed for implementing single and multi-player Multi-Armed Bandit (MAB) algorithms, including stochastic, adversarial, and contextual settings. It is primarily used by researchers and practitioners in the field of adaptive experimentation and bandit algorithms.",
    "use_cases": [],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for multi-armed bandits",
      "how to implement A/B testing in python",
      "MAB algorithms in python",
      "contextual bandits framework python",
      "stochastic bandits library python",
      "adversarial bandits implementation python"
    ],
    "primary_use_cases": [
      "A/B test analysis",
      "multi-player MAB algorithms"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0095
  },
  {
    "name": "crewai",
    "description": "Framework for orchestrating role-playing autonomous AI agents. Multi-agent collaboration made intuitive.",
    "category": "Agentic AI",
    "docs_url": "https://docs.crewai.com/",
    "github_url": "https://github.com/crewAIInc/crewAI",
    "url": "https://www.crewai.com/",
    "install": "pip install crewai",
    "tags": [
      "agents",
      "multi-agent",
      "orchestration",
      "roles"
    ],
    "best_for": "Role-based multi-agent teams",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "crewai is a framework designed for orchestrating role-playing autonomous AI agents, making multi-agent collaboration intuitive. It is suitable for developers and researchers interested in agent-based systems.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for multi-agent collaboration",
      "how to orchestrate AI agents in python",
      "framework for role-playing AI agents",
      "autonomous AI agents in python",
      "python multi-agent orchestration",
      "collaborative AI agents framework"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0092
  },
  {
    "name": "langchain",
    "description": "Framework for developing LLM-powered applications. Chains, tools, memory, and retrieval.",
    "category": "Agentic AI",
    "docs_url": "https://python.langchain.com/",
    "github_url": "https://github.com/langchain-ai/langchain",
    "url": "https://python.langchain.com/",
    "install": "pip install langchain",
    "tags": [
      "LLM",
      "chains",
      "tools",
      "RAG"
    ],
    "best_for": "LLM framework \u2014 chains, tools, memory",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Langchain is a framework designed for developing applications powered by large language models (LLMs). It is used by developers and data scientists looking to create sophisticated AI-driven solutions.",
    "use_cases": [
      "Building conversational agents",
      "Creating data retrieval systems"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for LLM applications",
      "how to build chains in python",
      "tools for memory in langchain",
      "RAG in langchain",
      "developing AI applications with langchain",
      "langchain framework documentation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0078
  },
  {
    "name": "langgraph",
    "description": "Framework for building stateful, multi-actor LLM applications. Graph-based agent workflows with persistence.",
    "category": "Agentic AI",
    "docs_url": "https://langchain-ai.github.io/langgraph/",
    "github_url": "https://github.com/langchain-ai/langgraph",
    "url": "https://langchain-ai.github.io/langgraph/",
    "install": "pip install langgraph",
    "tags": [
      "agents",
      "LLM",
      "workflows",
      "multi-agent"
    ],
    "best_for": "Production agent workflows with state management",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Langgraph is a framework designed for building stateful, multi-actor LLM applications. It enables users to create graph-based agent workflows with persistence, making it suitable for developers working on complex AI systems.",
    "use_cases": [
      "Developing chatbots with multiple interacting agents",
      "Creating AI-driven applications that require state management"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for building LLM applications",
      "how to create multi-agent workflows in python",
      "stateful LLM application framework",
      "graph-based agent workflows in python",
      "langgraph documentation",
      "examples of langgraph usage"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0078
  },
  {
    "name": "openai-agents",
    "description": "OpenAI's lightweight, production-ready SDK for building agentic AI applications. Fast prototyping.",
    "category": "Agentic AI",
    "docs_url": "https://openai.github.io/openai-agents-python/",
    "github_url": "https://github.com/openai/openai-agents-python",
    "url": "https://openai.github.io/openai-agents-python/",
    "install": "pip install openai-agents",
    "tags": [
      "agents",
      "OpenAI",
      "tools",
      "lightweight"
    ],
    "best_for": "OpenAI's lightweight SDK \u2014 fast prototyping",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "OpenAI's openai-agents is a lightweight SDK designed for building agentic AI applications, enabling fast prototyping. It is suitable for developers looking to create AI-driven solutions efficiently.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for agentic AI",
      "how to build AI applications in python",
      "OpenAI SDK for agents",
      "lightweight AI tools in python",
      "fast prototyping AI applications",
      "using openai-agents for AI development"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active",
    "model_score": 0.0078
  },
  {
    "name": "CausalMatch",
    "description": "Implements Propensity Score Matching (PSM) and Coarsened Exact Matching (CEM) with ML flexibility for propensity score estimation.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://github.com/bytedance/CausalMatch",
    "github_url": null,
    "url": "https://github.com/bytedance/CausalMatch",
    "install": "pip install causalmatch",
    "tags": [
      "causal inference",
      "matching"
    ],
    "best_for": "Estimating treatment effects, propensity score matching, observational studies",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "matching"
    ],
    "summary": "CausalMatch implements Propensity Score Matching (PSM) and Coarsened Exact Matching (CEM) with machine learning flexibility for propensity score estimation. It is useful for researchers and data scientists interested in causal inference methodologies.",
    "use_cases": [
      "Estimating treatment effects in observational studies",
      "Conducting A/B tests with matched samples"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for propensity score matching",
      "how to perform causal inference in python",
      "CausalMatch documentation",
      "examples of Coarsened Exact Matching in python",
      "best practices for matching in causal analysis",
      "python library for causal inference"
    ],
    "primary_use_cases": [
      "propensity score matching",
      "coarsened exact matching"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.007
  },
  {
    "name": "DoubleML",
    "description": "Implements the double/debiased ML framework (Chernozhukov et al.) for estimating causal parameters (ATE, LATE, POM) with ML nuisances.",
    "category": "Double/Debiased Machine Learning (DML)",
    "docs_url": "https://docs.doubleml.org/",
    "github_url": "https://github.com/DoubleML/doubleml-for-py",
    "url": "https://github.com/DoubleML/doubleml-for-py",
    "install": "pip install DoubleML",
    "tags": [
      "machine learning",
      "causal inference"
    ],
    "best_for": "High-dimensional controls, ML-based causal inference",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "DoubleML implements the double/debiased ML framework for estimating causal parameters such as ATE, LATE, and POM using machine learning nuisances. It is used by data scientists and researchers focused on causal inference.",
    "use_cases": [
      "Estimating average treatment effects",
      "Conducting A/B test analysis"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to estimate ATE in python",
      "double ML framework python",
      "machine learning for causal parameters",
      "using DoubleML for LATE estimation",
      "python package for debiased ML"
    ],
    "primary_use_cases": [
      "causal forest estimation"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Chernozhukov et al. (2018)",
    "related_packages": [
      "causalml",
      "econml"
    ],
    "maintenance_status": "active",
    "model_score": 0.007
  },
  {
    "name": "LightweightMMM",
    "description": "Bayesian Marketing Mix Modeling (see Marketing Mix Models section).",
    "category": "Bayesian Econometrics",
    "docs_url": null,
    "github_url": "https://github.com/google/lightweight_mmm",
    "url": "https://github.com/google/lightweight_mmm",
    "install": "pip install lightweight_mmm",
    "tags": [
      "Bayesian",
      "inference"
    ],
    "best_for": "Uncertainty quantification, prior-informed inference, probabilistic modeling",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "bayesian",
      "marketing-mix-modeling"
    ],
    "summary": "LightweightMMM is a package designed for Bayesian Marketing Mix Modeling, allowing users to analyze the effectiveness of marketing strategies. It is primarily used by data scientists and marketing analysts looking to optimize their marketing spend.",
    "use_cases": [
      "Evaluating marketing campaign effectiveness",
      "Optimizing advertising spend"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for Bayesian Marketing Mix Modeling",
      "how to perform marketing mix analysis in python",
      "Bayesian inference for marketing",
      "LightweightMMM usage examples",
      "best practices for marketing mix models in python",
      "how to analyze marketing effectiveness with python"
    ],
    "primary_use_cases": [
      "marketing mix modeling"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "PyMC3",
      "Stan"
    ],
    "maintenance_status": "active",
    "model_score": 0.0061
  },
  {
    "name": "bnlearn",
    "description": "Bayesian network structure learning, parameter estimation, and inference. Implements constraint-based (PC, GS), score-based (HC, TABU), and hybrid algorithms for DAG learning with discrete and continuous data.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": "https://www.bnlearn.com/",
    "github_url": null,
    "url": "https://cran.r-project.org/package=bnlearn",
    "install": "install.packages(\"bnlearn\")",
    "tags": [
      "Bayesian-networks",
      "structure-learning",
      "parameter-estimation",
      "probabilistic-graphical-models",
      "inference"
    ],
    "best_for": "Bayesian network learning and inference with constraint-based and score-based algorithms",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "bayesian"
    ],
    "summary": "bnlearn is a package for Bayesian network structure learning, parameter estimation, and inference. It is used by data scientists and researchers interested in causal discovery and probabilistic graphical models.",
    "use_cases": [
      "Learning the structure of a Bayesian network from data",
      "Estimating parameters of a Bayesian network"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for Bayesian networks",
      "how to perform structure learning in R",
      "Bayesian network inference in R",
      "parameter estimation for Bayesian networks R",
      "constraint-based algorithms for DAG learning R",
      "score-based learning in R"
    ],
    "primary_use_cases": [
      "Bayesian network structure learning",
      "parameter estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0042
  },
  {
    "name": "dagitty",
    "description": "Analysis of structural causal models represented as DAGs. Computes adjustment sets, identifies instrumental variables, tests conditional independencies, and finds minimal sufficient adjustment sets for causal identification.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": "http://www.dagitty.net/",
    "github_url": "https://github.com/jtextor/dagitty",
    "url": "https://cran.r-project.org/package=dagitty",
    "install": "install.packages(\"dagitty\")",
    "tags": [
      "DAG",
      "causal-graphs",
      "adjustment-sets",
      "d-separation",
      "instrumental-variables"
    ],
    "best_for": "DAG-based causal analysis with adjustment set computation and d-separation testing",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "Dagitty is a tool for analyzing structural causal models represented as directed acyclic graphs (DAGs). It is used by researchers and practitioners in the field of causal inference to compute adjustment sets and identify instrumental variables.",
    "use_cases": [
      "Identifying instrumental variables for causal analysis",
      "Testing conditional independencies in datasets"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for causal graphs",
      "how to compute adjustment sets in R",
      "DAG analysis in R",
      "R library for instrumental variables",
      "conditional independence testing in R",
      "R package for d-separation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0042
  },
  {
    "name": "causal-llm-bfs",
    "description": "LLM + BFS hybrid for efficient causal graph discovery. Uses language models to guide structure search.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": "https://github.com/superkaiba/causal-llm-bfs",
    "github_url": "https://github.com/superkaiba/causal-llm-bfs",
    "url": "https://github.com/superkaiba/causal-llm-bfs",
    "install": "pip install causal-llm-bfs",
    "tags": [
      "causal discovery",
      "LLM",
      "graphs"
    ],
    "best_for": "LLM-assisted causal discovery",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "graphs"
    ],
    "summary": "The causal-llm-bfs package combines language models with breadth-first search techniques to enhance the discovery of causal graphs. It is useful for researchers and practitioners in causal inference and data science.",
    "use_cases": [
      "Discovering causal relationships in datasets",
      "Guiding structure search in complex data environments"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for causal discovery",
      "how to use LLM for graph discovery",
      "efficient causal graph discovery in python",
      "BFS algorithm for causal graphs",
      "causal inference with language models",
      "graphical models in python"
    ],
    "primary_use_cases": [
      "causal graph discovery"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0042
  },
  {
    "name": "DoWhy",
    "description": "End-to-end framework for causal inference based on causal graphs (DAGs) and potential outcomes. Covers identification, estimation, refutation.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://www.pywhy.org/dowhy/",
    "github_url": "https://github.com/py-why/dowhy",
    "url": "https://github.com/py-why/dowhy",
    "install": "pip install dowhy",
    "tags": [
      "causal inference",
      "matching"
    ],
    "best_for": "Estimating treatment effects, propensity score matching, observational studies",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "DoWhy is an end-to-end framework for causal inference that utilizes causal graphs (DAGs) and potential outcomes. It is designed for researchers and practitioners who need to identify, estimate, and refute causal relationships.",
    "use_cases": [
      "Analyzing causal relationships in observational data",
      "Conducting A/B tests with causal inference"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to perform causal analysis in python",
      "DoWhy package usage",
      "causal graphs in python",
      "potential outcomes framework python",
      "causal inference tools python"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "EconML",
      "CausalML"
    ],
    "maintenance_status": "active",
    "model_score": 0.0039
  },
  {
    "name": "DoWhy",
    "description": "Microsoft's causal inference library with four-step Model-Identify-Estimate-Refute workflow",
    "category": "Causal Inference",
    "docs_url": "https://www.pywhy.org/dowhy/",
    "github_url": "https://github.com/py-why/dowhy",
    "url": "https://www.pywhy.org/dowhy/",
    "install": "pip install dowhy",
    "tags": [
      "causal inference",
      "DAG",
      "refutation",
      "Microsoft"
    ],
    "best_for": "End-to-end causal analysis with automated robustness checks",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "DAG",
      "refutation"
    ],
    "summary": "DoWhy is a causal inference library developed by Microsoft that provides a systematic approach to causal analysis through a four-step workflow: Model, Identify, Estimate, and Refute. It is designed for data scientists and researchers interested in understanding causal relationships in their data.",
    "use_cases": [
      "Analyzing the effect of a treatment in observational studies",
      "Evaluating the impact of marketing campaigns on sales"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to perform causal analysis in python",
      "DoWhy library examples",
      "Microsoft causal inference tools",
      "DAG in DoWhy",
      "refutation methods in DoWhy"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "EconML",
      "CausalML"
    ],
    "maintenance_status": "active",
    "model_score": 0.0039
  },
  {
    "name": "NumPyro",
    "description": "Probabilistic programming library built on JAX for scalable Bayesian inference, often faster than PyMC.",
    "category": "Bayesian Econometrics",
    "docs_url": "https://num.pyro.ai/",
    "github_url": "https://github.com/pyro-ppl/numpyro",
    "url": "https://github.com/pyro-ppl/numpyro",
    "install": "pip install numpyro",
    "tags": [
      "Bayesian",
      "inference"
    ],
    "best_for": "Uncertainty quantification, prior-informed inference, probabilistic modeling",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "bayesian"
    ],
    "summary": "NumPyro is a probabilistic programming library built on JAX that facilitates scalable Bayesian inference. It is designed for users looking for efficient and faster alternatives to traditional Bayesian frameworks like PyMC.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for probabilistic programming",
      "how to do Bayesian inference in python",
      "NumPyro tutorial",
      "scalable Bayesian inference python",
      "JAX probabilistic programming",
      "compare NumPyro and PyMC"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "JAX"
    ],
    "related_packages": [
      "PyMC"
    ],
    "maintenance_status": "active",
    "model_score": 0.0038
  },
  {
    "name": "PyMC",
    "description": "Flexible probabilistic programming library for Bayesian modeling and inference using MCMC algorithms (NUTS).",
    "category": "Bayesian Econometrics",
    "docs_url": "https://www.pymc.io/",
    "github_url": "https://github.com/pymc-devs/pymc",
    "url": "https://github.com/pymc-devs/pymc",
    "install": "pip install pymc",
    "tags": [
      "Bayesian",
      "inference"
    ],
    "best_for": "Uncertainty quantification, prior-informed inference, probabilistic modeling",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "bayesian"
    ],
    "summary": "PyMC is a flexible probabilistic programming library designed for Bayesian modeling and inference using Markov Chain Monte Carlo (MCMC) algorithms, specifically the No-U-Turn Sampler (NUTS). It is used by data scientists and statisticians for building complex models and conducting inference.",
    "use_cases": [
      "modeling complex data distributions",
      "conducting Bayesian inference"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for Bayesian modeling",
      "how to perform inference in Python",
      "Bayesian inference with PyMC",
      "MCMC algorithms in Python",
      "using NUTS in PyMC",
      "probabilistic programming in Python"
    ],
    "primary_use_cases": [
      "Bayesian modeling",
      "inference using MCMC"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "Stan",
      "TensorFlow Probability"
    ],
    "maintenance_status": "active",
    "model_score": 0.0038
  },
  {
    "name": "PyMC",
    "description": "Probabilistic programming for Bayesian statistical modeling and MCMC sampling. Foundation for Bayesian econometrics in Python.",
    "category": "Bayesian Inference",
    "docs_url": "https://www.pymc.io/",
    "github_url": "https://github.com/pymc-devs/pymc",
    "url": "https://www.pymc.io/",
    "install": "pip install pymc",
    "tags": [
      "Bayesian",
      "MCMC",
      "probabilistic-programming",
      "statistical-modeling"
    ],
    "best_for": "Bayesian modeling and probabilistic machine learning",
    "language": "Python",
    "model_score": 0.0038
  },
  {
    "name": "LiNGAM",
    "description": "Specialized package for learning non-Gaussian linear causal models, implementing various versions of the LiNGAM algorithm including ICA-based methods.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": "https://lingam.readthedocs.io/",
    "github_url": "https://github.com/cdt15/lingam",
    "url": "https://github.com/cdt15/lingam",
    "install": "pip install lingam",
    "tags": [
      "causal inference",
      "graphs"
    ],
    "best_for": "Learning causal structure from data, DAG estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "graphs"
    ],
    "summary": "LiNGAM is a specialized package for learning non-Gaussian linear causal models, implementing various versions of the LiNGAM algorithm including ICA-based methods. It is used by researchers and practitioners interested in causal discovery and graphical models.",
    "use_cases": [
      "Analyzing causal relationships in observational data",
      "Developing models for causal inference in social sciences"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to learn non-Gaussian linear causal models in python",
      "LiNGAM algorithm implementation in python",
      "causal discovery tools in python",
      "graphs in causal inference python",
      "ICA-based methods for causal models python"
    ],
    "primary_use_cases": [
      "causal forest estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0037
  },
  {
    "name": "MCD",
    "description": "Mixture of Causal Graphs discovery for heterogeneous time series (ICML 2024). Finds time-varying causal structures.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": null,
    "github_url": "https://github.com/Rose-STL-Lab/MCD",
    "url": "https://pypi.org/project/MCD/",
    "install": "pip install mcd",
    "tags": [
      "causal discovery",
      "time series",
      "heterogeneous"
    ],
    "best_for": "Time-varying causal structure discovery",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "time-series",
      "graphical-models"
    ],
    "summary": "MCD is a Python package designed for discovering time-varying causal structures in heterogeneous time series data. It is useful for researchers and practitioners in the fields of causal discovery and graphical models.",
    "use_cases": [
      "Analyzing causal relationships in economic data",
      "Studying the impact of interventions over time"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for causal discovery",
      "how to analyze time series with causal graphs",
      "MCD package for heterogeneous time series",
      "discovering causal structures in Python",
      "time-varying causal analysis library",
      "causal graphs in Python"
    ],
    "primary_use_cases": [
      "causal forest estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0037
  },
  {
    "name": "SDCI",
    "description": "State-dependent causal inference for conditionally stationary processes (ICML 2025). Handles regime-switching causal graphs.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": null,
    "github_url": "https://github.com/charlio23/SDCI",
    "url": "https://pypi.org/project/SDCI/",
    "install": "pip install sdci",
    "tags": [
      "causal discovery",
      "time series",
      "regime switching"
    ],
    "best_for": "State-dependent causal discovery",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "time-series",
      "regime-switching"
    ],
    "summary": "SDCI is a Python package designed for state-dependent causal inference in conditionally stationary processes. It is particularly useful for researchers and practitioners working with regime-switching causal graphs.",
    "use_cases": [
      "Analyzing causal relationships in time series data",
      "Studying the effects of regime changes on causal inference"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for causal discovery",
      "how to perform regime switching analysis in python",
      "time series causal inference python",
      "SDCI package usage",
      "regime-switching causal graphs in python",
      "causal inference for time series"
    ],
    "primary_use_cases": [
      "causal forest estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0037
  },
  {
    "name": "Tigramite",
    "description": "Specialized package for causal inference in time series data implementing PCMCI, PCMCIplus, LPCMCI algorithms with conditional independence tests.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": "https://jakobrunge.github.io/tigramite/",
    "github_url": "https://github.com/jakobrunge/tigramite",
    "url": "https://github.com/jakobrunge/tigramite",
    "install": "pip install tigramite",
    "tags": [
      "causal inference",
      "graphs"
    ],
    "best_for": "Learning causal structure from data, DAG estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "time-series"
    ],
    "summary": "Tigramite is a specialized package for causal inference in time series data that implements PCMCI, PCMCIplus, and LPCMCI algorithms along with conditional independence tests. It is used by researchers and data scientists working on causal discovery in temporal datasets.",
    "use_cases": [
      "Analyzing causal relationships in economic time series",
      "Evaluating the impact of interventions over time"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to perform causal discovery in time series",
      "Tigramite package usage",
      "conditional independence tests in Python",
      "time series causal inference library",
      "implementing PCMCI in Python"
    ],
    "primary_use_cases": [
      "causal forest estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0037
  },
  {
    "name": "boot",
    "description": "Classic bootstrap methods implementing the approaches described in Davison & Hinkley (1997). Provides functions for both parametric and nonparametric resampling with various confidence interval methods.",
    "category": "Bootstrap & Inference",
    "docs_url": "https://cran.r-project.org/web/packages/boot/boot.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=boot",
    "install": "install.packages(\"boot\")",
    "tags": [
      "bootstrap",
      "resampling",
      "confidence-intervals",
      "nonparametric",
      "parametric"
    ],
    "best_for": "Classic bootstrap methods from Davison & Hinkley (1997) for general resampling inference",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "bootstrap",
      "resampling"
    ],
    "summary": "The 'boot' package provides classic bootstrap methods for statistical inference as described in Davison & Hinkley (1997). It is used by statisticians and data scientists for both parametric and nonparametric resampling to create confidence intervals.",
    "use_cases": [
      "Estimating confidence intervals for a sample mean",
      "Conducting hypothesis tests using bootstrap methods"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for bootstrap methods",
      "how to perform resampling in R",
      "confidence intervals in R",
      "nonparametric bootstrap in R",
      "parametric bootstrap techniques R",
      "statistical inference R package"
    ],
    "primary_use_cases": [
      "confidence interval estimation",
      "hypothesis testing"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Davison & Hinkley (1997)",
    "maintenance_status": "active",
    "model_score": 0.0036
  },
  {
    "name": "fwildclusterboot",
    "description": "Fast wild cluster bootstrap implementation following Roodman et al. (2019)\u2014up to 1000\u00d7 faster than alternatives. Critical for panel data with few clusters. Integrates with fixest and lfe for efficient inference.",
    "category": "Bootstrap & Inference",
    "docs_url": "https://s3alfisc.github.io/fwildclusterboot/",
    "github_url": "https://github.com/s3alfisc/fwildclusterboot",
    "url": "https://cran.r-project.org/package=fwildclusterboot",
    "install": "install.packages(\"fwildclusterboot\")",
    "tags": [
      "wild-bootstrap",
      "cluster-robust",
      "few-clusters",
      "panel-data",
      "fixest"
    ],
    "best_for": "Fast wild cluster bootstrap for panel data with few clusters, implementing Roodman et al. (2019)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "fwildclusterboot is a fast wild cluster bootstrap implementation that significantly speeds up analysis for panel data with few clusters. It is particularly useful for researchers and data scientists working with econometric models.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for wild bootstrap",
      "how to perform cluster-robust inference in R",
      "fast bootstrap methods for panel data",
      "wild cluster bootstrap implementation R",
      "R package for efficient inference with fixest",
      "bootstrap methods for few clusters in R"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "fixest",
      "lfe"
    ],
    "implements_paper": "Roodman et al. (2019)",
    "maintenance_status": "active",
    "model_score": 0.0036
  },
  {
    "name": "rsample",
    "description": "Modern tidyverse-compatible resampling infrastructure. Provides functions for creating resamples (bootstrap, cross-validation, time series splits) that integrate seamlessly with tidymodels workflows.",
    "category": "Bootstrap & Inference",
    "docs_url": "https://rsample.tidymodels.org/",
    "github_url": "https://github.com/tidymodels/rsample",
    "url": "https://cran.r-project.org/package=rsample",
    "install": "install.packages(\"rsample\")",
    "tags": [
      "resampling",
      "cross-validation",
      "bootstrap",
      "tidymodels",
      "time-series-cv"
    ],
    "best_for": "Tidyverse-native resampling for bootstrap, cross-validation, and time series splits",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "resampling",
      "cross-validation",
      "bootstrap",
      "time-series"
    ],
    "summary": "The rsample package provides modern resampling infrastructure that is compatible with the tidyverse. It offers functions for creating various types of resamples, such as bootstrap and cross-validation, which integrate seamlessly with tidymodels workflows.",
    "use_cases": [
      "Creating bootstrap samples for model training",
      "Performing time series cross-validation for predictive modeling"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R library for resampling",
      "how to perform cross-validation in R",
      "bootstrap methods in R",
      "tidymodels resampling functions",
      "time series cross-validation in R",
      "R package for model evaluation"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "tidymodels"
    ],
    "related_packages": [
      "boot",
      "caret"
    ],
    "maintenance_status": "active",
    "model_score": 0.0036
  },
  {
    "name": "bayesplot",
    "description": "Extensive library of ggplot2-based plotting functions for posterior analysis, MCMC diagnostics, and prior/posterior predictive checks supporting the applied Bayesian workflow for any MCMC-fitted model.",
    "category": "Bayesian Inference",
    "docs_url": "https://mc-stan.org/bayesplot/",
    "github_url": "https://github.com/stan-dev/bayesplot",
    "url": "https://cran.r-project.org/package=bayesplot",
    "install": "install.packages(\"bayesplot\")",
    "tags": [
      "visualization",
      "MCMC-diagnostics",
      "posterior-predictive-checks",
      "ggplot2",
      "Bayesian"
    ],
    "best_for": "Diagnostic plots and posterior visualization for MCMC-based Bayesian models, implementing Gabry et al. (2019)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "bayesian"
    ],
    "summary": "bayesplot is an extensive library of ggplot2-based plotting functions designed for posterior analysis, MCMC diagnostics, and prior/posterior predictive checks. It is used by statisticians and data scientists who work with MCMC-fitted models in Bayesian analysis.",
    "use_cases": [
      "Visualizing MCMC diagnostics",
      "Creating posterior predictive checks",
      "Analyzing Bayesian model outputs"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for MCMC diagnostics",
      "how to visualize posterior distributions in R",
      "bayesian plotting functions in R",
      "ggplot2 for Bayesian analysis",
      "posterior predictive checks in R",
      "R package for MCMC analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0036
  },
  {
    "name": "brms",
    "description": "High-level interface for fitting Bayesian generalized multilevel models using Stan, with lme4-style formula syntax supporting linear, count, survival, ordinal, zero-inflated, hurdle, and mixture models with flexible prior specification.",
    "category": "Bayesian Inference",
    "docs_url": "https://paul-buerkner.github.io/brms/",
    "github_url": "https://github.com/paul-buerkner/brms",
    "url": "https://cran.r-project.org/package=brms",
    "install": "install.packages(\"brms\")",
    "tags": [
      "Bayesian",
      "multilevel-models",
      "Stan",
      "regression",
      "distributional-regression"
    ],
    "best_for": "Complex hierarchical Bayesian regression with familiar R formula syntax, implementing B\u00fcrkner (2017)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "bayesian"
    ],
    "summary": "The brms package provides a high-level interface for fitting Bayesian generalized multilevel models using Stan. It is particularly useful for statisticians and data scientists who need to specify complex models with flexible prior distributions.",
    "use_cases": [
      "Fitting linear mixed models",
      "Conducting Bayesian regression analyses"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for Bayesian multilevel models",
      "how to fit a Bayesian model in R",
      "Stan interface for R",
      "Bayesian regression in R",
      "multilevel modeling with brms",
      "brms package documentation"
    ],
    "primary_use_cases": [
      "Fitting Bayesian generalized multilevel models",
      "Specifying flexible prior distributions"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "rstan",
      "lme4"
    ],
    "maintenance_status": "active",
    "model_score": 0.0036
  },
  {
    "name": "rstan",
    "description": "Core R interface to the Stan probabilistic programming language, providing full Bayesian inference via NUTS/HMC, approximate inference via ADVI, and penalized maximum likelihood via L-BFGS for custom Bayesian models.",
    "category": "Bayesian Inference",
    "docs_url": "https://mc-stan.org/rstan/",
    "github_url": "https://github.com/stan-dev/rstan",
    "url": "https://cran.r-project.org/package=rstan",
    "install": "install.packages(\"rstan\")",
    "tags": [
      "Stan",
      "MCMC",
      "HMC",
      "probabilistic-programming",
      "Bayesian"
    ],
    "best_for": "Custom Bayesian models requiring direct Stan language access for maximum flexibility, implementing Carpenter et al. (2017)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "bayesian"
    ],
    "summary": "rstan is the core R interface to the Stan probabilistic programming language, enabling full Bayesian inference and approximate inference for custom Bayesian models. It is used by statisticians and data scientists who require advanced modeling techniques.",
    "use_cases": [
      "Bayesian data analysis",
      "Statistical modeling of complex data"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for Bayesian inference",
      "how to perform MCMC in R",
      "Stan interface for R",
      "Bayesian modeling in R",
      "R probabilistic programming",
      "using HMC with R",
      "L-BFGS in R for Bayesian models"
    ],
    "primary_use_cases": [
      "Bayesian data analysis",
      "Statistical modeling"
    ],
    "api_complexity": "advanced",
    "related_packages": [
      "brms",
      "rstanarm"
    ],
    "maintenance_status": "active",
    "model_score": 0.0036
  },
  {
    "name": "rstanarm",
    "description": "Pre-compiled Bayesian regression models using Stan that mimic familiar R functions (lm, glm, lmer) with customary formula syntax, weakly informative default priors, and zero model compilation time.",
    "category": "Bayesian Inference",
    "docs_url": "https://mc-stan.org/rstanarm/",
    "github_url": "https://github.com/stan-dev/rstanarm",
    "url": "https://cran.r-project.org/package=rstanarm",
    "install": "install.packages(\"rstanarm\")",
    "tags": [
      "Bayesian",
      "Stan",
      "regression",
      "mixed-effects",
      "pre-compiled"
    ],
    "best_for": "Applied Bayesian regression with minimal learning curve for lm/glm/lmer users",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "bayesian"
    ],
    "summary": "rstanarm provides pre-compiled Bayesian regression models that mimic familiar R functions, allowing users to perform regression analysis with minimal setup time. It is primarily used by statisticians and data scientists who require Bayesian modeling capabilities.",
    "use_cases": [
      "Performing Bayesian regression analysis",
      "Conducting mixed-effects modeling"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for Bayesian regression",
      "how to use rstanarm for mixed-effects models",
      "Bayesian modeling in R",
      "pre-compiled Bayesian models in R",
      "rstanarm documentation",
      "install rstanarm package",
      "examples of rstanarm usage"
    ],
    "primary_use_cases": [
      "Bayesian regression analysis",
      "mixed-effects modeling"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "rstan",
      "brms"
    ],
    "maintenance_status": "active",
    "model_score": 0.0036
  },
  {
    "name": "ggdag",
    "description": "Visualize and analyze causal DAGs using ggplot2. Provides tidy interface to dagitty with publication-quality DAG plots, path highlighting, and adjustment set visualization.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": "https://r-causal.github.io/ggdag/",
    "github_url": "https://github.com/malcolmbarrett/ggdag",
    "url": "https://cran.r-project.org/package=ggdag",
    "install": "install.packages(\"ggdag\")",
    "tags": [
      "DAG",
      "visualization",
      "ggplot2",
      "causal-diagrams",
      "adjustment-sets"
    ],
    "best_for": "Publication-quality DAG visualization using ggplot2 with dagitty integration",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The ggdag package allows users to visualize and analyze causal Directed Acyclic Graphs (DAGs) using ggplot2. It is particularly useful for researchers and practitioners in causal discovery who need publication-quality DAG plots and path highlighting.",
    "use_cases": [
      "Creating publication-quality causal DAGs",
      "Highlighting paths in causal diagrams"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for causal DAG visualization",
      "how to create DAGs in R",
      "ggplot2 DAG plotting",
      "analyze causal diagrams in R",
      "visualize adjustment sets R",
      "path highlighting in causal analysis R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0033
  },
  {
    "name": "pcalg",
    "description": "Causal structure learning from observational data using the PC algorithm and variants. Estimates Markov equivalence class of DAGs from conditional independence tests with intervention support.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": "https://cran.r-project.org/web/packages/pcalg/pcalg.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=pcalg",
    "install": "install.packages(\"pcalg\")",
    "tags": [
      "causal-discovery",
      "PC-algorithm",
      "structure-learning",
      "DAG",
      "conditional-independence"
    ],
    "best_for": "Causal structure learning from observational data using PC algorithm",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The pcalg package provides tools for causal structure learning from observational data using the PC algorithm and its variants. It is primarily used by researchers and practitioners in the field of causal inference.",
    "use_cases": [],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for causal structure learning",
      "how to use PC algorithm in R",
      "conditional independence tests in R",
      "DAG estimation in R",
      "causal discovery with pcalg",
      "intervention support in causal analysis"
    ],
    "primary_use_cases": [
      "causal structure learning",
      "Markov equivalence class estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0033
  },
  {
    "name": "gCastle",
    "description": "Huawei Noah's Ark Lab end-to-end causal structure learning toolchain emphasizing gradient-based methods with GPU acceleration (NOTEARS, GOLEM).",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": "https://gcastle.readthedocs.io/",
    "github_url": "https://github.com/huawei-noah/trustworthyAI",
    "url": "https://github.com/huawei-noah/trustworthyAI",
    "install": "pip install gcastle",
    "tags": [
      "causal inference",
      "graphs"
    ],
    "best_for": "Learning causal structure from data, DAG estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "graphs"
    ],
    "summary": "gCastle is an end-to-end causal structure learning toolchain developed by Huawei Noah's Ark Lab, focusing on gradient-based methods with GPU acceleration. It is used by researchers and practitioners in the field of causal inference and graphical models.",
    "use_cases": [
      "learning causal structures from data",
      "analyzing causal relationships in complex systems"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to do causal structure learning in python",
      "gradient-based methods for graphs in python",
      "GPU acceleration for causal inference python",
      "causal discovery tools in python",
      "Huawei Noah's Ark Lab causal tools"
    ],
    "primary_use_cases": [
      "causal structure learning",
      "gradient-based causal inference"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0033
  },
  {
    "name": "py-tetrad",
    "description": "Python interface to Tetrad Java library using JPype, providing direct access to Tetrad's causal discovery algorithms with efficient data translation.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": null,
    "github_url": "https://github.com/cmu-phil/py-tetrad",
    "url": "https://github.com/cmu-phil/py-tetrad",
    "install": "Available on GitHub (installation via git clone)",
    "tags": [
      "causal inference",
      "graphs"
    ],
    "best_for": "Learning causal structure from data, DAG estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "graphs"
    ],
    "summary": "py-tetrad is a Python interface to the Tetrad Java library that provides direct access to Tetrad's causal discovery algorithms. It is used by data scientists and researchers interested in causal inference and graphical models.",
    "use_cases": [
      "Analyzing causal relationships in data",
      "Conducting experiments to infer causality"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for causal discovery",
      "how to use Tetrad in python",
      "causal inference with python",
      "graphs in python for causal analysis",
      "python interface for Tetrad",
      "Tetrad causal discovery algorithms in python"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0033
  },
  {
    "name": "Statsmodels",
    "description": "Comprehensive library for estimating statistical models (OLS, GLM, etc.), conducting tests, and data exploration. Core tool.",
    "category": "Core Libraries & Linear Models",
    "docs_url": "https://www.statsmodels.org/",
    "github_url": "https://github.com/statsmodels/statsmodels",
    "url": "https://github.com/statsmodels/statsmodels",
    "install": "pip install statsmodels",
    "tags": [
      "regression",
      "linear models"
    ],
    "best_for": "OLS regression, basic econometrics, data manipulation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "linear models"
    ],
    "summary": "Statsmodels is a comprehensive library for estimating statistical models such as OLS and GLM, conducting statistical tests, and exploring data. It is widely used by data scientists and statisticians for various statistical analyses.",
    "use_cases": [
      "Estimating linear regression models",
      "Conducting hypothesis tests on statistical data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for statistical models",
      "how to conduct regression analysis in python",
      "python library for data exploration",
      "how to perform OLS in python",
      "python GLM examples",
      "best python libraries for regression"
    ],
    "primary_use_cases": [
      "linear regression analysis",
      "generalized linear model fitting"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "scikit-learn",
      "stats",
      "pandas"
    ],
    "maintenance_status": "active",
    "model_score": 0.003
  },
  {
    "name": "CausalInference",
    "description": "Implements classical causal inference methods like propensity score matching, inverse probability weighting, stratification.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://causalinferenceinpython.org",
    "github_url": "https://github.com/laurencium/causalinference",
    "url": "https://github.com/laurencium/causalinference",
    "install": "pip install CausalInference",
    "tags": [
      "causal inference",
      "matching"
    ],
    "best_for": "Estimating treatment effects, propensity score matching, observational studies",
    "language": "Python",
    "difficulty": "beginner|intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "matching"
    ],
    "summary": "CausalInference implements classical causal inference methods such as propensity score matching, inverse probability weighting, and stratification. It is used by data scientists and researchers to analyze causal relationships in data.",
    "use_cases": [
      "Analyzing treatment effects in observational studies",
      "Evaluating the impact of interventions in healthcare"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to do propensity score matching in python",
      "inverse probability weighting python package",
      "stratification methods in python",
      "matching techniques in causal analysis",
      "causal inference tools for data science"
    ],
    "primary_use_cases": [
      "propensity score matching",
      "inverse probability weighting"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "causalml",
      "DoWhy"
    ],
    "maintenance_status": "active",
    "model_score": 0.003
  },
  {
    "name": "y0",
    "description": "Causal inference framework providing tools for causal graph manipulation and effect identification.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://y0.readthedocs.io/",
    "github_url": "https://github.com/y0-causal-inference/y0",
    "url": "https://github.com/y0-causal-inference/y0",
    "install": "pip install y0",
    "tags": [
      "causal inference",
      "graphs",
      "identification"
    ],
    "best_for": "Causal graph manipulation and do-calculus",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "graphs"
    ],
    "summary": "y0 is a causal inference framework that provides tools for manipulating causal graphs and identifying effects. It is used by data scientists and researchers interested in causal analysis.",
    "use_cases": [
      "Analyzing the impact of interventions",
      "Understanding causal relationships in data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to manipulate causal graphs in python",
      "tools for effect identification in python",
      "causal inference framework python",
      "python causal graphs library",
      "identifying effects with python"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0025
  },
  {
    "name": "Benchpress",
    "description": "Benchmarking 41+ structure learning algorithms for causal discovery. Standardized evaluation framework.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": "https://benchpressdocs.readthedocs.io/",
    "github_url": "https://github.com/felixleopoldo/benchpress",
    "url": "https://github.com/felixleopoldo/benchpress",
    "install": "pip install benchpress",
    "tags": [
      "causal discovery",
      "benchmarking",
      "structure learning"
    ],
    "best_for": "Comparing causal discovery algorithms",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "graphical-models"
    ],
    "summary": "Benchpress is a benchmarking tool for evaluating over 41 structure learning algorithms used in causal discovery. It provides a standardized evaluation framework for researchers and practitioners in the field.",
    "use_cases": [
      "Comparing different structure learning algorithms",
      "Evaluating causal discovery methods in research"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for causal discovery",
      "how to benchmark structure learning algorithms in python",
      "evaluate structure learning algorithms python",
      "benchmarking causal discovery methods python",
      "python causal inference library",
      "structure learning algorithms evaluation python"
    ],
    "primary_use_cases": [
      "benchmarking structure learning algorithms",
      "evaluating causal discovery methods"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0023
  },
  {
    "name": "Causal Discovery Toolbox (CDT)",
    "description": "Implements algorithms for causal discovery (recovering causal graph structure) from observational data.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": "https://fentechsolutions.github.io/CausalDiscoveryToolbox/html/index.html",
    "github_url": "https://github.com/FenTechSolutions/CausalDiscoveryToolbox",
    "url": "https://github.com/FenTechSolutions/CausalDiscoveryToolbox",
    "install": "pip install cdt",
    "tags": [
      "causal inference",
      "graphs"
    ],
    "best_for": "Learning causal structure from data, DAG estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "graphs"
    ],
    "summary": "The Causal Discovery Toolbox (CDT) implements algorithms for recovering causal graph structures from observational data. It is used by researchers and practitioners in fields such as statistics, data science, and economics to understand causal relationships.",
    "use_cases": [
      "Analyzing causal relationships in observational studies",
      "Building causal models for decision-making"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for causal discovery",
      "how to recover causal graphs in python",
      "causal inference tools in python",
      "causal discovery algorithms python",
      "graphs in causal inference python",
      "observational data causal analysis python"
    ],
    "primary_use_cases": [
      "causal graph structure recovery"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0023
  },
  {
    "name": "CausalNex",
    "description": "Uses Bayesian Networks for causal reasoning, combining ML with expert knowledge to model relationships.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": null,
    "github_url": "https://github.com/mckinsey/causalnex",
    "url": "https://github.com/mckinsey/causalnex",
    "install": "pip install causalnex",
    "tags": [
      "causal inference",
      "graphs",
      "Bayesian"
    ],
    "best_for": "Learning causal structure from data, DAG estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "bayesian"
    ],
    "summary": "CausalNex is a Python library that utilizes Bayesian Networks for causal reasoning, effectively combining machine learning with expert knowledge to model relationships. It is used by data scientists and researchers who are interested in understanding causal relationships in their data.",
    "use_cases": [
      "Analyzing the impact of marketing strategies on sales",
      "Understanding the causal relationships in healthcare data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to model relationships with Bayesian networks in python",
      "CausalNex documentation",
      "causal discovery tools in python",
      "using Bayesian networks for causal reasoning",
      "best practices for causal inference in python"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "pgmpy",
      "DoWhy"
    ],
    "maintenance_status": "active",
    "model_score": 0.0023
  },
  {
    "name": "DeepCTR",
    "description": "Easy-to-use implementations of deep CTR models including Wide&Deep, DeepFM, DIN, xDeepFM, and multi-task architectures",
    "category": "CTR Prediction",
    "docs_url": "https://deepctr-doc.readthedocs.io/",
    "github_url": "https://github.com/shenweichen/DeepCTR",
    "url": "https://deepctr-doc.readthedocs.io/",
    "install": "pip install deepctr",
    "tags": [
      "CTR",
      "deep learning",
      "recommender",
      "Wide&Deep"
    ],
    "best_for": "Implementing and benchmarking deep CTR prediction models",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "DeepCTR provides easy-to-use implementations of various deep learning models for click-through rate (CTR) prediction. It is suitable for data scientists and machine learning practitioners working on recommendation systems.",
    "use_cases": [
      "Building recommendation systems",
      "Optimizing ad placements"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for CTR prediction",
      "how to implement deep learning models for recommendation",
      "easy CTR models in python",
      "DeepCTR usage examples",
      "CTR prediction with DeepCTR",
      "deep learning for recommender systems in python"
    ],
    "primary_use_cases": [
      "CTR prediction",
      "Model benchmarking"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "TensorFlow",
      "PyTorch"
    ],
    "maintenance_status": "active",
    "model_score": 0.0023
  },
  {
    "name": "CausalLib",
    "description": "IBM-developed package that provides a scikit-learn-inspired API for causal inference with meta-algorithms supporting arbitrary machine learning models.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://causallib.readthedocs.io/",
    "github_url": "https://github.com/IBM/causallib",
    "url": "https://github.com/IBM/causallib",
    "install": "pip install causallib",
    "tags": [
      "causal inference",
      "matching"
    ],
    "best_for": "Estimating treatment effects, propensity score matching, observational studies",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "matching"
    ],
    "summary": "CausalLib is an IBM-developed package that provides a scikit-learn-inspired API for causal inference. It is designed for users who need to apply causal inference techniques using various machine learning models.",
    "use_cases": [
      "Estimating causal effects in observational studies",
      "Conducting A/B tests using machine learning models"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to do matching in python",
      "causal inference with machine learning",
      "scikit-learn inspired causal inference library",
      "using CausalLib for A/B testing",
      "meta-algorithms for causal inference in python"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "DoWhy",
      "EconML"
    ],
    "maintenance_status": "active",
    "model_score": 0.0023
  },
  {
    "name": "fastmatch",
    "description": "Fast k-nearest-neighbor matching for large datasets using Facebook's FAISS library.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://github.com/py-econometrics/fastmatch",
    "github_url": null,
    "url": "https://github.com/py-econometrics/fastmatch",
    "install": "pip install fastmatch",
    "tags": [
      "causal inference",
      "matching"
    ],
    "best_for": "Estimating treatment effects, propensity score matching, observational studies",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "matching"
    ],
    "summary": "Fastmatch is a Python package designed for efficient k-nearest-neighbor matching on large datasets, leveraging Facebook's FAISS library. It is particularly useful for researchers and data scientists working in causal inference and matching scenarios.",
    "use_cases": [
      "Matching large datasets for causal analysis",
      "Conducting A/B tests with efficient neighbor matching"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for k-nearest-neighbor matching",
      "how to perform causal inference in python",
      "fastmatch package documentation",
      "efficient matching for large datasets in python",
      "using FAISS for matching in python",
      "best practices for causal inference with python"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0023
  },
  {
    "name": "CausalML",
    "description": "Focuses on uplift modeling and heterogeneous treatment effect estimation using machine learning techniques.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://causalml.readthedocs.io/",
    "github_url": "https://github.com/uber/causalml",
    "url": "https://github.com/uber/causalml",
    "install": "pip install causalml",
    "tags": [
      "causal inference",
      "matching"
    ],
    "best_for": "Estimating treatment effects, propensity score matching, observational studies",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "matching"
    ],
    "summary": "CausalML focuses on uplift modeling and heterogeneous treatment effect estimation using machine learning techniques. It is used by data scientists and researchers interested in causal inference and treatment effect analysis.",
    "use_cases": [
      "Estimating treatment effects in marketing campaigns",
      "Analyzing A/B test results"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for uplift modeling",
      "how to estimate treatment effects in python",
      "CausalML documentation",
      "machine learning for causal inference",
      "best practices for matching in python",
      "python library for heterogeneous treatment effects"
    ],
    "primary_use_cases": [
      "uplift modeling",
      "heterogeneous treatment effect estimation"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "DoWhy",
      "EconML"
    ],
    "maintenance_status": "active",
    "model_score": 0.0018
  },
  {
    "name": "CausalML",
    "description": "Uber's package for uplift modeling and causal inference. Includes meta-learners (S, T, X, R), tree-based methods, and propensity score approaches. Focus on heterogeneous treatment effects.",
    "category": "Healthcare Economics & Health-Tech",
    "docs_url": "https://causalml.readthedocs.io/",
    "github_url": "https://github.com/uber/causalml",
    "url": "https://github.com/uber/causalml",
    "install": "pip install causalml",
    "tags": [
      "causal inference",
      "uplift modeling",
      "treatment effects"
    ],
    "best_for": "Heterogeneous treatment effect estimation and uplift modeling",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "uplift-modeling"
    ],
    "summary": "CausalML is a Python package developed by Uber for uplift modeling and causal inference. It provides tools for analyzing heterogeneous treatment effects, making it useful for data scientists and researchers in healthcare economics and health-tech.",
    "use_cases": [
      "Estimating treatment effects in clinical trials",
      "Optimizing marketing strategies through uplift modeling"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to perform uplift modeling in python",
      "CausalML documentation",
      "best practices for treatment effects analysis in python",
      "python package for meta-learners",
      "how to use CausalML for A/B testing",
      "CausalML examples",
      "understanding heterogeneous treatment effects in python"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "EconML",
      "DoWhy"
    ],
    "maintenance_status": "active",
    "model_score": 0.0018
  },
  {
    "name": "CausalML",
    "description": "Uber's Python library for uplift modeling and causal inference with meta-learners, uplift trees, and propensity methods",
    "category": "Causal Inference",
    "docs_url": "https://causalml.readthedocs.io/",
    "github_url": "https://github.com/uber/causalml",
    "url": "https://causalml.readthedocs.io/",
    "install": "pip install causalml",
    "tags": [
      "uplift modeling",
      "CATE",
      "meta-learners",
      "treatment effects"
    ],
    "best_for": "Estimating heterogeneous treatment effects for ad targeting optimization",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "uplift-modeling"
    ],
    "summary": "CausalML is Uber's Python library designed for uplift modeling and causal inference. It is utilized by data scientists and researchers to analyze treatment effects and improve decision-making processes.",
    "use_cases": [
      "analyzing the impact of marketing campaigns",
      "optimizing customer targeting strategies"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for uplift modeling",
      "how to perform causal inference in python",
      "CausalML documentation",
      "best practices for uplift trees in python",
      "using meta-learners for treatment effects",
      "CausalML examples",
      "how to implement propensity methods in python"
    ],
    "primary_use_cases": [
      "uplift modeling",
      "causal inference analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "EconML",
      "DoWhy"
    ],
    "maintenance_status": "active",
    "model_score": 0.0018
  },
  {
    "name": "CausalML",
    "description": "Uber's library for uplift modeling and heterogeneous treatment effect estimation. Implements meta-learners (S, T, X, R, DR), uplift trees, and CATE estimation for targeting optimization.",
    "category": "MarTech & Customer Analytics",
    "docs_url": "https://causalml.readthedocs.io/",
    "github_url": "https://github.com/uber/causalml",
    "url": "https://github.com/uber/causalml",
    "install": "pip install causalml",
    "tags": [
      "uplift",
      "causal-inference",
      "targeting",
      "treatment-effects"
    ],
    "best_for": "Identifying which customers respond best to marketing interventions",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "uplift",
      "treatment-effects"
    ],
    "summary": "CausalML is Uber's library designed for uplift modeling and heterogeneous treatment effect estimation. It is primarily used by data scientists and researchers looking to optimize targeting strategies through advanced causal inference techniques.",
    "use_cases": [
      "Estimating heterogeneous treatment effects",
      "Optimizing marketing campaigns",
      "Conducting A/B tests with uplift modeling"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for uplift modeling",
      "how to estimate treatment effects in python",
      "CausalML documentation",
      "uplift modeling with CausalML",
      "CausalML examples",
      "how to use CausalML for targeting optimization"
    ],
    "primary_use_cases": [
      "uplift trees",
      "CATE estimation"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "EconML",
      "DoWhy"
    ],
    "maintenance_status": "active",
    "model_score": 0.0018
  },
  {
    "name": "Dolo",
    "description": "Framework for describing and solving economic models (DSGE, OLG, etc.) using a declarative YAML-based format.",
    "category": "Structural Econometrics & Estimation",
    "docs_url": "https://dolo.readthedocs.io/en/latest/",
    "github_url": "https://github.com/EconForge/dolo",
    "url": "https://github.com/EconForge/dolo",
    "install": "pip install dolo",
    "tags": [
      "structural",
      "estimation"
    ],
    "best_for": "Structural models, GMM estimation, BLP-style demand",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "structural-econometrics",
      "estimation"
    ],
    "summary": "Dolo is a framework designed for describing and solving economic models such as DSGE and OLG using a declarative YAML-based format. It is useful for economists and data scientists working with structural econometrics.",
    "use_cases": [
      "Modeling dynamic stochastic general equilibrium (DSGE) models",
      "Estimating overlapping generations (OLG) models"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for economic modeling",
      "how to solve DSGE models in python",
      "declarative framework for econometrics",
      "YAML-based economic model solver",
      "structural estimation in python",
      "OLG model implementation in python"
    ],
    "primary_use_cases": [
      "solving economic models",
      "structural estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0015
  },
  {
    "name": "Dolo",
    "description": "Rational expectations and DSGE model solver using YAML model definitions. Part of the EconForge ecosystem with Julia companion.",
    "category": "Simulation & Computational Economics",
    "docs_url": "https://www.econforge.org/dolo.py/",
    "github_url": "https://github.com/EconForge/dolo.py",
    "url": "https://www.econforge.org/dolo.py/",
    "install": "pip install dolo",
    "tags": [
      "DSGE",
      "rational-expectations",
      "macroeconomics",
      "dynamic-programming"
    ],
    "best_for": "Solving and simulating DSGE and rational expectations models",
    "language": "Python",
    "model_score": 0.0015
  },
  {
    "name": "Lifetimes",
    "description": "Analyze customer lifetime value (CLV) using probabilistic models (BG/NBD, Pareto/NBD) to predict purchases.",
    "category": "Marketing Mix Models (MMM) & Business Analytics",
    "docs_url": "https://lifetimes.readthedocs.io/en/latest/",
    "github_url": "https://github.com/CamDavidsonPilon/lifetimes",
    "url": "https://github.com/CamDavidsonPilon/lifetimes",
    "install": "pip install lifetimes",
    "tags": [
      "marketing",
      "analytics"
    ],
    "best_for": "Marketing ROI, media mix optimization, attribution",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "bayesian",
      "analytics"
    ],
    "summary": "Lifetimes is a Python package designed to analyze customer lifetime value (CLV) using probabilistic models such as BG/NBD and Pareto/NBD. It is used by data scientists and marketers to predict customer purchases and optimize marketing strategies.",
    "use_cases": [
      "Predicting customer purchases over time",
      "Estimating customer lifetime value for marketing campaigns"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for customer lifetime value analysis",
      "how to predict purchases in python",
      "BG/NBD model implementation in python",
      "Pareto/NBD model for marketing",
      "analyze customer behavior with python",
      "customer analytics library python"
    ],
    "primary_use_cases": [
      "customer lifetime value analysis",
      "purchase prediction"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0014
  },
  {
    "name": "lifetimes",
    "description": "Industry-standard library for CLV modeling. Implements BG/NBD, Pareto/NBD for transaction prediction and Gamma-Gamma for monetary value modeling in non-contractual settings.",
    "category": "MarTech & Customer Analytics",
    "docs_url": "https://lifetimes.readthedocs.io/",
    "github_url": "https://github.com/CamDavidsonPilon/lifetimes",
    "url": "https://github.com/CamDavidsonPilon/lifetimes",
    "install": "pip install lifetimes",
    "tags": [
      "CLV",
      "BTYD",
      "customer-analytics",
      "RFM"
    ],
    "best_for": "Implementing probabilistic CLV models (BG/NBD, Pareto/NBD) from transaction data",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "customer-analytics",
      "monetary-value-modeling"
    ],
    "summary": "The 'lifetimes' package is an industry-standard library for customer lifetime value (CLV) modeling. It is used by data scientists and marketers to predict customer transactions and monetary value in non-contractual settings.",
    "use_cases": [
      "Predicting future customer transactions",
      "Estimating customer lifetime value",
      "Analyzing customer behavior over time"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for CLV modeling",
      "how to predict customer transactions in python",
      "customer lifetime value analysis python",
      "BG/NBD model implementation python",
      "Gamma-Gamma model in python",
      "customer analytics tools in python"
    ],
    "primary_use_cases": [
      "transaction prediction",
      "monetary value modeling"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "PyMC-Marketing",
      "CLVTools"
    ],
    "maintenance_status": "active",
    "model_score": 0.0014
  },
  {
    "name": "CausalPlayground",
    "description": "Python library for causal research that addresses the scarcity of real-world datasets with known causal relations. Provides fine-grained control over structural causal models.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://causal-playground.readthedocs.io/",
    "github_url": "https://github.com/sa-and/CausalPlayground",
    "url": "https://github.com/sa-and/CausalPlayground",
    "install": "pip install causal-playground",
    "tags": [
      "causal inference",
      "matching"
    ],
    "best_for": "Estimating treatment effects, propensity score matching, observational studies",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "matching"
    ],
    "summary": "CausalPlayground is a Python library designed for causal research, focusing on the scarcity of real-world datasets with known causal relations. It provides users with fine-grained control over structural causal models, making it suitable for researchers and data scientists interested in causal inference.",
    "use_cases": [
      "Analyzing causal relationships in observational data",
      "Designing experiments with known causal structures"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to perform causal research in python",
      "matching techniques in python",
      "structural causal models in python",
      "real-world datasets for causal analysis",
      "causal inference tools for data scientists"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0012
  },
  {
    "name": "scikit-uplift",
    "description": "Focuses on uplift modeling and estimating heterogeneous treatment effects using various ML-based methods.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://scikit-uplift.readthedocs.io/en/latest/",
    "github_url": "https://github.com/maks-sh/scikit-uplift",
    "url": "https://github.com/maks-sh/scikit-uplift",
    "install": "pip install scikit-uplift",
    "tags": [
      "causal inference",
      "matching"
    ],
    "best_for": "Estimating treatment effects, propensity score matching, observational studies",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "matching"
    ],
    "summary": "scikit-uplift focuses on uplift modeling and estimating heterogeneous treatment effects using various ML-based methods. It is used by data scientists and researchers interested in causal inference and treatment effect estimation.",
    "use_cases": [
      "Estimating the effect of marketing campaigns",
      "Personalizing treatment recommendations"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for uplift modeling",
      "how to estimate treatment effects in python",
      "causal inference library in python",
      "matching methods in python",
      "scikit-uplift documentation",
      "uplift modeling techniques python"
    ],
    "primary_use_cases": [
      "uplift modeling",
      "heterogeneous treatment effect estimation"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "causalml",
      "econml"
    ],
    "maintenance_status": "active",
    "model_score": 0.0011
  },
  {
    "name": "HARK",
    "description": "Toolkit for solving, simulating, and estimating models with heterogeneous agents (e.g., consumption-saving).",
    "category": "Structural Econometrics & Estimation",
    "docs_url": "https://hark.readthedocs.io/en/latest/",
    "github_url": "https://github.com/econ-ark/HARK",
    "url": "https://github.com/econ-ark/HARK",
    "install": "pip install econ-ark",
    "tags": [
      "structural",
      "estimation"
    ],
    "best_for": "Structural models, GMM estimation, BLP-style demand",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "structural",
      "estimation"
    ],
    "summary": "HARK is a toolkit designed for solving, simulating, and estimating models with heterogeneous agents, particularly in the context of consumption-saving behavior. It is used by researchers and practitioners in structural econometrics to analyze complex economic models.",
    "use_cases": [
      "Simulating consumption-saving models",
      "Estimating parameters in heterogeneous agent models"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for heterogeneous agents",
      "how to simulate consumption-saving models in python",
      "HARK toolkit for structural econometrics",
      "estimating models with HARK",
      "HARK package documentation",
      "using HARK for economic modeling"
    ],
    "primary_use_cases": [
      "modeling consumption-saving behavior",
      "estimating heterogeneous agent models"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.001
  },
  {
    "name": "mcf (Modified Causal Forest)",
    "description": "Comprehensive Python implementation for heterogeneous treatment effect estimation. Handles binary/multiple discrete treatments with optimal policy learning via Policy Trees.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://mcfpy.github.io/mcf/",
    "github_url": "https://github.com/MCFpy/mcf",
    "url": "https://github.com/MCFpy/mcf",
    "install": "pip install mcf",
    "tags": [
      "causal inference",
      "treatment effects",
      "policy learning"
    ],
    "best_for": "CATE estimation with policy tree optimization",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "policy-learning"
    ],
    "summary": "The mcf package provides a comprehensive Python implementation for estimating heterogeneous treatment effects. It is designed for researchers and practitioners interested in causal inference and optimal policy learning.",
    "use_cases": [
      "Estimating treatment effects in clinical trials",
      "Optimizing marketing strategies based on treatment outcomes"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to estimate treatment effects in python",
      "policy learning with python",
      "modified causal forest implementation",
      "A/B testing in python",
      "python library for heterogeneous treatment effects"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "policy learning"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.001
  },
  {
    "name": "wildboottest",
    "description": "Fast implementation of various wild cluster bootstrap algorithms (WCR, WCU) for robust inference, especially with few clusters.",
    "category": "Standard Errors, Bootstrapping & Reporting",
    "docs_url": "https://py-econometrics.github.io/wildboottest/",
    "github_url": "https://github.com/py-econometrics/wildboottest",
    "url": "https://github.com/py-econometrics/wildboottest",
    "install": "pip install wildboottest",
    "tags": [
      "bootstrap",
      "standard errors"
    ],
    "best_for": "Robust inference, clustered SEs, result presentation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "bootstrapping",
      "standard errors"
    ],
    "summary": "The wildboottest package provides a fast implementation of various wild cluster bootstrap algorithms for robust inference, particularly useful in scenarios with few clusters. It is designed for statisticians and data scientists who need reliable statistical methods.",
    "use_cases": [
      "Performing robust statistical inference with limited clusters",
      "Conducting bootstrapping for standard error estimation"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for wild cluster bootstrap",
      "how to perform bootstrapping in python",
      "wildboottest documentation",
      "robust inference with wildboottest",
      "bootstrap algorithms in python",
      "standard errors with wildboottest"
    ],
    "primary_use_cases": [
      "wild cluster bootstrap inference"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.001
  },
  {
    "name": "Polars",
    "description": "Blazingly fast DataFrame library for Rust and Python with SQL-like syntax, lazy evaluation, and excellent time series handling.",
    "category": "Core Libraries & Linear Models",
    "docs_url": "https://pola.rs/",
    "github_url": "https://github.com/pola-rs/polars",
    "url": "https://crates.io/crates/polars",
    "install": "cargo add polars",
    "tags": [
      "rust",
      "dataframe",
      "data manipulation",
      "performance"
    ],
    "best_for": "High-performance data manipulation (pandas alternative)",
    "language": "Rust",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "data manipulation",
      "time-series"
    ],
    "summary": "Polars is a high-performance DataFrame library designed for Rust and Python, offering SQL-like syntax and lazy evaluation. It is particularly useful for data manipulation and handling time series data efficiently.",
    "use_cases": [
      "Data analysis with large datasets",
      "Time series data processing"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for data manipulation",
      "how to handle time series in python",
      "fast dataframe library for python",
      "rust dataframe library",
      "sql-like syntax for data analysis",
      "lazy evaluation in data processing",
      "performance data manipulation library"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "pandas",
      "dask"
    ],
    "maintenance_status": "active",
    "model_score": 0.0009
  },
  {
    "name": "Nashpy",
    "description": "Computation of Nash equilibria for 2-player games. Support enumeration and Lemke-Howson algorithm.",
    "category": "Game Theory & Mechanism Design",
    "docs_url": "https://nashpy.readthedocs.io/",
    "github_url": "https://github.com/drvinceknight/Nashpy",
    "url": "https://github.com/drvinceknight/Nashpy",
    "install": "pip install nashpy",
    "tags": [
      "game theory",
      "Nash equilibrium"
    ],
    "best_for": "2-player Nash equilibrium computation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "game theory"
    ],
    "summary": "Nashpy is a Python library designed for the computation of Nash equilibria in 2-player games. It supports enumeration methods and the Lemke-Howson algorithm, making it useful for researchers and practitioners in game theory.",
    "use_cases": [
      "Computing Nash equilibria for strategic games",
      "Analyzing 2-player game scenarios",
      "Research in game theory applications"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for Nash equilibria",
      "how to compute Nash equilibria in python",
      "Nashpy documentation",
      "game theory library python",
      "Lemke-Howson algorithm python",
      "enumerate Nash equilibria python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0009
  },
  {
    "name": "(PySAL Core)",
    "description": "The broader PySAL ecosystem contains many tools for spatial data handling, weights, visualization, and analysis.",
    "category": "Spatial Econometrics",
    "docs_url": "https://pysal.org/",
    "github_url": "https://github.com/pysal/pysal",
    "url": "https://github.com/pysal/pysal",
    "install": "pip install pysal",
    "tags": [
      "spatial",
      "geography"
    ],
    "best_for": "Geographic data, spatial autocorrelation, regional analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "spatial-econometrics"
    ],
    "summary": "PySAL Core is part of the broader PySAL ecosystem, providing tools for spatial data handling, weights, visualization, and analysis. It is used by researchers and practitioners in the fields of spatial econometrics and geography.",
    "use_cases": [
      "Analyzing spatial data patterns",
      "Creating spatial weights for econometric models"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for spatial data analysis",
      "how to visualize spatial data in python",
      "tools for spatial econometrics in python",
      "spatial weights in python",
      "geography analysis with python",
      "spatial data handling in python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0009
  },
  {
    "name": "FactorAnalyzer",
    "description": "Specialized library for Exploratory (EFA) and Confirmatory (CFA) Factor Analysis with rotation options for interpretability.",
    "category": "Dimensionality Reduction",
    "docs_url": "https://factor-analyzer.readthedocs.io/en/latest/",
    "github_url": "https://github.com/EducationalTestingService/factor_analyzer",
    "url": "https://github.com/EducationalTestingService/factor_analyzer",
    "install": "pip install factor_analyzer",
    "tags": [
      "machine learning",
      "dimensionality"
    ],
    "best_for": "Feature extraction, PCA, high-dimensional data",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "FactorAnalyzer is a specialized library designed for Exploratory and Confirmatory Factor Analysis, providing various rotation options for better interpretability. It is used by data scientists and researchers who need to analyze complex datasets and uncover latent structures.",
    "use_cases": [
      "Analyzing survey data to identify underlying factors",
      "Reducing dimensionality in large datasets"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for factor analysis",
      "how to perform EFA in python",
      "CFA library in python",
      "factor analysis with rotation options python",
      "exploratory factor analysis python",
      "confirmatory factor analysis python"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "factor_analyzer"
    ],
    "maintenance_status": "active",
    "model_score": 0.0009
  },
  {
    "name": "MaMiMo",
    "description": "Lightweight Python library focused specifically on Marketing Mix Modeling implementation.",
    "category": "Marketing Mix Models (MMM) & Business Analytics",
    "docs_url": null,
    "github_url": "https://github.com/Garve/mamimo",
    "url": "https://github.com/Garve/mamimo",
    "install": "pip install mamimo",
    "tags": [
      "marketing",
      "analytics"
    ],
    "best_for": "Marketing ROI, media mix optimization, attribution",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "marketing",
      "analytics"
    ],
    "summary": "MaMiMo is a lightweight Python library designed for implementing Marketing Mix Modeling. It is particularly useful for analysts and data scientists working in marketing analytics.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for marketing mix modeling",
      "how to do marketing analytics in python",
      "lightweight python library for MMM",
      "best practices for marketing mix modeling in python",
      "implementing marketing mix models in python",
      "python tools for business analytics"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active",
    "model_score": 0.0008
  },
  {
    "name": "lifelines",
    "description": "Comprehensive library for survival analysis: Kaplan-Meier, Nelson-Aalen, Cox regression, AFT models, handling censored data.",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": "https://lifelines.readthedocs.io/en/latest/",
    "github_url": "https://github.com/CamDavidsonPilon/lifelines",
    "url": "https://github.com/CamDavidsonPilon/lifelines",
    "install": "pip install lifelines",
    "tags": [
      "inference",
      "hypothesis testing"
    ],
    "best_for": "Hypothesis tests, confidence intervals, multiple testing",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "survival-analysis",
      "statistical-inference"
    ],
    "summary": "Lifelines is a comprehensive library for survival analysis that includes methods like Kaplan-Meier and Cox regression. It is used by data scientists and statisticians to analyze time-to-event data, particularly in fields such as healthcare and reliability engineering.",
    "use_cases": [
      "Analyzing patient survival times in clinical trials",
      "Estimating the time until an event occurs in reliability engineering"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for survival analysis",
      "how to perform Cox regression in python",
      "Kaplan-Meier estimator python",
      "analyze censored data python",
      "survival analysis with lifelines",
      "python lifelines documentation"
    ],
    "primary_use_cases": [
      "Kaplan-Meier estimation",
      "Cox proportional hazards model"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "statsmodels",
      "scikit-survival"
    ],
    "maintenance_status": "active",
    "model_score": 0.0008
  },
  {
    "name": "lifelines",
    "description": "Complete survival analysis library with Kaplan-Meier, Cox regression, AFT models, and rich plotting capabilities for time-to-event data",
    "category": "Insurance & Actuarial",
    "docs_url": "https://lifelines.readthedocs.io/",
    "github_url": "https://github.com/CamDavidsonPilon/lifelines",
    "url": "https://github.com/CamDavidsonPilon/lifelines",
    "install": "pip install lifelines",
    "tags": [
      "survival-analysis",
      "Kaplan-Meier",
      "Cox-regression",
      "time-to-event",
      "hazard-models"
    ],
    "best_for": "Customer churn analysis, mortality modeling, and survival curve estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "survival-analysis",
      "time-to-event"
    ],
    "summary": "Lifelines is a complete survival analysis library that provides tools for Kaplan-Meier estimation, Cox regression, and Accelerated Failure Time (AFT) models, along with rich plotting capabilities for analyzing time-to-event data. It is commonly used by data scientists and statisticians in fields such as insurance and actuarial science.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for survival analysis",
      "how to perform Kaplan-Meier in python",
      "Cox regression in python",
      "time-to-event analysis python",
      "AFT models in python",
      "hazard models python"
    ],
    "use_cases": [
      "Analyzing patient survival times in medical research",
      "Estimating insurance risk based on policyholder data"
    ],
    "primary_use_cases": [
      "Kaplan-Meier estimation",
      "Cox regression analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "lifelines"
    ],
    "maintenance_status": "active",
    "model_score": 0.0008
  },
  {
    "name": "lifelines",
    "description": "Pure Python survival analysis library. Kaplan-Meier estimation, Cox PH regression, parametric models (Weibull, log-normal), and time-varying covariates. Excellent documentation.",
    "category": "Healthcare Economics & Health-Tech",
    "docs_url": "https://lifelines.readthedocs.io/",
    "github_url": "https://github.com/CamDavidsonPilon/lifelines",
    "url": "https://lifelines.readthedocs.io/",
    "install": "pip install lifelines",
    "tags": [
      "survival analysis",
      "Kaplan-Meier",
      "Cox regression"
    ],
    "best_for": "Classical survival analysis with intuitive API",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "survival analysis"
    ],
    "summary": "Lifelines is a pure Python library designed for survival analysis, providing tools for Kaplan-Meier estimation, Cox proportional hazards regression, and parametric models. It is particularly useful for researchers and practitioners in healthcare economics and health-tech who need to analyze time-to-event data.",
    "use_cases": [
      "Analyzing patient survival times",
      "Evaluating treatment effects in clinical trials"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for survival analysis",
      "how to perform Kaplan-Meier estimation in python",
      "Cox regression in python",
      "time-varying covariates in python",
      "survival analysis with lifelines",
      "healthcare analytics python library"
    ],
    "primary_use_cases": [
      "Kaplan-Meier estimation",
      "Cox PH regression"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "statsmodels",
      "scikit-survival"
    ],
    "maintenance_status": "active",
    "model_score": 0.0008
  },
  {
    "name": "appelpy",
    "description": "Applied Econometrics Library bridging Stata-like syntax with Python. Built on statsmodels with convenient API.",
    "category": "Core Libraries & Linear Models",
    "docs_url": "https://appelpy.readthedocs.io/",
    "github_url": "https://github.com/mfarragher/appelpy",
    "url": "https://github.com/mfarragher/appelpy",
    "install": "pip install appelpy",
    "tags": [
      "regression",
      "linear models",
      "Stata"
    ],
    "best_for": "Stata-like econometrics workflow in Python",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "statsmodels"
    ],
    "topic_tags": [
      "regression",
      "linear models"
    ],
    "summary": "Appelpy is an Applied Econometrics Library that bridges Stata-like syntax with Python, providing a convenient API built on statsmodels. It is designed for users who are familiar with econometric modeling and want to leverage Python's capabilities.",
    "use_cases": [
      "Conducting regression analysis",
      "Performing econometric modeling"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for applied econometrics",
      "how to perform regression in python",
      "Stata-like syntax in python",
      "linear models in python",
      "applying econometrics with python",
      "python statsmodels tutorial"
    ],
    "primary_use_cases": [
      "regression analysis",
      "econometric modeling"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "statsmodels",
      "pandas"
    ],
    "maintenance_status": "active",
    "model_score": 0.0008
  },
  {
    "name": "Computational Methods for practitioners",
    "description": "Open-source textbook by Richard Evans on computational methods for researchers using Python.",
    "category": "Inference & Reporting Tools",
    "docs_url": "https://opensourceecon.github.io/CompMethods/",
    "github_url": "https://github.com/OpenSourceEcon/CompMethods",
    "url": "https://opensourceecon.github.io/CompMethods/",
    "install": "",
    "tags": [
      "education",
      "computation",
      "textbook"
    ],
    "best_for": "Comprehensive computational economics course",
    "language": "Python",
    "difficulty": "beginner|intermediate",
    "prerequisites": [],
    "topic_tags": [
      "computation",
      "education"
    ],
    "summary": "This package is an open-source textbook designed for researchers to learn computational methods using Python. It is suitable for those interested in applying these methods in various research contexts.",
    "use_cases": [],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for computational methods",
      "how to use Python for research",
      "open-source textbook on Python",
      "learning computational methods in Python",
      "Python tools for researchers",
      "educational resources for Python"
    ],
    "api_complexity": "simple",
    "model_score": 0.0008
  },
  {
    "name": "Transformers",
    "description": "Access to thousands of pre-trained models for NLP tasks like text classification, summarization, embeddings, etc.",
    "category": "Natural Language Processing for Economics",
    "docs_url": "https://huggingface.co/transformers/",
    "github_url": "https://github.com/huggingface/transformers",
    "url": "https://github.com/huggingface/transformers",
    "install": "pip install transformers",
    "tags": [
      "NLP",
      "text analysis"
    ],
    "best_for": "Text analysis, sentiment analysis, document classification",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Transformers provides access to thousands of pre-trained models specifically designed for various natural language processing tasks such as text classification, summarization, and embeddings. It is widely used by data scientists and researchers in the field of economics to enhance their NLP capabilities.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for NLP",
      "how to use pre-trained models in python",
      "text classification with Transformers",
      "summarization using Transformers",
      "NLP tasks in python",
      "accessing pre-trained models for text analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0008
  },
  {
    "name": "upper-envelope",
    "description": "Fast upper envelope scan for discrete-continuous dynamic programming. JAX and numba implementations.",
    "category": "Structural Econometrics & Estimation",
    "docs_url": null,
    "github_url": "https://github.com/OpenSourceEconomics/upper-envelope",
    "url": "https://github.com/OpenSourceEconomics/upper-envelope",
    "install": "pip install upper-envelope",
    "tags": [
      "structural",
      "dynamic programming",
      "optimization"
    ],
    "best_for": "Fast upper envelope computation for DC-EGM",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "optimization"
    ],
    "summary": "The upper-envelope package provides fast upper envelope scans for discrete-continuous dynamic programming problems. It is useful for researchers and practitioners in structural econometrics and estimation.",
    "use_cases": [
      "Dynamic programming for economic models",
      "Optimization in structural estimation"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for upper envelope scan",
      "how to perform dynamic programming in python",
      "JAX implementation for optimization",
      "numba for dynamic programming",
      "structural econometrics tools in python",
      "fast upper envelope algorithms in python"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "JAX",
      "numba"
    ],
    "maintenance_status": "active",
    "model_score": 0.0008
  },
  {
    "name": "QuantEcon.py",
    "description": "Core library for quantitative economics: dynamic programming, Markov chains, game theory, numerical methods.",
    "category": "Structural Econometrics & Estimation",
    "docs_url": "https://quantecon.org/python-lectures/",
    "github_url": "https://github.com/QuantEcon/QuantEcon.py",
    "url": "https://github.com/QuantEcon/QuantEcon.py",
    "install": "pip install quantecon",
    "tags": [
      "structural",
      "estimation"
    ],
    "best_for": "Structural models, GMM estimation, BLP-style demand",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "QuantEcon.py is a core library designed for quantitative economics, providing tools for dynamic programming, Markov chains, game theory, and numerical methods. It is primarily used by economists and data scientists working in the field of structural econometrics.",
    "use_cases": [
      "Modeling economic scenarios using dynamic programming",
      "Analyzing Markov processes in economic models"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for quantitative economics",
      "how to implement dynamic programming in python",
      "Markov chains in python",
      "game theory library python",
      "numerical methods for economics python",
      "structural estimation tools in python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0008
  },
  {
    "name": "SynapseML",
    "description": "Microsoft's distributed ML library with native Double ML (DoubleMLEstimator) for heterogeneous treatment effects at scale.",
    "category": "Double/Debiased Machine Learning (DML)",
    "docs_url": "https://microsoft.github.io/SynapseML/",
    "github_url": "https://github.com/microsoft/SynapseML",
    "url": "https://github.com/microsoft/SynapseML",
    "install": "pip install synapseml",
    "tags": [
      "spark",
      "causal inference",
      "double ML",
      "distributed"
    ],
    "best_for": "Causal inference at 100M+ rows on Spark clusters",
    "language": "Spark",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "distributed-machine-learning"
    ],
    "summary": "SynapseML is a distributed machine learning library developed by Microsoft that provides native support for Double ML, specifically designed for estimating heterogeneous treatment effects at scale. It is used by data scientists and researchers working in causal inference and machine learning.",
    "use_cases": [
      "Estimating treatment effects in clinical trials",
      "Analyzing the impact of marketing campaigns"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for distributed machine learning",
      "how to perform causal inference in Spark",
      "Double ML implementation in Python",
      "Microsoft ML library for treatment effects",
      "using SynapseML for A/B testing",
      "causal inference with Spark library",
      "distributed ML for heterogeneous treatment effects"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "Spark"
    ],
    "maintenance_status": "active",
    "model_score": 0.0007
  },
  {
    "name": "XLogit",
    "description": "Fast estimation of Multinomial Logit and Mixed Logit models, optimized for performance.",
    "category": "Discrete Choice Models",
    "docs_url": "https://xlogit.readthedocs.io/",
    "github_url": "https://github.com/arteagac/xlogit",
    "url": "https://github.com/arteagac/xlogit",
    "install": "pip install xlogit",
    "tags": [
      "discrete choice",
      "logit"
    ],
    "best_for": "Logit/probit models, consumer choice, demand estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "discrete choice"
    ],
    "summary": "XLogit is a Python package designed for the fast estimation of Multinomial Logit and Mixed Logit models, focusing on performance optimization. It is primarily used by data scientists and researchers working in the field of discrete choice modeling.",
    "use_cases": [
      "Estimating consumer choice behavior",
      "Analyzing survey data for preference modeling"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for Multinomial Logit",
      "how to estimate Mixed Logit models in python",
      "XLogit package documentation",
      "discrete choice modeling in python",
      "fast estimation of logit models python",
      "performance optimization in logit models"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0007
  },
  {
    "name": "xlogit",
    "description": "GPU-accelerated estimation of mixed logit models using CuPy/NumPy. Orders of magnitude faster than traditional packages for large datasets.",
    "category": "Transportation Economics & Technology",
    "docs_url": "https://xlogit.readthedocs.io/",
    "github_url": "https://github.com/arteagac/xlogit",
    "url": "https://github.com/arteagac/xlogit",
    "install": "pip install xlogit",
    "tags": [
      "discrete choice",
      "mixed logit",
      "GPU",
      "machine learning"
    ],
    "best_for": "Large-scale mixed logit estimation with GPU acceleration",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "xlogit is a GPU-accelerated library for estimating mixed logit models using CuPy and NumPy, offering significant speed improvements for large datasets. It is particularly useful for researchers and practitioners in transportation economics and machine learning.",
    "use_cases": [
      "Estimating mixed logit models for transportation data",
      "Analyzing consumer choice behavior in large datasets"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for mixed logit models",
      "how to estimate mixed logit models in python",
      "GPU accelerated discrete choice modeling python",
      "fast estimation of logit models python",
      "using CuPy for mixed logit models",
      "machine learning for transportation economics"
    ],
    "primary_use_cases": [
      "Discrete choice modeling"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "Biogeme",
      "PyLogit",
      "CuPy"
    ],
    "maintenance_status": "active",
    "model_score": 0.0007
  },
  {
    "name": "OSMnx",
    "description": "Download, model, analyze, and visualize street networks and urban infrastructure from OpenStreetMap. Essential for transportation network analysis.",
    "category": "Transportation Economics & Technology",
    "docs_url": "https://osmnx.readthedocs.io/",
    "github_url": "https://github.com/gboeing/osmnx",
    "url": "https://geoffboeing.com/publications/osmnx-complex-street-networks/",
    "install": "pip install osmnx",
    "tags": [
      "networks",
      "OpenStreetMap",
      "urban",
      "GIS",
      "routing"
    ],
    "best_for": "Street network analysis, urban form metrics, routing algorithms",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "OSMnX is a Python package that allows users to download, model, analyze, and visualize street networks and urban infrastructure from OpenStreetMap. It is essential for transportation network analysis and is used by urban planners, researchers, and data scientists.",
    "use_cases": [
      "Analyzing urban transportation networks",
      "Visualizing street layouts from OpenStreetMap"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for street network analysis",
      "how to visualize urban infrastructure in python",
      "download OpenStreetMap data in python",
      "analyze transportation networks with python",
      "model street networks using OSMnx",
      "routing analysis in python"
    ],
    "primary_use_cases": [
      "Network analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "NetworkX",
      "GeoPandas",
      "Shapely"
    ],
    "maintenance_status": "active",
    "model_score": 0.0007
  },
  {
    "name": "gtfs-kit",
    "description": "Analyze General Transit Feed Specification (GTFS) data. Compute route statistics, service frequencies, and visualize transit networks.",
    "category": "Transportation Economics & Technology",
    "docs_url": "https://mrcagney.github.io/gtfs_kit_docs/",
    "github_url": "https://github.com/mrcagney/gtfs_kit",
    "url": "https://github.com/mrcagney/gtfs_kit",
    "install": "pip install gtfs-kit",
    "tags": [
      "GTFS",
      "transit",
      "public transportation",
      "scheduling"
    ],
    "best_for": "GTFS feed analysis, transit service metrics, schedule validation",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [],
    "summary": "gtfs-kit is a Python package designed to analyze General Transit Feed Specification (GTFS) data. It helps users compute route statistics, service frequencies, and visualize transit networks, making it useful for transit agencies and researchers in transportation economics.",
    "use_cases": [
      "Analyzing transit service frequencies",
      "Visualizing transit networks",
      "Computing route statistics"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for GTFS analysis",
      "how to visualize transit networks in python",
      "compute service frequencies with python",
      "analyze public transportation data python",
      "GTFS data statistics python",
      "transit scheduling analysis python"
    ],
    "primary_use_cases": [
      "Transit analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "tidytransit",
      "partridge",
      "peartree"
    ],
    "maintenance_status": "active",
    "model_score": 0.0007
  },
  {
    "name": "Apollo",
    "description": "Comprehensive R package for advanced choice modeling including mixed logit, latent class, hybrid choice, and integrated choice-latent variable models.",
    "category": "Transportation Economics & Technology",
    "docs_url": "http://www.apollochoicemodelling.com/",
    "github_url": "https://github.com/apollochoicemodelling/apollo",
    "url": "http://www.apollochoicemodelling.com/",
    "install": "install.packages('apollo')",
    "tags": [
      "discrete choice",
      "R",
      "mixed logit",
      "latent class"
    ],
    "best_for": "Advanced choice models with latent variables and hybrid specifications",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Apollo is a comprehensive R package designed for advanced choice modeling. It is used by researchers and practitioners in transportation economics and related fields to analyze complex decision-making processes.",
    "use_cases": [
      "Analyzing consumer choice behavior in transportation",
      "Estimating preferences for different transport modes"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for advanced choice modeling",
      "how to perform mixed logit analysis in R",
      "latent class modeling in R",
      "integrated choice-latent variable models in R",
      "R discrete choice modeling package",
      "hybrid choice models in R"
    ],
    "primary_use_cases": [
      "Discrete choice modeling"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "mlogit",
      "gmnl",
      "Biogeme"
    ],
    "maintenance_status": "active",
    "model_score": 0.0007
  },
  {
    "name": "mlogit",
    "description": "The standard R package for multinomial logit estimation. Clean formula interface, nested logit support, and integration with R's modeling ecosystem.",
    "category": "Transportation Economics & Technology",
    "docs_url": "https://cran.r-project.org/web/packages/mlogit/",
    "github_url": "https://github.com/ycroissant/mlogit",
    "url": "https://cran.r-project.org/web/packages/mlogit/",
    "install": "install.packages('mlogit')",
    "tags": [
      "discrete choice",
      "R",
      "logit",
      "econometrics"
    ],
    "best_for": "Standard multinomial and nested logit models in R",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "econometrics"
    ],
    "summary": "mlogit is the standard R package for multinomial logit estimation, offering a clean formula interface and nested logit support. It is widely used by researchers and practitioners in the field of transportation economics and technology.",
    "use_cases": [
      "Estimating travel mode choice",
      "Analyzing consumer preferences for products"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for multinomial logit estimation",
      "how to perform multinomial logit in R",
      "mlogit package documentation",
      "R discrete choice modeling",
      "nested logit model in R",
      "econometrics tools in R"
    ],
    "primary_use_cases": [
      "Discrete choice modeling"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "Apollo",
      "gmnl",
      "nnet"
    ],
    "maintenance_status": "active",
    "model_score": 0.0007
  },
  {
    "name": "gmnl",
    "description": "R package for generalized multinomial logit models including G-MNL, LC-MNL, and MM-MNL for flexible preference heterogeneity.",
    "category": "Transportation Economics & Technology",
    "docs_url": "https://cran.r-project.org/web/packages/gmnl/",
    "github_url": "https://github.com/edsandorf/gmnl",
    "url": "https://cran.r-project.org/web/packages/gmnl/",
    "install": "install.packages('gmnl')",
    "tags": [
      "discrete choice",
      "R",
      "heterogeneity",
      "mixed logit"
    ],
    "best_for": "Flexible preference heterogeneity models (G-MNL, scale heterogeneity)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "transportation-economics",
      "discrete-choice"
    ],
    "summary": "The gmnl package provides tools for estimating generalized multinomial logit models, including G-MNL, LC-MNL, and MM-MNL, which allow for flexible modeling of preference heterogeneity. It is primarily used by researchers and practitioners in transportation economics and related fields.",
    "use_cases": [
      "Estimating consumer preferences in transportation studies",
      "Analyzing survey data for choice modeling"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for generalized multinomial logit models",
      "how to model preference heterogeneity in R",
      "gmnl package documentation",
      "R discrete choice modeling",
      "flexible preference modeling in R",
      "R transportation economics package"
    ],
    "primary_use_cases": [
      "Discrete choice modeling"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "mlogit",
      "Apollo",
      "mixl"
    ],
    "maintenance_status": "active",
    "model_score": 0.0007
  },
  {
    "name": "mixl",
    "description": "Fast maximum simulated likelihood estimation of mixed logit models in R. Optimized for speed with large datasets.",
    "category": "Transportation Economics & Technology",
    "docs_url": "https://cran.r-project.org/web/packages/mixl/",
    "github_url": "https://github.com/joemolloy/mixl",
    "url": "https://cran.r-project.org/web/packages/mixl/",
    "install": "install.packages('mixl')",
    "tags": [
      "discrete choice",
      "R",
      "mixed logit",
      "performance"
    ],
    "best_for": "Fast mixed logit estimation for large stated preference datasets",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "transportation-economics",
      "mixed-logit"
    ],
    "summary": "The mixl package provides fast maximum simulated likelihood estimation for mixed logit models in R, optimized for handling large datasets. It is primarily used by researchers and practitioners in the field of transportation economics and technology.",
    "use_cases": [
      "Estimating consumer preferences in transportation studies",
      "Analyzing choice behavior in discrete choice experiments"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for mixed logit models",
      "how to estimate mixed logit in R",
      "maximum simulated likelihood estimation in R",
      "fast estimation for large datasets R",
      "discrete choice modeling in R",
      "performance optimization for R packages"
    ],
    "primary_use_cases": [
      "Discrete choice modeling"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "mlogit",
      "gmnl",
      "Apollo"
    ],
    "maintenance_status": "active",
    "model_score": 0.0007
  },
  {
    "name": "tidytransit",
    "description": "Read and analyze GTFS transit feeds in the tidyverse style. Integrates with sf for spatial analysis and dplyr for data manipulation.",
    "category": "Transportation Economics & Technology",
    "docs_url": "https://r-transit.github.io/tidytransit/",
    "github_url": "https://github.com/r-transit/tidytransit",
    "url": "https://r-transit.github.io/tidytransit/",
    "install": "install.packages('tidytransit')",
    "tags": [
      "GTFS",
      "transit",
      "R",
      "tidyverse",
      "spatial"
    ],
    "best_for": "GTFS analysis with tidyverse workflows",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The tidytransit package allows users to read and analyze GTFS transit feeds using the tidyverse style. It is particularly useful for those involved in transportation economics and technology, providing tools for spatial analysis and data manipulation.",
    "use_cases": [
      "Analyzing public transit data",
      "Visualizing transit routes",
      "Performing spatial analysis on transit feeds"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "R package for GTFS analysis",
      "how to analyze transit data in R",
      "tidyverse tools for spatial analysis",
      "R transit feed integration",
      "GTFS data manipulation in R",
      "R package for transportation economics"
    ],
    "primary_use_cases": [
      "Transit analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "gtfs-kit",
      "sf",
      "dplyr"
    ],
    "maintenance_status": "active",
    "framework_compatibility": [
      "tidyverse",
      "sf",
      "dplyr"
    ],
    "model_score": 0.0007
  },
  {
    "name": "SUMO",
    "description": "Simulation of Urban Mobility - open source traffic simulation suite for modeling road networks, public transit, pedestrians, and multimodal scenarios.",
    "category": "Transportation Economics & Technology",
    "docs_url": "https://sumo.dlr.de/docs/",
    "github_url": "https://github.com/eclipse/sumo",
    "url": "https://eclipse.dev/sumo/",
    "install": "brew install sumo  # or apt-get install sumo",
    "tags": [
      "simulation",
      "traffic",
      "microsimulation",
      "multimodal"
    ],
    "best_for": "Traffic microsimulation, signal timing optimization, scenario analysis",
    "language": "C++/Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "SUMO is an open source traffic simulation suite designed for modeling road networks, public transit, pedestrians, and multimodal scenarios. It is used by researchers and practitioners in the field of transportation economics and technology to analyze urban mobility.",
    "use_cases": [
      "Modeling traffic flow in urban areas",
      "Simulating public transit systems",
      "Analyzing pedestrian movement in city environments"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for traffic simulation",
      "how to model urban mobility in C++",
      "open source traffic simulation tools",
      "SUMO traffic simulation tutorial",
      "multimodal transportation modeling software",
      "urban mobility simulation package"
    ],
    "primary_use_cases": [
      "Traffic simulation"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "TraCI",
      "MATSIM",
      "VISSIM"
    ],
    "maintenance_status": "active",
    "model_score": 0.0007
  },
  {
    "name": "OpenTripPlanner",
    "description": "Open source multimodal trip planning engine. Combines GTFS transit, OpenStreetMap streets, and bike-share for routing and isochrone analysis.",
    "category": "Transportation Economics & Technology",
    "docs_url": "https://docs.opentripplanner.org/",
    "github_url": "https://github.com/opentripplanner/OpenTripPlanner",
    "url": "https://www.opentripplanner.org/",
    "install": "java -jar otp.jar --build --serve",
    "tags": [
      "routing",
      "multimodal",
      "GTFS",
      "isochrones",
      "accessibility"
    ],
    "best_for": "Multimodal routing, transit accessibility analysis, isochrone mapping",
    "language": "Java",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "OpenTripPlanner is an open source multimodal trip planning engine that integrates GTFS transit data, OpenStreetMap streets, and bike-share information for effective routing and isochrone analysis. It is used by transportation planners and developers to enhance mobility solutions.",
    "use_cases": [
      "Planning public transportation routes",
      "Analyzing accessibility for urban areas"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "open source trip planner",
      "how to plan multimodal trips",
      "routing with GTFS",
      "isochrone analysis tool",
      "bike-share routing software",
      "transit planning engine"
    ],
    "primary_use_cases": [
      "Routing and accessibility"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "r5",
      "Valhalla",
      "OSRM"
    ],
    "maintenance_status": "active",
    "model_score": 0.0007
  },
  {
    "name": "mmm_stan",
    "description": "Python/STAN implementation of Bayesian Marketing Mix Models.",
    "category": "Marketing Mix Models (MMM) & Business Analytics",
    "docs_url": null,
    "github_url": "https://github.com/sibylhe/mmm_stan",
    "url": "https://github.com/sibylhe/mmm_stan",
    "install": "GitHub Repository",
    "tags": [
      "marketing",
      "analytics",
      "Bayesian"
    ],
    "best_for": "Marketing ROI, media mix optimization, attribution",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "bayesian",
      "marketing",
      "analytics"
    ],
    "summary": "mmm_stan is a Python implementation of Bayesian Marketing Mix Models, designed to help businesses analyze and optimize their marketing strategies. It is primarily used by data scientists and marketers looking to leverage Bayesian methods for marketing analytics.",
    "use_cases": [
      "optimizing marketing spend",
      "analyzing the effectiveness of advertising campaigns"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for Bayesian Marketing Mix Models",
      "how to analyze marketing data with Python",
      "Bayesian analytics for marketing",
      "marketing mix modeling in Python",
      "using STAN for marketing analysis",
      "Bayesian methods for business analytics"
    ],
    "primary_use_cases": [
      "marketing mix modeling",
      "Bayesian analysis of marketing data"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0007
  },
  {
    "name": "Linearmodels",
    "description": "Estimation of fixed, random, pooled OLS models for panel data. Also Fama-MacBeth and between/first-difference estimators.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://bashtage.github.io/linearmodels/",
    "github_url": "https://github.com/bashtage/linearmodels",
    "url": "https://github.com/bashtage/linearmodels",
    "install": "pip install linearmodels",
    "tags": [
      "panel data",
      "fixed effects"
    ],
    "best_for": "Longitudinal analysis, controlling for unobserved heterogeneity",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "panel data",
      "fixed effects"
    ],
    "summary": "Linearmodels is a Python package designed for the estimation of fixed, random, and pooled OLS models for panel data. It is useful for researchers and data scientists working with econometric models and time-series data.",
    "use_cases": [
      "Estimating fixed effects models for economic data",
      "Conducting Fama-MacBeth regression analysis"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for panel data analysis",
      "how to estimate fixed effects in python",
      "panel data regression in python",
      "Fama-MacBeth estimator python",
      "random effects model python",
      "how to use linearmodels package"
    ],
    "primary_use_cases": [
      "fixed effects estimation",
      "random effects estimation"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "statsmodels"
    ],
    "maintenance_status": "active",
    "model_score": 0.0007
  },
  {
    "name": "Python Packages for Applied Economists",
    "description": "Curated collection of Python packages for applied researchers organized by functionality.",
    "category": "Inference & Reporting Tools",
    "docs_url": null,
    "github_url": "https://github.com/clibassi/python-packages-for-applied-economists",
    "url": "https://github.com/clibassi/python-packages-for-applied-economists",
    "install": "",
    "tags": [
      "curated list",
      "resources"
    ],
    "best_for": "Discovering econometrics packages by use case",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "This package provides a curated collection of Python packages specifically designed for applied researchers. It is useful for those looking to enhance their research with various tools organized by functionality.",
    "use_cases": [],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for applied economists",
      "how to analyze data in python",
      "python packages for statistical analysis",
      "best python tools for econometrics",
      "resources for econometric analysis in python",
      "python libraries for data reporting"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active",
    "model_score": 0.0007
  },
  {
    "name": "miceforest",
    "description": "LightGBM-accelerated multiple imputation by chained equations. Fast MICE for large datasets.",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": "https://miceforest.readthedocs.io/",
    "github_url": "https://github.com/AnotherSamWilson/miceforest",
    "url": "https://github.com/AnotherSamWilson/miceforest",
    "install": "pip install miceforest",
    "tags": [
      "missing data",
      "imputation",
      "machine learning"
    ],
    "best_for": "Fast MICE imputation with LightGBM",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "missing data",
      "machine learning"
    ],
    "summary": "miceforest is a Python package that provides a fast implementation of multiple imputation by chained equations, accelerated by LightGBM. It is designed for handling large datasets with missing values, making it useful for data scientists and statisticians.",
    "use_cases": [
      "Imputing missing values in large datasets",
      "Preparing data for machine learning models"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for multiple imputation",
      "how to handle missing data in python",
      "fast MICE implementation python",
      "LightGBM for imputation",
      "imputation techniques in machine learning",
      "best practices for missing data in datasets"
    ],
    "primary_use_cases": [
      "multiple imputation",
      "data preprocessing for machine learning"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "mice",
      "missForest"
    ],
    "maintenance_status": "active",
    "model_score": 0.0007
  },
  {
    "name": "FilterPy",
    "description": "Focuses on Kalman filters (standard, EKF, UKF) and smoothers with a clear, pedagogical implementation style.",
    "category": "State Space & Volatility Models",
    "docs_url": "https://filterpy.readthedocs.io/en/latest/",
    "github_url": "https://github.com/rlabbe/filterpy",
    "url": "https://github.com/rlabbe/filterpy",
    "install": "pip install filterpy",
    "tags": [
      "volatility",
      "state space"
    ],
    "best_for": "GARCH, stochastic volatility, Kalman filtering",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "state space",
      "time-series"
    ],
    "summary": "FilterPy focuses on Kalman filters, including standard, EKF, and UKF implementations, along with smoothers. It is designed for those looking to understand and apply these concepts in a clear and pedagogical manner.",
    "use_cases": [
      "Tracking objects in motion",
      "Estimating the state of a system over time"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for Kalman filters",
      "how to implement EKF in python",
      "python library for state space models",
      "using FilterPy for smoothing",
      "FilterPy documentation",
      "FilterPy examples",
      "FilterPy installation guide"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0007
  },
  {
    "name": "PySAL (spreg)",
    "description": "The spatial regression `spreg` module of PySAL. Implements spatial lag, error, IV models, and diagnostics.",
    "category": "Spatial Econometrics",
    "docs_url": "https://pysal.org/spreg/",
    "github_url": "https://github.com/pysal/spreg",
    "url": "https://github.com/pysal/spreg",
    "install": "pip install spreg",
    "tags": [
      "spatial",
      "geography"
    ],
    "best_for": "Geographic data, spatial autocorrelation, regional analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "spatial-econometrics",
      "geography"
    ],
    "summary": "The `spreg` module of PySAL provides tools for spatial regression analysis, including spatial lag, error, and instrumental variable models. It is used by researchers and practitioners in spatial econometrics to analyze spatially correlated data.",
    "use_cases": [
      "Analyzing spatially correlated economic data",
      "Evaluating the impact of geographical factors on economic outcomes"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for spatial regression",
      "how to perform spatial econometrics in python",
      "spatial lag model in python",
      "spatial error model python",
      "IV models for spatial data",
      "diagnostics for spatial regression python"
    ],
    "primary_use_cases": [
      "spatial lag model estimation",
      "spatial error model analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "GeoPandas",
      "statsmodels"
    ],
    "maintenance_status": "active",
    "model_score": 0.0007
  },
  {
    "name": "expectation",
    "description": "E-values and game-theoretic probability for sequential testing. Enables early signal detection with proper error control.",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": null,
    "github_url": "https://github.com/jakorostami/expectation",
    "url": "https://pypi.org/project/expectation/",
    "install": "pip install expectation",
    "tags": [
      "sequential testing",
      "e-values",
      "hypothesis testing"
    ],
    "best_for": "E-value based sequential hypothesis testing",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python"
    ],
    "topic_tags": [
      "statistical-inference",
      "hypothesis-testing"
    ],
    "summary": "The expectation package provides tools for E-values and game-theoretic probability, facilitating early signal detection while maintaining proper error control. It is primarily used by statisticians and data scientists involved in sequential testing.",
    "use_cases": [
      "Detecting signals in clinical trials",
      "Evaluating A/B tests with proper error control"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for e-values",
      "how to perform sequential testing in python",
      "game-theoretic probability in python",
      "hypothesis testing with python",
      "early signal detection python",
      "statistical inference library python"
    ],
    "primary_use_cases": [
      "sequential testing",
      "hypothesis testing"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0007
  },
  {
    "name": "PyBLP",
    "description": "Tools for estimating demand for differentiated products using the Berry-Levinsohn-Pakes (BLP) method.",
    "category": "Discrete Choice Models",
    "docs_url": "https://pyblp.readthedocs.io/",
    "github_url": "https://github.com/jeffgortmaker/pyblp",
    "url": "https://github.com/jeffgortmaker/pyblp",
    "install": "pip install pyblp",
    "tags": [
      "discrete choice",
      "logit"
    ],
    "best_for": "Logit/probit models, consumer choice, demand estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "PyBLP is a Python package designed for estimating demand for differentiated products using the Berry-Levinsohn-Pakes (BLP) method. It is primarily used by researchers and practitioners in economics and data science who are interested in discrete choice modeling.",
    "use_cases": [
      "Estimating demand for consumer products",
      "Analyzing market competition"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for estimating demand",
      "how to use BLP method in python",
      "tools for discrete choice modeling in python",
      "python package for differentiated products demand estimation",
      "how to estimate demand using PyBLP",
      "BLP method implementation in python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0007
  },
  {
    "name": "pyqreg",
    "description": "Fast quantile regression solver using interior point methods, supporting robust and clustered standard errors.",
    "category": "Quantile Regression & Distributional Methods",
    "docs_url": "https://github.com/mozjay0619/pyqreg",
    "github_url": null,
    "url": "https://github.com/mozjay0619/pyqreg",
    "install": "pip install pyqreg",
    "tags": [
      "quantile",
      "regression"
    ],
    "best_for": "Heterogeneous effects, distributional analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "pyqreg is a fast quantile regression solver that utilizes interior point methods to provide robust and clustered standard errors. It is designed for users who need efficient quantile regression analysis in their statistical modeling.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for quantile regression",
      "how to perform quantile regression in python",
      "fast quantile regression solver in python",
      "interior point methods for regression",
      "robust standard errors in python",
      "clustered standard errors in quantile regression"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0007
  },
  {
    "name": "PyMC Statespace",
    "description": "(See Bayesian) Bayesian state-space modeling using PyMC, integrating Kalman filtering within MCMC for parameter estimation.",
    "category": "State Space & Volatility Models",
    "docs_url": "https://www.pymc.io/projects/examples/en/latest/blog/tag/time-series.html",
    "github_url": "https://github.com/jessegrabowski/pymc_statespace",
    "url": "https://github.com/jessegrabowski/pymc_statespace",
    "install": "pip install pymc-statespace",
    "tags": [
      "volatility",
      "state space",
      "Bayesian"
    ],
    "best_for": "GARCH, stochastic volatility, Kalman filtering",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "bayesian",
      "state space",
      "time-series"
    ],
    "summary": "PyMC Statespace is a package for Bayesian state-space modeling using PyMC. It integrates Kalman filtering within MCMC for parameter estimation, making it useful for statisticians and data scientists working with time-series data.",
    "use_cases": [
      "Estimating parameters in time-series data",
      "Modeling volatility in financial data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for Bayesian state-space modeling",
      "how to use Kalman filtering in MCMC with PyMC",
      "Bayesian modeling in Python",
      "state space models in Python",
      "PyMC for time-series analysis",
      "volatility modeling with PyMC"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0006
  },
  {
    "name": "fairpy",
    "description": "Fair division algorithms from academic papers. Implements cake-cutting and item allocation procedures.",
    "category": "Game Theory & Mechanism Design",
    "docs_url": "https://fairpy.readthedocs.io/",
    "github_url": "https://github.com/erelsgl/fairpy",
    "url": "https://github.com/erelsgl/fairpy",
    "install": "pip install fairpy",
    "tags": [
      "fair division",
      "allocation",
      "mechanism design"
    ],
    "best_for": "Fair division and cake-cutting algorithms",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "game theory",
      "mechanism design"
    ],
    "summary": "Fairpy provides implementations of fair division algorithms based on academic research, focusing on cake-cutting and item allocation procedures. It is useful for researchers and practitioners in game theory and mechanism design.",
    "use_cases": [
      "Dividing resources fairly among multiple parties",
      "Allocating items to participants in a fair manner"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for fair division",
      "how to implement cake-cutting in python",
      "item allocation algorithms in python",
      "fair allocation procedures python",
      "mechanism design library python",
      "fairpy documentation"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active",
    "model_score": 0.0006
  },
  {
    "name": "PyMC-Marketing",
    "description": "Bayesian Marketing Mix Modeling and Customer Lifetime Value with PyMC, including GPU acceleration",
    "category": "Marketing Analytics",
    "docs_url": "https://www.pymc-marketing.io/",
    "github_url": "https://github.com/pymc-labs/pymc-marketing",
    "url": "https://www.pymc-marketing.io/",
    "install": "pip install pymc-marketing",
    "tags": [
      "MMM",
      "Bayesian",
      "CLV",
      "PyMC"
    ],
    "best_for": "Bayesian MMM with uncertainty quantification and GPU acceleration",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "bayesian",
      "marketing-mix-modeling",
      "customer-lifetime-value"
    ],
    "summary": "PyMC-Marketing is a package designed for Bayesian Marketing Mix Modeling and Customer Lifetime Value analysis using PyMC. It is suitable for marketers and data scientists looking to leverage Bayesian methods for marketing analytics.",
    "use_cases": [
      "Estimating the impact of marketing channels on sales",
      "Predicting customer lifetime value for targeted marketing campaigns"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for marketing mix modeling",
      "how to calculate customer lifetime value in python",
      "bayesian marketing analytics python",
      "PyMC for marketing analysis",
      "using PyMC for customer lifetime value",
      "marketing mix modeling with python",
      "bayesian CLV modeling in python"
    ],
    "primary_use_cases": [
      "marketing mix modeling",
      "customer lifetime value estimation"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "PyMC3",
      "statsmodels"
    ],
    "maintenance_status": "active",
    "model_score": 0.0006
  },
  {
    "name": "PyMC-Marketing",
    "description": "Bayesian marketing analytics toolkit from PyMC Labs. Combines Marketing Mix Modeling, CLV estimation, and discrete choice models with full uncertainty quantification and GPU acceleration.",
    "category": "MarTech & Customer Analytics",
    "docs_url": "https://www.pymc-marketing.io/",
    "github_url": "https://github.com/pymc-labs/pymc-marketing",
    "url": "https://www.pymc-marketing.io/",
    "install": "pip install pymc-marketing",
    "tags": [
      "CLV",
      "MMM",
      "Bayesian",
      "marketing-analytics"
    ],
    "best_for": "Production-grade Bayesian CLV and media mix modeling with uncertainty estimates",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "bayesian",
      "marketing-analytics"
    ],
    "summary": "PyMC-Marketing is a Bayesian marketing analytics toolkit that combines Marketing Mix Modeling, Customer Lifetime Value estimation, and discrete choice models. It is designed for marketers and data scientists looking to quantify uncertainty in their analyses.",
    "use_cases": [
      "Estimating customer lifetime value",
      "Conducting marketing mix modeling",
      "Analyzing discrete choice data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for marketing analytics",
      "how to perform marketing mix modeling in python",
      "bayesian analysis for customer lifetime value",
      "discrete choice models in python",
      "PyMC for marketing",
      "marketing analytics toolkit python"
    ],
    "primary_use_cases": [
      "Marketing Mix Modeling",
      "CLV estimation"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "lifetimes",
      "PyMC",
      "Robyn"
    ],
    "maintenance_status": "active",
    "model_score": 0.0006
  },
  {
    "name": "PyMC Marketing",
    "description": "Developed by PyMC Labs, focuses specifically on causal inference in quasi-experimental settings. Specializes in scenarios where randomization is impossible or expensive.",
    "category": "Marketing Mix Models (MMM) & Business Analytics",
    "docs_url": "https://www.pymc-marketing.io/",
    "github_url": "https://github.com/pymc-labs/pymc-marketing",
    "url": "https://github.com/pymc-labs/pymc-marketing",
    "install": "pip install pymc-marketing",
    "tags": [
      "causal inference",
      "matching",
      "marketing",
      "analytics",
      "Bayesian"
    ],
    "best_for": "Estimating treatment effects, propensity score matching, observational studies",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "bayesian",
      "analytics"
    ],
    "summary": "PyMC Marketing is a Python package developed by PyMC Labs that focuses on causal inference in quasi-experimental settings. It is particularly useful for scenarios where randomization is not feasible or is too costly.",
    "use_cases": [
      "Analyzing marketing campaign effectiveness",
      "Evaluating the impact of pricing changes"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to do marketing analytics in python",
      "causal inference package python",
      "Bayesian marketing mix modeling python",
      "matching techniques in python",
      "analytics tools for marketing in python"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "DoWhy",
      "EconML"
    ],
    "maintenance_status": "active",
    "model_score": 0.0006
  },
  {
    "name": "causaldata",
    "description": "Unified collection of datasets from three major causal inference textbooks: 'The Effect' (Huntington-Klein), 'Causal Inference: The Mixtape' (Cunningham), and 'Causal Inference: What If?' (Hern\u00e1n & Robins).",
    "category": "Datasets",
    "docs_url": "https://cran.r-project.org/web/packages/causaldata/causaldata.pdf",
    "github_url": "https://github.com/NickCH-K/causaldata",
    "url": "https://cran.r-project.org/package=causaldata",
    "install": "install.packages(\"causaldata\")",
    "tags": [
      "datasets",
      "causal-inference",
      "textbook",
      "The-Effect",
      "Mixtape"
    ],
    "best_for": "Datasets from The Effect, Causal Inference: The Mixtape, and What If? textbooks",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The causaldata package provides a unified collection of datasets from three major causal inference textbooks, making it easier for users to access and utilize these resources for causal analysis. It is primarily used by students and practitioners in the field of causal inference.",
    "use_cases": [],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R library for causal inference datasets",
      "how to use datasets from The Effect in R",
      "datasets for causal analysis in R",
      "R package for causal inference textbooks",
      "access causal inference datasets in R",
      "causaldata R package usage"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active",
    "model_score": 0.0006
  },
  {
    "name": "CausalLift",
    "description": "Uplift modeling for observational (non-RCT) data using inverse probability weighting.",
    "category": "Uplift Modeling",
    "docs_url": "https://causallift.readthedocs.io/",
    "github_url": "https://github.com/Minyus/causallift",
    "url": "https://github.com/Minyus/causallift",
    "install": "pip install causallift",
    "tags": [
      "uplift modeling",
      "observational data",
      "IPW"
    ],
    "best_for": "Uplift from observational data with IPW",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "uplift-modeling"
    ],
    "summary": "CausalLift is a Python package designed for uplift modeling using observational data through inverse probability weighting. It is primarily used by data scientists and researchers interested in causal inference and treatment effect estimation.",
    "use_cases": [
      "Estimating treatment effects in marketing campaigns",
      "Analyzing customer behavior in observational studies"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for uplift modeling",
      "how to perform causal inference in python",
      "observational data analysis in python",
      "inverse probability weighting python",
      "uplift modeling techniques",
      "python package for treatment effect estimation"
    ],
    "primary_use_cases": [
      "uplift modeling",
      "treatment effect estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0006
  },
  {
    "name": "Metran",
    "description": "Specialized package for estimating Dynamic Factor Models (DFM) using state-space methods and Kalman filtering.",
    "category": "State Space & Volatility Models",
    "docs_url": null,
    "github_url": "https://github.com/pastas/metran",
    "url": "https://github.com/pastas/metran",
    "install": "pip install metran",
    "tags": [
      "volatility",
      "state space"
    ],
    "best_for": "GARCH, stochastic volatility, Kalman filtering",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "time-series",
      "state-space",
      "volatility"
    ],
    "summary": "Metran is a specialized package for estimating Dynamic Factor Models (DFM) using state-space methods and Kalman filtering. It is primarily used by data scientists and researchers working with time-series data to analyze volatility and state-space models.",
    "use_cases": [
      "Estimating dynamic factors in economic data",
      "Analyzing volatility in financial time-series"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for estimating Dynamic Factor Models",
      "how to use Kalman filtering in python",
      "state-space models in python",
      "volatility modeling with python",
      "Dynamic Factor Models python package",
      "python library for time-series analysis"
    ],
    "primary_use_cases": [
      "dynamic factor estimation",
      "volatility analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0006
  },
  {
    "name": "pyregadj",
    "description": "Regression and ML adjustments to treatment effects in RCTs. Implements List et al. (2024) methods.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://github.com/vyasenov/pyregadj",
    "github_url": "https://github.com/vyasenov/pyregadj",
    "url": "https://github.com/vyasenov/pyregadj",
    "install": "pip install pyregadj",
    "tags": [
      "RCT",
      "regression adjustment",
      "treatment effects"
    ],
    "best_for": "Covariate adjustment in experiments",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "machine-learning"
    ],
    "summary": "pyregadj provides methods for regression and machine learning adjustments to treatment effects in randomized controlled trials (RCTs). It is useful for researchers and data scientists working in causal inference.",
    "use_cases": [
      "Adjusting treatment effects in clinical trials",
      "Analyzing data from randomized controlled trials"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for regression adjustment",
      "how to analyze treatment effects in RCTs with python",
      "machine learning adjustments for causal inference in python",
      "pyregadj documentation",
      "RCT analysis tools in python",
      "methods for treatment effects in python"
    ],
    "primary_use_cases": [
      "regression adjustment",
      "treatment effect estimation"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "List et al. (2024)",
    "maintenance_status": "active",
    "model_score": 0.0006
  },
  {
    "name": "torch-choice",
    "description": "PyTorch framework for flexible estimation of complex discrete choice models, leveraging GPU acceleration.",
    "category": "Discrete Choice Models",
    "docs_url": "https://gsbdbi.github.io/torch-choice/",
    "github_url": "https://github.com/gsbDBI/torch-choice",
    "url": "https://github.com/gsbDBI/torch-choice",
    "install": "pip install torch-choice",
    "tags": [
      "discrete choice",
      "logit"
    ],
    "best_for": "Logit/probit models, consumer choice, demand estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "torch-choice is a PyTorch framework designed for flexible estimation of complex discrete choice models, utilizing GPU acceleration. It is primarily used by data scientists and researchers working on discrete choice analysis.",
    "use_cases": [
      "Estimating consumer preferences in marketing",
      "Modeling transportation choices"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for discrete choice models",
      "how to estimate complex discrete choice models in python",
      "torch-choice usage examples",
      "PyTorch discrete choice modeling",
      "GPU acceleration for discrete choice models",
      "discrete choice analysis in python"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "PyTorch"
    ],
    "maintenance_status": "active",
    "model_score": 0.0005
  },
  {
    "name": "duckreg",
    "description": "Out-of-core regression (OLS/IV) for very large datasets using DuckDB aggregation. Handles data that doesn't fit in memory.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://github.com/py-econometrics/duckreg",
    "github_url": null,
    "url": "https://github.com/py-econometrics/duckreg",
    "install": "pip install duckreg",
    "tags": [
      "panel data",
      "fixed effects"
    ],
    "best_for": "Longitudinal analysis, controlling for unobserved heterogeneity",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "duckdb"
    ],
    "topic_tags": [
      "panel data",
      "fixed effects"
    ],
    "summary": "duckreg is an out-of-core regression package designed for very large datasets using DuckDB aggregation. It is particularly useful for users needing to handle data that exceeds memory capacity.",
    "use_cases": [
      "Analyzing large panel datasets",
      "Performing regression analysis on data that doesn't fit in memory"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for out-of-core regression",
      "how to perform OLS regression in python",
      "duckdb aggregation for large datasets",
      "panel data analysis in python",
      "fixed effects regression in python",
      "handling large datasets in python",
      "python regression for big data"
    ],
    "primary_use_cases": [
      "out-of-core regression",
      "large dataset analysis"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "DuckDB"
    ],
    "maintenance_status": "active",
    "model_score": 0.0005
  },
  {
    "name": "fixes",
    "description": "Streamlined event study workflows with simple run_es() and plot_es() functions built on fixest. New 2025 package providing convenient wrappers for common event study specifications.",
    "category": "Causal Inference (Event Study)",
    "docs_url": "https://cran.r-project.org/web/packages/fixes/fixes.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=fixes",
    "install": "install.packages(\"fixes\")",
    "tags": [
      "event-study",
      "fixest",
      "DiD",
      "streamlined",
      "visualization"
    ],
    "best_for": "Streamlined event study workflows with simple run_es() and plot_es() functions on fixest",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "event-study"
    ],
    "summary": "The 'fixes' package streamlines event study workflows by providing simple functions like run_es() and plot_es() built on the fixest framework. It is designed for users looking for convenient wrappers for common event study specifications.",
    "use_cases": [
      "Analyzing the impact of a policy change on stock prices",
      "Visualizing event study results for academic research"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "R package for event study",
      "how to conduct event studies in R",
      "fixest event study tutorial",
      "visualization of event studies in R",
      "streamlined event study workflows R",
      "event study analysis with fixes package"
    ],
    "api_complexity": "simple",
    "framework_compatibility": [
      "fixest"
    ],
    "maintenance_status": "active",
    "model_score": 0.0005
  },
  {
    "name": "rddapp",
    "description": "Supports multi-assignment RDD with two running variables, power analysis for RDD designs, and includes a Shiny interface for interactive analysis. Handles both sharp and fuzzy designs with bandwidth selection.",
    "category": "Causal Inference (RDD)",
    "docs_url": "https://cran.r-project.org/web/packages/rddapp/rddapp.pdf",
    "github_url": "https://github.com/felixthoemmes/rddapp",
    "url": "https://cran.r-project.org/package=rddapp",
    "install": "install.packages(\"rddapp\")",
    "tags": [
      "RDD",
      "multi-assignment",
      "power-analysis",
      "Shiny",
      "fuzzy-RDD"
    ],
    "best_for": "Multi-assignment RDD with two running variables and power analysis with Shiny interface",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The rddapp package supports multi-assignment regression discontinuity designs (RDD) with two running variables and provides power analysis for RDD designs. It includes a Shiny interface for interactive analysis, making it useful for researchers and practitioners in causal inference.",
    "use_cases": [
      "Analyzing the impact of policy changes using RDD",
      "Conducting power analysis for experimental designs"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for multi-assignment RDD",
      "how to perform power analysis for RDD in R",
      "interactive analysis for RDD using Shiny",
      "fuzzy RDD analysis in R",
      "bandwidth selection for RDD",
      "causal inference tools in R"
    ],
    "primary_use_cases": [
      "multi-assignment RDD analysis",
      "power analysis for RDD designs"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0005
  },
  {
    "name": "fairpyx",
    "description": "Course-seat allocation with capacity constraints. Practical fair division for university course assignment.",
    "category": "Game Theory & Mechanism Design",
    "docs_url": null,
    "github_url": "https://github.com/ariel-research/fairpyx",
    "url": "https://github.com/ariel-research/fairpyx",
    "install": "pip install fairpyx",
    "tags": [
      "fair division",
      "course allocation",
      "mechanism design"
    ],
    "best_for": "Course-seat allocation with constraints",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "fairpyx is a Python package designed for practical fair division in university course assignment, focusing on course-seat allocation with capacity constraints. It is useful for universities and educational institutions looking to optimize course assignments fairly.",
    "use_cases": [
      "Allocating seats in university courses fairly",
      "Optimizing course assignments based on student preferences"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for course allocation",
      "how to implement fair division in python",
      "course-seat allocation in python",
      "mechanism design for course assignment",
      "fairpyx documentation",
      "python fair division package",
      "university course assignment tools",
      "capacity constraints in course allocation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0005
  },
  {
    "name": "TFP CausalImpact",
    "description": "TensorFlow Probability port of Google's CausalImpact. Bayesian structural time-series for intervention effects.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": "https://github.com/google/tfp-causalimpact",
    "github_url": "https://github.com/google/tfp-causalimpact",
    "url": "https://github.com/google/tfp-causalimpact",
    "install": "pip install tfcausalimpact",
    "tags": [
      "causal impact",
      "time series",
      "Bayesian"
    ],
    "best_for": "TensorFlow-based causal impact analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "time-series",
      "bayesian"
    ],
    "summary": "TFP CausalImpact is a TensorFlow Probability port of Google's CausalImpact, designed for analyzing intervention effects using Bayesian structural time-series models. It is primarily used by data scientists and researchers interested in causal inference and time-series analysis.",
    "use_cases": [
      "Evaluating the impact of marketing campaigns",
      "Assessing policy changes on economic indicators"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for causal impact analysis",
      "how to analyze intervention effects in python",
      "time series analysis with Bayesian methods in python",
      "TFP CausalImpact tutorial",
      "using TensorFlow Probability for causal inference",
      "how to implement CausalImpact in python"
    ],
    "primary_use_cases": [
      "causal impact analysis",
      "intervention effect estimation"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "TensorFlow Probability"
    ],
    "related_packages": [
      "CausalImpact"
    ],
    "maintenance_status": "active",
    "model_score": 0.0005
  },
  {
    "name": "py-econometrics `gmm`",
    "description": "Lightweight package for setting up and estimating custom GMM models based on user-defined moment conditions.",
    "category": "Instrumental Variables",
    "docs_url": "https://github.com/py-econometrics/gmm",
    "github_url": null,
    "url": "https://github.com/py-econometrics/gmm",
    "install": "pip install gmm",
    "tags": [
      "IV",
      "GMM"
    ],
    "best_for": "Endogeneity correction, 2SLS, moment estimation",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "py-econometrics `gmm` is a lightweight package designed for setting up and estimating custom Generalized Method of Moments (GMM) models based on user-defined moment conditions. It is useful for researchers and practitioners in econometrics who need to implement GMM estimation tailored to their specific requirements.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for GMM estimation",
      "how to set up custom GMM models in python",
      "GMM models in econometrics python",
      "estimating moment conditions in python",
      "lightweight GMM package python",
      "using GMM for econometric analysis in python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0005
  },
  {
    "name": "Doubly-Debiased-Lasso",
    "description": "High-dimensional inference under hidden confounding. Doubly debiased Lasso for valid inference.",
    "category": "Double/Debiased Machine Learning (DML)",
    "docs_url": "https://github.com/zijguo/Doubly-Debiased-Lasso",
    "github_url": "https://github.com/zijguo/Doubly-Debiased-Lasso",
    "url": "https://github.com/zijguo/Doubly-Debiased-Lasso",
    "install": "Install from GitHub",
    "tags": [
      "high-dimensional",
      "Lasso",
      "debiased"
    ],
    "best_for": "High-dim inference with confounding",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "high-dimensional"
    ],
    "summary": "Doubly-Debiased-Lasso is a Python package designed for high-dimensional inference under hidden confounding. It is primarily used by data scientists and researchers who need valid inference methods in causal analysis.",
    "use_cases": [
      "Estimating causal effects in high-dimensional settings",
      "Conducting valid inference in the presence of confounding variables"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for high-dimensional inference",
      "how to use Lasso for causal inference in python",
      "Doubly Debiased Lasso tutorial",
      "valid inference methods in python",
      "high-dimensional data analysis python",
      "causal inference library python"
    ],
    "primary_use_cases": [
      "causal effect estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0005
  },
  {
    "name": "gEconpy",
    "description": "DSGE modeling tools inspired by R's gEcon. Automatic first-order condition derivation with Dynare export.",
    "category": "Structural Econometrics & Estimation",
    "docs_url": null,
    "github_url": "https://github.com/jessegrabowski/gEconpy",
    "url": "https://github.com/jessegrabowski/gEconpy",
    "install": "pip install gEconpy",
    "tags": [
      "structural",
      "DSGE",
      "estimation"
    ],
    "best_for": "Symbolic DSGE derivation with Dynare compatibility",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "numpy"
    ],
    "topic_tags": [
      "structural-econometrics",
      "DSGE",
      "estimation"
    ],
    "summary": "gEconpy provides tools for Dynamic Stochastic General Equilibrium (DSGE) modeling, allowing users to automatically derive first-order conditions and export them to Dynare. It is designed for economists and researchers working in structural econometrics.",
    "use_cases": [
      "Modeling economic scenarios using DSGE",
      "Exporting models to Dynare for analysis"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for DSGE modeling",
      "how to derive first-order conditions in python",
      "DSGE estimation tools in python",
      "gEconpy documentation",
      "python econometrics library",
      "how to use Dynare with python"
    ],
    "primary_use_cases": [
      "DSGE modeling",
      "first-order condition derivation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0005
  },
  {
    "name": "hypothetical",
    "description": "Library focused on hypothesis testing: ANOVA/MANOVA, t-tests, chi-square, Fisher's exact, nonparametric tests (Mann-Whitney, Kruskal-Wallis, etc.).",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": null,
    "github_url": "https://github.com/aschleg/hypothetical",
    "url": "https://github.com/aschleg/hypothetical",
    "install": "pip install hypothetical",
    "tags": [
      "inference",
      "hypothesis testing"
    ],
    "best_for": "Hypothesis tests, confidence intervals, multiple testing",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "statistical-inference",
      "hypothesis-testing"
    ],
    "summary": "The hypothetical package is a library designed for performing various hypothesis testing methods including ANOVA, t-tests, and chi-square tests. It is useful for statisticians and data scientists who need to conduct rigorous statistical analyses.",
    "use_cases": [
      "Conducting A/B tests",
      "Analyzing experimental data",
      "Comparing group means",
      "Testing independence in categorical data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for hypothesis testing",
      "how to perform ANOVA in python",
      "python library for statistical inference",
      "how to do t-tests in python",
      "python chi-square test example",
      "Fisher's exact test in python",
      "nonparametric tests in python"
    ],
    "primary_use_cases": [
      "A/B test analysis",
      "ANOVA testing"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "scipy",
      "statsmodels"
    ],
    "maintenance_status": "active",
    "model_score": 0.0005
  },
  {
    "name": "Prophet",
    "description": "Forecasting procedure for time series with strong seasonality and trend components, developed by Facebook.",
    "category": "Time Series Forecasting",
    "docs_url": "https://facebook.github.io/prophet/",
    "github_url": "https://github.com/facebook/prophet",
    "url": "https://github.com/facebook/prophet",
    "install": "pip install prophet",
    "tags": [
      "forecasting",
      "time series"
    ],
    "best_for": "Prediction, demand forecasting, trend analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "time-series"
    ],
    "summary": "Prophet is a forecasting procedure designed for time series data that exhibits strong seasonal effects and trends. It is widely used by data scientists and analysts for making predictions in various domains.",
    "use_cases": [
      "Forecasting sales for retail",
      "Predicting website traffic",
      "Estimating demand for products"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for time series forecasting",
      "how to forecast with Prophet in python",
      "time series analysis using Prophet",
      "forecasting seasonal data in python",
      "Prophet forecasting tutorial",
      "best practices for using Prophet"
    ],
    "primary_use_cases": [
      "sales forecasting",
      "traffic prediction"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "statsmodels",
      "ARIMA",
      "Facebook's Kats"
    ],
    "maintenance_status": "active",
    "model_score": 0.0005
  },
  {
    "name": "prophet",
    "description": "Automatic forecasting procedure based on an additive decomposable model with non-linear trends, yearly/weekly/daily seasonality, and holiday effects. Robust to missing data, trend shifts, and outliers; designed for business time series with strong seasonal patterns.",
    "category": "Time Series Forecasting",
    "docs_url": "https://facebook.github.io/prophet/",
    "github_url": "https://github.com/facebook/prophet",
    "url": "https://cran.r-project.org/package=prophet",
    "install": "install.packages(\"prophet\")",
    "tags": [
      "time-series",
      "Facebook",
      "decomposable-model",
      "seasonality",
      "holidays"
    ],
    "best_for": "Business time series forecasting with multiple seasonalities, holiday effects, and automated tunable forecasts, implementing Taylor & Letham (2018)",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "time-series",
      "seasonality"
    ],
    "summary": "Prophet is an automatic forecasting procedure that uses an additive decomposable model to handle non-linear trends and seasonal patterns. It is particularly useful for business time series data with strong seasonal effects and is robust to missing data and outliers.",
    "use_cases": [
      "Forecasting sales with seasonal patterns",
      "Predicting website traffic during holidays"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "R library for time series forecasting",
      "how to forecast with seasonality in R",
      "automatic forecasting in R",
      "business time series forecasting R package",
      "R package for handling missing data in time series",
      "forecasting with holidays in R"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "forecast",
      "tsibble"
    ],
    "maintenance_status": "active",
    "model_score": 0.0005
  },
  {
    "name": "Prophet",
    "description": "Facebook/Meta's time series forecasting with trend, seasonality, and holiday effects. Excellent for electricity load forecasting with automatic changepoint detection.",
    "category": "Energy & Utilities Economics",
    "docs_url": "https://facebook.github.io/prophet/",
    "github_url": "https://github.com/facebook/prophet",
    "url": "https://facebook.github.io/prophet/",
    "install": "pip install prophet",
    "tags": [
      "forecasting",
      "time series",
      "load forecasting"
    ],
    "best_for": "Load forecasting with seasonality and trend decomposition",
    "language": "Python/R",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "time-series"
    ],
    "summary": "Prophet is a forecasting tool developed by Facebook/Meta that allows users to make time series predictions with considerations for trend, seasonality, and holiday effects. It is particularly useful for applications like electricity load forecasting.",
    "use_cases": [
      "Electricity load forecasting",
      "Sales forecasting during holidays"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for time series forecasting",
      "how to forecast electricity load in python",
      "Prophet time series forecasting tutorial",
      "Facebook Prophet usage examples",
      "time series analysis with Prophet",
      "load forecasting using Prophet"
    ],
    "primary_use_cases": [
      "Time series forecasting"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "statsmodels",
      "scikit-forecast"
    ],
    "maintenance_status": "active",
    "model_score": 0.0005
  },
  {
    "name": "LocalProjections",
    "description": "Community implementations of Jord\u00e0 (2005) Local Projections for estimating impulse responses without VAR assumptions.",
    "category": "Time Series Econometrics",
    "docs_url": null,
    "github_url": "https://github.com/elenev/localprojections",
    "url": "https://github.com/elenev/localprojections",
    "install": "Install from source",
    "tags": [
      "time series",
      "econometrics"
    ],
    "best_for": "ARIMA, cointegration, VAR models",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "time series",
      "econometrics"
    ],
    "summary": "LocalProjections is a Python package that implements Jord\u00e0's (2005) Local Projections method for estimating impulse responses without relying on VAR assumptions. It is useful for economists and data scientists working with time series data.",
    "use_cases": [
      "Estimating impulse responses from economic shocks",
      "Analyzing the effects of policy changes on economic indicators"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for local projections",
      "how to estimate impulse responses in python",
      "local projections time series python",
      "community implementations of local projections",
      "time series econometrics python",
      "Jord\u00e0 local projections python"
    ],
    "primary_use_cases": [
      "estimating impulse responses"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Jord\u00e0 (2005)",
    "maintenance_status": "active",
    "model_score": 0.0005
  },
  {
    "name": "rddtools",
    "description": "Regression discontinuity design toolkit with clustered inference for geographic discontinuities. Provides bandwidth selection, specification tests, and visualization tools.",
    "category": "Causal Inference (RDD)",
    "docs_url": "https://cran.r-project.org/web/packages/rddtools/rddtools.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=rddtools",
    "install": "install.packages(\"rddtools\")",
    "tags": [
      "RDD",
      "clustered-inference",
      "bandwidth-selection",
      "geographic-discontinuity",
      "visualization"
    ],
    "best_for": "RDD with clustered inference for geographic discontinuities",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The rddtools package provides a toolkit for regression discontinuity design, focusing on clustered inference for geographic discontinuities. It is useful for researchers and practitioners who need to perform causal inference analyses with a focus on bandwidth selection and visualization.",
    "use_cases": [
      "Analyzing the impact of policy changes at geographic boundaries",
      "Evaluating educational interventions using RDD"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for regression discontinuity design",
      "how to visualize geographic discontinuities in R",
      "bandwidth selection in RDD",
      "clustered inference in R",
      "specification tests for RDD",
      "tools for causal inference in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0005
  },
  {
    "name": "nfl-data-py",
    "description": "Python package for accessing nflverse NFL play-by-play data with built-in EPA and win probability models",
    "category": "Sports Analytics",
    "docs_url": "https://nfl-data-py.readthedocs.io/",
    "github_url": "https://github.com/nflverse/nfl_data_py",
    "url": "https://github.com/nflverse/nfl_data_py",
    "install": "pip install nfl_data_py",
    "tags": [
      "football",
      "sports-analytics",
      "NFL",
      "EPA",
      "play-by-play"
    ],
    "best_for": "NFL analytics, expected points analysis, and fourth-down decision modeling",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "sports-analytics"
    ],
    "summary": "nfl-data-py is a Python package designed for accessing NFL play-by-play data from nflverse. It is useful for analysts and developers interested in sports analytics, particularly in evaluating game performance using EPA and win probability models.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for NFL data",
      "how to analyze NFL plays in python",
      "nflverse play-by-play data access",
      "EPA models for NFL in python",
      "win probability models in sports analytics",
      "football analytics python package"
    ],
    "use_cases": [
      "Analyzing NFL game performance",
      "Building predictive models for game outcomes"
    ],
    "primary_use_cases": [
      "EPA calculation for plays",
      "Win probability analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0005
  },
  {
    "name": "Differences",
    "description": "Implements modern difference-in-differences methods for staggered adoption designs (e.g., Callaway & Sant'Anna).",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": "https://bernardodionisi.github.io/differences/",
    "github_url": "https://github.com/bernardodionisi/differences",
    "url": "https://github.com/bernardodionisi/differences",
    "install": "pip install differences",
    "tags": [
      "DiD",
      "synthetic control",
      "RDD"
    ],
    "best_for": "Policy evaluation, natural experiments, quasi-experiments",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "program-evaluation"
    ],
    "summary": "The Differences package implements modern difference-in-differences methods specifically designed for staggered adoption designs, such as those proposed by Callaway & Sant'Anna. It is useful for researchers and practitioners in program evaluation who need to analyze the effects of interventions over time.",
    "use_cases": [
      "Evaluating the impact of a policy change over time",
      "Analyzing the effects of staggered treatment adoption across groups"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for difference-in-differences",
      "how to implement staggered adoption in python",
      "difference-in-differences methods in python",
      "Callaway & Sant'Anna implementation in python",
      "program evaluation methods in python",
      "synthetic control methods python"
    ],
    "primary_use_cases": [
      "difference-in-differences analysis",
      "program evaluation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0005
  },
  {
    "name": "OpenSpiel",
    "description": "DeepMind's 70+ game environments with multi-agent RL algorithms including Alpha-Rank, Neural Fictitious Self-Play, and CFR variants.",
    "category": "Game Theory & Mechanism Design",
    "docs_url": "https://openspiel.readthedocs.io/",
    "github_url": "https://github.com/deepmind/open_spiel",
    "url": "https://github.com/deepmind/open_spiel",
    "install": "pip install open_spiel",
    "tags": [
      "game theory",
      "reinforcement learning",
      "multi-agent"
    ],
    "best_for": "Multi-agent RL and game-theoretic algorithms",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "game theory",
      "reinforcement learning",
      "multi-agent"
    ],
    "summary": "OpenSpiel is a library developed by DeepMind that provides over 70 game environments for multi-agent reinforcement learning. It includes various algorithms such as Alpha-Rank, Neural Fictitious Self-Play, and CFR variants, making it suitable for researchers and practitioners in the field of game theory and AI.",
    "use_cases": [
      "Testing multi-agent reinforcement learning algorithms",
      "Simulating game environments for research",
      "Developing AI strategies for competitive games"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for game theory",
      "how to implement multi-agent reinforcement learning in python",
      "DeepMind OpenSpiel tutorial",
      "game environments for reinforcement learning in python",
      "reinforcement learning algorithms in OpenSpiel",
      "multi-agent game theory library python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0005
  },
  {
    "name": "matched_markets",
    "description": "Google's time-based regression with greedy search for optimal geo experiment groups.",
    "category": "Geo-Experiments & Lift Measurement",
    "docs_url": null,
    "github_url": "https://github.com/google/matched_markets",
    "url": "https://github.com/google/matched_markets",
    "install": "pip install matched-markets",
    "tags": [
      "geo-experiments",
      "market matching",
      "incrementality"
    ],
    "best_for": "Optimal geo experiment group selection",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "geo-experiments"
    ],
    "summary": "The matched_markets package provides a time-based regression method with a greedy search algorithm to create optimal groups for geo experiments. It is useful for data scientists and researchers looking to measure lift and incrementality in marketing experiments.",
    "use_cases": [
      "Optimizing geo experiment groups",
      "Measuring marketing lift",
      "Conducting A/B tests with geographic considerations"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for geo experiments",
      "how to measure incrementality in python",
      "optimal market matching in python",
      "time-based regression for experiments",
      "greedy search for geo experiments",
      "python package for lift measurement",
      "market matching algorithms in python",
      "geo experiment analysis in python"
    ],
    "primary_use_cases": [
      "geo experiment design",
      "incrementality measurement"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0005
  },
  {
    "name": "PyKalman",
    "description": "Implements Kalman filter, smoother, and EM algorithm for parameter estimation, including support for missing values and UKF.",
    "category": "State Space & Volatility Models",
    "docs_url": "https://pypi.org/project/pykalman/",
    "github_url": "https://github.com/pykalman/pykalman",
    "url": "https://github.com/pykalman/pykalman",
    "install": "pip install pykalman",
    "tags": [
      "volatility",
      "state space"
    ],
    "best_for": "GARCH, stochastic volatility, Kalman filtering",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "time-series"
    ],
    "summary": "PyKalman implements Kalman filters, smoothers, and the EM algorithm for parameter estimation. It is useful for those working with state space models and is applicable in various fields such as finance and engineering.",
    "use_cases": [
      "Estimating parameters in financial models",
      "Smoothing time series data with missing values"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for Kalman filter",
      "how to estimate parameters with Kalman filter in python",
      "Kalman smoother python",
      "EM algorithm for state space models python",
      "handling missing values in time series python",
      "UKF implementation in python"
    ],
    "primary_use_cases": [
      "parameter estimation",
      "time series forecasting"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "pydlm",
      "filterpy"
    ],
    "maintenance_status": "active",
    "model_score": 0.0005
  },
  {
    "name": "gegravity",
    "description": "General equilibrium structural gravity modeling for trade policy analysis. Only Python package for Anderson-van Wincoop GE gravity.",
    "category": "Structural Econometrics & Estimation",
    "docs_url": null,
    "github_url": "https://github.com/peter-herman/gegravity",
    "url": "https://pypi.org/project/gegravity/",
    "install": "pip install gegravity",
    "tags": [
      "trade",
      "gravity models",
      "structural"
    ],
    "best_for": "GE structural gravity for trade policy",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "structural",
      "trade"
    ],
    "summary": "Gegravity is a Python package designed for general equilibrium structural gravity modeling, specifically for trade policy analysis. It is the only package that implements the Anderson-van Wincoop GE gravity model, making it a valuable tool for researchers and practitioners in trade economics.",
    "use_cases": [
      "Analyzing the impact of trade policies on economic equilibrium",
      "Estimating trade flows between countries using structural models"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for gravity modeling",
      "how to analyze trade policy in python",
      "structural gravity models in python",
      "gegravity package usage",
      "trade analysis with python",
      "Anderson-van Wincoop model in python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0005
  },
  {
    "name": "pydoublelasso",
    "description": "Double\u2011post\u00a0Lasso estimator for high\u2011dimensional treatment effects (Belloni\u2011Chernozhukov\u2011Hansen\u202f2014).",
    "category": "Double/Debiased Machine Learning (DML)",
    "docs_url": "https://pypi.org/project/pydoublelasso/",
    "github_url": null,
    "url": "https://pypi.org/project/pydoublelasso/",
    "install": "pip install pydoublelasso",
    "tags": [
      "machine learning",
      "causal inference"
    ],
    "best_for": "High-dimensional controls, ML-based causal inference",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "machine-learning"
    ],
    "summary": "The pydoublelasso package provides a Double-post Lasso estimator for analyzing high-dimensional treatment effects, based on the methodology established by Belloni, Chernozhukov, and Hansen in 2014. It is primarily used by researchers and practitioners in causal inference and econometrics.",
    "use_cases": [
      "Estimating treatment effects in observational studies",
      "Analyzing high-dimensional data for causal inference"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for double post lasso",
      "how to estimate treatment effects in python",
      "pydoublelasso documentation",
      "double lasso estimator python",
      "causal inference python package",
      "high-dimensional treatment effects in python"
    ],
    "primary_use_cases": [
      "high-dimensional treatment effect estimation"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Belloni-Chernozhukov-Hansen (2014)",
    "maintenance_status": "active",
    "model_score": 0.0004
  },
  {
    "name": "causalToolbox",
    "description": "Implements meta-learner algorithms (S-learner, T-learner, X-learner) for heterogeneous treatment effect estimation using flexible base learners including honest Random Forests and BART for personalized CATE estimation.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://rdrr.io/github/soerenkuenzel/causalToolbox/",
    "github_url": "https://github.com/forestry-labs/causalToolbox",
    "url": "https://github.com/forestry-labs/causalToolbox",
    "install": "devtools::install_github(\"forestry-labs/causalToolbox\")",
    "tags": [
      "metalearners",
      "X-learner",
      "T-learner",
      "S-learner",
      "CATE"
    ],
    "best_for": "Comparing and benchmarking different CATE meta-learner strategies (S/T/X-learner) with BART or RF base learners, implementing K\u00fcnzel et al. (2019)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "causalToolbox implements meta-learner algorithms for estimating heterogeneous treatment effects using flexible base learners. It is useful for researchers and practitioners in causal inference looking to personalize treatment effect estimation.",
    "use_cases": [],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for causal inference",
      "how to estimate treatment effects in R",
      "R meta-learner algorithms",
      "personalized CATE estimation in R",
      "using Random Forests for causal inference",
      "BART for treatment effect estimation"
    ],
    "primary_use_cases": [
      "heterogeneous treatment effect estimation",
      "personalized CATE estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0004
  },
  {
    "name": "conflictcartographer",
    "description": "Python package for conflict event data visualization and geospatial analysis",
    "category": "Defense Research",
    "docs_url": "https://github.com/conflictcartographer/conflictcartographer",
    "github_url": "https://github.com/conflictcartographer/conflictcartographer",
    "url": "https://github.com/conflictcartographer/conflictcartographer",
    "install": "pip install conflictcartographer",
    "tags": [
      "conflict",
      "mapping",
      "ACLED",
      "visualization"
    ],
    "best_for": "Visualizing conflict events from ACLED and similar datasets",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "geospatial-analysis",
      "data-visualization"
    ],
    "summary": "conflictcartographer is a Python package designed for visualizing conflict event data and performing geospatial analysis. It is useful for researchers and analysts working in the field of defense and conflict studies.",
    "use_cases": [
      "Visualizing conflict events on a map",
      "Analyzing trends in conflict data over time"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for conflict data visualization",
      "how to analyze conflict events in python",
      "geospatial analysis of conflict data",
      "mapping conflict events with python",
      "visualization tools for ACLED data",
      "python package for defense research"
    ],
    "primary_use_cases": [
      "Conflict visualization",
      "Event mapping"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "geopandas",
      "folium"
    ],
    "maintenance_status": "active",
    "model_score": 0.0004
  },
  {
    "name": "NetworkCausalTree",
    "description": "Estimates both direct treatment effects and spillover effects under clustered network interference (Bargagli-Stoffi et al. 2025).",
    "category": "Causal Inference & Matching",
    "docs_url": null,
    "github_url": "https://github.com/fbargaglistoffi/NetworkCausalTree",
    "url": "https://github.com/fbargaglistoffi/NetworkCausalTree",
    "install": "pip install networkcausaltree",
    "tags": [
      "causal inference",
      "networks",
      "spillovers"
    ],
    "best_for": "Treatment effects with network interference",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "networks",
      "spillovers"
    ],
    "summary": "NetworkCausalTree estimates both direct treatment effects and spillover effects in the context of clustered network interference. It is useful for researchers and practitioners in causal inference who are dealing with network data.",
    "use_cases": [
      "Estimating treatment effects in social networks",
      "Analyzing spillover effects in public health interventions"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to estimate spillover effects in python",
      "network interference analysis in python",
      "direct treatment effects estimation python",
      "causal inference with networks",
      "spillover effects in clustered networks"
    ],
    "primary_use_cases": [
      "causal forest estimation"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Bargagli-Stoffi et al. (2025)",
    "maintenance_status": "active",
    "model_score": 0.0004
  },
  {
    "name": "CatBoost",
    "description": "Gradient boosting library excelling with categorical features (minimal preprocessing needed). Robust against overfitting.",
    "category": "Tree & Ensemble Methods for Prediction",
    "docs_url": "https://catboost.ai/docs/",
    "github_url": "https://github.com/catboost/catboost",
    "url": "https://github.com/catboost/catboost",
    "install": "pip install catboost",
    "tags": [
      "machine learning",
      "prediction"
    ],
    "best_for": "Random forests, gradient boosting, prediction tasks",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "CatBoost is a gradient boosting library that excels in handling categorical features with minimal preprocessing. It is robust against overfitting, making it suitable for various machine learning tasks.",
    "use_cases": [
      "Predicting customer churn",
      "Forecasting sales trends"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for gradient boosting",
      "how to use CatBoost in Python",
      "CatBoost for categorical features",
      "machine learning with CatBoost",
      "gradient boosting library Python",
      "CatBoost tutorial",
      "CatBoost vs XGBoost",
      "how to prevent overfitting in CatBoost"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "XGBoost",
      "LightGBM"
    ],
    "maintenance_status": "active",
    "model_score": 0.0004
  },
  {
    "name": "FixedEffectModelPyHDFE",
    "description": "Solves linear models with high-dimensional fixed effects, supporting robust variance calculation and IV.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://pypi.org/project/FixedEffectModelPyHDFE/",
    "github_url": null,
    "url": "https://pypi.org/project/FixedEffectModelPyHDFE/",
    "install": "pip install FixedEffectModelPyHDFE",
    "tags": [
      "panel data",
      "fixed effects"
    ],
    "best_for": "Longitudinal analysis, controlling for unobserved heterogeneity",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "panel data",
      "fixed effects"
    ],
    "summary": "FixedEffectModelPyHDFE solves linear models with high-dimensional fixed effects, allowing for robust variance calculation and instrumental variable support. It is primarily used by data scientists and researchers working with panel data.",
    "use_cases": [
      "Analyzing panel data with multiple fixed effects",
      "Estimating treatment effects in longitudinal studies"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for fixed effects models",
      "how to calculate robust variance in python",
      "linear models with high-dimensional fixed effects python",
      "IV regression in python",
      "panel data analysis in python",
      "fixed effects estimation python"
    ],
    "primary_use_cases": [
      "high-dimensional fixed effects estimation",
      "robust variance calculation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0004
  },
  {
    "name": "pyhtelasso",
    "description": "Debiased\u2011Lasso detector of heterogeneous treatment effects in randomized experiments.",
    "category": "Double/Debiased Machine Learning (DML)",
    "docs_url": "https://pypi.org/project/pyhtelasso/",
    "github_url": null,
    "url": "https://pypi.org/project/pyhtelasso/",
    "install": "pip install pyhtelasso",
    "tags": [
      "machine learning",
      "causal inference"
    ],
    "best_for": "High-dimensional controls, ML-based causal inference",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "pyhtelasso is a Python package designed for detecting heterogeneous treatment effects in randomized experiments using a debiased Lasso approach. It is useful for researchers and practitioners in the field of causal inference and machine learning.",
    "use_cases": [
      "Analyzing treatment effects in clinical trials",
      "Evaluating policy interventions in social sciences"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for debiased Lasso",
      "how to detect heterogeneous treatment effects in python",
      "causal inference tools in python",
      "randomized experiments analysis python",
      "machine learning for treatment effects",
      "debiased Lasso implementation python"
    ],
    "primary_use_cases": [
      "causal forest estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0004
  },
  {
    "name": "LightGBM",
    "description": "Fast, distributed gradient boosting (also supports RF). Known for speed, low memory usage, and handling large datasets.",
    "category": "Tree & Ensemble Methods for Prediction",
    "docs_url": "https://lightgbm.readthedocs.io/",
    "github_url": "https://github.com/microsoft/LightGBM",
    "url": "https://github.com/microsoft/LightGBM",
    "install": "pip install lightgbm",
    "tags": [
      "machine learning",
      "prediction"
    ],
    "best_for": "Random forests, gradient boosting, prediction tasks",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "LightGBM is a fast, distributed gradient boosting framework that is particularly known for its speed and low memory usage. It is widely used in machine learning for handling large datasets and making predictions.",
    "use_cases": [
      "Predicting outcomes in large datasets",
      "Optimizing model performance in machine learning competitions"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for gradient boosting",
      "how to use LightGBM in python",
      "LightGBM for large datasets",
      "machine learning with LightGBM",
      "LightGBM speed comparison",
      "LightGBM tutorial"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "XGBoost",
      "CatBoost"
    ],
    "maintenance_status": "active",
    "model_score": 0.0004
  },
  {
    "name": "LightGBM",
    "description": "Microsoft's fast gradient boosting with histogram-based algorithm, widely used in ad tech for CTR prediction",
    "category": "Machine Learning",
    "docs_url": "https://lightgbm.readthedocs.io/",
    "github_url": "https://github.com/microsoft/LightGBM",
    "url": "https://lightgbm.readthedocs.io/",
    "install": "pip install lightgbm",
    "tags": [
      "gradient boosting",
      "fast",
      "categorical",
      "Microsoft"
    ],
    "best_for": "Large-scale CTR prediction with native categorical feature support",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "LightGBM is a fast gradient boosting framework that uses a histogram-based algorithm, making it efficient for large datasets. It is widely utilized in ad tech for click-through rate (CTR) prediction.",
    "use_cases": [
      "Click-through rate prediction in advertising",
      "Predictive modeling for user engagement"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for gradient boosting",
      "how to use LightGBM for CTR prediction",
      "LightGBM tutorial",
      "fast gradient boosting in Python",
      "LightGBM vs XGBoost",
      "LightGBM installation guide"
    ],
    "primary_use_cases": [
      "CTR prediction"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "XGBoost",
      "CatBoost"
    ],
    "maintenance_status": "active",
    "model_score": 0.0004
  },
  {
    "name": "pygambit",
    "description": "N-player extensive form games with Alan Turing Institute support. Computes Nash, perfect, and sequential equilibria.",
    "category": "Game Theory & Mechanism Design",
    "docs_url": "https://gambitproject.readthedocs.io/",
    "github_url": "https://github.com/gambitproject/gambit",
    "url": "https://github.com/gambitproject/gambit",
    "install": "pip install pygambit",
    "tags": [
      "game theory",
      "extensive form",
      "equilibrium"
    ],
    "best_for": "N-player extensive form game solving",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "pygambit is a Python package designed for analyzing N-player extensive form games, providing tools to compute Nash, perfect, and sequential equilibria. It is particularly useful for researchers and practitioners in game theory and mechanism design.",
    "use_cases": [
      "Analyzing strategic interactions in economics",
      "Studying competitive behaviors in multi-agent systems"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for game theory",
      "how to compute Nash equilibrium in python",
      "extensive form games analysis python",
      "tools for game theory in python",
      "sequential equilibria computation python",
      "pygambit documentation",
      "N-player game analysis python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0004
  },
  {
    "name": "bacondecomp",
    "description": "Performs Goodman-Bacon decomposition showing how two-way fixed effects (TWFE) estimates are weighted averages of all possible 2\u00d72 DiD comparisons. Essential for diagnosing negative weights problems in staggered adoption designs.",
    "category": "Causal Inference (DiD)",
    "docs_url": "https://cran.r-project.org/web/packages/bacondecomp/bacondecomp.pdf",
    "github_url": "https://github.com/evanjflack/bacondecomp",
    "url": "https://cran.r-project.org/package=bacondecomp",
    "install": "install.packages(\"bacondecomp\")",
    "tags": [
      "DiD",
      "TWFE",
      "Goodman-Bacon",
      "decomposition",
      "staggered-adoption"
    ],
    "best_for": "Goodman-Bacon decomposition for diagnosing negative weights in TWFE staggered DiD designs",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "decomposition"
    ],
    "summary": "The bacondecomp package performs Goodman-Bacon decomposition, which helps in understanding how two-way fixed effects estimates are weighted averages of all possible 2\u00d72 DiD comparisons. It is essential for diagnosing negative weights problems in staggered adoption designs.",
    "use_cases": [
      "Analyzing staggered adoption designs",
      "Diagnosing negative weights in DiD estimates"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for Goodman-Bacon decomposition",
      "how to diagnose negative weights in staggered adoption",
      "two-way fixed effects in R",
      "R library for causal inference",
      "Goodman-Bacon decomposition in R",
      "staggered adoption analysis in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0004
  },
  {
    "name": "causal-curve",
    "description": "Continuous treatment dose-response curve estimation. GPS and TMLE methods for continuous treatments.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://causal-curve.readthedocs.io/",
    "github_url": "https://github.com/ronikobrosly/causal-curve",
    "url": "https://github.com/ronikobrosly/causal-curve",
    "install": "pip install causal-curve",
    "tags": [
      "dose-response",
      "continuous treatment",
      "GPS"
    ],
    "best_for": "Dose-response curves for continuous treatments",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "dose-response"
    ],
    "summary": "The causal-curve package provides methods for estimating continuous treatment dose-response curves using Generalized Propensity Score (GPS) and Targeted Maximum Likelihood Estimation (TMLE). It is useful for researchers and practitioners in causal inference who are working with continuous treatments.",
    "use_cases": [
      "Estimating treatment effects in clinical trials",
      "Analyzing the impact of dosage levels in drug studies"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for dose-response estimation",
      "how to estimate continuous treatment effects in python",
      "GPS methods in python",
      "TMLE methods for continuous treatments",
      "causal inference tools in python",
      "continuous treatment analysis python"
    ],
    "primary_use_cases": [
      "causal forest estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0004
  },
  {
    "name": "ortools",
    "description": "Google's operations research toolkit. Constraint programming, routing, linear/integer programming, and scheduling.",
    "category": "Optimization",
    "docs_url": "https://developers.google.com/optimization",
    "github_url": "https://github.com/google/or-tools",
    "url": "https://developers.google.com/optimization",
    "install": "pip install ortools",
    "tags": [
      "OR",
      "routing",
      "scheduling",
      "constraint programming"
    ],
    "best_for": "Production-ready combinatorial optimization",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "OR-Tools is Google's operations research toolkit designed for solving optimization problems. It is used by data scientists and operations researchers for tasks such as routing, scheduling, and constraint programming.",
    "use_cases": [
      "Optimizing delivery routes",
      "Scheduling tasks in a manufacturing process"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for optimization",
      "how to do routing in python",
      "python scheduling library",
      "constraint programming in python",
      "google operations research toolkit",
      "linear programming in python",
      "integer programming python library"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "PuLP",
      "SciPy"
    ],
    "maintenance_status": "active",
    "model_score": 0.0004
  },
  {
    "name": "testinterference",
    "description": "Statistical tests for SUTVA violations and spillover hypotheses. Detects network interference in experiments.",
    "category": "Interference & Spillovers",
    "docs_url": "https://github.com/tkhdyanagi/testinterference",
    "github_url": "https://github.com/tkhdyanagi/testinterference",
    "url": "https://github.com/tkhdyanagi/testinterference",
    "install": "pip install testinterference",
    "tags": [
      "SUTVA",
      "spillovers",
      "hypothesis testing"
    ],
    "best_for": "Testing for spillover effects",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "hypothesis-testing"
    ],
    "summary": "The testinterference package provides statistical tests to identify violations of the Stable Unit Treatment Value Assumption (SUTVA) and to analyze spillover effects in experimental designs. It is useful for researchers and practitioners in the field of causal inference who are investigating network interference in experiments.",
    "use_cases": [
      "Analyzing network interference in randomized controlled trials",
      "Testing for SUTVA violations in experimental data"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for SUTVA violations",
      "how to test for spillover effects in python",
      "statistical tests for interference in experiments",
      "detecting network interference in python",
      "hypothesis testing for spillovers",
      "python package for causal inference"
    ],
    "primary_use_cases": [
      "SUTVA violation detection",
      "spillover hypothesis testing"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0004
  },
  {
    "name": "trimmed_match",
    "description": "Google's robust analysis for paired geo experiments using trimmed statistics. Handles outliers in geo-level data.",
    "category": "Geo-Experiments & Lift Measurement",
    "docs_url": null,
    "github_url": "https://github.com/google/trimmed_match",
    "url": "https://github.com/google/trimmed_match",
    "install": "pip install trimmed-match",
    "tags": [
      "geo-experiments",
      "robust statistics",
      "incrementality"
    ],
    "best_for": "Robust paired geo experiment analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "geo-experiments",
      "robust statistics",
      "incrementality"
    ],
    "summary": "trimmed_match is a Python package designed for robust analysis of paired geo experiments using trimmed statistics. It is particularly useful for handling outliers in geo-level data, making it suitable for data scientists and researchers in the field of causal inference.",
    "use_cases": [
      "Analyzing the impact of marketing campaigns across different regions",
      "Evaluating the effectiveness of policy changes in urban areas"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for geo experiments",
      "how to analyze paired geo data in python",
      "robust statistics in geo-level analysis",
      "incrementality measurement in python",
      "handling outliers in geo experiments",
      "A/B testing with trimmed statistics in python"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0004
  },
  {
    "name": "CATENets",
    "description": "JAX-accelerated neural network CATE estimators implementing SNet, FlexTENet, TARNet, CFRNet, and DragonNet architectures.",
    "category": "Causal Inference & Matching",
    "docs_url": null,
    "github_url": "https://github.com/AliciaCurth/CATENets",
    "url": "https://github.com/AliciaCurth/CATENets",
    "install": "pip install catenets",
    "tags": [
      "causal inference",
      "deep learning",
      "JAX"
    ],
    "best_for": "GPU-accelerated neural CATE estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "deep-learning"
    ],
    "summary": "CATENets is a library for JAX-accelerated neural network CATE estimators that implements various architectures such as SNet, FlexTENet, TARNet, CFRNet, and DragonNet. It is designed for researchers and practitioners in causal inference seeking advanced deep learning solutions.",
    "use_cases": [],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to estimate CATE in python",
      "deep learning for causal inference",
      "JAX neural network estimators",
      "CATE estimators in python",
      "using JAX for deep learning",
      "neural networks for causal analysis",
      "implementing TARNet in python"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "JAX"
    ],
    "maintenance_status": "active",
    "model_score": 0.0004
  },
  {
    "name": "CMAverse",
    "description": "Unified interface for six causal mediation approaches including traditional regression, inverse odds weighting, and g-formula. Supports multiple sequential mediators and exposure-mediator interactions.",
    "category": "Causal Inference (Mediation)",
    "docs_url": "https://bs1125.github.io/CMAverse/",
    "github_url": null,
    "url": "https://cran.r-project.org/package=CMAverse",
    "install": "install.packages(\"CMAverse\")",
    "tags": [
      "mediation",
      "g-formula",
      "multiple-mediators",
      "causal-mechanisms",
      "unified-interface"
    ],
    "best_for": "Unified causal mediation analysis with six approaches and multiple sequential mediators",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "CMAverse provides a unified interface for various causal mediation approaches, allowing users to analyze complex relationships involving multiple mediators and exposure-mediator interactions. It is useful for researchers and practitioners in causal inference who need to implement mediation analysis.",
    "use_cases": [
      "Analyzing the effect of a treatment on an outcome through multiple mediators",
      "Evaluating the impact of exposure-mediator interactions in a study"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for causal mediation analysis",
      "how to use CMAverse for mediation",
      "CMAverse examples",
      "causal mediation approaches in R",
      "R unified interface for mediation",
      "multiple mediators analysis in R",
      "g-formula in R",
      "inverse odds weighting in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0004
  },
  {
    "name": "gurobipy",
    "description": "Python interface for Gurobi, the best-in-class commercial solver. LP, QP, MIP, and MIQP.",
    "category": "Optimization",
    "docs_url": "https://www.gurobi.com/documentation/",
    "github_url": null,
    "url": "https://www.gurobi.com/",
    "install": "pip install gurobipy",
    "tags": [
      "optimization",
      "solver",
      "MIP",
      "commercial"
    ],
    "best_for": "Best-in-class solver \u2014 free for academics",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Gurobipy is a Python interface for the Gurobi optimizer, which is a leading commercial solver for linear programming (LP), quadratic programming (QP), mixed-integer programming (MIP), and mixed-integer quadratic programming (MIQP). It is used by data scientists and operations researchers to solve complex optimization problems.",
    "use_cases": [
      "Optimizing supply chain logistics",
      "Resource allocation in project management"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for optimization",
      "how to solve MIP in python",
      "Gurobi python interface",
      "best commercial solver for LP",
      "using Gurobi for optimization",
      "Gurobi python examples"
    ],
    "api_complexity": "advanced",
    "related_packages": [
      "cvxpy",
      "Pyomo"
    ],
    "maintenance_status": "active",
    "model_score": 0.0004
  },
  {
    "name": "ADOpy",
    "description": "Bayesian Adaptive Design Optimization (ADO) for tuning experiments in real-time, with models for psychometric tasks.",
    "category": "Power Simulation & Design of Experiments",
    "docs_url": "https://adopy.readthedocs.io/en/latest/",
    "github_url": "https://github.com/adopy/adopy",
    "url": "https://github.com/adopy/adopy",
    "install": "pip install adopy",
    "tags": [
      "power analysis",
      "experiments",
      "Bayesian"
    ],
    "best_for": "Sample size calculation, experimental design, power analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "bayesian",
      "experiments"
    ],
    "summary": "ADOpy is a Python package designed for Bayesian Adaptive Design Optimization, enabling real-time tuning of experiments with models tailored for psychometric tasks. It is useful for researchers and practitioners involved in experimental design and analysis.",
    "use_cases": [
      "Optimizing experimental designs for psychometric tasks",
      "Real-time tuning of experiments based on Bayesian methods"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for Bayesian Adaptive Design Optimization",
      "how to tune experiments in real-time with Python",
      "Bayesian methods for experiments in Python",
      "psychometric task modeling in Python",
      "power analysis tools in Python",
      "experiments optimization library Python"
    ],
    "primary_use_cases": [
      "real-time experiment tuning",
      "Bayesian optimization for experiments"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0004
  },
  {
    "name": "rdd",
    "description": "Toolkit for sharp RDD analysis, including bandwidth calculation and estimation, integrating with pandas.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": null,
    "github_url": "https://github.com/evan-magnusson/rdd",
    "url": "https://github.com/evan-magnusson/rdd",
    "install": "pip install rdd",
    "tags": [
      "DiD",
      "synthetic control",
      "RDD"
    ],
    "best_for": "Policy evaluation, natural experiments, quasi-experiments",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "causal-inference",
      "program-evaluation"
    ],
    "summary": "The rdd package provides a toolkit for sharp Regression Discontinuity Design (RDD) analysis, including bandwidth calculation and estimation. It is useful for researchers and practitioners in program evaluation who need to analyze causal effects.",
    "use_cases": [
      "Analyzing the impact of policy changes at a threshold",
      "Estimating treatment effects in educational interventions"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for RDD analysis",
      "how to calculate bandwidth in RDD",
      "RDD estimation in python",
      "sharp RDD toolkit python",
      "program evaluation methods in python",
      "python pandas RDD"
    ],
    "primary_use_cases": [
      "bandwidth calculation",
      "RDD estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0004
  },
  {
    "name": "contextual",
    "description": "Multi-armed bandit algorithms including Thompson Sampling, UCB, and LinUCB. Directly applicable to adaptive A/B testing and recommendation optimization with simulation and evaluation tools.",
    "category": "Experimental Design",
    "docs_url": "https://nth-iteration-labs.github.io/contextual/",
    "github_url": "https://github.com/Nth-iteration-labs/contextual",
    "url": "https://cran.r-project.org/package=contextual",
    "install": "install.packages(\"contextual\")",
    "tags": [
      "bandits",
      "Thompson-sampling",
      "UCB",
      "adaptive-experiments",
      "A/B-testing"
    ],
    "best_for": "Multi-armed bandits for adaptive A/B testing with Thompson Sampling, UCB, LinUCB",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "bandits",
      "adaptive-experiments",
      "A/B-testing"
    ],
    "summary": "The 'contextual' package implements multi-armed bandit algorithms such as Thompson Sampling, UCB, and LinUCB. It is designed for adaptive A/B testing and recommendation optimization, providing simulation and evaluation tools for users.",
    "use_cases": [
      "Adaptive A/B testing",
      "Recommendation system optimization"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for multi-armed bandits",
      "how to implement Thompson Sampling in R",
      "adaptive A/B testing in R",
      "recommendation optimization with R",
      "UCB algorithm in R",
      "evaluate bandit algorithms in R"
    ],
    "primary_use_cases": [
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0004
  },
  {
    "name": "rdrobust",
    "description": "Comprehensive tools for Regression Discontinuity Designs (RDD), including optimal bandwidth selection, estimation, inference.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": "https://pypi.org/project/rdrobust/",
    "github_url": "https://github.com/rdpackages/rdrobust",
    "url": "https://github.com/rdpackages/rdrobust",
    "install": "pip install rdrobust",
    "tags": [
      "DiD",
      "synthetic control",
      "RDD"
    ],
    "best_for": "Policy evaluation, natural experiments, quasi-experiments",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "program-evaluation"
    ],
    "summary": "rdrobust provides comprehensive tools for conducting Regression Discontinuity Designs (RDD), including optimal bandwidth selection and estimation. It is used by researchers and practitioners in the field of program evaluation to analyze causal effects.",
    "use_cases": [
      "Evaluating the impact of a policy change using RDD",
      "Analyzing educational interventions with RDD"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for regression discontinuity",
      "how to perform RDD in python",
      "optimal bandwidth selection in python",
      "RDD estimation tools python",
      "program evaluation methods in python",
      "causal inference library python"
    ],
    "primary_use_cases": [
      "optimal bandwidth selection",
      "RDD estimation"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "rdd",
      "rdl"
    ],
    "maintenance_status": "active",
    "model_score": 0.0004
  },
  {
    "name": "didet",
    "description": "DiD with general treatment patterns. Handles effective treatment timing beyond simple staggered adoption.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": "https://github.com/tkhdyanagi/didet",
    "github_url": "https://github.com/tkhdyanagi/didet",
    "url": "https://github.com/tkhdyanagi/didet",
    "install": "pip install didet",
    "tags": [
      "DiD",
      "treatment timing",
      "causal inference"
    ],
    "best_for": "DiD with general treatment patterns",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "program-evaluation"
    ],
    "summary": "The didet package provides tools for Difference-in-Differences (DiD) analysis with a focus on general treatment patterns and effective treatment timing. It is useful for researchers and practitioners in economics and social sciences who are analyzing causal effects.",
    "use_cases": [
      "Evaluating the impact of policy changes over time",
      "Analyzing treatment effects in social experiments"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for DiD analysis",
      "how to analyze treatment timing in python",
      "causal inference tools in python",
      "difference-in-differences package python",
      "program evaluation methods in python",
      "python library for causal analysis"
    ],
    "primary_use_cases": [
      "causal inference analysis",
      "treatment timing evaluation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0004
  },
  {
    "name": "quantile-forest",
    "description": "Scikit-learn compatible implementation of Quantile Regression Forests for non-parametric estimation.",
    "category": "Quantile Regression & Distributional Methods",
    "docs_url": "https://zillow.github.io/quantile-forest/",
    "github_url": "https://github.com/zillow/quantile-forest",
    "url": "https://github.com/zillow/quantile-forest",
    "install": "pip install quantile-forest",
    "tags": [
      "quantile",
      "regression"
    ],
    "best_for": "Heterogeneous effects, distributional analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "Quantile Forest is a Scikit-learn compatible library that provides a non-parametric method for estimating quantiles using regression forests. It is useful for statisticians and data scientists who need to perform quantile regression analysis.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for quantile regression",
      "how to estimate quantiles in python",
      "scikit-learn compatible quantile forest",
      "quantile regression forests implementation",
      "non-parametric quantile estimation in python",
      "quantile forest tutorial"
    ],
    "primary_use_cases": [
      "quantile regression analysis"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "scikit-learn"
    ],
    "related_packages": [
      "quantregForest"
    ],
    "maintenance_status": "active",
    "model_score": 0.0004
  },
  {
    "name": "ddml",
    "description": "Streamlined double/debiased machine learning estimation with emphasis on (short-)stacking to combine multiple base learners, increasing robustness to unknown data generating processes. Designed as a complement to DoubleML with simpler syntax.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://thomaswiemann.com/ddml/",
    "github_url": "https://github.com/thomaswiemann/ddml",
    "url": "https://cran.r-project.org/package=ddml",
    "install": "install.packages(\"ddml\")",
    "tags": [
      "double-machine-learning",
      "stacking",
      "model-averaging",
      "treatment-effects",
      "causal-inference"
    ],
    "best_for": "Quick, robust DML estimation using short-stacking to ensemble multiple ML learners without extensive tuning, implementing Ahrens, Hansen, Schaffer & Wiemann (2024)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "ddml is a package designed for streamlined double/debiased machine learning estimation, focusing on stacking to enhance robustness against unknown data generating processes. It is particularly useful for researchers and practitioners in causal inference who seek a simpler syntax compared to DoubleML.",
    "use_cases": [
      "Estimating treatment effects in observational studies",
      "Combining multiple machine learning models for improved predictions"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for double machine learning",
      "how to perform causal inference in R",
      "stacking models in R",
      "treatment effects estimation in R",
      "double machine learning R tutorial",
      "robust machine learning methods R"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "treatment effect estimation"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "DoubleML"
    ],
    "maintenance_status": "active",
    "model_score": 0.0004
  },
  {
    "name": "data.table",
    "description": "Extension of data.frame providing fast aggregation of large data (100GB+), ordered joins, and memory-efficient operations. Uses reference semantics for in-place modification with concise syntax [:=, .SD, by=].",
    "category": "Data Workflow",
    "docs_url": "https://rdatatable.gitlab.io/data.table/",
    "github_url": "https://github.com/Rdatatable/data.table",
    "url": "https://cran.r-project.org/package=data.table",
    "install": "install.packages(\"data.table\")",
    "tags": [
      "data-manipulation",
      "fast",
      "large-data",
      "reference-semantics",
      "aggregation"
    ],
    "best_for": "Fast operations on large datasets (100GB+) with memory-efficient reference semantics",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "data.table is an extension of data.frame in R that provides fast aggregation of large datasets, ordered joins, and memory-efficient operations. It is commonly used by data scientists and statisticians working with large data sets.",
    "use_cases": [
      "Aggregating large datasets for analysis",
      "Performing ordered joins on large data tables"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for fast data manipulation",
      "how to aggregate large data in R",
      "R package for memory-efficient data operations",
      "data.table vs data.frame in R",
      "efficient joins in R",
      "R data manipulation tools"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "dplyr",
      "data.frame"
    ],
    "maintenance_status": "active",
    "model_score": 0.0004
  },
  {
    "name": "cplm",
    "description": "Compound Poisson linear models for insurance claims with exact zero mass - handles the mixed discrete-continuous nature of claims data",
    "category": "Insurance & Actuarial",
    "docs_url": "https://cran.r-project.org/web/packages/cplm/cplm.pdf",
    "github_url": "https://github.com/actuarialvoodoo/cplm",
    "url": "https://cran.r-project.org/package=cplm",
    "install": "install.packages(\"cplm\")",
    "tags": [
      "Tweedie",
      "compound-Poisson",
      "claims-modeling",
      "zero-inflation",
      "GLM"
    ],
    "best_for": "Insurance claims modeling with Tweedie distributions handling zero claims",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The cplm package provides tools for modeling insurance claims data using Compound Poisson linear models. It is particularly useful for actuaries and data scientists working with mixed discrete-continuous claims data.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for Compound Poisson models",
      "how to model zero-inflated claims data in R",
      "insurance claims modeling with R",
      "Tweedie distribution in R",
      "R package for GLM with zero mass",
      "analyzing claims data in R"
    ],
    "use_cases": [
      "Modeling insurance claims with zero-inflation",
      "Analyzing mixed discrete-continuous data"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0004
  },
  {
    "name": "Pingouin",
    "description": "User-friendly interface for common statistical tests (ANOVA, ANCOVA, t-tests, correlations, chi\u00b2, reliability) built on pandas & scipy.",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": "https://pingouin-stats.org/",
    "github_url": "https://github.com/raphaelvallat/pingouin",
    "url": "https://github.com/raphaelvallat/pingouin",
    "install": "pip install pingouin",
    "tags": [
      "inference",
      "hypothesis testing"
    ],
    "best_for": "Hypothesis tests, confidence intervals, multiple testing",
    "language": "Python",
    "difficulty": "beginner|intermediate",
    "prerequisites": [
      "python-pandas",
      "scipy"
    ],
    "topic_tags": [],
    "summary": "Pingouin provides a user-friendly interface for conducting common statistical tests such as ANOVA, t-tests, and correlations. It is designed for users who need to perform statistical analysis without deep expertise in the underlying libraries.",
    "use_cases": [
      "Conducting ANOVA tests",
      "Performing t-tests for comparing means"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for statistical tests",
      "how to perform ANOVA in python",
      "user-friendly statistical analysis python",
      "Pingouin library usage",
      "statistical inference in python",
      "python hypothesis testing library"
    ],
    "primary_use_cases": [
      "ANOVA analysis",
      "t-test analysis"
    ],
    "api_complexity": "simple",
    "related_packages": [
      "scipy",
      "statsmodels"
    ],
    "maintenance_status": "active",
    "model_score": 0.0004
  },
  {
    "name": "maketables",
    "description": "Publication-ready regression tables for pyfixest, statsmodels, linearmodels. Outputs HTML (great-tables), LaTeX, Word.",
    "category": "Inference & Reporting Tools",
    "docs_url": null,
    "github_url": "https://github.com/py-econometrics/maketables",
    "url": "https://github.com/py-econometrics/maketables",
    "install": "pip install maketables",
    "tags": [
      "reporting",
      "tables",
      "visualization"
    ],
    "best_for": "Multi-format regression tables from pyfixest/statsmodels",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "maketables is a Python package that generates publication-ready regression tables for various statistical modeling libraries such as pyfixest, statsmodels, and linearmodels. It is useful for researchers and data scientists who need to present their regression results in a clear and professional format.",
    "use_cases": [
      "Generating regression tables for academic publications",
      "Creating HTML reports for data analysis"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for regression tables",
      "how to create publication-ready tables in python",
      "maketables package documentation",
      "best python libraries for reporting",
      "generate LaTeX tables in python",
      "visualization tools for regression results"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active",
    "model_score": 0.0004
  },
  {
    "name": "Biogeme",
    "description": "Maximum likelihood estimation of parametric models, with strong support for complex discrete choice models.",
    "category": "Discrete Choice Models",
    "docs_url": "https://biogeme.epfl.ch/index.html",
    "github_url": "https://github.com/michelbierlaire/biogeme",
    "url": "https://github.com/michelbierlaire/biogeme",
    "install": "pip install biogeme",
    "tags": [
      "discrete choice",
      "logit"
    ],
    "best_for": "Logit/probit models, consumer choice, demand estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "Biogeme is a Python library designed for maximum likelihood estimation of parametric models, particularly excelling in complex discrete choice models. It is primarily used by researchers and practitioners in fields such as transportation, marketing, and economics.",
    "use_cases": [
      "Estimating consumer choice behavior",
      "Analyzing transportation mode choice"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for discrete choice models",
      "how to estimate parametric models in python",
      "maximum likelihood estimation in python",
      "discrete choice modeling with python",
      "Biogeme installation guide",
      "examples of using Biogeme"
    ],
    "primary_use_cases": [
      "maximum likelihood estimation",
      "discrete choice modeling"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "choicepy",
      "pylogit"
    ],
    "maintenance_status": "active",
    "model_score": 0.0003
  },
  {
    "name": "Biogeme",
    "description": "The reference Python package for discrete choice model estimation (logit, nested logit, mixed logit). Developed by Michel Bierlaire at EPFL, widely used in transportation research.",
    "category": "Transportation Economics & Technology",
    "docs_url": "https://biogeme.epfl.ch/",
    "github_url": "https://github.com/michelbierlaire/biogeme",
    "url": "https://biogeme.epfl.ch/",
    "install": "pip install biogeme",
    "tags": [
      "discrete choice",
      "logit",
      "transportation",
      "mode choice"
    ],
    "best_for": "Discrete choice modeling, mode choice analysis, stated preference experiments",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "Biogeme is a Python package designed for estimating discrete choice models, including logit, nested logit, and mixed logit. It is widely used in transportation research, particularly for analyzing mode choice.",
    "use_cases": [
      "Estimating mode choice preferences",
      "Analyzing transportation survey data"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for discrete choice modeling",
      "how to estimate logit models in python",
      "Biogeme installation guide",
      "examples of mixed logit in python",
      "transportation research tools in python",
      "nested logit model python package"
    ],
    "primary_use_cases": [
      "Discrete choice modeling"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "xlogit",
      "PyLogit",
      "Apollo"
    ],
    "maintenance_status": "active",
    "model_score": 0.0003
  },
  {
    "name": "openTSNE",
    "description": "Optimized, parallel implementation of t-distributed Stochastic Neighbor Embedding (t-SNE) for large datasets.",
    "category": "Dimensionality Reduction",
    "docs_url": "https://opentsne.readthedocs.io/en/stable/",
    "github_url": "https://github.com/pavlin-policar/openTSNE",
    "url": "https://github.com/pavlin-policar/openTSNE",
    "install": "pip install opentsne",
    "tags": [
      "machine learning",
      "dimensionality"
    ],
    "best_for": "Feature extraction, PCA, high-dimensional data",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "openTSNE is an optimized and parallel implementation of t-distributed Stochastic Neighbor Embedding (t-SNE) designed for handling large datasets. It is commonly used by data scientists and researchers for visualizing high-dimensional data in a lower-dimensional space.",
    "use_cases": [
      "Visualizing clusters in high-dimensional data",
      "Reducing dimensionality for machine learning preprocessing"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for dimensionality reduction",
      "how to use t-SNE in python",
      "visualizing high-dimensional data in python",
      "openTSNE tutorial",
      "parallel t-SNE implementation python",
      "large dataset t-SNE python"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "scikit-learn",
      "umap-learn"
    ],
    "maintenance_status": "active",
    "model_score": 0.0003
  },
  {
    "name": "aipyw",
    "description": "Minimal, fast AIPW (Augmented Inverse Probability Weighting) implementation for discrete treatments. Sklearn-compatible with cross-fitting.",
    "category": "Causal Inference & Matching",
    "docs_url": null,
    "github_url": "https://github.com/apoorvalal/aipyw",
    "url": "https://github.com/apoorvalal/aipyw",
    "install": "pip install aipyw",
    "tags": [
      "causal inference",
      "AIPW",
      "treatment effects"
    ],
    "best_for": "Fast AIPW estimation with sklearn models",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "Aipyw is a minimal and fast implementation of Augmented Inverse Probability Weighting (AIPW) for discrete treatments. It is designed to be compatible with Scikit-learn and supports cross-fitting, making it suitable for users interested in causal inference.",
    "use_cases": [
      "Estimating treatment effects in clinical trials",
      "Analyzing A/B test results"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for AIPW",
      "how to do causal inference in python",
      "AIPW implementation in python",
      "fast AIPW for discrete treatments",
      "cross-fitting in causal inference python",
      "scikit-learn compatible AIPW"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "scikit-learn"
    ],
    "related_packages": [
      "causalml",
      "econml"
    ],
    "maintenance_status": "active",
    "model_score": 0.0003
  },
  {
    "name": "pynare",
    "description": "Python wrapper/interface to Dynare for DSGE model solving. Bridge between Python workflows and Dynare.",
    "category": "Structural Econometrics & Estimation",
    "docs_url": null,
    "github_url": "https://github.com/gboehl/pynare",
    "url": "https://github.com/gboehl/pynare",
    "install": "pip install pynare",
    "tags": [
      "structural",
      "DSGE",
      "Dynare"
    ],
    "best_for": "Running Dynare models from Python",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "structural",
      "DSGE"
    ],
    "summary": "Pynare is a Python wrapper that facilitates the use of Dynare for solving Dynamic Stochastic General Equilibrium (DSGE) models. It serves as a bridge between Python workflows and the Dynare software, enabling users to leverage the capabilities of both tools.",
    "use_cases": [
      "Solving DSGE models",
      "Integrating Dynare with Python workflows"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for DSGE modeling",
      "how to use Dynare with Python",
      "pynare installation guide",
      "DSGE model solving in Python",
      "pynare examples",
      "Python wrapper for Dynare"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0003
  },
  {
    "name": "momentfit",
    "description": "Modern S4-based implementation of Generalized Method of Moments supporting systems of equations, nonlinear moment conditions, and hypothesis testing. Successor to gmm package with object-oriented design.",
    "category": "Instrumental Variables",
    "docs_url": "https://cran.r-project.org/web/packages/momentfit/momentfit.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=momentfit",
    "install": "install.packages(\"momentfit\")",
    "tags": [
      "GMM",
      "S4-class",
      "systems-estimation",
      "moment-conditions",
      "hypothesis-testing"
    ],
    "best_for": "Modern object-oriented GMM estimation for systems of equations",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "instrumental-variables",
      "hypothesis-testing"
    ],
    "summary": "Momentfit is a modern S4-based implementation of the Generalized Method of Moments (GMM) that supports systems of equations and nonlinear moment conditions. It is designed for users who need to perform hypothesis testing in econometric models.",
    "use_cases": [
      "Estimating parameters in econometric models",
      "Testing hypotheses in economic research"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for Generalized Method of Moments",
      "how to perform hypothesis testing in R",
      "R GMM implementation",
      "S4 class in R for econometrics",
      "nonlinear moment conditions in R",
      "systems of equations in R"
    ],
    "primary_use_cases": [
      "parameter estimation in econometrics",
      "hypothesis testing in econometric models"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "gmm"
    ],
    "maintenance_status": "active",
    "model_score": 0.0003
  },
  {
    "name": "tidysynth",
    "description": "Brings synthetic control method into the tidyverse with cleaner syntax and built-in placebo inference. Provides pipe-friendly workflows for SCM estimation and visualization.",
    "category": "Causal Inference (Synthetic Control)",
    "docs_url": "https://cran.r-project.org/web/packages/tidysynth/tidysynth.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=tidysynth",
    "install": "install.packages(\"tidysynth\")",
    "tags": [
      "synthetic-control",
      "tidyverse",
      "placebo-inference",
      "causal-inference",
      "policy-evaluation"
    ],
    "best_for": "Tidyverse-friendly synthetic control method with clean syntax and built-in placebo inference",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "policy-evaluation"
    ],
    "summary": "tidysynth brings the synthetic control method into the tidyverse, providing a cleaner syntax and built-in placebo inference. It offers pipe-friendly workflows for estimating and visualizing synthetic control models, making it accessible for users in the data science community.",
    "use_cases": [
      "Evaluating the impact of a policy intervention",
      "Comparing treatment and control groups in observational studies"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for synthetic control",
      "how to use synthetic control in R",
      "tidyverse package for causal inference",
      "visualizing synthetic control results in R",
      "placebo inference in R",
      "synthetic control method tidyverse"
    ],
    "primary_use_cases": [
      "synthetic control estimation",
      "visualization of causal effects"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "tidyverse"
    ],
    "maintenance_status": "active",
    "model_score": 0.0003
  },
  {
    "name": "pmdarima",
    "description": "ARIMA modeling with automatic parameter selection (auto-ARIMA), similar to R's `forecast::auto.arima`.",
    "category": "Time Series Forecasting",
    "docs_url": "https://alkaline-ml.com/pmdarima/",
    "github_url": "https://github.com/alkaline-ml/pmdarima",
    "url": "https://github.com/alkaline-ml/pmdarima",
    "install": "pip install pmdarima",
    "tags": [
      "forecasting",
      "time series"
    ],
    "best_for": "Prediction, demand forecasting, trend analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "time-series"
    ],
    "summary": "pmdarima is a Python package for ARIMA modeling with automatic parameter selection, similar to R's `forecast::auto.arima`. It is used by data scientists and statisticians for time series forecasting tasks.",
    "use_cases": [
      "Forecasting sales data",
      "Predicting stock prices"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for ARIMA modeling",
      "how to do time series forecasting in python",
      "auto-ARIMA package for python",
      "forecasting with pmdarima",
      "time series analysis in python",
      "best practices for ARIMA in python"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "statsmodels"
    ],
    "maintenance_status": "active",
    "model_score": 0.0003
  },
  {
    "name": "CausalNLP",
    "description": "Causal inference for text data. Estimate treatment effects from unstructured text using NLP.",
    "category": "Natural Language Processing for Economics",
    "docs_url": null,
    "github_url": "https://github.com/amaiya/causalnlp",
    "url": "https://github.com/amaiya/causalnlp",
    "install": "pip install causalnlp",
    "tags": [
      "NLP",
      "causal inference",
      "text"
    ],
    "best_for": "Causal effects from text data",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "natural-language-processing"
    ],
    "summary": "CausalNLP is a package designed for causal inference in text data, enabling users to estimate treatment effects from unstructured text using natural language processing techniques. It is particularly useful for researchers and practitioners in economics who work with text data.",
    "use_cases": [
      "Analyzing the impact of a marketing campaign based on customer reviews",
      "Estimating the effect of policy changes on public sentiment from social media posts"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference in text",
      "how to estimate treatment effects from text in python",
      "NLP for causal analysis in python",
      "causal inference tools for text data",
      "using CausalNLP for text analysis",
      "text data treatment effects estimation python"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "causalml",
      "DoWhy"
    ],
    "maintenance_status": "active",
    "model_score": 0.0003
  },
  {
    "name": "JAX",
    "description": "High-performance numerical computing with autograd and XLA compilation on CPU/GPU/TPU.",
    "category": "Numerical Optimization & Computational Tools",
    "docs_url": "https://jax.readthedocs.io/",
    "github_url": "https://github.com/google/jax",
    "url": "https://github.com/google/jax",
    "install": "pip install jax",
    "tags": [
      "optimization",
      "computation"
    ],
    "best_for": "Solving optimization problems, numerical methods",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "JAX is a high-performance numerical computing library that supports automatic differentiation and XLA compilation for efficient execution on CPU, GPU, and TPU. It is used by researchers and developers in machine learning and scientific computing to optimize and accelerate their computations.",
    "use_cases": [],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for numerical computing",
      "how to perform autograd in python",
      "JAX optimization techniques",
      "XLA compilation in JAX",
      "high-performance computing with JAX",
      "JAX for machine learning"
    ],
    "api_complexity": "advanced",
    "related_packages": [
      "NumPy",
      "TensorFlow",
      "PyTorch"
    ],
    "maintenance_status": "active",
    "model_score": 0.0003
  },
  {
    "name": "pyrifreg",
    "description": "Recentered Influence\u2011Function (RIF) regression for unconditional quantile & distributional effects (Firpo\u202fet\u202fal.,\u202f2008).",
    "category": "Quantile Regression & Distributional Methods",
    "docs_url": "https://github.com/vyasenov/pyrifreg",
    "github_url": null,
    "url": "https://github.com/vyasenov/pyrifreg",
    "install": "pip install pyrifreg",
    "tags": [
      "quantile",
      "regression"
    ],
    "best_for": "Heterogeneous effects, distributional analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "quantile-regression",
      "distributional-methods"
    ],
    "summary": "pyrifreg is a Python package that implements Recentered Influence-Function (RIF) regression for analyzing unconditional quantile and distributional effects. It is primarily used by researchers and practitioners interested in causal inference and econometrics.",
    "use_cases": [
      "Analyzing the impact of policy changes on income distribution",
      "Estimating the effects of treatment on different quantiles of the outcome variable"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for RIF regression",
      "how to perform quantile regression in python",
      "RIF regression for distributional effects",
      "quantile regression analysis python",
      "unconditional quantile effects python",
      "distributional methods in python"
    ],
    "primary_use_cases": [
      "unconditional quantile analysis",
      "distributional effect estimation"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Firpo et al. (2008)",
    "maintenance_status": "active",
    "model_score": 0.0003
  },
  {
    "name": "econpizza",
    "description": "Solve nonlinear heterogeneous agent models (HANK) with perfect foresight. Efficient perturbation and projection methods.",
    "category": "Structural Econometrics & Estimation",
    "docs_url": "https://econpizza.readthedocs.io/",
    "github_url": "https://github.com/gboehl/econpizza",
    "url": "https://github.com/gboehl/econpizza",
    "install": "pip install econpizza",
    "tags": [
      "structural",
      "DSGE",
      "HANK"
    ],
    "best_for": "Nonlinear HANK models with aggregate shocks",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "structural",
      "DSGE",
      "HANK"
    ],
    "summary": "Econpizza is a Python package designed to solve nonlinear heterogeneous agent models (HANK) using efficient perturbation and projection methods. It is primarily used by researchers and practitioners in structural econometrics.",
    "use_cases": [
      "Solving complex economic models",
      "Conducting simulations for policy analysis"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for solving HANK models",
      "how to use perturbation methods in python",
      "efficient projection methods in python",
      "nonlinear heterogeneous agent models python",
      "structural econometrics tools in python",
      "DSGE modeling in python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0003
  },
  {
    "name": "tidytext",
    "description": "Tidy data principles for text mining. Converts text to tidy format (one-token-per-row), enabling analysis with dplyr, ggplot2, and other tidyverse tools. Accompanies the book 'Text Mining with R'.",
    "category": "Text Analysis",
    "docs_url": "https://juliasilge.github.io/tidytext/",
    "github_url": "https://github.com/juliasilge/tidytext",
    "url": "https://cran.r-project.org/package=tidytext",
    "install": "install.packages(\"tidytext\")",
    "tags": [
      "text-mining",
      "tidyverse",
      "tokenization",
      "sentiment-analysis",
      "NLP"
    ],
    "best_for": "Tidy text mining with dplyr and ggplot2 integration\u2014accompanies 'Text Mining with R'",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "text-mining",
      "NLP"
    ],
    "summary": "The tidytext package applies tidy data principles to text mining by converting text into a tidy format, which allows for easier analysis using tools from the tidyverse such as dplyr and ggplot2. It is particularly useful for data scientists and researchers working with text data.",
    "use_cases": [
      "Analyzing sentiment in customer reviews",
      "Tokenizing text for further analysis"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "R package for text mining",
      "how to tokenize text in R",
      "tidy data principles for text analysis",
      "text analysis with tidyverse in R",
      "R library for sentiment analysis",
      "how to use tidytext for NLP"
    ],
    "api_complexity": "simple",
    "framework_compatibility": [
      "tidyverse"
    ],
    "related_packages": [
      "tm",
      "textclean"
    ],
    "maintenance_status": "active",
    "model_score": 0.0003
  },
  {
    "name": "DoEgen",
    "description": "Automates generation and optimization of designs, especially for mixed factor-level experiments; computes efficiency metrics.",
    "category": "Power Simulation & Design of Experiments",
    "docs_url": null,
    "github_url": "https://github.com/sebhaan/DoEgen",
    "url": "https://github.com/sebhaan/DoEgen",
    "install": "pip install DoEgen",
    "tags": [
      "power analysis",
      "experiments"
    ],
    "best_for": "Sample size calculation, experimental design, power analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "DoEgen automates the generation and optimization of designs for mixed factor-level experiments and computes efficiency metrics. It is useful for researchers and practitioners involved in experimental design and analysis.",
    "use_cases": [
      "Optimizing experimental designs for agricultural studies",
      "Generating designs for clinical trials"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for design of experiments",
      "how to optimize mixed factor-level experiments in python",
      "automate design generation in python",
      "efficiency metrics for experiments in python",
      "python power analysis tools",
      "experiments optimization python library"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0003
  },
  {
    "name": "pyleebounds",
    "description": "Lee\u00a0(2009) sample\u2011selection bounds for treatment effects; trims treated distribution to match selection rates.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": "https://pypi.org/project/pyleebounds/",
    "github_url": null,
    "url": "https://pypi.org/project/pyleebounds/",
    "install": "pip install pyleebounds",
    "tags": [
      "DiD",
      "synthetic control",
      "RDD",
      "causal inference"
    ],
    "best_for": "Policy evaluation, natural experiments, quasi-experiments",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The pyleebounds package implements sample-selection bounds for treatment effects as described by Lee (2009). It is used by researchers and practitioners in program evaluation to adjust treated distributions to match selection rates.",
    "use_cases": [
      "Evaluating treatment effects in observational studies",
      "Adjusting for selection bias in experimental data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for treatment effects",
      "how to implement sample selection bounds in python",
      "python causal inference tools",
      "program evaluation methods in python",
      "python library for DiD analysis",
      "how to use synthetic control in python"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Lee (2009)",
    "maintenance_status": "active",
    "model_score": 0.0003
  },
  {
    "name": "pyDOE2",
    "description": "Implements classical Design of Experiments: factorial (full/fractional), response surface (Box-Behnken, CCD), Latin Hypercube.",
    "category": "Power Simulation & Design of Experiments",
    "docs_url": "https://pythonhosted.org/pyDOE2/",
    "github_url": "https://github.com/clicumu/pyDOE2",
    "url": "https://github.com/clicumu/pyDOE2",
    "install": "pip install pyDOE2",
    "tags": [
      "power analysis",
      "experiments"
    ],
    "best_for": "Sample size calculation, experimental design, power analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "pyDOE2 implements classical Design of Experiments techniques, including factorial designs, response surface methods, and Latin Hypercube sampling. It is used by researchers and practitioners in fields requiring experimental design and analysis.",
    "use_cases": [
      "Designing experiments for product testing",
      "Optimizing processes in manufacturing"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for Design of Experiments",
      "how to perform factorial design in python",
      "response surface methodology in python",
      "Latin Hypercube sampling python",
      "pyDOE2 examples",
      "experimental design tools in python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0003
  },
  {
    "name": "cuML (RAPIDS)",
    "description": "GPU-accelerated implementation of Random Forests for significant speedups on large datasets. Scikit-learn compatible API.",
    "category": "Tree & Ensemble Methods for Prediction",
    "docs_url": "https://docs.rapids.ai/api/cuml/stable/",
    "github_url": "https://github.com/rapidsai/cuml",
    "url": "https://github.com/rapidsai/cuml",
    "install": "conda install ... (See RAPIDS docs)",
    "tags": [
      "machine learning",
      "prediction"
    ],
    "best_for": "Random forests, gradient boosting, prediction tasks",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "cuML is a GPU-accelerated implementation of Random Forests designed for significant speedups on large datasets. It offers a Scikit-learn compatible API, making it accessible for users familiar with Scikit-learn.",
    "use_cases": [
      "large dataset classification",
      "regression tasks on big data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for GPU-accelerated Random Forests",
      "how to use cuML for machine learning",
      "scikit-learn compatible GPU library",
      "fast Random Forests in Python",
      "machine learning with cuML",
      "GPU machine learning libraries"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "scikit-learn"
    ],
    "maintenance_status": "active",
    "model_score": 0.0003
  },
  {
    "name": "ARCH",
    "description": "Specialized library for modeling and forecasting conditional volatility using ARCH, GARCH, EGARCH, and related models.",
    "category": "Time Series Econometrics",
    "docs_url": "https://arch.readthedocs.io/",
    "github_url": "https://github.com/bashtage/arch",
    "url": "https://github.com/bashtage/arch",
    "install": "pip install arch",
    "tags": [
      "time series",
      "econometrics"
    ],
    "best_for": "ARIMA, cointegration, VAR models",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "time-series",
      "econometrics"
    ],
    "summary": "ARCH is a specialized library for modeling and forecasting conditional volatility using various models such as ARCH, GARCH, and EGARCH. It is primarily used by data scientists and researchers in the field of econometrics to analyze time series data.",
    "use_cases": [
      "Forecasting financial market volatility",
      "Analyzing time series data for economic indicators"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for modeling conditional volatility",
      "how to forecast volatility in python",
      "GARCH model implementation in python",
      "time series econometrics library python",
      "using ARCH for financial modeling",
      "EGARCH model in python"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "statsmodels"
    ],
    "maintenance_status": "active",
    "model_score": 0.0003
  },
  {
    "name": "FixedEffectModel",
    "description": "Panel data modeling with IV tests (weak IV, over-identification, endogeneity) and 2-step GMM estimation.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": null,
    "github_url": "https://github.com/ksecology/FixedEffectModel",
    "url": "https://github.com/ksecology/FixedEffectModel",
    "install": "pip install FixedEffectModel",
    "tags": [
      "panel data",
      "fixed effects",
      "IV"
    ],
    "best_for": "Panel regression with comprehensive IV diagnostics",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "causal-inference",
      "panel-data"
    ],
    "summary": "The FixedEffectModel package provides tools for panel data modeling, including instrumental variable tests and two-step GMM estimation. It is useful for researchers and data scientists working with panel datasets who need to address issues like endogeneity and weak instruments.",
    "use_cases": [
      "Analyzing economic data with fixed effects",
      "Testing for endogeneity in panel datasets"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for panel data modeling",
      "how to perform IV tests in python",
      "python package for GMM estimation",
      "panel data analysis in python",
      "how to handle endogeneity in panel data",
      "weak IV tests in python",
      "over-identification tests in python"
    ],
    "primary_use_cases": [
      "panel data modeling",
      "IV tests",
      "GMM estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0003
  },
  {
    "name": "causalweight",
    "description": "Semiparametric causal inference methods based on inverse probability weighting and double machine learning for average treatment effects, causal mediation analysis (direct/indirect effects), and dynamic treatment evaluation. Supports LATE estimation with instrumental variables.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://cran.r-project.org/web/packages/causalweight/causalweight.pdf",
    "github_url": "https://github.com/hbodory/causalweight",
    "url": "https://cran.r-project.org/package=causalweight",
    "install": "install.packages(\"causalweight\")",
    "tags": [
      "inverse-probability-weighting",
      "causal-mediation",
      "double-machine-learning",
      "LATE",
      "instrumental-variables"
    ],
    "best_for": "Mediation analysis and LATE estimation using weighting-based approaches with flexible nuisance estimation, implementing Huber (2014) and Fr\u00f6lich & Huber (2017)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The causalweight package provides semiparametric causal inference methods using inverse probability weighting and double machine learning techniques. It is designed for researchers and practitioners interested in estimating average treatment effects, causal mediation analysis, and dynamic treatment evaluation.",
    "use_cases": [
      "Estimating average treatment effects in clinical trials",
      "Conducting causal mediation analysis in social sciences"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R library for causal inference",
      "how to perform causal mediation analysis in R",
      "R package for double machine learning",
      "LATE estimation in R",
      "inverse probability weighting in R",
      "instrumental variables in R"
    ],
    "primary_use_cases": [
      "average treatment effects estimation",
      "causal mediation analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0003
  },
  {
    "name": "PyWhy-Stats",
    "description": "Part of the PyWhy ecosystem providing statistical methods specifically for causal applications, including various independence tests and power-divergence methods.",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": "https://pywhy-stats.readthedocs.io/",
    "github_url": "https://github.com/py-why/pywhy-stats",
    "url": "https://github.com/py-why/pywhy-stats",
    "install": "pip install pywhy-stats",
    "tags": [
      "inference",
      "hypothesis testing"
    ],
    "best_for": "Hypothesis tests, confidence intervals, multiple testing",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "statistical-methods"
    ],
    "summary": "PyWhy-Stats is part of the PyWhy ecosystem that provides statistical methods tailored for causal applications. It is used by data scientists and researchers focusing on causal inference and hypothesis testing.",
    "use_cases": [
      "conducting independence tests",
      "performing power-divergence analysis"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to perform hypothesis testing in python",
      "statistical methods for causal applications",
      "independence tests in python",
      "power-divergence methods in python",
      "PyWhy-Stats documentation"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0003
  },
  {
    "name": "StatsForecast",
    "description": "Fast, scalable implementations of popular statistical forecasting models (ETS, ARIMA, Theta, etc.) optimized for performance.",
    "category": "Time Series Forecasting",
    "docs_url": "https://nixtla.github.io/statsforecast/",
    "github_url": "https://github.com/Nixtla/statsforecast",
    "url": "https://github.com/Nixtla/statsforecast",
    "install": "pip install statsforecast",
    "tags": [
      "forecasting",
      "time series"
    ],
    "best_for": "Prediction, demand forecasting, trend analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "time-series"
    ],
    "summary": "StatsForecast provides fast and scalable implementations of popular statistical forecasting models such as ETS, ARIMA, and Theta. It is designed for data scientists and analysts who need efficient forecasting solutions.",
    "use_cases": [
      "Forecasting sales trends",
      "Predicting stock prices"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for statistical forecasting",
      "how to implement ARIMA in python",
      "best python package for time series forecasting",
      "fast forecasting models in python",
      "scalable time series analysis python",
      "using ETS model in python"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "statsmodels",
      "prophet"
    ],
    "maintenance_status": "active",
    "model_score": 0.0003
  },
  {
    "name": "Linregress",
    "description": "Simple linear regression for Rust with R-style formula syntax, standard errors, t-stats, and p-values.",
    "category": "Core Libraries & Linear Models",
    "docs_url": "https://docs.rs/linregress",
    "github_url": "https://github.com/n1m3/linregress",
    "url": "https://crates.io/crates/linregress",
    "install": "cargo add linregress",
    "tags": [
      "rust",
      "regression",
      "OLS",
      "statistics"
    ],
    "best_for": "Simple no-frills OLS regression in Rust",
    "language": "Rust",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "statistics"
    ],
    "summary": "Linregress is a Rust library that provides simple linear regression capabilities using R-style formula syntax. It is useful for statisticians and data scientists looking to perform regression analysis in Rust.",
    "use_cases": [
      "Performing linear regression analysis on datasets",
      "Calculating standard errors and p-values for regression models"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "rust library for linear regression",
      "how to perform OLS in Rust",
      "statistics package in Rust",
      "Rust regression analysis",
      "simple linear regression Rust",
      "Rust library for statistics"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active",
    "model_score": 0.0003
  },
  {
    "name": "Skyfield",
    "description": "Elegant astronomy library for computing satellite and celestial positions using JPL ephemeris data",
    "category": "Space & Orbital Analysis",
    "docs_url": "https://rhodesmill.org/skyfield/",
    "github_url": "https://github.com/skyfielders/python-skyfield",
    "url": "https://rhodesmill.org/skyfield/",
    "install": "pip install skyfield",
    "tags": [
      "satellites",
      "astronomy",
      "orbital mechanics",
      "ephemeris"
    ],
    "best_for": "Computing satellite positions and passes for space economics research",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "astronomy",
      "orbital mechanics"
    ],
    "summary": "Skyfield is an elegant astronomy library designed for computing satellite and celestial positions using JPL ephemeris data. It is used by astronomers and space enthusiasts to accurately track celestial objects.",
    "use_cases": [
      "Calculating the position of satellites",
      "Tracking celestial events"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for astronomy",
      "how to compute satellite positions in python",
      "JPL ephemeris data in python",
      "astronomy calculations python",
      "orbital mechanics library python",
      "satellite tracking python"
    ],
    "primary_use_cases": [
      "Satellite tracking",
      "Orbital analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "SGP4",
      "astropy"
    ],
    "maintenance_status": "active",
    "model_score": 0.0003
  },
  {
    "name": "KECENI",
    "description": "Doubly robust, non-parametric estimation of node-wise counterfactual means under network interference (arXiv 2024).",
    "category": "Causal Inference & Matching",
    "docs_url": null,
    "github_url": "https://github.com/HeejongBong/KECENI",
    "url": "https://pypi.org/project/KECENI/",
    "install": "pip install keceni",
    "tags": [
      "networks",
      "spillovers",
      "causal inference"
    ],
    "best_for": "Network interference with node heterogeneity",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "networks"
    ],
    "summary": "KECENI provides a method for doubly robust, non-parametric estimation of counterfactual means in the presence of network interference. It is particularly useful for researchers and practitioners in causal inference who are dealing with complex network data.",
    "use_cases": [
      "Estimating treatment effects in networked populations",
      "Analyzing spillover effects in social networks"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to estimate counterfactual means in python",
      "network interference estimation python",
      "doubly robust estimation in python",
      "spillover effects analysis python",
      "non-parametric causal inference python"
    ],
    "primary_use_cases": [
      "causal forest estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0003
  },
  {
    "name": "CausalGPS",
    "description": "Machine learning-based generalized propensity score estimation for continuous treatments. Uses SuperLearner ensemble methods for flexible estimation of dose-response curves.",
    "category": "Causal Inference (Continuous Treatment)",
    "docs_url": "https://cran.r-project.org/web/packages/CausalGPS/CausalGPS.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=CausalGPS",
    "install": "install.packages(\"CausalGPS\")",
    "tags": [
      "GPS",
      "continuous-treatment",
      "machine-learning",
      "SuperLearner",
      "dose-response"
    ],
    "best_for": "ML-based generalized propensity scores for continuous treatment dose-response estimation",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "machine-learning"
    ],
    "summary": "CausalGPS is a machine learning package designed for generalized propensity score estimation for continuous treatments. It employs SuperLearner ensemble methods to provide flexible estimation of dose-response curves, making it useful for researchers and practitioners in causal inference.",
    "use_cases": [
      "Estimating treatment effects in observational studies",
      "Analyzing dose-response relationships in clinical trials"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for generalized propensity score",
      "how to estimate dose-response curves in R",
      "machine learning for continuous treatments R",
      "CausalGPS documentation",
      "SuperLearner for causal inference",
      "R library for dose-response analysis"
    ],
    "primary_use_cases": [
      "generalized propensity score estimation",
      "dose-response curve estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0003
  },
  {
    "name": "cvxpy",
    "description": "Domain-specific language for convex optimization problems. Write math as code \u2014 the standard for convex problems.",
    "category": "Optimization",
    "docs_url": "https://www.cvxpy.org/",
    "github_url": "https://github.com/cvxpy/cvxpy",
    "url": "https://www.cvxpy.org/",
    "install": "pip install cvxpy",
    "tags": [
      "convex optimization",
      "linear programming",
      "quadratic programming"
    ],
    "best_for": "Convex optimization with intuitive syntax",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Cvxpy is a domain-specific language designed for formulating and solving convex optimization problems. It allows users to express mathematical problems in a code format, making it suitable for researchers and practitioners in optimization.",
    "use_cases": [
      "Solving linear programming problems",
      "Formulating and solving quadratic programming problems"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for convex optimization",
      "how to solve linear programming in python",
      "quadratic programming in python",
      "cvxpy tutorial",
      "convex optimization problems in python",
      "cvxpy examples"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "cvxopt",
      "scipy.optimize"
    ],
    "maintenance_status": "active",
    "model_score": 0.0003
  },
  {
    "name": "Gensim",
    "description": "Library focused on topic modeling (LDA, LSI) and document similarity analysis.",
    "category": "Natural Language Processing for Economics",
    "docs_url": "https://radimrehurek.com/gensim/",
    "github_url": "https://github.com/RaRe-Technologies/gensim",
    "url": "https://github.com/RaRe-Technologies/gensim",
    "install": "pip install gensim",
    "tags": [
      "NLP",
      "text analysis"
    ],
    "best_for": "Text analysis, sentiment analysis, document classification",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "Gensim is a library focused on topic modeling and document similarity analysis. It is widely used by researchers and practitioners in the field of natural language processing to analyze large text corpora.",
    "use_cases": [
      "Analyzing customer feedback for insights",
      "Building recommendation systems based on text data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for topic modeling",
      "how to do document similarity analysis in python",
      "Gensim tutorial",
      "Gensim LDA example",
      "text analysis with Gensim",
      "using Gensim for NLP"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "spaCy",
      "NLTK"
    ],
    "maintenance_status": "active",
    "model_score": 0.0003
  },
  {
    "name": "scipy.optimize",
    "description": "Optimization algorithms built into SciPy. Minimization, root finding, curve fitting, and linear programming.",
    "category": "Optimization",
    "docs_url": "https://docs.scipy.org/doc/scipy/reference/optimize.html",
    "github_url": "https://github.com/scipy/scipy",
    "url": "https://docs.scipy.org/doc/scipy/reference/optimize.html",
    "install": "pip install scipy",
    "tags": [
      "optimization",
      "minimization",
      "root finding"
    ],
    "best_for": "General-purpose optimization \u2014 start here for basics",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "scipy.optimize provides a collection of optimization algorithms for tasks such as minimization, root finding, curve fitting, and linear programming. It is widely used by data scientists and researchers in various fields requiring optimization techniques.",
    "use_cases": [
      "Minimizing a cost function in machine learning",
      "Finding roots of equations",
      "Fitting a curve to data points"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for optimization",
      "how to minimize a function in python",
      "root finding algorithms in python",
      "curve fitting with scipy",
      "linear programming in python",
      "scipy.optimize examples",
      "using scipy for optimization tasks"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "numpy",
      "scikit-learn"
    ],
    "maintenance_status": "active",
    "model_score": 0.0003
  },
  {
    "name": "tea-tasting",
    "description": "Calculate A/B test statistics directly within data warehouses (BigQuery, ClickHouse, Snowflake, Spark) via Ibis interface. Supports CUPED/CUPAC.",
    "category": "Power Simulation & Design of Experiments",
    "docs_url": "https://tea-tasting.e10v.me/",
    "github_url": "https://github.com/e10v/tea-tasting",
    "url": "https://github.com/e10v/tea-tasting",
    "install": "pip install tea-tasting",
    "tags": [
      "A/B testing",
      "experimentation",
      "data warehouses"
    ],
    "best_for": "In-warehouse A/B test analysis with variance reduction",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "causal-inference",
      "experimentation"
    ],
    "summary": "The tea-tasting package allows users to calculate A/B test statistics directly within data warehouses using the Ibis interface. It is particularly useful for data scientists and analysts working with large datasets in environments like BigQuery, ClickHouse, Snowflake, and Spark.",
    "use_cases": [
      "Analyzing A/B test results in BigQuery",
      "Implementing CUPED for variance reduction",
      "Conducting experiments in Snowflake",
      "Performing statistical analysis in ClickHouse"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for A/B testing",
      "how to calculate A/B test statistics in python",
      "data warehouse experimentation tools",
      "Ibis interface for A/B testing",
      "CUPED in python",
      "CUPAC implementation in python",
      "python A/B testing library",
      "analyze A/B tests with data warehouses"
    ],
    "primary_use_cases": [
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0003
  },
  {
    "name": "HiGHS",
    "description": "State-of-the-art open-source LP/MIP solver. Now the default solver in PyPSA, JuMP, and SciPy. Competitive with commercial solvers on many problem types.",
    "category": "Energy & Utilities Economics",
    "docs_url": "https://highs.dev/",
    "github_url": "https://github.com/ERGO-Code/HiGHS",
    "url": "https://highs.dev/",
    "install": "pip install highspy",
    "tags": [
      "solver",
      "optimization",
      "LP",
      "MIP"
    ],
    "best_for": "Free, high-performance linear and mixed-integer optimization",
    "language": "C++/Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "HiGHS is a state-of-the-art open-source LP/MIP solver that is competitive with commercial solvers on many problem types. It is now the default solver in PyPSA, JuMP, and SciPy, making it widely used in optimization tasks.",
    "use_cases": [
      "Optimizing energy dispatch in power systems",
      "Solving large-scale linear programming problems"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for LP solver",
      "how to optimize with HiGHS",
      "MIP solver in Python",
      "best open-source LP solver",
      "using HiGHS with PyPSA",
      "JuMP optimization with HiGHS",
      "SciPy LP solver options"
    ],
    "primary_use_cases": [
      "Optimization solving"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "Gurobi",
      "CPLEX",
      "CBC"
    ],
    "maintenance_status": "active",
    "framework_compatibility": [
      "PyPSA",
      "JuMP",
      "SciPy"
    ],
    "model_score": 0.0003
  },
  {
    "name": "HiGHS",
    "description": "High-performance open-source linear and mixed-integer programming solver",
    "category": "Optimization",
    "docs_url": "https://highs.dev/",
    "github_url": "https://github.com/ERGO-Code/HiGHS",
    "url": "https://highs.dev/",
    "install": "pip install highspy",
    "tags": [
      "solver",
      "LP",
      "MIP",
      "optimization",
      "open-source"
    ],
    "best_for": "Solving large-scale linear and mixed-integer programs for energy optimization",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "HiGHS is a high-performance open-source solver designed for linear and mixed-integer programming problems. It is used by researchers and practitioners in optimization to efficiently solve complex mathematical models.",
    "use_cases": [
      "Solving linear programming problems",
      "Optimizing resource allocation",
      "Finding optimal solutions for mixed-integer problems"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for linear programming",
      "how to solve mixed-integer programming in python",
      "open-source LP solver",
      "HiGHS optimization solver",
      "python optimization tools",
      "best libraries for optimization in python"
    ],
    "primary_use_cases": [
      "LP/MIP solving",
      "Energy optimization"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "Pyomo",
      "PuLP",
      "CPLEX"
    ],
    "maintenance_status": "active",
    "model_score": 0.0003
  },
  {
    "name": "gcimpute",
    "description": "Gaussian copula imputation for mixed variable types with streaming capability (Journal of Statistical Software 2024).",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": null,
    "github_url": "https://github.com/udellgroup/gcimpute",
    "url": "https://github.com/udellgroup/gcimpute",
    "install": "pip install gcimpute",
    "tags": [
      "missing data",
      "imputation"
    ],
    "best_for": "Mixed-type missing data imputation with copulas",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "gcimpute is a Python package designed for Gaussian copula imputation, specifically tailored for mixed variable types. It is useful for statisticians and data scientists dealing with missing data in their datasets.",
    "use_cases": [
      "Imputing missing values in survey data",
      "Handling incomplete datasets in machine learning"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for Gaussian copula imputation",
      "how to handle missing data in Python",
      "imputation techniques in Python",
      "best practices for mixed variable imputation",
      "streaming data imputation in Python",
      "Gaussian copula methods for data analysis"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Journal of Statistical Software (2024)",
    "maintenance_status": "active",
    "model_score": 0.0003
  },
  {
    "name": "pydsge",
    "description": "DSGE model simulation, filtering, and Bayesian estimation. Handles occasionally binding constraints.",
    "category": "Structural Econometrics & Estimation",
    "docs_url": null,
    "github_url": "https://github.com/gboehl/pydsge",
    "url": "https://github.com/gboehl/pydsge",
    "install": "pip install pydsge",
    "tags": [
      "structural",
      "DSGE",
      "Bayesian"
    ],
    "best_for": "DSGE estimation with occasionally binding constraints",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "structural-econometrics",
      "DSGE",
      "bayesian"
    ],
    "summary": "pydsge is a Python package designed for simulating, filtering, and estimating DSGE models using Bayesian methods. It is particularly useful for economists and researchers working with macroeconomic models that involve occasionally binding constraints.",
    "use_cases": [
      "Simulating economic scenarios using DSGE models",
      "Estimating parameters of macroeconomic models"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for DSGE modeling",
      "how to estimate DSGE models in python",
      "Bayesian estimation for DSGE",
      "filtering DSGE models in python",
      "structural econometrics python package",
      "pydsge documentation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0003
  },
  {
    "name": "algmatch",
    "description": "Student-Project Allocation with lecturer preferences. Extends matching to three-sided markets.",
    "category": "Matching & Market Design",
    "docs_url": null,
    "github_url": null,
    "url": "https://pypi.org/project/algmatch/",
    "install": "pip install algmatch",
    "tags": [
      "matching",
      "market design",
      "allocation"
    ],
    "best_for": "Student-project-lecturer allocation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "algmatch is a Python package designed for student-project allocation that incorporates lecturer preferences, extending the matching process to three-sided markets. It is useful for educational institutions and researchers involved in allocation problems.",
    "use_cases": [
      "Allocating students to projects based on preferences",
      "Facilitating lecturer involvement in project assignments"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "python library for student-project allocation",
      "how to match students with projects in python",
      "python matching algorithms for three-sided markets",
      "allocation problems in python",
      "lecturer preferences in project allocation python",
      "student project matching library",
      "market design algorithms in python",
      "how to implement matching in python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0003
  },
  {
    "name": "ziln_cltv",
    "description": "Google's Zero-Inflated Lognormal loss for heavily-tailed LTV distributions. Outputs both predicted LTV and churn probability.",
    "category": "Marketing Mix Models (MMM) & Business Analytics",
    "docs_url": null,
    "github_url": "https://github.com/google/lifetime_value",
    "url": "https://github.com/google/lifetime_value",
    "install": "pip install lifetime-value",
    "tags": [
      "LTV",
      "customer analytics",
      "churn"
    ],
    "best_for": "Customer LTV with zero-inflated distributions",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "customer analytics",
      "churn prediction"
    ],
    "summary": "The ziln_cltv package provides Google's Zero-Inflated Lognormal loss model for predicting customer Lifetime Value (LTV) and churn probability, specifically designed for heavily-tailed distributions. It is useful for marketers and data scientists looking to analyze customer behavior and retention.",
    "use_cases": [
      "Predicting customer lifetime value for subscription services",
      "Analyzing churn rates in e-commerce",
      "Estimating LTV for marketing campaigns"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for LTV prediction",
      "how to predict churn probability in python",
      "zero-inflated lognormal model python",
      "customer analytics tools in python",
      "python package for marketing mix models",
      "predicting customer lifetime value using python",
      "churn analysis with python",
      "business analytics library in python"
    ],
    "primary_use_cases": [
      "customer lifetime value prediction",
      "churn probability estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0003
  },
  {
    "name": "GeoLift",
    "description": "Meta's end-to-end synthetic control for geo experiments with multi-cell testing and power calculations.",
    "category": "Geo-Experiments & Lift Measurement",
    "docs_url": "https://facebookincubator.github.io/GeoLift/",
    "github_url": "https://github.com/facebookincubator/GeoLift",
    "url": "https://github.com/facebookincubator/GeoLift",
    "install": "pip install geolift",
    "tags": [
      "geo-experiments",
      "synthetic control",
      "incrementality"
    ],
    "best_for": "Meta's geo-level incrementality measurement",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "geo-experiments"
    ],
    "summary": "GeoLift is a Python package designed for conducting geo experiments using synthetic control methods. It allows users to perform multi-cell testing and power calculations, making it useful for researchers and data scientists in the field of causal inference.",
    "use_cases": [
      "Evaluating the impact of marketing campaigns in different geographical areas",
      "Testing policy changes across multiple regions"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for geo experiments",
      "how to perform synthetic control in python",
      "incrementality testing in python",
      "multi-cell testing in python",
      "power calculations for experiments",
      "geo-lift measurement in python"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0003
  },
  {
    "name": "GeoLift",
    "description": "Meta's geo-experimental methodology combining Augmented Synthetic Control with power analysis",
    "category": "Causal Inference",
    "docs_url": "https://facebookincubator.github.io/GeoLift/",
    "github_url": "https://github.com/facebookincubator/GeoLift",
    "url": "https://facebookincubator.github.io/GeoLift/",
    "install": "devtools::install_github('facebookincubator/GeoLift')",
    "tags": [
      "geo experiments",
      "synthetic control",
      "power analysis",
      "Meta"
    ],
    "best_for": "Designing and analyzing geo-holdout experiments for ad measurement",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "geo-experiments"
    ],
    "summary": "GeoLift is a methodology developed by Meta that combines Augmented Synthetic Control with power analysis to conduct geo-experimental studies. It is primarily used by researchers and data scientists interested in causal inference in geographical contexts.",
    "use_cases": [
      "Evaluating the impact of a policy change in a specific region",
      "Assessing the effectiveness of a marketing campaign across different geographical areas"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for geo experiments",
      "how to conduct synthetic control in R",
      "power analysis for geo experiments in R"
    ],
    "primary_use_cases": [
      "Geo holdout tests",
      "Incrementality measurement"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "Robyn",
      "CausalImpact"
    ],
    "maintenance_status": "active",
    "model_score": 0.0003
  },
  {
    "name": "stargazer",
    "description": "Produces well-formatted LaTeX, HTML/CSS, and ASCII regression tables with multiple models side-by-side, plus summary statistics tables. Widely used in economics with journal-specific formatting styles (AER, QJE, ASR).",
    "category": "Regression Output",
    "docs_url": "https://cran.r-project.org/web/packages/stargazer/vignettes/stargazer.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=stargazer",
    "install": "install.packages(\"stargazer\")",
    "tags": [
      "LaTeX-tables",
      "regression-output",
      "academic-publishing",
      "economics",
      "HTML-tables"
    ],
    "best_for": "Quick, publication-ready LaTeX tables for economics journals with classic formatting",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The stargazer package produces well-formatted regression tables in LaTeX, HTML/CSS, and ASCII formats, allowing users to display multiple models side-by-side along with summary statistics. It is widely used in the field of economics, particularly for academic publishing.",
    "use_cases": [
      "Generating regression tables for academic papers",
      "Creating formatted output for presentations"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for regression tables",
      "how to create LaTeX tables in R",
      "generate HTML regression output in R",
      "stargazer package documentation",
      "R academic publishing tools",
      "create summary statistics tables in R",
      "multiple regression models side by side in R"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active",
    "model_score": 0.0003
  },
  {
    "name": "Stargazer",
    "description": "Python port of R's stargazer for creating publication-quality regression tables (HTML, LaTeX) from `statsmodels` & `linearmodels` results.",
    "category": "Standard Errors, Bootstrapping & Reporting",
    "docs_url": null,
    "github_url": "https://github.com/StatsReporting/stargazer",
    "url": "https://github.com/StatsReporting/stargazer",
    "install": "pip install stargazer",
    "tags": [
      "bootstrap",
      "standard errors"
    ],
    "best_for": "Robust inference, clustered SEs, result presentation",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Stargazer is a Python package that creates publication-quality regression tables in HTML and LaTeX formats from results obtained using statsmodels and linearmodels. It is primarily used by researchers and data scientists who need to present regression results in a professional format.",
    "use_cases": [
      "Generating regression tables for academic papers",
      "Creating reports with statistical results"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for regression tables",
      "how to create publication-quality tables in python",
      "stargazer python package",
      "generate LaTeX tables from statsmodels",
      "create HTML regression tables python",
      "reporting regression results in python"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active",
    "model_score": 0.0003
  },
  {
    "name": "spilled_t",
    "description": "Treatment and spillover effect estimation under network interference. Separates direct and indirect effects.",
    "category": "Interference & Spillovers",
    "docs_url": "https://github.com/mpleung/spilled_t",
    "github_url": "https://github.com/mpleung/spilled_t",
    "url": "https://github.com/mpleung/spilled_t",
    "install": "pip install spilled_t",
    "tags": [
      "network interference",
      "spillovers",
      "treatment effects"
    ],
    "best_for": "Separating direct and spillover effects",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "network-analysis"
    ],
    "summary": "The 'spilled_t' package estimates treatment and spillover effects in the presence of network interference. It is useful for researchers and practitioners who need to separate direct and indirect effects in their analyses.",
    "use_cases": [
      "Estimating treatment effects in social networks",
      "Analyzing the impact of interventions in connected populations"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for treatment effect estimation",
      "how to analyze spillover effects in python",
      "network interference analysis in python",
      "estimating indirect effects with python",
      "spillover effect estimation library",
      "causal inference tools in python"
    ],
    "primary_use_cases": [
      "treatment effect estimation",
      "spillover effect analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "catalystcoop-pudl",
    "description": "Public Utility Data Liberation - cleaned, integrated U.S. energy data. Combines EIA, FERC, and EPA data into analysis-ready formats with comprehensive documentation.",
    "category": "Energy & Utilities Economics",
    "docs_url": "https://catalystcoop-pudl.readthedocs.io/",
    "github_url": "https://github.com/catalyst-cooperative/pudl",
    "url": "https://catalyst.coop/pudl/",
    "install": "pip install catalystcoop.pudl",
    "tags": [
      "data integration",
      "EIA",
      "FERC",
      "EPA",
      "open source"
    ],
    "best_for": "Integrated U.S. energy data analysis",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The catalystcoop-pudl package provides cleaned and integrated U.S. energy data by combining information from EIA, FERC, and EPA into analysis-ready formats. It is useful for researchers and analysts in the energy and utilities sector.",
    "use_cases": [
      "Analyzing energy consumption trends",
      "Comparing utility performance across states"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for energy data integration",
      "how to analyze U.S. energy data in python",
      "public utility data analysis python",
      "EIA FERC EPA data python library",
      "open source energy data tools",
      "cleaning energy data in python"
    ],
    "primary_use_cases": [
      "Integrated data access"
    ],
    "api_complexity": "simple",
    "related_packages": [
      "eiapy",
      "gridstatus"
    ],
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "catalystcoop-pudl",
    "description": "Public Utility Data Liberation - integrated energy data from EIA, FERC, and EPA",
    "category": "Data Access",
    "docs_url": "https://catalystcoop-pudl.readthedocs.io/",
    "github_url": "https://github.com/catalyst-cooperative/pudl",
    "url": "https://catalyst.coop/pudl/",
    "install": "pip install catalystcoop-pudl",
    "tags": [
      "EIA",
      "FERC",
      "EPA",
      "integrated data",
      "ETL"
    ],
    "best_for": "Working with clean, integrated U.S. utility and energy data",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The catalystcoop-pudl package provides integrated energy data from EIA, FERC, and EPA, facilitating public utility data liberation. It is useful for data analysts and researchers working with energy data.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for public utility data",
      "how to access EIA data in python",
      "integrated energy data analysis in python",
      "ETL for energy data using python"
    ],
    "primary_use_cases": [
      "Data integration",
      "Energy research"
    ],
    "api_complexity": "simple",
    "related_packages": [
      "pandas",
      "sqlalchemy"
    ],
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "H2O Sparkling Water",
    "description": "H2O's distributed ML engine on Spark with GLM/GAM that provides p-values, confidence intervals, and Tweedie/Gamma distributions.",
    "category": "Core Libraries & Linear Models",
    "docs_url": "https://docs.h2o.ai/sparkling-water/3.3/latest-stable/doc/index.html",
    "github_url": "https://github.com/h2oai/sparkling-water",
    "url": "https://github.com/h2oai/sparkling-water",
    "install": "pip install h2o_pysparkling_3.4",
    "tags": [
      "spark",
      "GLM",
      "GAM",
      "distributed",
      "p-values"
    ],
    "best_for": "Econometric inference (p-values, CIs) at Spark scale",
    "language": "Spark",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "H2O Sparkling Water is a distributed machine learning engine that integrates with Spark, providing functionalities for Generalized Linear Models (GLM) and Generalized Additive Models (GAM). It is used by data scientists and machine learning practitioners who require scalable solutions for statistical modeling and inference.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "H2O Sparkling Water library",
      "how to use GLM in Spark",
      "distributed machine learning with Spark",
      "H2O Sparkling Water examples",
      "GAM implementation in Spark",
      "p-values in machine learning",
      "Tweedie distribution in Spark",
      "H2O machine learning engine"
    ],
    "primary_use_cases": [
      "statistical modeling",
      "machine learning inference"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "Spark"
    ],
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "HonestDiD",
    "description": "Constructs robust confidence intervals for DiD and event-study designs under violations of parallel trends. Allows researchers to conduct sensitivity analysis by relaxing the parallel trends assumption using smoothness or relative magnitude restrictions on pre-trend violations.",
    "category": "Causal Inference (DiD)",
    "docs_url": "https://github.com/asheshrambachan/HonestDiD",
    "github_url": "https://github.com/asheshrambachan/HonestDiD",
    "url": "https://cran.r-project.org/package=HonestDiD",
    "install": "install.packages(\"HonestDiD\")",
    "tags": [
      "sensitivity-analysis",
      "parallel-trends",
      "robust-inference",
      "confidence-intervals",
      "event-study"
    ],
    "best_for": "Assessing how treatment effect conclusions change under plausible parallel trends violations, implementing Rambachan & Roth (2023)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "HonestDiD constructs robust confidence intervals for Difference-in-Differences (DiD) and event-study designs, addressing violations of parallel trends. It is primarily used by researchers conducting sensitivity analysis in causal inference.",
    "use_cases": [
      "Analyzing the impact of policy changes using DiD",
      "Conducting sensitivity analysis for pre-trend violations"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for causal inference",
      "how to conduct sensitivity analysis in R",
      "robust confidence intervals for DiD",
      "event-study analysis in R",
      "parallel trends violation analysis",
      "R library for event-study designs"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "BCEA",
    "description": "Bayesian Cost-Effectiveness Analysis in R. Processes MCMC output from JAGS/Stan, generates CEACs, CEAFs, and expected value of information calculations.",
    "category": "Healthcare Economics & Health-Tech",
    "docs_url": "https://cran.r-project.org/web/packages/BCEA/",
    "github_url": "https://github.com/giabaio/BCEA",
    "url": "https://cran.r-project.org/web/packages/BCEA/",
    "install": "install.packages('BCEA')",
    "tags": [
      "Bayesian",
      "cost-effectiveness",
      "VOI",
      "R"
    ],
    "best_for": "Bayesian cost-effectiveness analysis and value of information",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "bayesian",
      "cost-effectiveness"
    ],
    "summary": "BCEA is an R package designed for Bayesian Cost-Effectiveness Analysis. It processes MCMC output from JAGS/Stan and generates cost-effectiveness acceptability curves (CEACs), cost-effectiveness analysis frameworks (CEAFs), and expected value of information calculations.",
    "use_cases": [
      "Analyzing healthcare interventions",
      "Evaluating cost-effectiveness of medical treatments"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for Bayesian cost-effectiveness analysis",
      "how to perform MCMC in R",
      "generate CEACs in R",
      "cost-effectiveness analysis using R",
      "expected value of information calculations in R",
      "BCEA R package documentation"
    ],
    "primary_use_cases": [
      "cost-effectiveness analysis",
      "expected value of information calculations"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "hesim",
      "heemod",
      "rjags"
    ],
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "PyLogit",
    "description": "Flexible implementation of conditional/multinomial logit models with utilities for data preparation.",
    "category": "Discrete Choice Models",
    "docs_url": null,
    "github_url": "https://github.com/timothyb0912/pylogit",
    "url": "https://github.com/timothyb0912/pylogit",
    "install": "pip install pylogit",
    "tags": [
      "discrete choice",
      "logit"
    ],
    "best_for": "Logit/probit models, consumer choice, demand estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "PyLogit is a flexible implementation of conditional and multinomial logit models, providing utilities for data preparation. It is primarily used by data scientists and researchers in fields involving discrete choice modeling.",
    "use_cases": [
      "Modeling consumer choice behavior",
      "Analyzing survey data with multiple alternatives"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for discrete choice models",
      "how to implement multinomial logit in python",
      "conditional logit model python",
      "data preparation for logit models in python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "PyLogit",
    "description": "Sklearn-style API for discrete choice models (MNL, nested, mixed logit). Integrates well with pandas DataFrames and scikit-learn workflows.",
    "category": "Transportation Economics & Technology",
    "docs_url": "https://github.com/timothyb0912/pylogit",
    "github_url": "https://github.com/timothyb0912/pylogit",
    "url": "https://github.com/timothyb0912/pylogit",
    "install": "pip install pylogit",
    "tags": [
      "discrete choice",
      "logit",
      "sklearn",
      "pandas"
    ],
    "best_for": "Choice modeling with familiar sklearn-style interface",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "PyLogit provides a sklearn-style API for estimating discrete choice models such as multinomial logit, nested logit, and mixed logit. It is designed for users who work with pandas DataFrames and want to integrate choice modeling into their scikit-learn workflows.",
    "use_cases": [
      "Estimating consumer choice behavior",
      "Analyzing transportation mode preferences"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for discrete choice models",
      "how to use logit in python",
      "sklearn API for choice modeling",
      "pandas integration with logit models",
      "mixed logit model in python",
      "nested logit analysis in python"
    ],
    "primary_use_cases": [
      "Discrete choice modeling"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "Biogeme",
      "xlogit",
      "scikit-learn"
    ],
    "maintenance_status": "active",
    "framework_compatibility": [
      "scikit-learn"
    ],
    "model_score": 0.0002
  },
  {
    "name": "pysyncon",
    "description": "Synthetic control method implementation compatible with R's Synth and augsynth packages.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": null,
    "github_url": "https://github.com/sdfordham/pysyncon",
    "url": "https://github.com/sdfordham/pysyncon",
    "install": "pip install pysyncon",
    "tags": [
      "synthetic control",
      "causal inference",
      "panel data"
    ],
    "best_for": "R Synth-compatible synthetic control in Python",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "panel-data"
    ],
    "summary": "pysyncon is a Python package that implements the synthetic control method, allowing users to analyze causal effects in observational data. It is particularly useful for researchers and practitioners in program evaluation who require robust methods for causal inference.",
    "use_cases": [
      "Evaluating the impact of a policy intervention",
      "Comparing treatment effects across different groups"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for synthetic control",
      "how to implement synthetic control in python",
      "causal inference methods in python",
      "panel data analysis in python",
      "synthetic control method python",
      "python package for program evaluation"
    ],
    "primary_use_cases": [
      "causal inference analysis",
      "program evaluation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "collapse",
    "description": "High-performance data transformation package designed by an economist. Provides fast grouped operations, time series functions, and panel data tools with 10-100\u00d7 speedups over dplyr on large data.",
    "category": "Data Workflow",
    "docs_url": "https://sebkrantz.github.io/collapse/",
    "github_url": "https://github.com/SebKrantz/collapse",
    "url": "https://cran.r-project.org/package=collapse",
    "install": "install.packages(\"collapse\")",
    "tags": [
      "data-transformation",
      "high-performance",
      "panel-data",
      "time-series",
      "grouped-operations"
    ],
    "best_for": "High-performance data transformation optimized for economists\u201410-100\u00d7 faster than dplyr",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "data-transformation",
      "time-series",
      "panel-data"
    ],
    "summary": "The 'collapse' package is a high-performance data transformation tool designed for economists, providing fast grouped operations, time series functions, and panel data tools. It is particularly useful for users dealing with large datasets who require efficient data manipulation.",
    "use_cases": [
      "Transforming large datasets efficiently",
      "Conducting time series analysis",
      "Performing grouped operations on panel data"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for high-performance data transformation",
      "how to perform grouped operations in R",
      "R time series functions",
      "efficient panel data tools in R",
      "data manipulation package for economists",
      "fast data transformation in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "respy",
    "description": "Simulation and estimation of finite-horizon dynamic discrete choice (DDC) models (e.g., labor/education choice).",
    "category": "Structural Econometrics & Estimation",
    "docs_url": "https://respy.readthedocs.io/en/latest/",
    "github_url": "https://github.com/OpenSourceEconomics/respy",
    "url": "https://github.com/OpenSourceEconomics/respy",
    "install": "pip install respy",
    "tags": [
      "structural",
      "estimation"
    ],
    "best_for": "Structural models, GMM estimation, BLP-style demand",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "structural",
      "estimation"
    ],
    "summary": "Respy is a Python package designed for the simulation and estimation of finite-horizon dynamic discrete choice models, which are commonly used in labor and education choice contexts. It is useful for researchers and practitioners in structural econometrics who need to model decision-making processes over time.",
    "use_cases": [
      "Estimating labor market decisions",
      "Modeling educational choices over time"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for dynamic discrete choice models",
      "how to estimate finite-horizon models in python",
      "simulation of labor choice models in python",
      "education choice modeling with python",
      "structural econometrics tools in python",
      "respy package usage examples",
      "dynamic choice simulation library python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "SCtools",
    "description": "Automates placebo tests and multi-treated-unit ATT calculations for synthetic control. Provides utilities for generating in-space and in-time placebos with visualization.",
    "category": "Causal Inference (Synthetic Control)",
    "docs_url": "https://cran.r-project.org/web/packages/SCtools/SCtools.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=SCtools",
    "install": "install.packages(\"SCtools\")",
    "tags": [
      "synthetic-control",
      "placebo-tests",
      "multi-unit",
      "ATT",
      "visualization"
    ],
    "best_for": "Automated placebo tests and multi-treated-unit ATT calculations for synthetic control",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "visualization"
    ],
    "summary": "SCtools automates placebo tests and multi-treated-unit ATT calculations for synthetic control. It provides utilities for generating in-space and in-time placebos with visualization, making it useful for researchers in causal inference.",
    "use_cases": [
      "Conducting placebo tests for causal inference studies",
      "Calculating ATT for multiple treated units"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for synthetic control",
      "how to perform placebo tests in R",
      "multi-treated-unit ATT calculations in R",
      "visualization tools for synthetic control",
      "automate placebo tests in R",
      "R utilities for synthetic control"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "CausalPy",
    "description": "Bayesian causal inference library from PyMC Labs. Implements synthetic control, difference-in-differences, and interrupted time series for geo experiments and marketing measurement.",
    "category": "MarTech & Customer Analytics",
    "docs_url": "https://causalpy.readthedocs.io/",
    "github_url": "https://github.com/pymc-labs/CausalPy",
    "url": "https://github.com/pymc-labs/CausalPy",
    "install": "pip install CausalPy",
    "tags": [
      "causal-inference",
      "synthetic-control",
      "DiD",
      "Bayesian"
    ],
    "best_for": "Measuring geo experiments and quasi-experimental marketing effects",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "time-series",
      "bayesian"
    ],
    "summary": "CausalPy is a Bayesian causal inference library that implements methods such as synthetic control, difference-in-differences, and interrupted time series. It is designed for geo experiments and marketing measurement, making it useful for data scientists and researchers in these fields.",
    "use_cases": [
      "Analyzing the impact of a marketing campaign",
      "Evaluating policy changes using synthetic control"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to do difference-in-differences in python",
      "Bayesian causal analysis in python",
      "synthetic control methods python",
      "interrupted time series analysis python",
      "marketing measurement with python"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "DoWhy",
      "EconML"
    ],
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "tidygraph",
    "description": "Tidy data interface for network/graph data. Extends dplyr verbs to work with nodes and edges, enabling pipe-friendly network manipulation that integrates seamlessly with ggraph for visualization.",
    "category": "Network Analysis",
    "docs_url": "https://tidygraph.data-imaginist.com/",
    "github_url": "https://github.com/thomasp85/tidygraph",
    "url": "https://cran.r-project.org/package=tidygraph",
    "install": "install.packages(\"tidygraph\")",
    "tags": [
      "networks",
      "tidyverse",
      "graph-manipulation",
      "dplyr",
      "pipes"
    ],
    "best_for": "Tidy manipulation of network data with dplyr-style verbs for nodes and edges",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "network analysis",
      "data manipulation",
      "visualization"
    ],
    "summary": "The tidygraph package provides a tidy data interface for network and graph data, allowing users to manipulate nodes and edges using dplyr verbs. It is designed for R users who want to perform network analysis and visualization seamlessly with ggraph.",
    "use_cases": [
      "Analyzing social networks",
      "Visualizing relationships in data",
      "Manipulating graph structures for analysis"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "R package for network analysis",
      "how to manipulate graph data in R",
      "tidygraph documentation",
      "visualizing networks in R",
      "dplyr for graph data",
      "network manipulation with tidyverse"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "igraph",
      "ggraph"
    ],
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "ggraph",
    "description": "Grammar of graphics for network data built on ggplot2. Provides layouts, geometries, and faceting specifically designed for network visualization with publication-quality output.",
    "category": "Network Analysis",
    "docs_url": "https://ggraph.data-imaginist.com/",
    "github_url": "https://github.com/thomasp85/ggraph",
    "url": "https://cran.r-project.org/package=ggraph",
    "install": "install.packages(\"ggraph\")",
    "tags": [
      "networks",
      "visualization",
      "ggplot2",
      "graph-layouts",
      "publication-ready"
    ],
    "best_for": "Publication-quality network visualization using ggplot2 grammar",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "network visualization",
      "graphics"
    ],
    "summary": "ggraph is a package that provides a grammar of graphics specifically designed for visualizing network data using ggplot2. It is used by data scientists and researchers who need to create publication-quality network visualizations.",
    "use_cases": [
      "Creating visualizations for social network analysis",
      "Visualizing complex relationships in data"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for network visualization",
      "how to create network graphs in R",
      "ggplot2 network layouts",
      "visualizing networks with ggraph",
      "network data graphics in R",
      "publication-quality network visualizations R"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "ggplot2"
    ],
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "CausalPy",
    "description": "Bayesian causal inference on PyMC including synthetic control, difference-in-differences, and regression discontinuity.",
    "category": "Causal Inference (Synthetic Control)",
    "docs_url": "https://causalpy.readthedocs.io/",
    "github_url": "https://github.com/pymc-labs/CausalPy",
    "url": "https://causalpy.readthedocs.io/",
    "install": "pip install causalpy",
    "tags": [
      "causal-inference",
      "Bayesian",
      "synthetic-control",
      "DiD",
      "RDD",
      "PyMC"
    ],
    "best_for": "Bayesian causal inference with uncertainty quantification",
    "language": "Python",
    "model_score": 0.0002
  },
  {
    "name": "ebal",
    "description": "Implements entropy balancing, a reweighting method that finds weights for control units such that specified covariate moment conditions (means, variances) are exactly satisfied while staying as close as possible to uniform weights by minimizing Kullback-Leibler divergence. Primarily designed for ATT estimation.",
    "category": "Causal Inference (Matching)",
    "docs_url": "https://cran.r-project.org/web/packages/ebal/ebal.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=ebal",
    "install": "install.packages(\"ebal\")",
    "tags": [
      "entropy-balancing",
      "reweighting",
      "covariate-balance",
      "observational-studies",
      "ATT"
    ],
    "best_for": "When you need exact covariate balance on specified moments (means, variances) with minimal weight dispersion, implementing Hainmueller (2012)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The ebal package implements entropy balancing, a reweighting method that finds weights for control units to satisfy specified covariate moment conditions while minimizing Kullback-Leibler divergence. It is primarily designed for Average Treatment Effect on the Treated (ATT) estimation.",
    "use_cases": [
      "Estimating treatment effects in observational studies",
      "Balancing covariates in experimental designs"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for entropy balancing",
      "how to perform reweighting in R",
      "R package for covariate balance",
      "entropy balancing for observational studies",
      "ATT estimation in R",
      "how to minimize Kullback-Leibler divergence in R",
      "R methods for causal inference",
      "reweighting methods in R"
    ],
    "primary_use_cases": [
      "ATT estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "microsynth",
    "description": "Extends synthetic control method to micro-level data with many units. Implements permutation inference and handles high-dimensional settings where traditional SCM struggles.",
    "category": "Causal Inference (Synthetic Control)",
    "docs_url": "https://cran.r-project.org/web/packages/microsynth/microsynth.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=microsynth",
    "install": "install.packages(\"microsynth\")",
    "tags": [
      "synthetic-control",
      "micro-data",
      "permutation-inference",
      "high-dimensional",
      "many-units"
    ],
    "best_for": "Synthetic control for micro-level data with many units and permutation inference",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "high-dimensional"
    ],
    "summary": "Microsynth extends the synthetic control method to micro-level data with many units, allowing for permutation inference and handling high-dimensional settings. It is useful for researchers and practitioners in causal inference who work with complex datasets.",
    "use_cases": [
      "Analyzing treatment effects in observational studies",
      "Evaluating policy impacts on micro-level data"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for synthetic control",
      "how to perform permutation inference in R",
      "high-dimensional causal inference in R",
      "synthetic control for micro-data",
      "R library for many units analysis",
      "advanced synthetic control methods in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "PStrata",
    "description": "Principal stratification analysis for noncompliance and truncation-by-death using both Bayesian (Stan) and frequentist estimation. Implements Liu and Li (2023) methods for causal inference with post-treatment complications.",
    "category": "Causal Inference (Principal Stratification)",
    "docs_url": "https://cran.r-project.org/web/packages/PStrata/PStrata.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=PStrata",
    "install": "install.packages(\"PStrata\")",
    "tags": [
      "principal-stratification",
      "noncompliance",
      "truncation-by-death",
      "Bayesian",
      "Stan"
    ],
    "best_for": "Principal stratification for noncompliance and truncation-by-death with Bayesian/frequentist estimation",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "bayesian"
    ],
    "summary": "PStrata is designed for principal stratification analysis, focusing on noncompliance and truncation-by-death. It utilizes both Bayesian and frequentist estimation methods, making it suitable for researchers dealing with causal inference in complex scenarios.",
    "use_cases": [
      "Analyzing noncompliance in clinical trials",
      "Estimating causal effects with truncation-by-death"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for principal stratification",
      "how to perform causal inference in R",
      "Bayesian analysis for noncompliance",
      "truncation-by-death analysis in R",
      "PStrata package usage",
      "methods for causal inference with post-treatment complications"
    ],
    "primary_use_cases": [
      "principal stratification analysis",
      "causal inference with post-treatment complications"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Liu and Li (2023)",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "microsynth",
    "description": "Micro-level synthetic control with permutation-based inference. Extends synthetic control to individual-level data.",
    "category": "Causal Inference (Synthetic Control)",
    "docs_url": "https://cran.r-project.org/web/packages/microsynth/vignettes/microsynth-vignette.html",
    "github_url": null,
    "url": "https://cran.r-project.org/web/packages/microsynth/",
    "install": "install.packages('microsynth')",
    "tags": [
      "synthetic-control",
      "causal-inference",
      "permutation-test",
      "micro-data"
    ],
    "best_for": "Synthetic control at micro/individual level",
    "language": "R",
    "model_score": 0.0002
  },
  {
    "name": "deep-opt-auctions",
    "description": "Neural network optimal auction design. Implements RegretNet, RochetNet for mechanism design.",
    "category": "Matching & Market Design",
    "docs_url": "https://github.com/saisrivatsan/deep-opt-auctions",
    "github_url": "https://github.com/saisrivatsan/deep-opt-auctions",
    "url": "https://github.com/saisrivatsan/deep-opt-auctions",
    "install": "Install from GitHub",
    "tags": [
      "auctions",
      "mechanism design",
      "deep learning"
    ],
    "best_for": "Neural network auction design",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The deep-opt-auctions package provides neural network-based solutions for optimal auction design, specifically implementing RegretNet and RochetNet for mechanism design. It is useful for researchers and practitioners in the fields of economics and machine learning.",
    "use_cases": [
      "Designing optimal auctions using neural networks",
      "Analyzing auction mechanisms with deep learning"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for optimal auction design",
      "how to implement neural networks for auctions in python",
      "deep learning for mechanism design",
      "auctions with deep learning in python",
      "RegretNet implementation in python",
      "RochetNet auction design python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "umap-learn",
    "description": "Fast and scalable implementation of Uniform Manifold Approximation and Projection (UMAP) for non-linear reduction.",
    "category": "Dimensionality Reduction",
    "docs_url": "https://umap-learn.readthedocs.io/en/latest/",
    "github_url": "https://github.com/lmcinnes/umap",
    "url": "https://github.com/lmcinnes/umap",
    "install": "pip install umap-learn",
    "tags": [
      "machine learning",
      "dimensionality"
    ],
    "best_for": "Feature extraction, PCA, high-dimensional data",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "umap-learn is a fast and scalable implementation of Uniform Manifold Approximation and Projection (UMAP) for non-linear dimensionality reduction. It is used by data scientists and machine learning practitioners to visualize high-dimensional data in lower dimensions.",
    "use_cases": [
      "Visualizing high-dimensional datasets",
      "Reducing dimensions for clustering algorithms"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for dimensionality reduction",
      "how to use UMAP in python",
      "fast UMAP implementation python",
      "scalable UMAP for machine learning",
      "visualize high-dimensional data python",
      "UMAP for non-linear reduction python"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "scikit-learn"
    ],
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "savvi",
    "description": "Safe Anytime Valid Inference using e-processes and confidence sequences (Ramdas et al. 2023). Valid inference at any stopping time.",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": null,
    "github_url": "https://github.com/assuncaolfi/savvi",
    "url": "https://pypi.org/project/savvi/",
    "install": "pip install savvi",
    "tags": [
      "sequential testing",
      "A/B testing",
      "anytime valid"
    ],
    "best_for": "Always-valid sequential inference for experiments",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "statistical-inference",
      "hypothesis-testing"
    ],
    "summary": "Savvi is a Python package designed for safe anytime valid inference using e-processes and confidence sequences. It is particularly useful for researchers and data scientists conducting sequential testing and A/B testing.",
    "use_cases": [
      "Conducting A/B tests with valid inference",
      "Implementing sequential testing frameworks"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for valid inference",
      "how to perform A/B testing in python",
      "sequential testing library python",
      "safe inference python package",
      "confidence sequences in python",
      "statistical hypothesis testing python"
    ],
    "primary_use_cases": [
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Ramdas et al. (2023)",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "clusterbootstraps",
    "description": "Wild cluster bootstrap and pairs cluster bootstrap implementations for clustered standard errors.",
    "category": "Inference & Reporting Tools",
    "docs_url": null,
    "github_url": "https://github.com/BingkunLin/clusterbootstraps",
    "url": "https://pypi.org/project/clusterbootstraps/",
    "install": "pip install clusterbootstraps",
    "tags": [
      "bootstrap",
      "clustered errors",
      "inference"
    ],
    "best_for": "Alternative cluster bootstrap implementations",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The clusterbootstraps package provides implementations for wild cluster bootstrap and pairs cluster bootstrap methods, which are useful for calculating clustered standard errors. It is primarily used by data scientists and statisticians working with clustered data.",
    "use_cases": [],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for cluster bootstrap",
      "how to calculate clustered standard errors in python",
      "wild cluster bootstrap implementation python",
      "pairs cluster bootstrap python",
      "bootstrapping techniques in python",
      "clustered errors analysis python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "statsbombpy",
    "description": "Official Python API client for StatsBomb open data with 360 freeze-frame support for detailed soccer event analysis",
    "category": "Sports Analytics",
    "docs_url": "https://github.com/statsbomb/statsbombpy#readme",
    "github_url": "https://github.com/statsbomb/statsbombpy",
    "url": "https://github.com/statsbomb/statsbombpy",
    "install": "pip install statsbombpy",
    "tags": [
      "soccer",
      "football",
      "sports-analytics",
      "xG",
      "event-data"
    ],
    "best_for": "Soccer analytics, expected goals modeling, and tactical analysis",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "statsbombpy is an official Python API client for accessing StatsBomb open data, designed for detailed soccer event analysis with 360 freeze-frame support. It is used by analysts and developers interested in sports analytics, particularly in soccer.",
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for soccer analytics",
      "how to analyze soccer events in python",
      "statsbombpy documentation",
      "python API for StatsBomb data",
      "soccer event data analysis in python",
      "how to use statsbombpy"
    ],
    "use_cases": [
      "Analyzing soccer match events",
      "Extracting detailed statistics from StatsBomb data"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "csdid",
    "description": "Python adaptation of the R `did` package. Implements multi-period DiD with staggered treatment timing (Callaway & Sant\u2019Anna).",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": null,
    "github_url": "https://github.com/d2cml-ai/csdid",
    "url": "https://github.com/d2cml-ai/csdid",
    "install": "pip install csdid",
    "tags": [
      "DiD",
      "synthetic control",
      "RDD"
    ],
    "best_for": "Policy evaluation, natural experiments, quasi-experiments",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "program-evaluation"
    ],
    "summary": "The csdid package is a Python adaptation of the R `did` package, designed to implement multi-period Difference-in-Differences (DiD) with staggered treatment timing. It is primarily used by researchers and practitioners in program evaluation to analyze the effects of interventions over time.",
    "use_cases": [
      "Evaluating the impact of a policy change over multiple time periods",
      "Analyzing treatment effects with staggered implementation"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for difference-in-differences",
      "how to implement staggered treatment timing in python",
      "python causal inference tools",
      "multi-period DiD in python",
      "synthetic control methods in python",
      "RDD analysis in python"
    ],
    "primary_use_cases": [
      "multi-period DiD analysis",
      "program evaluation"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Callaway & Sant\u2019Anna (2021)",
    "related_packages": [
      "statsmodels",
      "causalml"
    ],
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "DynTxRegime",
    "description": "Comprehensive package for dynamic treatment regimes implementing Q-learning, value search, and outcome-weighted learning methods. Accompanies the textbook 'Dynamic Treatment Regimes' (Tsiatis et al., 2020).",
    "category": "Causal Inference (Dynamic Treatment)",
    "docs_url": "https://cran.r-project.org/web/packages/DynTxRegime/DynTxRegime.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=DynTxRegime",
    "install": "install.packages(\"DynTxRegime\")",
    "tags": [
      "dynamic-treatment",
      "Q-learning",
      "value-search",
      "reinforcement-learning",
      "personalized-medicine"
    ],
    "best_for": "Comprehensive dynamic treatment regimes with Q-learning and value search, from Tsiatis et al. (2020) textbook",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "DynTxRegime is a comprehensive R package designed for implementing dynamic treatment regimes using Q-learning, value search, and outcome-weighted learning methods. It is particularly useful for researchers and practitioners in the field of personalized medicine.",
    "use_cases": [],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for dynamic treatment regimes",
      "how to implement Q-learning in R",
      "outcome-weighted learning methods in R",
      "personalized medicine R package",
      "value search in dynamic treatment",
      "causal inference R tools"
    ],
    "primary_use_cases": [
      "dynamic treatment regimes",
      "Q-learning implementation"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Tsiatis et al. (2020)",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "tidyverse",
    "description": "Meta-package installing core tidyverse packages: ggplot2 (visualization), dplyr (manipulation), tidyr (tidying), readr (import), purrr (functional programming), tibble (data frames), stringr (strings), and forcats (factors).",
    "category": "Data Workflow",
    "docs_url": "https://www.tidyverse.org/",
    "github_url": "https://github.com/tidyverse/tidyverse",
    "url": "https://cran.r-project.org/package=tidyverse",
    "install": "install.packages(\"tidyverse\")",
    "tags": [
      "tidyverse",
      "data-science",
      "dplyr",
      "ggplot2",
      "meta-package"
    ],
    "best_for": "Core tidyverse ecosystem for consistent data science workflows",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The tidyverse is a collection of R packages designed for data science, providing tools for data manipulation, visualization, and import. It is widely used by data scientists and statisticians to streamline their workflow.",
    "use_cases": [
      "Creating visualizations with ggplot2",
      "Data manipulation with dplyr"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "R library for data manipulation",
      "how to visualize data in R",
      "R package for data import",
      "functional programming in R",
      "R tools for data frames",
      "tidyverse package overview"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "ggplot2",
      "dplyr",
      "tidyr",
      "readr",
      "purrr",
      "tibble",
      "stringr",
      "forcats"
    ],
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "Kats",
    "description": "Broad toolkit for time series analysis, including multivariate analysis, detection (outliers, change points, trends), feature extraction.",
    "category": "Time Series Econometrics",
    "docs_url": "https://facebookresearch.github.io/Kats/",
    "github_url": "https://github.com/facebookresearch/Kats",
    "url": "https://github.com/facebookresearch/Kats",
    "install": "pip install kats",
    "tags": [
      "time series",
      "econometrics"
    ],
    "best_for": "ARIMA, cointegration, VAR models",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "time-series",
      "econometrics"
    ],
    "summary": "Kats is a broad toolkit designed for time series analysis, providing functionalities for multivariate analysis, detection of outliers, change points, and trends, as well as feature extraction. It is used by data scientists and researchers working in econometrics and time series analysis.",
    "use_cases": [
      "Analyzing economic indicators over time",
      "Detecting anomalies in financial transactions"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for time series analysis",
      "how to detect outliers in time series python",
      "feature extraction in time series with python",
      "change point detection in python",
      "multivariate time series analysis python",
      "trends in time series data python"
    ],
    "primary_use_cases": [
      "outlier detection",
      "trend analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "statsmodels",
      "pandas"
    ],
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "gmm",
    "description": "Generalized Method of Moments estimation implementing two-step GMM, iterated GMM, and continuous updated estimator (CUE) with HAC covariance matrices. Supports linear and nonlinear moment conditions.",
    "category": "Instrumental Variables",
    "docs_url": "https://cran.r-project.org/web/packages/gmm/gmm.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=gmm",
    "install": "install.packages(\"gmm\")",
    "tags": [
      "GMM",
      "method-of-moments",
      "HAC",
      "instrumental-variables",
      "CUE"
    ],
    "best_for": "Generalized Method of Moments estimation with two-step, iterated, and CUE estimators",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "instrumental-variables"
    ],
    "summary": "The gmm package provides tools for Generalized Method of Moments estimation, including two-step GMM, iterated GMM, and continuous updated estimator (CUE) with HAC covariance matrices. It is used by statisticians and data scientists for estimating models with linear and nonlinear moment conditions.",
    "use_cases": [
      "Estimating parameters in econometric models",
      "Conducting hypothesis tests with GMM",
      "Analyzing time series data with HAC covariance"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for GMM estimation",
      "how to perform two-step GMM in R",
      "HAC covariance matrices in R",
      "GMM method-of-moments R",
      "instrumental variables estimation R",
      "CUE estimation in R"
    ],
    "primary_use_cases": [
      "parameter estimation using GMM",
      "hypothesis testing with moment conditions"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "UpliftML",
    "description": "Booking.com's enterprise uplift modeling via PySpark and H2O. Six meta-learners plus Uplift Random Forest with ROI-constrained optimization.",
    "category": "Uplift Modeling",
    "docs_url": null,
    "github_url": "https://github.com/bookingcom/upliftml",
    "url": "https://github.com/bookingcom/upliftml",
    "install": "pip install upliftml",
    "tags": [
      "uplift modeling",
      "treatment effects",
      "marketing"
    ],
    "best_for": "Enterprise-scale uplift with ROI optimization",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "uplift modeling",
      "treatment effects",
      "marketing"
    ],
    "summary": "UpliftML is a package designed for enterprise uplift modeling using PySpark and H2O. It is particularly useful for marketing professionals looking to optimize treatment effects and ROI.",
    "use_cases": [
      "Optimizing marketing campaigns",
      "Estimating treatment effects for customer segmentation"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for uplift modeling",
      "how to optimize marketing treatment effects in python",
      "UpliftML documentation",
      "examples of uplift modeling with PySpark",
      "H2O uplift modeling in python",
      "best practices for uplift random forest",
      "ROI-constrained optimization in marketing",
      "treatment effects analysis with UpliftML"
    ],
    "primary_use_cases": [
      "uplift modeling",
      "ROI-constrained optimization"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "PySpark",
      "H2O"
    ],
    "related_packages": [
      "CausalML",
      "EconML"
    ],
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "rdmulti",
    "description": "Provides tools for RD designs with multiple cutoffs or scores: rdmc() estimates pooled and cutoff-specific effects in multi-cutoff designs, rdmcplot() draws RD plots for multi-cutoff designs, and rdms() estimates effects in cumulative cutoffs or multi-score (geographic/boundary) designs.",
    "category": "Causal Inference (RDD)",
    "docs_url": "https://rdpackages.github.io/rdmulti/",
    "github_url": "https://github.com/rdpackages/rdmulti",
    "url": "https://cran.r-project.org/package=rdmulti",
    "install": "install.packages(\"rdmulti\")",
    "tags": [
      "multiple-cutoffs",
      "multi-score",
      "geographic-RD",
      "pooled-effects",
      "extrapolation"
    ],
    "best_for": "RDD with multiple cutoffs (e.g., different thresholds across regions) or multiple running variables (geographic boundaries), implementing Cattaneo, Titiunik & Vazquez-Bare (2020)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The rdmulti package provides tools for regression discontinuity designs with multiple cutoffs or scores. It is useful for researchers and practitioners in causal inference who need to estimate effects in complex RD designs.",
    "use_cases": [
      "Estimating pooled effects in multi-cutoff designs",
      "Visualizing regression discontinuity plots for multiple cutoffs"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for multiple cutoffs",
      "how to estimate effects in multi-score designs",
      "tools for regression discontinuity designs in R",
      "rdmc() function usage",
      "creating RD plots with rdmcplot()",
      "cumulative cutoffs analysis in R",
      "geographic RD analysis tools"
    ],
    "primary_use_cases": [
      "estimating effects in multi-cutoff designs",
      "drawing RD plots for multi-cutoff designs"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "NGBoost",
    "description": "Extends gradient boosting to probabilistic prediction, providing uncertainty estimates alongside point predictions. Built on scikit-learn.",
    "category": "Tree & Ensemble Methods for Prediction",
    "docs_url": "https://stanfordmlgroup.github.io/ngboost/",
    "github_url": "https://github.com/stanfordmlgroup/ngboost",
    "url": "https://github.com/stanfordmlgroup/ngboost",
    "install": "pip install ngboost",
    "tags": [
      "machine learning",
      "prediction"
    ],
    "best_for": "Random forests, gradient boosting, prediction tasks",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "NGBoost extends gradient boosting to probabilistic prediction, providing uncertainty estimates alongside point predictions. It is built on scikit-learn and is useful for practitioners looking to incorporate uncertainty into their predictive models.",
    "use_cases": [
      "Predicting outcomes with uncertainty estimates",
      "Enhancing models with probabilistic predictions"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for probabilistic prediction",
      "how to use NGBoost for uncertainty estimates",
      "gradient boosting with uncertainty in python",
      "scikit-learn extension for probabilistic models",
      "machine learning library for predictions with uncertainty",
      "NGBoost documentation",
      "install NGBoost in python"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "scikit-learn"
    ],
    "related_packages": [
      "XGBoost",
      "LightGBM"
    ],
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "Faer",
    "description": "State-of-the-art linear algebra for Rust with Cholesky, QR, SVD decompositions and multithreaded solvers for large systems.",
    "category": "Numerical Optimization & Computational Tools",
    "docs_url": "https://docs.rs/faer",
    "github_url": "https://github.com/sarah-quinones/faer-rs",
    "url": "https://crates.io/crates/faer",
    "install": "cargo add faer",
    "tags": [
      "rust",
      "linear algebra",
      "matrix",
      "performance"
    ],
    "best_for": "High-performance matrix decompositions for custom estimators",
    "language": "Rust",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Faer is a Rust library that provides state-of-the-art linear algebra capabilities, including Cholesky, QR, and SVD decompositions, along with multithreaded solvers for handling large systems. It is suitable for users needing high-performance matrix operations in Rust.",
    "use_cases": [
      "Solving large systems of equations",
      "Performing matrix decompositions for data analysis"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "rust library for linear algebra",
      "how to perform matrix decomposition in rust",
      "multithreaded solvers in rust",
      "best rust libraries for numerical optimization",
      "linear algebra performance in rust",
      "rust matrix operations library"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "DeclareDesign",
    "description": "Ex ante experimental design declaration and diagnosis. Enables researchers to formally describe their research design, diagnose statistical properties via simulation, and improve designs before data collection.",
    "category": "Experimental Design",
    "docs_url": "https://declaredesign.org/",
    "github_url": "https://github.com/DeclareDesign/DeclareDesign",
    "url": "https://cran.r-project.org/package=DeclareDesign",
    "install": "install.packages(\"DeclareDesign\")",
    "tags": [
      "experimental-design",
      "pre-registration",
      "power-analysis",
      "simulation",
      "design-diagnosis"
    ],
    "best_for": "Ex ante experimental design declaration and diagnosis via simulation",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "experimental-design"
    ],
    "summary": "DeclareDesign is a package that allows researchers to formally describe their research design and diagnose its statistical properties through simulation. It is primarily used by researchers looking to improve their experimental designs before data collection.",
    "use_cases": [
      "Improving experimental designs before data collection",
      "Diagnosing statistical properties of research designs"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for experimental design",
      "how to diagnose research design in R",
      "pre-registration tools for experiments",
      "power analysis in R",
      "simulation for experimental design",
      "design diagnosis R package"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "rddensity",
    "description": "Implements manipulation testing (density discontinuity testing) procedures using local polynomial density estimators to detect perfect self-selection around a cutoff. Provides rddensity() for hypothesis testing, rdbwdensity() for bandwidth selection, and rdplotdensity() for density plots with confidence bands.",
    "category": "Causal Inference (RDD)",
    "docs_url": "https://rdpackages.github.io/rddensity/",
    "github_url": "https://github.com/rdpackages/rddensity",
    "url": "https://cran.r-project.org/package=rddensity",
    "install": "install.packages(\"rddensity\")",
    "tags": [
      "manipulation-testing",
      "density-discontinuity",
      "McCrary-test",
      "falsification",
      "sorting"
    ],
    "best_for": "Testing RDD validity by detecting bunching/manipulation around the cutoff (McCrary-type tests), implementing Cattaneo, Jansson & Ma (2020)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "rddensity implements manipulation testing procedures using local polynomial density estimators to detect perfect self-selection around a cutoff. It is used primarily by researchers and practitioners in causal inference to test hypotheses and visualize density plots.",
    "use_cases": [
      "Testing for self-selection around a cutoff",
      "Visualizing density plots with confidence bands"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for manipulation testing",
      "how to perform density discontinuity testing in R",
      "R density plots with confidence bands",
      "local polynomial density estimators in R",
      "hypothesis testing for self-selection in R",
      "bandwidth selection in R for density tests"
    ],
    "primary_use_cases": [
      "density discontinuity testing",
      "hypothesis testing"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "CausalMotifs",
    "description": "Meta's library for estimating heterogeneous spillover effects in A/B tests. Handles network interference.",
    "category": "Interference & Spillovers",
    "docs_url": "https://github.com/facebookresearch/CausalMotifs",
    "github_url": "https://github.com/facebookresearch/CausalMotifs",
    "url": "https://github.com/facebookresearch/CausalMotifs",
    "install": "pip install causal-motifs",
    "tags": [
      "network interference",
      "spillovers",
      "A/B testing"
    ],
    "best_for": "Spillover effects in social networks",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "network-analysis"
    ],
    "summary": "CausalMotifs is a library designed to estimate heterogeneous spillover effects in A/B tests, particularly in the presence of network interference. It is useful for researchers and data scientists conducting experiments where interactions between subjects may affect outcomes.",
    "use_cases": [
      "Estimating spillover effects in marketing campaigns",
      "Analyzing the impact of social networks on treatment effects"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for estimating spillover effects",
      "how to analyze A/B tests with network interference in python",
      "CausalMotifs documentation",
      "best practices for A/B testing in python",
      "network interference analysis tools",
      "spillover effects in experiments python"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "torchonometrics",
    "description": "Econometrics implementations in PyTorch. Leverages autodiff and GPU acceleration for econometric methods.",
    "category": "Numerical Optimization & Computational Tools",
    "docs_url": null,
    "github_url": "https://github.com/apoorvalal/torchonometrics",
    "url": "https://github.com/apoorvalal/torchonometrics",
    "install": "GitHub Repository",
    "tags": [
      "optimization",
      "computation",
      "PyTorch"
    ],
    "best_for": "GPU-accelerated econometrics with PyTorch autodiff",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "econometrics",
      "optimization",
      "deep-learning"
    ],
    "summary": "torchonometrics provides econometric methods implemented in PyTorch, utilizing automatic differentiation and GPU acceleration. It is designed for users interested in applying econometric techniques in a computationally efficient manner.",
    "use_cases": [
      "Estimating econometric models",
      "Running simulations for econometric analysis"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for econometrics",
      "how to do econometric analysis in python",
      "PyTorch econometrics package",
      "optimizing econometric models with PyTorch",
      "GPU acceleration for econometrics in python",
      "autodiff econometrics library"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "PyTorch"
    ],
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "pyliferisk",
    "description": "Python library for life actuarial calculations including commutation functions, life annuities, and insurance present values",
    "category": "Insurance & Actuarial",
    "docs_url": "https://github.com/franciscogarate/pyliferisk",
    "github_url": "https://github.com/franciscogarate/pyliferisk",
    "url": "https://github.com/franciscogarate/pyliferisk",
    "install": "pip install pyliferisk",
    "tags": [
      "life-insurance",
      "actuarial",
      "annuities",
      "mortality",
      "commutation-functions"
    ],
    "best_for": "Life insurance calculations, annuity valuation, and actuarial education in Python",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "pyliferisk is a Python library designed for life actuarial calculations, including commutation functions, life annuities, and insurance present values. It is primarily used by actuaries and data scientists working in the insurance sector.",
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for life actuarial calculations",
      "how to calculate life annuities in python",
      "commutation functions in python",
      "insurance present values python library",
      "actuarial calculations python",
      "mortality calculations in python"
    ],
    "use_cases": [
      "Calculating life annuities for retirement planning",
      "Performing commutation functions for insurance policies"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "wooldridge",
    "description": "All 115 datasets from Wooldridge's 'Introductory Econometrics: A Modern Approach' (7th edition). Includes wage equations, crime data, housing prices, and classic econometrics teaching examples.",
    "category": "Datasets",
    "docs_url": "https://cran.r-project.org/web/packages/wooldridge/wooldridge.pdf",
    "github_url": "https://github.com/JustinMShea/wooldridge",
    "url": "https://cran.r-project.org/package=wooldridge",
    "install": "install.packages(\"wooldridge\")",
    "tags": [
      "datasets",
      "textbook",
      "teaching",
      "Wooldridge",
      "econometrics"
    ],
    "best_for": "115 datasets from Wooldridge's 'Introductory Econometrics' for teaching and examples",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "econometrics",
      "datasets",
      "teaching"
    ],
    "summary": "The wooldridge package provides access to all 115 datasets from Wooldridge's 'Introductory Econometrics: A Modern Approach' (7th edition). It is primarily used by students and educators in econometrics for teaching and learning purposes.",
    "use_cases": [
      "Teaching econometrics using real datasets",
      "Analyzing wage equations for educational purposes"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "R library for econometrics datasets",
      "how to access Wooldridge datasets in R",
      "datasets for teaching econometrics",
      "Wooldridge econometrics examples in R",
      "R package for wage equations",
      "crime data analysis in R",
      "housing prices datasets in R",
      "econometrics teaching resources in R"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "SHAP",
    "description": "Model-agnostic explainability using Shapley values for any ML model, essential for actuarial model interpretability and regulatory compliance",
    "category": "Insurance & Actuarial",
    "docs_url": "https://shap.readthedocs.io/",
    "github_url": "https://github.com/slundberg/shap",
    "url": "https://github.com/slundberg/shap",
    "install": "pip install shap",
    "tags": [
      "explainability",
      "interpretability",
      "Shapley-values",
      "model-agnostic",
      "feature-importance"
    ],
    "best_for": "Explaining insurance pricing models, regulatory compliance, and model governance",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "explainability",
      "interpretability"
    ],
    "summary": "SHAP is a Python package that provides model-agnostic explainability using Shapley values for any machine learning model. It is essential for actuarial model interpretability and regulatory compliance, making it useful for data scientists and actuaries.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for model explainability",
      "how to interpret machine learning models in python",
      "SHAP values in python",
      "explainability for ML models",
      "actuarial model interpretability python",
      "regulatory compliance ML python"
    ],
    "use_cases": [
      "Explaining predictions of insurance models",
      "Ensuring compliance with regulatory standards"
    ],
    "primary_use_cases": [
      "model interpretability",
      "feature importance analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "LIME",
      "Eli5"
    ],
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "PySensemakr",
    "description": "Implements Cinelli-Hazlett framework for assessing robustness to unobserved confounding. Computes confounder strength needed to invalidate results.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://github.com/carloscinelli/PySensemakr",
    "github_url": "https://github.com/carloscinelli/PySensemakr",
    "url": "https://github.com/carloscinelli/PySensemakr",
    "install": "pip install pysensemakr",
    "tags": [
      "causal inference",
      "sensitivity analysis",
      "robustness"
    ],
    "best_for": "Sensitivity analysis with publication-ready contour plots",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "sensitivity-analysis",
      "robustness"
    ],
    "summary": "PySensemakr implements the Cinelli-Hazlett framework to assess robustness against unobserved confounding in causal inference studies. It is useful for researchers and data scientists who need to evaluate the strength of confounders that could potentially invalidate their results.",
    "use_cases": [
      "Assessing the robustness of causal estimates",
      "Evaluating the impact of unobserved confounding on study results"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to perform sensitivity analysis in python",
      "robustness assessment in python",
      "Cinelli-Hazlett framework implementation",
      "confounder strength analysis python",
      "evaluate unobserved confounding python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "fastdid",
    "description": "High-performance implementation of Callaway & Sant'Anna estimators optimized for large datasets with millions of observations. Reduces computation time from hours to seconds while supporting time-varying covariates and multiple events per unit.",
    "category": "Causal Inference (DiD)",
    "docs_url": "https://tsailintung.github.io/fastdid",
    "github_url": "https://github.com/TsaiLintung/fastdid",
    "url": "https://cran.r-project.org/package=fastdid",
    "install": "install.packages(\"fastdid\")",
    "tags": [
      "high-performance",
      "large-scale",
      "staggered-DiD",
      "time-varying-covariates",
      "fast-computation"
    ],
    "best_for": "Large-scale applications where standard did package is computationally prohibitive, with support for time-varying covariates",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "fastdid is a high-performance implementation of Callaway & Sant'Anna estimators optimized for large datasets. It is designed for researchers and practitioners who need to conduct causal inference analysis efficiently.",
    "use_cases": [
      "Estimating treatment effects in large observational studies",
      "Analyzing the impact of policy changes over time"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for causal inference",
      "how to implement Callaway & Sant'Anna estimators in R",
      "fast computation for DiD analysis in R",
      "large-scale causal inference R package",
      "time-varying covariates in R",
      "high-performance DiD estimators R"
    ],
    "primary_use_cases": [
      "causal forest estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "hdm",
    "description": "High-dimensional statistical methods featuring heteroscedasticity-robust LASSO with theoretically-grounded penalty selection, post-double-selection inference, and treatment effect estimation under sparsity assumptions for high-dimensional controls.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://cran.r-project.org/web/packages/hdm/vignettes/hdm.html",
    "github_url": "https://github.com/MartinSpindler/hdm",
    "url": "https://cran.r-project.org/package=hdm",
    "install": "install.packages(\"hdm\")",
    "tags": [
      "lasso",
      "post-double-selection",
      "high-dimensional",
      "instrumental-variables",
      "sparsity"
    ],
    "best_for": "Post-double-selection LASSO inference and treatment effect estimation when the true model is sparse, implementing Belloni, Chernozhukov & Hansen (2014)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "high-dimensional"
    ],
    "summary": "The hdm package provides high-dimensional statistical methods that focus on heteroscedasticity-robust LASSO, enabling users to perform post-double-selection inference and treatment effect estimation under sparsity assumptions. It is primarily used by researchers and practitioners in causal inference and statistics.",
    "use_cases": [
      "Estimating treatment effects in high-dimensional settings",
      "Conducting post-double-selection inference for causal analysis"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for high-dimensional LASSO",
      "how to perform treatment effect estimation in R",
      "heteroscedasticity-robust methods in R",
      "post-double-selection inference R package",
      "sparsity assumptions in causal inference",
      "instrumental variables in high-dimensional settings"
    ],
    "primary_use_cases": [
      "treatment effect estimation",
      "post-double-selection inference"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "Robyn",
    "description": "Meta's AI/ML-powered Marketing Mix Modeling package with ridge regression and multi-objective optimization",
    "category": "Marketing Analytics",
    "docs_url": "https://facebookexperimental.github.io/Robyn/",
    "github_url": "https://github.com/facebookexperimental/Robyn",
    "url": "https://facebookexperimental.github.io/Robyn/",
    "install": "remotes::install_github('facebookexperimental/Robyn/R')",
    "tags": [
      "MMM",
      "marketing mix",
      "budget optimization",
      "Meta"
    ],
    "best_for": "Automated marketing mix modeling with budget allocation recommendations",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "marketing-analytics"
    ],
    "summary": "Robyn is an AI/ML-powered Marketing Mix Modeling package developed by Meta, designed to optimize marketing budgets using ridge regression and multi-objective optimization. It is primarily used by marketers and data scientists to analyze the effectiveness of marketing strategies.",
    "use_cases": [
      "Optimizing marketing budget allocation",
      "Evaluating the impact of marketing channels"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for marketing mix modeling",
      "how to optimize marketing budget in R",
      "R package for ridge regression",
      "best practices for marketing analytics in R"
    ],
    "primary_use_cases": [
      "budget optimization"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "PyMC-Marketing",
      "GeoLift"
    ],
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "actuar",
    "description": "Actuarial science functions for R including loss distributions, credibility theory, ruin theory, and simulation of compound models",
    "category": "Insurance & Actuarial",
    "docs_url": "https://cran.r-project.org/web/packages/actuar/vignettes/",
    "github_url": "https://gitlab.com/vigou3/actuar",
    "url": "https://cran.r-project.org/package=actuar",
    "install": "install.packages(\"actuar\")",
    "tags": [
      "actuarial",
      "loss-distributions",
      "credibility",
      "ruin-theory",
      "aggregate-claims"
    ],
    "best_for": "Core actuarial calculations in R including loss modeling and credibility premium",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "actuarial",
      "statistics"
    ],
    "summary": "The 'actuar' package provides functions for actuarial science in R, focusing on loss distributions, credibility theory, ruin theory, and the simulation of compound models. It is primarily used by actuaries and data scientists working in the insurance industry.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for actuarial science",
      "how to simulate compound models in R",
      "loss distributions in R",
      "credibility theory functions in R",
      "ruin theory analysis in R",
      "actuarial functions for insurance in R"
    ],
    "use_cases": [
      "Modeling insurance claims",
      "Estimating risk in insurance portfolios"
    ],
    "primary_use_cases": [
      "loss distribution modeling",
      "credibility theory application"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "sktime",
    "description": "Unified framework for various time series tasks, including forecasting with classical, ML, and deep learning models.",
    "category": "Time Series Forecasting",
    "docs_url": "https://www.sktime.net/en/latest/",
    "github_url": "https://github.com/sktime/sktime",
    "url": "https://github.com/sktime/sktime",
    "install": "pip install sktime",
    "tags": [
      "forecasting",
      "time series",
      "machine learning"
    ],
    "best_for": "Prediction, demand forecasting, trend analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "time-series",
      "machine-learning"
    ],
    "summary": "sktime is a unified framework designed for various time series tasks, including forecasting using classical, machine learning, and deep learning models. It is used by data scientists and researchers working with time series data.",
    "use_cases": [
      "Forecasting stock prices",
      "Predicting sales trends"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for time series forecasting",
      "how to forecast time series in python",
      "time series analysis with machine learning python",
      "sktime package for time series",
      "deep learning for time series forecasting python",
      "classical forecasting methods in python"
    ],
    "primary_use_cases": [
      "time series forecasting"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "statsmodels",
      "prophet"
    ],
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "AER",
    "description": "Companion package to 'Applied Econometrics with R' (Kleiber & Zeileis) plus datasets from Stock & Watson. Provides ivreg() for instrumental variables, tobit(), and econometric testing functions.",
    "category": "Datasets",
    "docs_url": "https://cran.r-project.org/web/packages/AER/AER.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=AER",
    "install": "install.packages(\"AER\")",
    "tags": [
      "datasets",
      "textbook",
      "instrumental-variables",
      "Stock-Watson",
      "Kleiber-Zeileis"
    ],
    "best_for": "Datasets and functions from 'Applied Econometrics with R' plus Stock & Watson data",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "econometrics",
      "instrumental-variables"
    ],
    "summary": "AER is a companion package to 'Applied Econometrics with R' that provides functions for instrumental variables and econometric testing. It is used by students and practitioners in econometrics for analysis and testing.",
    "use_cases": [
      "Performing instrumental variable regression",
      "Conducting tobit analysis",
      "Testing econometric hypotheses"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for instrumental variables",
      "how to perform econometric testing in R",
      "datasets for applied econometrics",
      "R package for tobit model",
      "AER R package documentation",
      "how to use ivreg in R",
      "Stock-Watson datasets in R"
    ],
    "primary_use_cases": [
      "instrumental variable regression",
      "tobit model analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "Apache Sedona",
    "description": "Distributed spatial analytics engine (formerly GeoSpark) with spatial SQL, K-NN joins, and range queries for spatial econometrics.",
    "category": "Spatial Econometrics",
    "docs_url": "https://sedona.apache.org/",
    "github_url": "https://github.com/apache/sedona",
    "url": "https://github.com/apache/sedona",
    "install": "pip install apache-sedona",
    "tags": [
      "spark",
      "spatial",
      "GIS",
      "distributed"
    ],
    "best_for": "Constructing spatial weight matrices and distance-based instruments at scale",
    "language": "Spark",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Apache Sedona is a distributed spatial analytics engine that allows users to perform spatial SQL queries, K-NN joins, and range queries, making it suitable for spatial econometrics. It is used by data scientists and researchers working with spatial data.",
    "use_cases": [
      "Analyzing spatial data for urban planning",
      "Performing K-NN joins for location-based services"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for spatial analytics",
      "how to perform K-NN joins in Spark",
      "spatial SQL queries in Apache Sedona",
      "distributed GIS tools",
      "range queries for spatial econometrics",
      "Apache Sedona documentation",
      "spatial data analysis with Spark"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "Spark"
    ],
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "HypoRS",
    "description": "Hypothesis testing library for Rust with T-tests, Z-tests, ANOVA, Chi-square, designed to work seamlessly with Polars DataFrames.",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": "https://lib.rs/crates/hypors",
    "github_url": "https://github.com/astronights/hypors",
    "url": "https://crates.io/crates/hypors",
    "install": "cargo add hypors",
    "tags": [
      "rust",
      "hypothesis testing",
      "t-test",
      "ANOVA",
      "polars"
    ],
    "best_for": "Statistical hypothesis testing with Polars integration",
    "language": "Rust",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "HypoRS is a hypothesis testing library for Rust that provides T-tests, Z-tests, ANOVA, and Chi-square functionalities. It is designed to work seamlessly with Polars DataFrames, making it suitable for data analysis tasks.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "rust library for hypothesis testing",
      "how to perform T-tests in Rust",
      "ANOVA in Rust with Polars",
      "Chi-square test in Rust",
      "Z-tests using HypoRS",
      "statistical analysis in Rust"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "Polars"
    ],
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "TS-Flint",
    "description": "Two Sigma's time-series library for Spark with optimized temporal joins, as-of joins, and distributed OLS for high-frequency data.",
    "category": "Time Series Econometrics",
    "docs_url": "https://ts-flint.readthedocs.io/",
    "github_url": "https://github.com/twosigma/flint",
    "url": "https://github.com/twosigma/flint",
    "install": "pip install ts-flint",
    "tags": [
      "spark",
      "time series",
      "temporal joins",
      "fintech"
    ],
    "best_for": "High-frequency financial data with inexact timestamp matching",
    "language": "Spark",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "time-series"
    ],
    "summary": "TS-Flint is a time-series library designed for Spark that provides optimized temporal joins, as-of joins, and distributed OLS for handling high-frequency data. It is primarily used in fintech applications for efficient data analysis.",
    "use_cases": [
      "Analyzing high-frequency financial data",
      "Performing temporal joins on large datasets"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for time series analysis",
      "how to perform temporal joins in Spark",
      "best practices for high-frequency data analysis",
      "distributed OLS in Spark",
      "fintech time series library",
      "using Spark for temporal data",
      "optimizing joins in Spark",
      "Two Sigma time series library"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "Spark"
    ],
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "survminer",
    "description": "Visualization tools for survival analysis in R with publication-ready Kaplan-Meier plots, risk tables, and Cox model forest plots",
    "category": "Insurance & Actuarial",
    "docs_url": "https://rpkgs.datanovia.com/survminer/",
    "github_url": "https://github.com/kassambara/survminer",
    "url": "https://cran.r-project.org/package=survminer",
    "install": "install.packages(\"survminer\")",
    "tags": [
      "survival-visualization",
      "Kaplan-Meier-plots",
      "ggplot2",
      "publication-ready",
      "risk-tables"
    ],
    "best_for": "Creating publication-quality survival curves and risk tables for actuarial reports",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The survminer package provides visualization tools for survival analysis in R, enabling users to create publication-ready Kaplan-Meier plots, risk tables, and Cox model forest plots. It is primarily used by statisticians and data scientists working in fields related to survival analysis.",
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for survival analysis visualization",
      "how to create Kaplan-Meier plots in R",
      "risk tables in R",
      "Cox model forest plots R",
      "visualization tools for survival analysis R",
      "publication-ready plots in R"
    ],
    "use_cases": [
      "Creating Kaplan-Meier survival curves",
      "Generating risk tables for clinical studies"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "eiapy",
    "description": "Python wrapper for the EIA Open Data API. Access generation, consumption, prices, and other energy data programmatically.",
    "category": "Energy & Utilities Economics",
    "docs_url": "https://github.com/rneal3/eiapy",
    "github_url": "https://github.com/rneal3/eiapy",
    "url": "https://github.com/rneal3/eiapy",
    "install": "pip install eiapy",
    "tags": [
      "EIA",
      "API",
      "energy data"
    ],
    "best_for": "Programmatic access to EIA energy data",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "eiapy is a Python wrapper for the EIA Open Data API that allows users to programmatically access various energy data, including generation, consumption, and prices. It is useful for anyone interested in analyzing energy-related economic data.",
    "use_cases": [
      "Accessing energy generation data",
      "Analyzing energy consumption trends"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for EIA data",
      "how to access energy data in python",
      "EIA Open Data API python wrapper",
      "energy consumption data python",
      "python energy prices API",
      "programmatic access to EIA data"
    ],
    "primary_use_cases": [
      "Data access"
    ],
    "api_complexity": "simple",
    "related_packages": [
      "gridstatus",
      "PUDL"
    ],
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "eiapy",
    "description": "Python wrapper for the U.S. Energy Information Administration (EIA) API",
    "category": "Data Access",
    "docs_url": "https://github.com/mra1385/eiapy",
    "github_url": "https://github.com/mra1385/eiapy",
    "url": "https://github.com/mra1385/eiapy",
    "install": "pip install eiapy",
    "tags": [
      "EIA",
      "API",
      "energy data",
      "government data"
    ],
    "best_for": "Accessing EIA energy statistics programmatically",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "eiapy is a Python wrapper for the U.S. Energy Information Administration (EIA) API, allowing users to easily access and retrieve energy data from the EIA. It is useful for researchers, analysts, and developers interested in energy statistics and government data.",
    "use_cases": [],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for EIA API",
      "how to access energy data in python",
      "EIA data retrieval python",
      "energy information API python",
      "government energy data python",
      "python wrapper for EIA"
    ],
    "primary_use_cases": [
      "Data access",
      "Energy statistics"
    ],
    "api_complexity": "simple",
    "related_packages": [
      "pandas",
      "requests"
    ],
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "flexsurv",
    "description": "Flexible parametric survival models including spline-based hazards, multi-state models, and cure models for complex time-to-event data",
    "category": "Insurance & Actuarial",
    "docs_url": "https://chjackson.github.io/flexsurv/",
    "github_url": "https://github.com/chjackson/flexsurv",
    "url": "https://cran.r-project.org/package=flexsurv",
    "install": "install.packages(\"flexsurv\")",
    "tags": [
      "flexible-survival",
      "parametric-models",
      "splines",
      "multi-state",
      "cure-models"
    ],
    "best_for": "Advanced survival modeling with flexible hazard shapes and multi-state transitions",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "survival-analysis",
      "parametric-models"
    ],
    "summary": "flexsurv provides flexible parametric survival models that can handle complex time-to-event data, including spline-based hazards and multi-state models. It is primarily used by statisticians and data scientists working in fields like insurance and actuarial science.",
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for flexible survival models",
      "how to model time-to-event data in R",
      "R spline-based hazards package",
      "multi-state models in R",
      "cure models in R",
      "flexsurv documentation",
      "flexsurv examples"
    ],
    "use_cases": [
      "Analyzing survival data in clinical trials",
      "Modeling time-to-event data in insurance"
    ],
    "primary_use_cases": [
      "spline-based hazard modeling",
      "multi-state survival analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "peacesciencer",
    "description": "R package for generating dyad-year and state-year datasets with conflict, democracy, alliance, and contiguity data",
    "category": "Defense Research",
    "docs_url": "http://svmiller.com/peacesciencer/",
    "github_url": "https://github.com/svmiller/peacesciencer",
    "url": "http://svmiller.com/peacesciencer/",
    "install": "install.packages('peacesciencer')",
    "tags": [
      "conflict data",
      "COW-MID",
      "UCDP",
      "dyad-year"
    ],
    "best_for": "Constructing datasets for quantitative defense and peace research",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The peacesciencer R package generates dyad-year and state-year datasets that include data on conflict, democracy, alliances, and contiguity. It is primarily used by researchers and analysts in the field of defense research.",
    "use_cases": [
      "Analyzing conflict patterns over time",
      "Studying the relationship between democracy and conflict"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "R package for conflict data",
      "how to generate dyad-year datasets in R",
      "R package for democracy data",
      "state-year datasets in R",
      "R package for alliance data",
      "how to analyze contiguity data in R"
    ],
    "primary_use_cases": [
      "Dataset construction",
      "Conflict research"
    ],
    "api_complexity": "simple",
    "related_packages": [
      "countrycode",
      "states"
    ],
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "countrycode",
    "description": "R package for converting between country naming and coding conventions essential for merging defense datasets",
    "category": "Data Wrangling",
    "docs_url": "https://vincentarelbundock.github.io/countrycode/",
    "github_url": "https://github.com/vincentarelbundock/countrycode",
    "url": "https://vincentarelbundock.github.io/countrycode/",
    "install": "install.packages('countrycode')",
    "tags": [
      "country codes",
      "data merging",
      "ISO",
      "COW"
    ],
    "best_for": "Merging datasets using different country coding schemes (ISO, COW, SIPRI)",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The 'countrycode' R package facilitates the conversion between different country naming and coding conventions, which is essential for merging defense datasets. It is primarily used by researchers and analysts working with international data.",
    "use_cases": [
      "Merging defense datasets from different sources",
      "Standardizing country codes for analysis"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "R package for converting country codes",
      "how to merge defense datasets in R",
      "country naming conventions in R",
      "R library for ISO country codes",
      "data merging techniques in R",
      "R package for country coding"
    ],
    "primary_use_cases": [
      "Country code conversion",
      "Data merging"
    ],
    "api_complexity": "simple",
    "related_packages": [
      "peacesciencer",
      "states"
    ],
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "fortuna",
    "description": "AWS library for uncertainty quantification in deep learning. Bayesian and conformal methods.",
    "category": "Conformal Prediction & Uncertainty",
    "docs_url": "https://aws-fortuna.readthedocs.io/",
    "github_url": "https://github.com/awslabs/fortuna",
    "url": "https://github.com/awslabs/fortuna",
    "install": "pip install fortuna",
    "tags": [
      "uncertainty",
      "Bayesian",
      "deep learning"
    ],
    "best_for": "Deep learning uncertainty quantification",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "bayesian",
      "uncertainty"
    ],
    "summary": "Fortuna is an AWS library designed for uncertainty quantification in deep learning, utilizing Bayesian and conformal methods. It is primarily used by data scientists and researchers working on uncertainty in machine learning models.",
    "use_cases": [
      "Quantifying uncertainty in deep learning models",
      "Implementing Bayesian methods for model predictions"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for uncertainty quantification",
      "how to implement Bayesian methods in python",
      "AWS library for deep learning uncertainty",
      "conformal prediction in python",
      "deep learning uncertainty quantification",
      "Bayesian deep learning library"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "stochvol",
    "description": "Efficient Bayesian estimation of stochastic volatility (SV) models using MCMC.",
    "category": "State Space & Volatility Models",
    "docs_url": "https://stochvol.readthedocs.io/en/latest/",
    "github_url": "https://github.com/gregorkastner/stochvol",
    "url": "https://github.com/gregorkastner/stochvol",
    "install": "pip install stochvol",
    "tags": [
      "volatility",
      "state space",
      "Bayesian"
    ],
    "best_for": "GARCH, stochastic volatility, Kalman filtering",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "bayesian",
      "volatility",
      "state space"
    ],
    "summary": "The stochvol package provides efficient Bayesian estimation of stochastic volatility models using MCMC techniques. It is primarily used by statisticians and data scientists working with financial time series data.",
    "use_cases": [
      "Estimating volatility in financial time series",
      "Modeling asset prices with stochastic volatility"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for stochastic volatility",
      "how to estimate volatility in python",
      "bayesian estimation of SV models",
      "MCMC for volatility models in python",
      "state space models in python",
      "efficient Bayesian estimation python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "latenetwork",
    "description": "Handles both noncompliance AND network interference of unknown form following Hoshino and Yanagi (2023 JASA). Provides valid inference when treatment effects spill over through network connections.",
    "category": "Causal Inference (Interference)",
    "docs_url": "https://cran.r-project.org/web/packages/latenetwork/latenetwork.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=latenetwork",
    "install": "install.packages(\"latenetwork\")",
    "tags": [
      "network-interference",
      "noncompliance",
      "LATE",
      "spillovers",
      "IV"
    ],
    "best_for": "LATE estimation with network interference and noncompliance, implementing Hoshino & Yanagi (2023 JASA)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "network-interference"
    ],
    "summary": "The latenetwork package handles noncompliance and network interference of unknown form, providing valid inference when treatment effects spill over through network connections. It is useful for researchers and practitioners in causal inference who need to account for these complexities in their analyses.",
    "use_cases": [
      "Analyzing treatment effects in social networks",
      "Estimating causal effects in experimental designs with noncompliance"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for network interference",
      "how to handle noncompliance in R",
      "R causal inference package",
      "spillover effects analysis in R",
      "network effects in treatment studies",
      "Hoshino and Yanagi causal inference R package"
    ],
    "primary_use_cases": [
      "valid inference in causal studies",
      "analyzing spillover effects"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Hoshino and Yanagi (2023)",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "chainladder-python",
    "description": "Python library for actuarial reserving implementing chain-ladder, Bornhuetter-Ferguson, Cape Cod, and stochastic methods for loss reserve estimation",
    "category": "Insurance & Actuarial",
    "docs_url": "https://chainladder-python.readthedocs.io/",
    "github_url": "https://github.com/casact/chainladder-python",
    "url": "https://github.com/casact/chainladder-python",
    "install": "pip install chainladder",
    "tags": [
      "actuarial",
      "reserving",
      "chain-ladder",
      "loss-triangles",
      "P&C-insurance"
    ],
    "best_for": "P&C insurance loss reserving, IBNR estimation, and actuarial analysis in Python",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "actuarial",
      "loss-reserving"
    ],
    "summary": "chainladder-python is a Python library designed for actuarial reserving, implementing various methods for loss reserve estimation. It is used by actuaries and data scientists in the insurance industry to analyze and predict loss reserves.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for actuarial reserving",
      "how to estimate loss reserves in python",
      "chain-ladder method implementation in python",
      "Bornhuetter-Ferguson method python",
      "Cape Cod reserving python",
      "stochastic methods for loss reserve estimation python"
    ],
    "use_cases": [
      "Estimating loss reserves for property and casualty insurance",
      "Analyzing loss triangles for actuarial reports"
    ],
    "primary_use_cases": [
      "chain-ladder estimation",
      "Bornhuetter-Ferguson estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "fhirclient",
    "description": "Official SMART on FHIR Python client. OAuth 2.0 authentication, resource CRUD operations, and search. Essential for building apps that connect to EHR systems.",
    "category": "Healthcare Economics & Health-Tech",
    "docs_url": "https://github.com/smart-on-fhir/client-py",
    "github_url": "https://github.com/smart-on-fhir/client-py",
    "url": "https://github.com/smart-on-fhir/client-py",
    "install": "pip install fhirclient",
    "tags": [
      "FHIR",
      "interoperability",
      "EHR",
      "API"
    ],
    "best_for": "Building SMART on FHIR applications that connect to EHRs",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The fhirclient is an official SMART on FHIR Python client that facilitates OAuth 2.0 authentication, resource CRUD operations, and search capabilities. It is essential for developers building applications that connect to Electronic Health Record (EHR) systems.",
    "use_cases": [
      "Connecting to EHR systems",
      "Building health-tech applications"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for FHIR",
      "how to connect to EHR in python",
      "SMART on FHIR python client",
      "OAuth 2.0 authentication in python",
      "FHIR API interactions in python",
      "CRUD operations with FHIR in python"
    ],
    "primary_use_cases": [
      "Healthcare interoperability"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "fhir.resources",
      "fhir-py"
    ],
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "CausalImpact",
    "description": "Python port of Google's R package for estimating causal effects of interventions on time series using Bayesian structural time-series models.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": "https://google.github.io/CausalImpact/CausalImpact/CausalImpact.html",
    "github_url": "https://github.com/tcassou/causal_impact",
    "url": "https://github.com/tcassou/causal_impact",
    "install": "pip install causalimpact",
    "tags": [
      "DiD",
      "synthetic control",
      "RDD",
      "Bayesian"
    ],
    "best_for": "Policy evaluation, natural experiments, quasi-experiments",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "time-series",
      "bayesian"
    ],
    "summary": "CausalImpact is a Python port of Google's R package designed to estimate causal effects of interventions on time series data using Bayesian structural time-series models. It is primarily used by data scientists and researchers interested in evaluating the impact of specific interventions over time.",
    "use_cases": [
      "Evaluating the impact of a marketing campaign on sales",
      "Assessing the effect of a policy change on economic indicators"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to estimate causal effects in python",
      "time series analysis with python",
      "bayesian structural time series python",
      "synthetic control methods in python",
      "RDD analysis in python"
    ],
    "primary_use_cases": [
      "causal impact analysis",
      "intervention effect estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "related_packages": [
      "CausalImpact-R"
    ],
    "model_score": 0.0002
  },
  {
    "name": "CausalImpact",
    "description": "Google's Bayesian structural time-series package for measuring intervention effects on time series",
    "category": "Causal Inference",
    "docs_url": "https://google.github.io/CausalImpact/",
    "github_url": "https://github.com/google/CausalImpact",
    "url": "https://google.github.io/CausalImpact/",
    "install": "install.packages('CausalImpact')",
    "tags": [
      "time series",
      "Bayesian",
      "intervention",
      "Google"
    ],
    "best_for": "Measuring causal impact of ad campaigns on time-series outcomes",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "time-series",
      "bayesian"
    ],
    "summary": "CausalImpact is a Bayesian structural time-series package developed by Google for measuring the effects of interventions on time series data. It is used by data scientists and analysts to assess the impact of changes in business strategies or policies on key performance indicators.",
    "use_cases": [
      "Evaluating the impact of a marketing campaign on sales",
      "Assessing the effect of a policy change on website traffic"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "Google CausalImpact package",
      "how to measure intervention effects in R",
      "Bayesian time series analysis R",
      "CausalImpact tutorial",
      "time series intervention analysis R",
      "CausalImpact examples"
    ],
    "primary_use_cases": [
      "measuring intervention effects"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "bsts",
      "pycausalimpact"
    ],
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "Pyomo",
    "description": "General-purpose algebraic optimization modeling in Python. Supports LP, MILP, NLP, and stochastic programming with interfaces to major solvers including HiGHS, Gurobi, and CPLEX.",
    "category": "Energy & Utilities Economics",
    "docs_url": "https://www.pyomo.org/",
    "github_url": "https://github.com/Pyomo/pyomo",
    "url": "https://www.pyomo.org/",
    "install": "pip install pyomo",
    "tags": [
      "optimization",
      "mathematical programming",
      "modeling"
    ],
    "best_for": "Building custom optimization models for energy systems",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Pyomo is a general-purpose algebraic optimization modeling library in Python that supports various programming types including LP, MILP, NLP, and stochastic programming. It is used by researchers and practitioners in fields such as operations research and applied mathematics for optimization tasks.",
    "use_cases": [
      "Solving linear programming problems",
      "Modeling complex optimization scenarios in energy systems"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for optimization",
      "how to model algebraic optimization in python",
      "Pyomo tutorial",
      "best practices for using Pyomo",
      "Pyomo examples",
      "optimization modeling with Pyomo"
    ],
    "primary_use_cases": [
      "Optimization modeling"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "PuLP",
      "Gurobi",
      "CPLEX"
    ],
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "Pyomo",
    "description": "Python-based open-source optimization modeling language supporting linear, mixed-integer, nonlinear programming",
    "category": "Optimization",
    "docs_url": "https://pyomo.readthedocs.io/",
    "github_url": "https://github.com/Pyomo/pyomo",
    "url": "https://www.pyomo.org/",
    "install": "pip install pyomo",
    "tags": [
      "optimization",
      "linear programming",
      "MILP",
      "nonlinear"
    ],
    "best_for": "Building and solving mathematical optimization models for energy systems",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Pyomo is a Python-based open-source optimization modeling language that supports various types of optimization problems including linear, mixed-integer, and nonlinear programming. It is used by researchers and practitioners in fields such as operations research, economics, and engineering to model and solve complex optimization problems.",
    "use_cases": [
      "Supply chain optimization",
      "Resource allocation problems"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for optimization",
      "how to do linear programming in python",
      "Pyomo tutorial",
      "mixed-integer programming in python",
      "nonlinear programming with Pyomo",
      "optimization modeling in python"
    ],
    "primary_use_cases": [
      "Mathematical optimization",
      "Energy modeling"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "PuLP",
      "Gurobi",
      "CVXPY"
    ],
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "MONAI",
    "description": "Medical Open Network for AI - PyTorch-based framework for deep learning in healthcare imaging. Domain-specific transforms, pre-built architectures (UNet, SegResNet), and MONAI Label for annotation.",
    "category": "Healthcare Economics & Health-Tech",
    "docs_url": "https://docs.monai.io/",
    "github_url": "https://github.com/Project-MONAI/MONAI",
    "url": "https://monai.io/",
    "install": "pip install monai",
    "tags": [
      "medical imaging",
      "deep learning",
      "PyTorch",
      "segmentation"
    ],
    "best_for": "Medical image analysis and deep learning in radiology",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python"
    ],
    "topic_tags": [
      "medical imaging",
      "deep learning",
      "PyTorch"
    ],
    "summary": "MONAI is a PyTorch-based framework designed for deep learning in healthcare imaging, providing domain-specific transforms and pre-built architectures. It is used by researchers and practitioners in the healthcare field to facilitate the development of AI solutions for medical imaging tasks.",
    "use_cases": [
      "Image segmentation in medical imaging",
      "Annotation of medical images using MONAI Label"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for medical imaging",
      "how to do deep learning in healthcare",
      "MONAI PyTorch framework",
      "medical AI annotation tools",
      "deep learning segmentation in Python",
      "using MONAI for healthcare imaging"
    ],
    "primary_use_cases": [
      "image segmentation",
      "medical image annotation"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "PyTorch",
      "TensorFlow"
    ],
    "maintenance_status": "active",
    "framework_compatibility": [
      "PyTorch"
    ],
    "model_score": 0.0002
  },
  {
    "name": "rmarkdown",
    "description": "Dynamic documents combining R code with Markdown text. Generates reproducible reports in HTML, PDF, Word, and slides. Foundation for literate programming and reproducible research in R.",
    "category": "Reproducibility",
    "docs_url": "https://rmarkdown.rstudio.com/",
    "github_url": "https://github.com/rstudio/rmarkdown",
    "url": "https://cran.r-project.org/package=rmarkdown",
    "install": "install.packages(\"rmarkdown\")",
    "tags": [
      "literate-programming",
      "reproducible-research",
      "dynamic-documents",
      "reporting",
      "Markdown"
    ],
    "best_for": "Literate programming and reproducible reports combining R code with Markdown",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "rmarkdown is a package that allows users to create dynamic documents by combining R code with Markdown text. It is widely used by researchers and data analysts for generating reproducible reports in various formats such as HTML, PDF, and Word.",
    "use_cases": [
      "Generating reports for academic research",
      "Creating presentations with R code output"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "how to create dynamic documents in R",
      "R package for reproducible reports",
      "generate PDF reports with Rmarkdown",
      "using Markdown with R code",
      "Rmarkdown tutorial",
      "Rmarkdown for data analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "Statrs",
    "description": "Comprehensive statistical distributions for Rust (Normal, T, Gamma, etc.) with PDF, CDF, quantile functions\u2014the scipy.stats equivalent.",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": "https://docs.rs/statrs",
    "github_url": "https://github.com/statrs-dev/statrs",
    "url": "https://crates.io/crates/statrs",
    "install": "cargo add statrs",
    "tags": [
      "rust",
      "statistics",
      "distributions",
      "probability"
    ],
    "best_for": "Probability distributions and basic statistics in Rust",
    "language": "Rust",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "statistical-inference",
      "hypothesis-testing"
    ],
    "summary": "Statrs is a Rust library that provides comprehensive statistical distributions, including Normal, T, and Gamma distributions. It is useful for statisticians and data scientists who require statistical functions similar to those in scipy.stats.",
    "use_cases": [
      "Performing statistical analysis in Rust applications",
      "Developing data-driven applications that require statistical functions"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "Rust library for statistical distributions",
      "how to calculate PDF in Rust",
      "Rust statistics library",
      "distributions in Rust",
      "Rust CDF functions",
      "statistical analysis in Rust"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "PyTorch",
    "description": "Popular deep learning framework with flexible automatic differentiation.",
    "category": "Numerical Optimization & Computational Tools",
    "docs_url": "https://pytorch.org/",
    "github_url": "https://github.com/pytorch/pytorch",
    "url": "https://github.com/pytorch/pytorch",
    "install": "(See PyTorch website)",
    "tags": [
      "optimization",
      "computation",
      "machine learning"
    ],
    "best_for": "Solving optimization problems, numerical methods",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "machine learning"
    ],
    "summary": "PyTorch is a popular deep learning framework that provides flexible automatic differentiation for building and training neural networks. It is widely used by researchers and practitioners in the field of machine learning.",
    "use_cases": [
      "Building neural networks",
      "Training models for image classification"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for deep learning",
      "how to use PyTorch for machine learning",
      "PyTorch tutorial",
      "deep learning framework comparison",
      "PyTorch automatic differentiation",
      "PyTorch optimization techniques"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "TensorFlow",
      "Keras"
    ],
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "ChainLadder",
    "description": "Comprehensive R package for claims reserving methods including Mack, Munich, and bootstrap chain-ladder with full uncertainty quantification",
    "category": "Insurance & Actuarial",
    "docs_url": "https://mages.github.io/ChainLadder/",
    "github_url": "https://github.com/mages/ChainLadder",
    "url": "https://cran.r-project.org/package=ChainLadder",
    "install": "install.packages(\"ChainLadder\")",
    "tags": [
      "actuarial",
      "reserving",
      "chain-ladder",
      "Mack-model",
      "bootstrap"
    ],
    "best_for": "P&C reserving in R with stochastic methods and uncertainty estimation",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "ChainLadder is a comprehensive R package designed for claims reserving methods, including Mack, Munich, and bootstrap chain-ladder techniques. It is primarily used by actuaries and insurance professionals for uncertainty quantification in claims reserving.",
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for claims reserving",
      "how to perform chain-ladder in R",
      "Mack model implementation in R",
      "bootstrap chain-ladder R package",
      "uncertainty quantification in insurance R",
      "actuarial reserving methods R"
    ],
    "use_cases": [
      "Estimating reserves for insurance claims",
      "Quantifying uncertainty in actuarial models"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "pylift",
    "description": "Wayfair's uplift modeling wrapping sklearn for speed with rigorous Qini curve evaluation.",
    "category": "Uplift Modeling",
    "docs_url": "https://pylift.readthedocs.io/",
    "github_url": "https://github.com/wayfair/pylift",
    "url": "https://github.com/wayfair/pylift",
    "install": "pip install pylift",
    "tags": [
      "uplift modeling",
      "treatment effects",
      "marketing"
    ],
    "best_for": "Fast uplift with Qini curve evaluation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "uplift modeling",
      "treatment effects",
      "marketing"
    ],
    "summary": "pylift is a Python package developed by Wayfair for uplift modeling, designed to enhance the speed of sklearn while providing rigorous Qini curve evaluation. It is primarily used by data scientists and researchers in marketing to analyze treatment effects.",
    "use_cases": [
      "Evaluating marketing campaign effectiveness",
      "Optimizing treatment allocation in experiments"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for uplift modeling",
      "how to evaluate Qini curve in python",
      "treatment effects analysis with python",
      "marketing uplift modeling python package"
    ],
    "primary_use_cases": [
      "uplift modeling",
      "treatment effect estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "evd",
    "description": "Functions for extreme value distributions including GEV, GPD, and point process models essential for catastrophe modeling",
    "category": "Insurance & Actuarial",
    "docs_url": "https://cran.r-project.org/web/packages/evd/evd.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=evd",
    "install": "install.packages(\"evd\")",
    "tags": [
      "extreme-values",
      "GEV",
      "GPD",
      "catastrophe-modeling",
      "tail-risk"
    ],
    "best_for": "Extreme value analysis for reinsurance pricing and catastrophe risk assessment",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The 'evd' package provides functions for extreme value distributions, including GEV and GPD, which are essential for modeling catastrophic events. It is primarily used by statisticians and data scientists working in the insurance and actuarial fields.",
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for extreme value distributions",
      "how to model catastrophe events in R",
      "functions for GEV in R",
      "GPD modeling in R",
      "tail risk analysis R package",
      "catastrophe modeling tools in R"
    ],
    "use_cases": [
      "Modeling extreme weather events",
      "Assessing financial risk from rare events"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "haven",
    "description": "Import and export Stata, SPSS, and SAS data files preserving variable labels and value labels. Handles .dta, .sav, .sas7bdat, and .xpt formats with labelled vectors for metadata.",
    "category": "Data Workflow",
    "docs_url": "https://haven.tidyverse.org/",
    "github_url": "https://github.com/tidyverse/haven",
    "url": "https://cran.r-project.org/package=haven",
    "install": "install.packages(\"haven\")",
    "tags": [
      "Stata",
      "SPSS",
      "SAS",
      "data-import",
      "labelled-data"
    ],
    "best_for": "Reading and writing Stata, SPSS, and SAS files with preserved labels",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The haven package allows users to import and export data files from Stata, SPSS, and SAS while preserving variable and value labels. It is useful for data analysts and researchers who work with these statistical software formats.",
    "use_cases": [
      "Importing .dta files from Stata",
      "Exporting .sav files to SPSS"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for importing Stata data",
      "how to export SPSS files in R",
      "R haven package documentation",
      "import SAS data into R",
      "R data import tools",
      "preserve variable labels in R",
      "R package for labelled data"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "DTRreg",
    "description": "Dynamic treatment regime estimation via G-estimation for sequential treatment decisions. Implements methods for finding optimal treatment rules that adapt over time based on patient characteristics.",
    "category": "Causal Inference (Dynamic Treatment)",
    "docs_url": "https://cran.r-project.org/web/packages/DTRreg/DTRreg.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=DTRreg",
    "install": "install.packages(\"DTRreg\")",
    "tags": [
      "dynamic-treatment",
      "G-estimation",
      "sequential-decisions",
      "optimal-treatment",
      "personalization"
    ],
    "best_for": "Dynamic treatment regime estimation via G-estimation for sequential treatment decisions",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "DTRreg is a package for estimating dynamic treatment regimes using G-estimation for sequential treatment decisions. It is used by researchers and practitioners in causal inference to find optimal treatment rules that adapt over time based on patient characteristics.",
    "use_cases": [
      "Estimating optimal treatment strategies in clinical trials",
      "Adapting treatment plans based on patient responses"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for dynamic treatment regimes",
      "how to perform G-estimation in R",
      "sequential treatment decisions in R",
      "optimal treatment rules R package",
      "personalization in treatment decisions R",
      "dynamic treatment estimation R"
    ],
    "primary_use_cases": [
      "dynamic treatment estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "mplsoccer",
    "description": "Python library for football/soccer pitch visualization with support for heat maps, shot maps, pass maps, and event plotting",
    "category": "Sports Analytics",
    "docs_url": "https://mplsoccer.readthedocs.io/",
    "github_url": "https://github.com/andrewRowlinson/mplsoccer",
    "url": "https://github.com/andrewRowlinson/mplsoccer",
    "install": "pip install mplsoccer",
    "tags": [
      "soccer",
      "football",
      "visualization",
      "sports-analytics"
    ],
    "best_for": "Soccer data visualization, pitch plotting, and tactical analysis presentation",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "mplsoccer is a Python library designed for visualizing football/soccer pitches, enabling users to create heat maps, shot maps, pass maps, and plot events. It is useful for analysts and enthusiasts in the sports analytics field.",
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for soccer visualization",
      "how to create heat maps in python",
      "football pitch visualization in python",
      "python library for sports analytics",
      "how to plot events in soccer",
      "create shot maps with python"
    ],
    "use_cases": [
      "Visualizing player movements on the pitch",
      "Analyzing shot distribution during a match"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "nflfastR",
    "description": "R package for NFL play-by-play data with built-in expected points (EPA) and win probability models from 1999-present",
    "category": "Sports Analytics",
    "docs_url": "https://www.nflfastr.com/",
    "github_url": "https://github.com/nflverse/nflfastR",
    "url": "https://github.com/nflverse/nflfastR",
    "install": "install.packages(\"nflfastR\")",
    "tags": [
      "football",
      "sports-analytics",
      "R",
      "NFL",
      "EPA"
    ],
    "best_for": "NFL analytics in R, expected points analysis, and game strategy research",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "nflfastR is an R package designed for analyzing NFL play-by-play data, incorporating built-in expected points (EPA) and win probability models from 1999 to the present. It is primarily used by sports analysts and data scientists interested in football analytics.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for NFL play-by-play data",
      "how to analyze NFL data in R",
      "expected points model in R",
      "R sports analytics package",
      "NFL win probability model R",
      "R football analytics tools"
    ],
    "use_cases": [
      "Analyzing game strategies based on play-by-play data",
      "Calculating expected points for different plays"
    ],
    "primary_use_cases": [
      "expected points analysis",
      "win probability modeling"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "lifelib",
    "description": "Open-source actuarial library with complete life insurance projection models including term, whole life, universal life, and variable annuities",
    "category": "Insurance & Actuarial",
    "docs_url": "https://lifelib.io/",
    "github_url": "https://github.com/lifelib-dev/lifelib",
    "url": "https://lifelib.io/",
    "install": "pip install lifelib",
    "tags": [
      "life-insurance",
      "actuarial-modeling",
      "cash-flow-projection",
      "reserving",
      "ALM"
    ],
    "best_for": "Life insurance product modeling, ALM analysis, and actuarial cash flow projections",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Lifelib is an open-source actuarial library that provides complete life insurance projection models, including term, whole life, universal life, and variable annuities. It is used by actuaries and data scientists working in the insurance industry.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for life insurance modeling",
      "how to project cash flows in insurance using python",
      "actuarial modeling in python",
      "life insurance projection models python",
      "open-source actuarial library",
      "python life insurance tools",
      "life insurance cash flow projection python",
      "variable annuities modeling in python"
    ],
    "use_cases": [
      "Modeling term life insurance policies",
      "Projecting cash flows for whole life insurance",
      "Analyzing universal life insurance products",
      "Estimating reserves for variable annuities"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0002
  },
  {
    "name": "WeightIt",
    "description": "Unified interface for generating balancing weights for causal effect estimation in observational studies. Supports binary, multi-category, and continuous treatments for point and longitudinal/marginal structural models. Methods include inverse probability weighting (IPW), entropy balancing, covariate balancing propensity score (CBPS), energy balancing, stable balancing weights, BART, and SuperLearner.",
    "category": "Causal Inference (Matching)",
    "docs_url": "https://ngreifer.github.io/WeightIt/",
    "github_url": "https://github.com/ngreifer/WeightIt",
    "url": "https://cran.r-project.org/package=WeightIt",
    "install": "install.packages(\"WeightIt\")",
    "tags": [
      "propensity-score-weighting",
      "inverse-probability-weighting",
      "entropy-balancing",
      "CBPS",
      "marginal-structural-models"
    ],
    "best_for": "Generating balancing weights using modern weighting methods (IPW, entropy balancing, CBPS, etc.) for point or longitudinal treatments",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "WeightIt provides a unified interface for generating balancing weights used in causal effect estimation for observational studies. It is utilized by researchers and practitioners in fields such as statistics and data science to ensure balanced treatment groups in various types of analyses.",
    "use_cases": [
      "Estimating treatment effects in observational studies",
      "Creating balanced datasets for analysis"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for causal effect estimation",
      "how to generate balancing weights in R",
      "R library for inverse probability weighting",
      "entropy balancing in R",
      "propensity score weighting R",
      "marginal structural models in R"
    ],
    "primary_use_cases": [
      "causal effect estimation",
      "balancing weights generation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "OpenDSS",
    "description": "EPRI's open-source distribution system simulator. Quasi-static time-series analysis, DER integration, and comprehensive distribution modeling. Industry standard.",
    "category": "Energy & Utilities Economics",
    "docs_url": "https://www.epri.com/pages/sa/opendss",
    "github_url": "https://github.com/dss-extensions",
    "url": "https://www.epri.com/pages/sa/opendss",
    "install": "pip install opendssdirect.py",
    "tags": [
      "distribution",
      "simulation",
      "DER",
      "EPRI"
    ],
    "best_for": "Distribution system simulation with high DER penetration",
    "language": "COM/Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "time-series",
      "DER integration"
    ],
    "summary": "OpenDSS is an open-source distribution system simulator developed by EPRI that allows for quasi-static time-series analysis and comprehensive distribution modeling. It is widely used in the energy sector for integrating distributed energy resources (DER).",
    "use_cases": [
      "Simulating the impact of DER on distribution networks",
      "Conducting time-series analysis for energy consumption patterns"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for distribution system simulation",
      "how to perform time-series analysis in OpenDSS",
      "EPRI OpenDSS usage examples",
      "DER integration with OpenDSS",
      "OpenDSS documentation",
      "OpenDSS installation guide"
    ],
    "primary_use_cases": [
      "Distribution simulation"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "pandapower",
      "GridLAB-D",
      "CYME"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "OpenDSS",
    "description": "Electric power distribution system simulator for distributed energy resources and smart grid",
    "category": "Energy Systems Modeling",
    "docs_url": "https://opendss.epri.com/",
    "github_url": "https://sourceforge.net/projects/electricdss/",
    "url": "https://www.epri.com/pages/sa/opendss",
    "install": "pip install OpenDSSDirect.py",
    "tags": [
      "distribution",
      "DER",
      "smart grid",
      "EPRI"
    ],
    "best_for": "Distribution system analysis with high penetration of distributed energy resources",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "OpenDSS is an electric power distribution system simulator designed for analyzing distributed energy resources and smart grid applications. It is used by researchers and engineers in the energy sector to model and simulate various scenarios in power distribution.",
    "use_cases": [],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for electric power distribution simulation",
      "how to simulate smart grid in python",
      "OpenDSS energy system modeling",
      "distributed energy resources simulation python",
      "smart grid analysis tools",
      "electric power distribution modeling software"
    ],
    "primary_use_cases": [
      "Distribution simulation",
      "DER analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "pandapower",
      "GridLAB-D"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "inferference",
    "description": "Computes inverse probability weighted (IPW) causal effects under partial interference following Tchetgen Tchetgen and VanderWeele (2012). Handles spillover effects within groups while maintaining independence across groups.",
    "category": "Causal Inference (Interference)",
    "docs_url": "https://cran.r-project.org/web/packages/inferference/inferference.pdf",
    "github_url": "https://github.com/bsaul/inferference",
    "url": "https://cran.r-project.org/package=inferference",
    "install": "install.packages(\"inferference\")",
    "tags": [
      "interference",
      "spillovers",
      "IPW",
      "partial-interference",
      "SUTVA-violations"
    ],
    "best_for": "IPW causal effects under partial interference with within-group spillovers, implementing Tchetgen Tchetgen & VanderWeele (2012)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The 'inferference' package computes inverse probability weighted (IPW) causal effects under partial interference, addressing spillover effects within groups while maintaining independence across groups. It is useful for researchers and practitioners in causal inference who are dealing with complex group interactions.",
    "use_cases": [],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R library for causal inference",
      "how to compute IPW causal effects in R",
      "spillover effects analysis in R",
      "partial interference methods in R",
      "Tchetgen Tchetgen and VanderWeele causal effects",
      "R package for handling SUTVA violations"
    ],
    "primary_use_cases": [
      "causal effect estimation under partial interference",
      "analyzing spillover effects"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Tchetgen Tchetgen and VanderWeele (2012)",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "blavaan",
    "description": "Bayesian latent variable analysis extending lavaan with MCMC estimation via Stan or JAGS, supporting Bayesian CFA, SEM, growth models, and model comparison with WAIC, LOO, and Bayes factors.",
    "category": "Structural Equation Modeling",
    "docs_url": "https://ecmerkle.github.io/blavaan/",
    "github_url": "https://github.com/ecmerkle/blavaan",
    "url": "https://cran.r-project.org/package=blavaan",
    "install": "install.packages(\"blavaan\")",
    "tags": [
      "Bayesian-SEM",
      "Stan",
      "JAGS",
      "MCMC",
      "latent-variables"
    ],
    "best_for": "Bayesian inference for SEM models using familiar lavaan syntax, implementing Merkle & Rosseel (2018)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "bayesian"
    ],
    "summary": "blavaan is a package for Bayesian latent variable analysis that extends the capabilities of lavaan by incorporating MCMC estimation through Stan or JAGS. It is utilized by researchers and practitioners interested in Bayesian approaches to confirmatory factor analysis (CFA), structural equation modeling (SEM), and growth models.",
    "use_cases": [
      "Conducting Bayesian CFA",
      "Estimating growth models using MCMC"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "Bayesian latent variable analysis in R",
      "how to perform Bayesian SEM in R",
      "blavaan package usage",
      "MCMC estimation with Stan in R",
      "latent variable modeling with JAGS",
      "model comparison in Bayesian analysis"
    ],
    "primary_use_cases": [
      "Bayesian CFA",
      "Bayesian SEM"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "lavaan"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "cjoint",
    "description": "Estimates Average Marginal Component Effects (AMCEs) for conjoint experiments following Hainmueller, Hopkins & Yamamoto (2014). Handles multi-dimensional preferences with clustered standard errors.",
    "category": "Conjoint Analysis",
    "docs_url": "https://cran.r-project.org/web/packages/cjoint/cjoint.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=cjoint",
    "install": "install.packages(\"cjoint\")",
    "tags": [
      "conjoint",
      "AMCE",
      "survey-experiments",
      "preferences",
      "political-science"
    ],
    "best_for": "AMCE estimation for conjoint experiments, implementing Hainmueller, Hopkins & Yamamoto (2014)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "conjoint-analysis",
      "survey-experiments",
      "political-science"
    ],
    "summary": "The cjoint package estimates Average Marginal Component Effects (AMCEs) for conjoint experiments, allowing researchers to analyze multi-dimensional preferences with clustered standard errors. It is primarily used by social scientists conducting survey experiments.",
    "use_cases": [
      "Analyzing voter preferences in political surveys",
      "Evaluating consumer choices in market research"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R library for conjoint analysis",
      "how to estimate AMCEs in R",
      "R package for survey experiments",
      "multi-dimensional preferences analysis in R",
      "clustered standard errors in R",
      "political science conjoint analysis R"
    ],
    "primary_use_cases": [
      "estimating average marginal component effects",
      "analyzing multi-dimensional preferences"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Hainmueller, Hopkins & Yamamoto (2014)",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "jaxonometrics",
    "description": "JAX-ecosystem implementations of standard econometrics routines for GPU computation.",
    "category": "Numerical Optimization & Computational Tools",
    "docs_url": null,
    "github_url": "https://github.com/py-econometrics/jaxonometrics",
    "url": "https://github.com/py-econometrics/jaxonometrics",
    "install": "GitHub Repository",
    "tags": [
      "optimization",
      "JAX",
      "GPU"
    ],
    "best_for": "GPU-accelerated econometrics with JAX",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "jaxonometrics provides implementations of standard econometrics routines optimized for GPU computation using the JAX ecosystem. It is suitable for users looking to leverage GPU capabilities for econometric analysis.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for econometrics",
      "GPU computation in econometrics",
      "JAX econometrics routines",
      "how to perform econometrics with JAX",
      "optimizing econometrics with GPU",
      "jaxonometrics package usage",
      "econometrics for data science in Python"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "JAX"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "MatchIt",
    "description": "Comprehensive matching package that selects matched samples of treated and control groups with similar covariate distributions. Provides a unified interface to multiple matching methods including nearest neighbor, optimal pair, optimal full, genetic, exact, coarsened exact (CEM), cardinality matching, and subclassification with propensity score estimation via GLM, GAM, random forest, and BART.",
    "category": "Causal Inference (Matching)",
    "docs_url": "https://kosukeimai.github.io/MatchIt/",
    "github_url": "https://github.com/kosukeimai/MatchIt",
    "url": "https://cran.r-project.org/package=MatchIt",
    "install": "install.packages(\"MatchIt\")",
    "tags": [
      "propensity-score-matching",
      "causal-inference",
      "observational-studies",
      "covariate-balance",
      "treatment-effects"
    ],
    "best_for": "Preprocessing observational data via matching to reduce confounding before estimating causal treatment effects, implementing Ho et al. (2007, 2011)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "MatchIt is a comprehensive matching package that selects matched samples of treated and control groups with similar covariate distributions. It provides a unified interface to multiple matching methods, making it useful for researchers conducting observational studies.",
    "use_cases": [
      "Selecting matched samples for treatment studies",
      "Improving covariate balance in observational research"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for propensity score matching",
      "how to perform matching in R",
      "matching methods in R",
      "causal inference with R",
      "observational studies R package",
      "covariate balance in R"
    ],
    "primary_use_cases": [
      "propensity score matching",
      "treatment effect estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "pensynth",
    "description": "Implements penalized synthetic control method from Abadie & L'Hour (2021). Adds regularization to improve pre-treatment fit and reduce interpolation bias in sparse donor pools.",
    "category": "Causal Inference (Synthetic Control)",
    "docs_url": "https://cran.r-project.org/web/packages/pensynth/pensynth.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=pensynth",
    "install": "install.packages(\"pensynth\")",
    "tags": [
      "synthetic-control",
      "penalized",
      "regularization",
      "interpolation-bias",
      "sparse-donors"
    ],
    "best_for": "Penalized synthetic control with regularization, implementing Abadie & L'Hour (2021)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "synthetic-control"
    ],
    "summary": "The pensynth package implements a penalized synthetic control method to enhance pre-treatment fit and mitigate interpolation bias in sparse donor pools. It is primarily used by researchers and practitioners in causal inference who require improved estimation techniques.",
    "use_cases": [
      "Evaluating treatment effects in observational studies",
      "Improving causal inference in economic research"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R library for synthetic control",
      "how to implement penalized synthetic control in R",
      "synthetic control method for sparse donors",
      "penalized synthetic control example",
      "R package for interpolation bias in causal inference",
      "best practices for synthetic control in R"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Abadie & L'Hour (2021)",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "pydtr",
    "description": "Dynamic treatment regimes using Iterative Q-Learning. Scikit-learn compatible for multi-stage optimal treatment sequencing.",
    "category": "Causal Inference & Matching",
    "docs_url": null,
    "github_url": "https://github.com/fullflu/pydtr",
    "url": "https://pypi.org/project/pydtr/",
    "install": "pip install pydtr",
    "tags": [
      "dynamic treatment",
      "reinforcement learning",
      "causal inference"
    ],
    "best_for": "Multi-stage dynamic treatment regimes",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "reinforcement-learning"
    ],
    "summary": "pydtr is a Python package designed for implementing dynamic treatment regimes using Iterative Q-Learning. It is compatible with Scikit-learn and is used for multi-stage optimal treatment sequencing in causal inference contexts.",
    "use_cases": [
      "optimizing treatment strategies in clinical trials",
      "analyzing multi-stage decision processes"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for dynamic treatment regimes",
      "how to implement iterative Q-learning in python",
      "scikit-learn compatible treatment sequencing",
      "dynamic treatment using reinforcement learning in python"
    ],
    "primary_use_cases": [
      "dynamic treatment optimization",
      "multi-stage treatment sequencing"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "scikit-learn"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "Ambrosia",
    "description": "End-to-end A/B testing from MobileTeleSystems with PySpark support. Covers experiment design, multi-group splitting, matching, and inference.",
    "category": "Power Simulation & Design of Experiments",
    "docs_url": null,
    "github_url": "https://github.com/MobileTeleSystems/Ambrosia",
    "url": "https://github.com/MobileTeleSystems/Ambrosia",
    "install": "pip install ambrosia",
    "tags": [
      "A/B testing",
      "experimentation",
      "Spark"
    ],
    "best_for": "End-to-end A/B testing with PySpark",
    "language": "Spark",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "experimentation"
    ],
    "summary": "Ambrosia is an end-to-end A/B testing tool that supports experiment design, multi-group splitting, matching, and inference using PySpark. It is primarily used by data scientists and researchers conducting experiments in various domains.",
    "use_cases": [
      "Conducting A/B tests for mobile applications",
      "Designing experiments for marketing strategies"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for A/B testing",
      "how to conduct experiments with PySpark",
      "end-to-end A/B testing in Python",
      "A/B testing framework for Spark",
      "experiment design tools in Python",
      "multi-group splitting in A/B testing"
    ],
    "primary_use_cases": [
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "PySpark"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "pydynpd",
    "description": "Estimation of dynamic panel data models using Arellano-Bond (Difference GMM) and Blundell-Bond (System GMM). Includes Windmeijer correction & tests.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://doi.org/10.21105/joss.04416",
    "github_url": "https://github.com/dazhwu/pydynpd",
    "url": "https://github.com/dazhwu/pydynpd",
    "install": "pip install pydynpd",
    "tags": [
      "panel data",
      "fixed effects"
    ],
    "best_for": "Longitudinal analysis, controlling for unobserved heterogeneity",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "panel data",
      "fixed effects"
    ],
    "summary": "pydynpd is a Python package designed for estimating dynamic panel data models using Arellano-Bond and Blundell-Bond methods. It is useful for researchers and practitioners working with panel data in econometrics.",
    "use_cases": [
      "Estimating dynamic panel data models",
      "Conducting econometric analysis on longitudinal data"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for dynamic panel data models",
      "how to estimate Arellano-Bond in python",
      "Blundell-Bond estimation in python",
      "panel data analysis with python",
      "difference GMM python package",
      "system GMM python library"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "Adaptive",
    "description": "Parallel active learning library for adaptive function sampling/evaluation, with live plotting for monitoring.",
    "category": "Power Simulation & Design of Experiments",
    "docs_url": "https://adaptive.readthedocs.io/en/latest/",
    "github_url": "https://github.com/python-adaptive/adaptive",
    "url": "https://github.com/python-adaptive/adaptive",
    "install": "pip install adaptive",
    "tags": [
      "power analysis",
      "experiments"
    ],
    "best_for": "Sample size calculation, experimental design, power analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "Adaptive is a parallel active learning library designed for adaptive function sampling and evaluation, featuring live plotting for monitoring purposes. It is useful for researchers and practitioners involved in power analysis and experimental design.",
    "use_cases": [
      "Sampling functions adaptively based on previous evaluations",
      "Monitoring experiments in real-time with live plotting"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for adaptive function sampling",
      "how to do active learning in python",
      "parallel active learning library",
      "live plotting for monitoring experiments",
      "power analysis in python",
      "experiments with adaptive sampling in python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "BTYDplus",
    "description": "Extended BTYD models for R including MBG/NBD, Pareto/GGG, and hierarchical Bayesian variants. Handles regular purchasing patterns and incorporates purchase timing.",
    "category": "MarTech & Customer Analytics",
    "docs_url": "https://cran.r-project.org/package=BTYDplus",
    "github_url": "https://github.com/mplatzer/BTYDplus",
    "url": "https://github.com/mplatzer/BTYDplus",
    "install": "install.packages('BTYDplus')",
    "tags": [
      "CLV",
      "BTYD",
      "R",
      "hierarchical-Bayes"
    ],
    "best_for": "Advanced BTYD variants for subscription and regular-purchase businesses",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "bayesian"
    ],
    "summary": "BTYDplus is an R package that extends BTYD models to analyze customer purchasing behavior, incorporating various Bayesian methods. It is useful for data scientists and analysts focused on customer lifetime value and purchasing patterns.",
    "use_cases": [
      "Analyzing customer lifetime value",
      "Modeling purchase timing",
      "Evaluating regular purchasing patterns"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for BTYD models",
      "how to analyze customer purchasing patterns in R",
      "bayesian models for customer analytics in R",
      "BTYDplus documentation",
      "purchase timing analysis in R",
      "hierarchical Bayesian models for customer lifetime value"
    ],
    "primary_use_cases": [
      "customer lifetime value analysis",
      "purchase timing modeling"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "CLVTools",
      "lifetimes"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "did",
    "description": "Implements group-time average treatment effects (ATT(g,t)) for staggered DiD designs with multiple periods and variation in treatment timing. Provides flexible aggregation into event-study plots or overall treatment effect estimates, addressing the well-documented negative weighting issues with conventional TWFE under staggered adoption.",
    "category": "Causal Inference (DiD)",
    "docs_url": "https://bcallaway11.github.io/did/",
    "github_url": "https://github.com/bcallaway11/did",
    "url": "https://cran.r-project.org/package=did",
    "install": "install.packages(\"did\")",
    "tags": [
      "difference-in-differences",
      "staggered-adoption",
      "event-study",
      "treatment-effects",
      "panel-data"
    ],
    "best_for": "Staggered rollout designs where different units adopt treatment at different times, implementing the Callaway & Sant'Anna (2021) estimator",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The 'did' package implements group-time average treatment effects for staggered DiD designs, allowing for flexible aggregation into event-study plots and overall treatment effect estimates. It is primarily used by researchers and practitioners in causal inference to address issues with conventional methods under staggered adoption.",
    "use_cases": [
      "Analyzing treatment effects in policy evaluations",
      "Creating event-study plots for staggered adoption scenarios"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for group-time average treatment effects",
      "how to create event-study plots in R",
      "difference-in-differences analysis in R",
      "staggered adoption treatment effects R",
      "R library for causal inference",
      "R package for panel data treatment effects"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "matching",
    "description": "Implements Stable Marriage, Hospital-Resident, Student-Allocation, and Stable Roommates using Gale-Shapley (JOSS paper).",
    "category": "Matching & Market Design",
    "docs_url": "https://daffidwilde.github.io/matching/",
    "github_url": "https://github.com/daffidwilde/matching",
    "url": "https://github.com/daffidwilde/matching",
    "install": "pip install matching",
    "tags": [
      "matching",
      "market design",
      "Gale-Shapley"
    ],
    "best_for": "Classic two-sided matching algorithms",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The matching package implements algorithms for Stable Marriage, Hospital-Resident, Student-Allocation, and Stable Roommates problems using the Gale-Shapley method. It is useful for researchers and practitioners in matching and market design.",
    "use_cases": [
      "Allocating students to schools",
      "Matching residents to hospitals",
      "Organizing stable marriages",
      "Creating roommate assignments"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for stable marriage",
      "how to implement Gale-Shapley in python",
      "matching algorithms in python",
      "hospital-resident matching python",
      "student allocation algorithms python",
      "stable roommates implementation python"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Gale & Shapley (1962)",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "staggered",
    "description": "Provides the efficient estimator for randomized staggered rollout designs, offering optimal weighting schemes for treatment effect estimation. Also implements Callaway & Sant'Anna and Sun & Abraham estimators with design-based Fisher inference for randomized experiments.",
    "category": "Causal Inference (DiD)",
    "docs_url": "https://cran.r-project.org/web/packages/staggered/readme/README.html",
    "github_url": "https://github.com/jonathandroth/staggered",
    "url": "https://cran.r-project.org/package=staggered",
    "install": "install.packages(\"staggered\")",
    "tags": [
      "staggered-rollout",
      "randomized-experiments",
      "efficient-estimation",
      "event-study",
      "fisher-inference"
    ],
    "best_for": "Randomized experiments with staggered treatment timing where efficiency gains matter, implementing Roth & Sant'Anna (2023)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The staggered package provides efficient estimators for randomized staggered rollout designs, implementing optimal weighting schemes for treatment effect estimation. It is useful for researchers and practitioners conducting randomized experiments.",
    "use_cases": [
      "Estimating treatment effects in randomized staggered rollout designs",
      "Conducting event studies with optimal weighting schemes"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for staggered rollout designs",
      "how to estimate treatment effects in R",
      "efficient estimation for randomized experiments in R",
      "Callaway & Sant'Anna estimator R package",
      "Sun & Abraham estimator in R",
      "Fisher inference for randomized experiments R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "CLVTools",
    "description": "R package for probabilistic CLV modeling. Implements Pareto/NBD and BG/NBD with time-varying covariates, spending models, and customer-level predictions.",
    "category": "MarTech & Customer Analytics",
    "docs_url": "https://www.clvtools.com/",
    "github_url": "https://github.com/bachmannpatrick/CLVTools",
    "url": "https://www.clvtools.com/",
    "install": "install.packages('CLVTools')",
    "tags": [
      "CLV",
      "BTYD",
      "R",
      "customer-analytics"
    ],
    "best_for": "Production CLV modeling in R with time-varying covariates",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "customer-analytics"
    ],
    "summary": "CLVTools is an R package designed for probabilistic customer lifetime value modeling. It is particularly useful for data analysts and marketers looking to implement advanced customer prediction models.",
    "use_cases": [
      "Modeling customer lifetime value for e-commerce businesses",
      "Predicting customer behavior over time"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for CLV modeling",
      "how to implement Pareto/NBD in R",
      "BG/NBD model in R",
      "customer-level predictions in R",
      "R tools for customer analytics",
      "probabilistic CLV modeling in R"
    ],
    "primary_use_cases": [
      "customer lifetime value modeling"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "BTYDplus",
      "lifetimes"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "augsynth",
    "description": "Implements the Augmented Synthetic Control Method, which uses an outcome model (ridge regression by default) to correct for bias when pre-treatment fit is imperfect. Uniquely supports staggered adoption across multiple treated units via multisynth() function.",
    "category": "Causal Inference (Synthetic Control)",
    "docs_url": "https://github.com/ebenmichael/augsynth/blob/master/vignettes/singlesynth-vignette.md",
    "github_url": "https://github.com/ebenmichael/augsynth",
    "url": "https://github.com/ebenmichael/augsynth",
    "install": "devtools::install_github(\"ebenmichael/augsynth\")",
    "tags": [
      "augmented-synthetic-control",
      "bias-correction",
      "staggered-adoption",
      "ridge-regression",
      "imperfect-fit"
    ],
    "best_for": "SC applications with imperfect pre-treatment fit or staggered adoption across units, implementing Ben-Michael, Feller & Rothstein (2021)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "AugSynth implements the Augmented Synthetic Control Method to correct for bias in pre-treatment fit using an outcome model. It is particularly useful for researchers dealing with staggered adoption across multiple treated units.",
    "use_cases": [
      "Analyzing the impact of policy changes across different regions",
      "Evaluating treatment effects in staggered adoption scenarios"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for augmented synthetic control",
      "how to correct bias in synthetic control in R",
      "R library for staggered adoption analysis",
      "implementing ridge regression in synthetic control",
      "R synthetic control method for causal inference",
      "using multisynth function in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "OpenMx",
    "description": "Extended SEM software with programmatic model specification via paths (RAM) or matrix algebra, supporting mixture distributions, item factor analysis, state space models, and behavior genetics twin studies.",
    "category": "Structural Equation Modeling",
    "docs_url": "https://openmx.ssri.psu.edu/",
    "github_url": "https://github.com/OpenMx/OpenMx",
    "url": "https://cran.r-project.org/package=OpenMx",
    "install": "install.packages(\"OpenMx\")",
    "tags": [
      "SEM",
      "matrix-algebra",
      "twin-studies",
      "behavior-genetics",
      "IFA"
    ],
    "best_for": "Complex/advanced SEM, behavior genetics, and researchers needing maximum specification flexibility, implementing Neale et al. (2016)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "structural-equation-modeling",
      "behavior-genetics"
    ],
    "summary": "OpenMx is an extended software package for structural equation modeling (SEM) that allows for programmatic model specification through paths or matrix algebra. It is used by researchers and practitioners in fields such as psychology, genetics, and social sciences for complex statistical modeling.",
    "use_cases": [
      "Modeling complex relationships in psychological research",
      "Analyzing twin studies for behavior genetics"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for structural equation modeling",
      "how to perform item factor analysis in R",
      "OpenMx tutorial",
      "R SEM software",
      "behavior genetics modeling in R",
      "mixture distributions in OpenMx"
    ],
    "primary_use_cases": [
      "item factor analysis",
      "state space models"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "igraph",
    "description": "Comprehensive network analysis library with efficient algorithms for network creation, manipulation, and analysis. Provides centrality measures, community detection, graph visualization, and network statistics.",
    "category": "Network Analysis",
    "docs_url": "https://igraph.org/r/",
    "github_url": "https://github.com/igraph/rigraph",
    "url": "https://cran.r-project.org/package=igraph",
    "install": "install.packages(\"igraph\")",
    "tags": [
      "networks",
      "graph-algorithms",
      "centrality",
      "community-detection",
      "network-statistics"
    ],
    "best_for": "Comprehensive network analysis with efficient algorithms for centrality, community detection, and visualization",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "igraph is a comprehensive network analysis library that provides efficient algorithms for creating, manipulating, and analyzing networks. It is used by data scientists and researchers for tasks such as centrality measures, community detection, and graph visualization.",
    "use_cases": [
      "Analyzing social networks",
      "Visualizing transportation networks"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for network analysis",
      "how to visualize graphs in R",
      "community detection in R",
      "centrality measures in R",
      "network statistics R package",
      "graph algorithms in R"
    ],
    "primary_use_cases": [
      "community detection",
      "graph visualization"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "network",
      "statnet"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "Fairlearn",
    "description": "Microsoft toolkit for assessing and improving ML model fairness, critical for insurance pricing compliance and avoiding discriminatory outcomes",
    "category": "Insurance & Actuarial",
    "docs_url": "https://fairlearn.org/",
    "github_url": "https://github.com/fairlearn/fairlearn",
    "url": "https://fairlearn.org/",
    "install": "pip install fairlearn",
    "tags": [
      "fairness",
      "bias-mitigation",
      "regulatory-compliance",
      "discrimination",
      "model-auditing"
    ],
    "best_for": "Detecting and mitigating bias in insurance underwriting and pricing models",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "Fairlearn is a toolkit developed by Microsoft to assess and improve the fairness of machine learning models. It is particularly useful for industries like insurance to ensure compliance with regulations and to mitigate discriminatory outcomes.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for model fairness",
      "how to assess ML model fairness in python",
      "tools for bias mitigation in ML",
      "regulatory compliance in machine learning",
      "how to avoid discrimination in ML models",
      "model auditing tools in python"
    ],
    "use_cases": [
      "Improving fairness in insurance pricing models",
      "Assessing bias in machine learning algorithms"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "pandapower",
    "description": "Power system analysis for distribution networks. Newton-Raphson power flow, state estimation, short circuit calculations, and network visualization.",
    "category": "Energy & Utilities Economics",
    "docs_url": "https://www.pandapower.org/",
    "github_url": "https://github.com/e2nIEE/pandapower",
    "url": "https://www.pandapower.org/",
    "install": "pip install pandapower",
    "tags": [
      "power flow",
      "distribution",
      "networks"
    ],
    "best_for": "Distribution system analysis and power flow calculations",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [],
    "summary": "pandapower is a Python package designed for power system analysis specifically for distribution networks. It provides functionalities such as Newton-Raphson power flow, state estimation, short circuit calculations, and network visualization, making it useful for engineers and researchers in the energy sector.",
    "use_cases": [
      "Analyzing power flow in distribution networks",
      "Estimating the state of a power system",
      "Performing short circuit calculations",
      "Visualizing electrical networks"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for power system analysis",
      "how to perform power flow calculations in python",
      "tools for network visualization in energy",
      "short circuit calculations in python",
      "state estimation for distribution networks",
      "pandapower documentation",
      "python energy utilities package"
    ],
    "primary_use_cases": [
      "Distribution analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "PyPSA",
      "OpenDSS",
      "GridLAB-D"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "pandapower",
    "description": "Easy-to-use Python library for power system modeling, analysis, and optimization",
    "category": "Energy Systems Modeling",
    "docs_url": "https://pandapower.readthedocs.io/",
    "github_url": "https://github.com/e2nIEE/pandapower",
    "url": "https://www.pandapower.org/",
    "install": "pip install pandapower",
    "tags": [
      "power flow",
      "short circuit",
      "optimal power flow",
      "distribution"
    ],
    "best_for": "Power flow calculations and distribution network analysis",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [],
    "summary": "Pandapower is an easy-to-use Python library designed for power system modeling, analysis, and optimization. It is primarily used by engineers and researchers in the energy sector to perform power flow and short circuit analysis.",
    "use_cases": [
      "Modeling electrical power systems",
      "Analyzing short circuits in power networks"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for power system modeling",
      "how to analyze power flow in python",
      "python library for optimal power flow",
      "short circuit analysis in python",
      "how to optimize power systems with python",
      "easy power system analysis in python"
    ],
    "primary_use_cases": [
      "Power flow",
      "Network analysis"
    ],
    "api_complexity": "simple",
    "related_packages": [
      "PyPSA",
      "networkx"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "igraph",
    "description": "Network analysis and visualization library for R and Python, applicable to defense supply chains and alliance networks",
    "category": "Network Analysis",
    "docs_url": "https://igraph.org/r/",
    "github_url": "https://github.com/igraph/rigraph",
    "url": "https://igraph.org/",
    "install": "install.packages('igraph')",
    "tags": [
      "networks",
      "graphs",
      "visualization",
      "analysis"
    ],
    "best_for": "Analyzing defense supply chain networks and alliance structures",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "igraph is a network analysis and visualization library that supports R and Python. It is used for analyzing complex networks, such as defense supply chains and alliance networks.",
    "use_cases": [
      "Analyzing defense supply chains",
      "Visualizing alliance networks"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "network analysis library for R",
      "visualization of graphs in Python",
      "how to analyze supply chains with igraph",
      "igraph examples for alliance networks",
      "R library for network visualization",
      "Python library for graph analysis"
    ],
    "primary_use_cases": [
      "Network analysis",
      "Visualization"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "NetworkX",
      "ggraph"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "DRDID",
    "description": "Implements locally efficient doubly robust DiD estimators that combine inverse probability weighting and outcome regression for improved statistical properties. Handles both panel data and repeated cross-sections in the canonical 2x2 DiD setting with covariates, providing robustness against model misspecification.",
    "category": "Causal Inference (DiD)",
    "docs_url": "https://psantanna.com/DRDID/",
    "github_url": "https://github.com/pedrohcgs/DRDID",
    "url": "https://cran.r-project.org/package=DRDID",
    "install": "install.packages(\"DRDID\")",
    "tags": [
      "doubly-robust",
      "difference-in-differences",
      "inverse-probability-weighting",
      "ATT",
      "covariates"
    ],
    "best_for": "Two-period DiD with covariates requiring robust estimation against model misspecification, implementing Sant'Anna & Zhao (2020)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "DRDID implements locally efficient doubly robust DiD estimators that combine inverse probability weighting and outcome regression for improved statistical properties. It is used by researchers and practitioners working with panel data and repeated cross-sections in causal inference.",
    "use_cases": [
      "Estimating treatment effects in policy evaluations",
      "Analyzing the impact of interventions in social sciences"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for doubly robust DiD",
      "how to implement difference-in-differences in R",
      "R library for causal inference with covariates",
      "inverse probability weighting in R",
      "outcome regression for DiD in R",
      "statistical properties of DiD estimators in R"
    ],
    "primary_use_cases": [
      "causal inference analysis",
      "treatment effect estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "optmatch",
    "description": "Distance-based bipartite matching using minimum cost network flow algorithms, oriented to matching treatment and control groups in observational studies. Provides optimal full matching and pair matching with support for propensity score distances, Mahalanobis distance, calipers, and exact matching constraints.",
    "category": "Causal Inference (Matching)",
    "docs_url": "https://markmfredrickson.github.io/optmatch",
    "github_url": "https://github.com/markmfredrickson/optmatch",
    "url": "https://cran.r-project.org/package=optmatch",
    "install": "install.packages(\"optmatch\")",
    "tags": [
      "optimal-matching",
      "propensity-score",
      "network-flow",
      "observational-studies",
      "full-matching"
    ],
    "best_for": "When you need mathematically optimal matching solutions that minimize total matched distance with flexible control:treatment ratios (full matching), implementing Hansen & Klopfer (2006)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The optmatch package provides distance-based bipartite matching using minimum cost network flow algorithms, specifically designed for matching treatment and control groups in observational studies. It is used by researchers and data scientists working in causal inference to achieve optimal matching.",
    "use_cases": [
      "Matching treatment and control groups in observational studies",
      "Conducting optimal full matching for causal analysis"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for optimal matching",
      "how to perform full matching in R",
      "distance-based matching in R",
      "matching treatment and control groups in R",
      "R library for network flow algorithms",
      "observational studies matching in R"
    ],
    "primary_use_cases": [
      "optimal full matching",
      "pair matching"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "Synth",
    "description": "The original synthetic control method implementation for comparative case studies. Constructs a weighted combination of comparison units to create a synthetic counterfactual for estimating effects of interventions on a single treated unit, as used in seminal studies of California tobacco program and German reunification.",
    "category": "Causal Inference (Synthetic Control)",
    "docs_url": "https://web.stanford.edu/~jhain/",
    "github_url": "https://github.com/cran/Synth",
    "url": "https://cran.r-project.org/package=Synth",
    "install": "install.packages(\"Synth\")",
    "tags": [
      "synthetic-control",
      "comparative-case-studies",
      "counterfactual",
      "policy-evaluation",
      "single-unit-treatment"
    ],
    "best_for": "Classic single-treated-unit policy evaluations, implementing Abadie, Diamond & Hainmueller (2010, 2011, 2015)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "comparative-case-studies",
      "policy-evaluation"
    ],
    "summary": "Synth is an implementation of the synthetic control method for comparative case studies. It constructs a synthetic counterfactual to estimate the effects of interventions on a single treated unit, making it useful for researchers in policy evaluation.",
    "use_cases": [
      "Evaluating the impact of the California tobacco program",
      "Analyzing the effects of German reunification"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R library for synthetic control",
      "how to perform comparative case studies in R",
      "synthetic counterfactual analysis R",
      "policy evaluation methods in R",
      "single unit treatment effects R",
      "synthetic control method tutorial"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "lmerTest",
    "description": "Provides p-values for lme4 model fits via Satterthwaite's or Kenward-Roger degrees of freedom methods, with Type I/II/III ANOVA tables, model selection tools (step, drop1), and least-squares means calculations.",
    "category": "Mixed Effects",
    "docs_url": "https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf",
    "github_url": "https://github.com/runehaubo/lmerTestR",
    "url": "https://cran.r-project.org/package=lmerTest",
    "install": "install.packages(\"lmerTest\")",
    "tags": [
      "p-values",
      "Satterthwaite",
      "Kenward-Roger",
      "ANOVA",
      "hypothesis-testing"
    ],
    "best_for": "Getting p-values and formal hypothesis tests for lme4 linear mixed models, implementing Kuznetsova et al. (2017)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "hypothesis-testing",
      "mixed-effects-models"
    ],
    "summary": "lmerTest provides p-values for lme4 model fits using Satterthwaite's or Kenward-Roger degrees of freedom methods. It is used by statisticians and data scientists for model selection and ANOVA analysis in mixed effects models.",
    "use_cases": [
      "Conducting ANOVA for mixed effects models",
      "Performing model selection for linear mixed models"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for mixed effects models",
      "how to get p-values for lme4 models in R",
      "ANOVA tables in R",
      "model selection tools in R",
      "Satterthwaite method in R",
      "Kenward-Roger method in R"
    ],
    "primary_use_cases": [
      "ANOVA analysis for mixed effects models",
      "p-value calculation for lme4 model fits"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "lme4"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "tmle3",
    "description": "A modular, extensible framework for targeted minimum loss-based estimation supporting custom TMLE parameters through a unified interface. Part of the tlverse ecosystem, designed to be as general as the mathematical TMLE framework itself for complex analyses.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://tlverse.org/tmle3/",
    "github_url": "https://github.com/tlverse/tmle3",
    "url": "https://github.com/tlverse/tmle3",
    "install": "remotes::install_github(\"tlverse/tmle3\")",
    "tags": [
      "TMLE",
      "tlverse",
      "modular",
      "extensible",
      "stochastic-interventions"
    ],
    "best_for": "Complex TMLE analyses requiring custom parameters, mediation, stochastic interventions, or optimal treatment regimes",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "tmle3 is a modular and extensible framework designed for targeted minimum loss-based estimation, allowing users to support custom TMLE parameters through a unified interface. It is part of the tlverse ecosystem and is suitable for complex analyses in causal inference.",
    "use_cases": [],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for targeted minimum loss-based estimation",
      "how to use tmle3 for causal inference",
      "tmle3 examples in R",
      "tlverse ecosystem packages",
      "modular framework for TMLE in R",
      "tmle3 documentation"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "tlverse"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "CBPS",
    "description": "Implements Covariate Balancing Propensity Score, which estimates propensity scores by jointly optimizing treatment prediction and covariate balance via generalized method of moments (GMM). Supports binary, multi-valued, and continuous treatments, as well as longitudinal settings for marginal structural models.",
    "category": "Causal Inference (Matching)",
    "docs_url": "https://cran.r-project.org/web/packages/CBPS/CBPS.pdf",
    "github_url": "https://github.com/kosukeimai/CBPS",
    "url": "https://cran.r-project.org/package=CBPS",
    "install": "install.packages(\"CBPS\")",
    "tags": [
      "propensity-score",
      "covariate-balance",
      "GMM",
      "weighting",
      "treatment-effects"
    ],
    "best_for": "When propensity score model specification is uncertain and you want simultaneous balance optimization, implementing Imai & Ratkovic (2014)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "CBPS implements the Covariate Balancing Propensity Score method, which estimates propensity scores by optimizing treatment prediction and covariate balance using generalized method of moments (GMM). It is useful for researchers and practitioners in causal inference who need to analyze treatment effects.",
    "use_cases": [
      "Estimating treatment effects in observational studies",
      "Balancing covariates in experimental designs"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for covariate balancing",
      "how to estimate propensity scores in R",
      "R package for treatment effects analysis",
      "propensity score matching in R",
      "GMM for treatment prediction R",
      "longitudinal treatment effects R"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "hesim",
    "description": "R package for health economic simulation modeling. Cohort discrete-time state transition models, partitioned survival analysis, and probabilistic sensitivity analysis with parallelization.",
    "category": "Healthcare Economics & Health-Tech",
    "docs_url": "https://hesim-dev.github.io/hesim/",
    "github_url": "https://github.com/hesim-dev/hesim",
    "url": "https://hesim-dev.github.io/hesim/",
    "install": "install.packages('hesim')",
    "tags": [
      "health economics",
      "simulation",
      "cost-effectiveness",
      "R"
    ],
    "best_for": "Health economic decision modeling and cost-effectiveness analysis",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "health economics",
      "simulation"
    ],
    "summary": "The hesim package is designed for health economic simulation modeling, allowing users to create cohort discrete-time state transition models and perform partitioned survival analysis. It is primarily used by health economists and researchers in health technology assessment.",
    "use_cases": [
      "Modeling health interventions",
      "Conducting cost-effectiveness analyses"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for health economic simulation",
      "how to perform cost-effectiveness analysis in R",
      "health simulation modeling in R",
      "R tools for probabilistic sensitivity analysis",
      "partitioned survival analysis R package",
      "R health economics package"
    ],
    "primary_use_cases": [
      "health intervention modeling",
      "probabilistic sensitivity analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "heemod",
      "BCEA",
      "dampack"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "gsynth",
    "description": "Implements generalized synthetic control with interactive fixed effects, extending SCM to multiple treated units with variable treatment timing. Uses factor models to impute counterfactuals, handling unbalanced panels and complex treatment patterns with latent factor structures.",
    "category": "Causal Inference (Synthetic Control)",
    "docs_url": "https://yiqingxu.org/packages/gsynth/",
    "github_url": "https://github.com/xuyiqing/gsynth",
    "url": "https://cran.r-project.org/package=gsynth",
    "install": "install.packages(\"gsynth\")",
    "tags": [
      "generalized-synthetic-control",
      "interactive-fixed-effects",
      "factor-models",
      "multiple-treated-units",
      "unbalanced-panels"
    ],
    "best_for": "Multiple treated units with staggered treatment timing and latent factor structures, implementing Xu (2017)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The gsynth package implements generalized synthetic control methods with interactive fixed effects, allowing for the analysis of multiple treated units with varying treatment timings. It is primarily used by researchers and practitioners in causal inference to handle complex treatment patterns in unbalanced panel data.",
    "use_cases": [
      "Analyzing the impact of policy changes across multiple regions",
      "Evaluating treatment effects in economic studies"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for generalized synthetic control",
      "how to implement interactive fixed effects in R",
      "R library for causal inference with unbalanced panels",
      "generalized synthetic control methods in R",
      "factor models for counterfactuals in R",
      "R package for multiple treated units analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "SciPy Bootstrap",
    "description": "Foundational module within SciPy for a wide range of statistical functions, distributions, and hypothesis tests (t-tests, ANOVA, chi\u00b2, KS, etc.).",
    "category": "Standard Errors, Bootstrapping & Reporting",
    "docs_url": "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html",
    "github_url": "https://github.com/scipy/scipy",
    "url": "https://github.com/scipy/scipy",
    "install": "pip install scipy",
    "tags": [
      "bootstrap",
      "standard errors",
      "inference",
      "hypothesis testing"
    ],
    "best_for": "Hypothesis tests, confidence intervals, multiple testing",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "SciPy Bootstrap is a foundational module within SciPy that provides a wide range of statistical functions, distributions, and hypothesis tests. It is used by data scientists and statisticians for performing statistical inference and hypothesis testing.",
    "use_cases": [
      "Performing t-tests for comparing two groups",
      "Conducting ANOVA for multiple group comparisons"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for statistical functions",
      "how to perform hypothesis testing in python",
      "python bootstrap methods",
      "using SciPy for ANOVA",
      "SciPy statistical distributions",
      "python library for t-tests"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "primary_use_cases": [
      "hypothesis testing",
      "statistical inference"
    ],
    "related_packages": [
      "NumPy",
      "statsmodels"
    ],
    "model_score": 0.0001
  },
  {
    "name": "scpi",
    "description": "Provides rigorous prediction intervals for synthetic control methods following Cattaneo et al. (2021, 2025). Supports staggered adoption designs with valid uncertainty quantification.",
    "category": "Causal Inference (Synthetic Control)",
    "docs_url": "https://nppackages.github.io/scpi/",
    "github_url": null,
    "url": "https://cran.r-project.org/package=scpi",
    "install": "install.packages(\"scpi\")",
    "tags": [
      "synthetic-control",
      "prediction-intervals",
      "uncertainty-quantification",
      "staggered-adoption",
      "inference"
    ],
    "best_for": "Rigorous prediction intervals for synthetic control, implementing Cattaneo et al. (2021, 2025)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "uncertainty-quantification"
    ],
    "summary": "The scpi package provides rigorous prediction intervals for synthetic control methods, allowing users to perform valid uncertainty quantification in staggered adoption designs. It is particularly useful for researchers and practitioners in causal inference.",
    "use_cases": [],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R library for synthetic control methods",
      "how to create prediction intervals in R",
      "uncertainty quantification in R",
      "staggered adoption designs in R",
      "Cattaneo 2021 synthetic control",
      "Cattaneo 2025 prediction intervals"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Cattaneo et al. (2021)",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "Synthpop",
    "description": "Port of the R package for generating synthetic populations based on sample survey data.",
    "category": "Synthetic Data Generation",
    "docs_url": null,
    "github_url": "https://github.com/alan-turing-institute/synthpop",
    "url": "https://github.com/alan-turing-institute/synthpop",
    "install": "pip install synthpop",
    "tags": [
      "synthetic data",
      "simulation"
    ],
    "best_for": "Privacy-preserving data, simulation, augmentation",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "Synthpop is a Python package that generates synthetic populations based on sample survey data. It is useful for researchers and data scientists who need to create realistic datasets for analysis and simulation.",
    "use_cases": [
      "Creating synthetic datasets for testing algorithms",
      "Simulating survey data for research purposes"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for synthetic data generation",
      "how to generate synthetic populations in python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "cobalt",
    "description": "Generates standardized balance tables and plots for covariates after preprocessing via matching, weighting, or subclassification. Provides unified balance assessment across multiple R packages (MatchIt, WeightIt, twang, Matching, optmatch, CBPS, ebal, cem, sbw, designmatch). Supports multi-category, continuous, and longitudinal treatments with clustered and multiply imputed data.",
    "category": "Causal Inference (Matching)",
    "docs_url": "https://ngreifer.github.io/cobalt/",
    "github_url": "https://github.com/ngreifer/cobalt",
    "url": "https://cran.r-project.org/package=cobalt",
    "install": "install.packages(\"cobalt\")",
    "tags": [
      "covariate-balance",
      "balance-diagnostics",
      "love-plot",
      "standardized-mean-difference",
      "balance-tables"
    ],
    "best_for": "Assessing and visualizing covariate balance before/after matching or weighting to validate causal inference preprocessing",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The cobalt package generates standardized balance tables and plots for covariates after preprocessing via matching, weighting, or subclassification. It is used by researchers and practitioners in causal inference to assess balance across multiple R packages.",
    "use_cases": [
      "Assessing covariate balance after matching",
      "Visualizing balance diagnostics for treatment groups"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for covariate balance",
      "how to assess balance in causal inference R",
      "R balance diagnostics tools",
      "generate love plot in R",
      "standardized mean difference in R",
      "R package for matching and weighting"
    ],
    "primary_use_cases": [
      "balance assessment across multiple R packages",
      "generating balance tables and plots"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "MatchIt",
      "WeightIt",
      "twang",
      "Matching",
      "optmatch"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "rdlocrand",
    "description": "Provides tools for RD analysis under local randomization: rdrandinf() performs hypothesis testing using randomization inference, rdwinselect() selects a window around the cutoff where randomization likely holds, rdsensitivity() assesses sensitivity to different windows, and rdrbounds() constructs Rosenbaum bounds for unobserved confounders.",
    "category": "Causal Inference (RDD)",
    "docs_url": "https://rdpackages.github.io/rdlocrand/",
    "github_url": "https://github.com/rdpackages/rdlocrand",
    "url": "https://cran.r-project.org/package=rdlocrand",
    "install": "install.packages(\"rdlocrand\")",
    "tags": [
      "local-randomization",
      "randomization-inference",
      "finite-sample",
      "window-selection",
      "sensitivity-analysis"
    ],
    "best_for": "Finite-sample inference in RDD when local randomization assumption is plausible near the cutoff, implementing Cattaneo, Frandsen & Titiunik (2015)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The rdlocrand package provides tools for regression discontinuity analysis under local randomization. It is useful for researchers and practitioners who need to perform hypothesis testing and sensitivity analysis in causal inference settings.",
    "use_cases": [
      "Performing hypothesis testing using randomization inference",
      "Assessing sensitivity to different windows in regression discontinuity designs"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for randomization inference",
      "how to perform sensitivity analysis in R",
      "tools for regression discontinuity analysis in R",
      "R local randomization methods",
      "hypothesis testing with randomization in R",
      "window selection for RDD in R"
    ],
    "primary_use_cases": [
      "hypothesis testing",
      "sensitivity analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "synthdid",
    "description": "Implements synthetic difference-in-differences, a hybrid method combining insights from both DiD and synthetic control that reweights and matches pre-treatment trends. Provides improved robustness properties compared to either method alone by combining their strengths.",
    "category": "Causal Inference (Synthetic Control)",
    "docs_url": "https://synth-inference.github.io/synthdid/",
    "github_url": "https://github.com/synth-inference/synthdid",
    "url": "https://cran.r-project.org/package=synthdid",
    "install": "install.packages(\"synthdid\")",
    "tags": [
      "synthetic-control",
      "difference-in-differences",
      "hybrid-estimator",
      "panel-data",
      "robust-estimation"
    ],
    "best_for": "Settings where neither pure DiD nor pure SC is ideal, implementing Arkhangelsky, Athey, Hirshberg, Imbens & Wager (2021)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "synthetic-control"
    ],
    "summary": "The synthdid package implements synthetic difference-in-differences, a method that combines insights from both difference-in-differences and synthetic control. It is used by researchers and practitioners in causal inference to improve robustness in estimating treatment effects.",
    "use_cases": [],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for synthetic difference-in-differences",
      "how to implement synthetic control in R",
      "difference-in-differences method in R",
      "hybrid estimator for causal inference R",
      "robust estimation techniques in R",
      "panel data analysis in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "mlsynth",
    "description": "Implements advanced synthetic control methods: forward DiD, cluster SC, factor models, and proximal SC. Designed for single-treated-unit settings.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": "https://mlsynth.readthedocs.io/en/latest/",
    "github_url": "https://github.com/jgreathouse9/mlsynth",
    "url": "https://github.com/jgreathouse9/mlsynth",
    "install": "pip install mlsynth",
    "tags": [
      "DiD",
      "synthetic control",
      "RDD"
    ],
    "best_for": "Policy evaluation, natural experiments, quasi-experiments",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "program-evaluation",
      "synthetic-control"
    ],
    "summary": "mlsynth implements advanced synthetic control methods for evaluating causal effects in single-treated-unit settings. It is designed for researchers and practitioners in program evaluation.",
    "use_cases": [
      "Evaluating the impact of a policy intervention on a single unit",
      "Comparing outcomes before and after treatment using synthetic control methods"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for synthetic control",
      "how to implement DiD in python",
      "python package for causal inference",
      "advanced synthetic control methods in python",
      "cluster synthetic control in python",
      "factor models for program evaluation in python"
    ],
    "primary_use_cases": [
      "synthetic control analysis",
      "forward DiD implementation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "GenX",
    "description": "Capacity expansion model from MIT/Princeton in Julia. Highly configurable with unit commitment, long-duration storage, and transmission expansion. Used for Net-Zero America.",
    "category": "Energy & Utilities Economics",
    "docs_url": "https://genxproject.github.io/GenX/",
    "github_url": "https://github.com/GenXProject/GenX",
    "url": "https://genxproject.github.io/GenX/",
    "install": "Julia package",
    "tags": [
      "capacity expansion",
      "Julia",
      "decarbonization"
    ],
    "best_for": "Policy-relevant capacity expansion with detailed operational constraints",
    "language": "Julia",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "GenX is a capacity expansion model developed by MIT and Princeton, implemented in Julia. It is highly configurable and is utilized for scenarios involving unit commitment, long-duration storage, and transmission expansion, particularly in the context of achieving net-zero emissions.",
    "use_cases": [
      "Modeling energy system transitions",
      "Evaluating long-term storage solutions"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "GenX Julia package for capacity expansion",
      "how to model decarbonization in Julia",
      "capacity expansion model for energy utilities",
      "Julia package for transmission expansion",
      "long-duration storage modeling in Julia",
      "Net-Zero America capacity expansion tool"
    ],
    "primary_use_cases": [
      "Capacity expansion"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "PyPSA",
      "SWITCH",
      "ReEDS"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "GenX",
    "description": "Julia-based capacity expansion model for electricity systems with detailed operational constraints",
    "category": "Energy Systems Modeling",
    "docs_url": "https://genxproject.github.io/GenX/",
    "github_url": "https://github.com/GenXProject/GenX",
    "url": "https://genxproject.github.io/GenX/",
    "install": "using Pkg; Pkg.add(\"GenX\")",
    "tags": [
      "capacity expansion",
      "Julia",
      "electricity planning",
      "decarbonization"
    ],
    "best_for": "Long-term electricity system planning and decarbonization pathway analysis",
    "language": "Julia",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "GenX is a Julia-based capacity expansion model designed for electricity systems, incorporating detailed operational constraints. It is used by researchers and practitioners in the field of energy systems modeling to analyze and plan electricity generation and distribution.",
    "use_cases": [
      "Modeling the expansion of renewable energy sources",
      "Analyzing the impact of operational constraints on electricity systems"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "Julia library for capacity expansion modeling",
      "how to model electricity systems in Julia",
      "decarbonization strategies using GenX",
      "electricity planning tools in Julia"
    ],
    "primary_use_cases": [
      "Capacity expansion",
      "Long-term planning"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "PowerModels.jl",
      "JuMP"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "grf",
    "description": "Forest-based statistical estimation and inference for heterogeneous treatment effects, supporting multiple treatment arms, instrumental variables, survival outcomes, and quantile regression\u2014all with honest estimation and valid confidence intervals. The most widely-used R package for CATE estimation.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://grf-labs.github.io/grf/",
    "github_url": "https://github.com/grf-labs/grf",
    "url": "https://cran.r-project.org/package=grf",
    "install": "install.packages(\"grf\")",
    "tags": [
      "causal-forest",
      "heterogeneous-treatment-effects",
      "CATE",
      "machine-learning",
      "econometrics"
    ],
    "best_for": "Estimating individual-level treatment effects (CATE) with valid statistical inference in RCTs or observational studies, implementing Athey, Tibshirani & Wager (2019)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "econometrics"
    ],
    "summary": "The 'grf' package provides forest-based statistical estimation and inference for heterogeneous treatment effects, making it suitable for various complex scenarios such as multiple treatment arms and instrumental variables. It is widely used by researchers and practitioners in fields like econometrics and machine learning for causal analysis.",
    "use_cases": [
      "Estimating causal effects in clinical trials",
      "Analyzing treatment effects in observational studies"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for causal forest",
      "how to estimate treatment effects in R",
      "R package for heterogeneous treatment effects",
      "using grf for CATE estimation",
      "forest-based inference in R",
      "R package for quantile regression"
    ],
    "primary_use_cases": [
      "causal forest estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "ltmle",
    "description": "Targeted maximum likelihood estimation for treatment/censoring-specific mean outcomes with time-varying treatments and confounders. Supports longitudinal settings, marginal structural models, and dynamic treatment regimes alongside IPTW and G-computation.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://joshuaschwab.github.io/ltmle/",
    "github_url": "https://github.com/joshuaschwab/ltmle",
    "url": "https://cran.r-project.org/package=ltmle",
    "install": "install.packages(\"ltmle\")",
    "tags": [
      "TMLE",
      "longitudinal",
      "time-varying-treatment",
      "dynamic-regimes",
      "MSM"
    ],
    "best_for": "Causal inference with time-varying treatments, time-varying confounders, and right-censored longitudinal data, implementing Lendle et al. (2017)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "longitudinal-data"
    ],
    "summary": "The ltmle package provides targeted maximum likelihood estimation for treatment and censoring-specific mean outcomes in longitudinal settings. It is particularly useful for researchers dealing with time-varying treatments and confounders in causal inference.",
    "use_cases": [
      "Estimating treatment effects in clinical trials",
      "Analyzing longitudinal health data"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for targeted maximum likelihood estimation",
      "how to use ltmle for longitudinal data analysis",
      "time-varying treatment analysis in R",
      "causal inference with ltmle",
      "dynamic treatment regimes in R",
      "marginal structural models R package"
    ],
    "primary_use_cases": [
      "causal forest estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "rdpower",
    "description": "Provides tools for power, sample size, and minimum detectable effects (MDE) calculations in RD designs using robust bias-corrected local polynomial inference: rdpower() calculates power, rdsampsi() calculates required sample size for desired power, and rdmde() computes minimum detectable effects.",
    "category": "Causal Inference (RDD)",
    "docs_url": "https://rdpackages.github.io/rdpower/",
    "github_url": "https://github.com/rdpackages/rdpower",
    "url": "https://cran.r-project.org/package=rdpower",
    "install": "install.packages(\"rdpower\")",
    "tags": [
      "power-analysis",
      "sample-size",
      "MDE",
      "study-design",
      "ex-ante-analysis"
    ],
    "best_for": "Planning RDD studies\u2014calculating required sample sizes, statistical power, or minimum detectable effects, implementing Cattaneo, Titiunik & Vazquez-Bare (2019)",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The rdpower package provides tools for power, sample size, and minimum detectable effects calculations in regression discontinuity designs. It is useful for researchers and practitioners working in causal inference who need to determine the necessary sample sizes and power for their studies.",
    "use_cases": [
      "Calculating power for a regression discontinuity design study",
      "Determining sample size needed for desired power in an RD analysis"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "R package for power analysis",
      "how to calculate sample size in R",
      "minimum detectable effects in R",
      "tools for study design in R",
      "power calculations for RD designs",
      "rdpower package documentation",
      "R package for ex-ante analysis"
    ],
    "primary_use_cases": [
      "power calculation",
      "sample size determination",
      "minimum detectable effects computation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "fabricatr",
    "description": "Simulates realistic social science data for power analysis and design testing. Creates hierarchical data structures with correlated variables matching real-world patterns.",
    "category": "Experimental Design",
    "docs_url": "https://declaredesign.org/r/fabricatr/",
    "github_url": "https://github.com/DeclareDesign/fabricatr",
    "url": "https://cran.r-project.org/package=fabricatr",
    "install": "install.packages(\"fabricatr\")",
    "tags": [
      "data-simulation",
      "power-analysis",
      "hierarchical-data",
      "synthetic-data",
      "design-testing"
    ],
    "best_for": "Simulating realistic hierarchical data for experimental power analysis and design testing",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "fabricatr simulates realistic social science data for power analysis and design testing. It is used by researchers and data scientists to create hierarchical data structures with correlated variables that reflect real-world patterns.",
    "use_cases": [
      "Simulating data for social science research",
      "Testing design methodologies in experimental studies"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R library for data simulation",
      "how to simulate social science data in R",
      "R package for power analysis",
      "creating hierarchical data in R",
      "synthetic data generation in R",
      "design testing with R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "survival",
    "description": "Core R package for survival analysis with Cox regression, Kaplan-Meier estimation, and parametric survival models - the foundation for time-to-event analysis",
    "category": "Insurance & Actuarial",
    "docs_url": "https://cran.r-project.org/web/packages/survival/vignettes/survival.pdf",
    "github_url": "https://github.com/therneau/survival",
    "url": "https://cran.r-project.org/package=survival",
    "install": "install.packages(\"survival\")",
    "tags": [
      "survival-analysis",
      "Cox-regression",
      "Kaplan-Meier",
      "time-to-event",
      "hazard-models"
    ],
    "best_for": "Foundation for survival analysis in R, mortality studies, and duration modeling",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "survival-analysis",
      "time-to-event"
    ],
    "summary": "The survival package is a core R package designed for survival analysis, including techniques such as Cox regression and Kaplan-Meier estimation. It is widely used by statisticians and data scientists working in fields like insurance and actuarial science for time-to-event analysis.",
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for survival analysis",
      "how to perform Cox regression in R",
      "Kaplan-Meier estimation in R",
      "time-to-event analysis in R",
      "hazard models in R",
      "survival analysis techniques in R"
    ],
    "use_cases": [
      "Analyzing patient survival times in clinical trials",
      "Estimating the time until an event occurs in insurance claims"
    ],
    "primary_use_cases": [
      "Cox regression analysis",
      "Kaplan-Meier survival curves"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "survminer",
      "survival",
      "flexsurv"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "survival",
    "description": "Core survival analysis package in R. Kaplan-Meier, Cox regression, parametric models, and diagnostic tools. Foundation for most R survival packages.",
    "category": "Healthcare Economics & Health-Tech",
    "docs_url": "https://cran.r-project.org/web/packages/survival/",
    "github_url": "https://github.com/therneau/survival",
    "url": "https://cran.r-project.org/web/packages/survival/",
    "install": "install.packages('survival')",
    "tags": [
      "survival analysis",
      "Cox regression",
      "Kaplan-Meier",
      "R"
    ],
    "best_for": "Core survival analysis in R",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "survival analysis"
    ],
    "summary": "The 'survival' package is a core tool for survival analysis in R, providing functions for Kaplan-Meier estimates, Cox regression, and parametric models. It is widely used by statisticians and data scientists working in healthcare economics and health technology.",
    "use_cases": [
      "Analyzing patient survival times",
      "Evaluating treatment effects in clinical trials"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for survival analysis",
      "how to perform Cox regression in R",
      "Kaplan-Meier analysis in R",
      "survival analysis tools in R",
      "R survival package documentation",
      "best practices for survival analysis in R"
    ],
    "primary_use_cases": [
      "Kaplan-Meier estimation",
      "Cox proportional hazards modeling"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "survminer",
      "survivalanalysis"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "lavaan",
    "description": "Free, open-source latent variable analysis providing commercial-quality functionality for path analysis, confirmatory factor analysis, structural equation modeling, and growth curve models with intuitive model syntax.",
    "category": "Structural Equation Modeling",
    "docs_url": "https://lavaan.ugent.be/",
    "github_url": "https://github.com/yrosseel/lavaan",
    "url": "https://cran.r-project.org/package=lavaan",
    "install": "install.packages(\"lavaan\")",
    "tags": [
      "SEM",
      "CFA",
      "path-analysis",
      "latent-variables",
      "psychometrics"
    ],
    "best_for": "General-purpose structural equation modeling with accessible syntax for researchers, implementing Rosseel (2012)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "latent-variables",
      "psychometrics"
    ],
    "summary": "The lavaan package provides free, open-source functionality for latent variable analysis, including path analysis, confirmatory factor analysis, and structural equation modeling. It is commonly used by researchers and practitioners in psychology and social sciences for modeling complex relationships between variables.",
    "use_cases": [
      "Analyzing survey data to understand underlying factors",
      "Modeling growth trajectories in longitudinal studies"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for latent variable analysis",
      "how to perform SEM in R",
      "R confirmatory factor analysis tutorial",
      "path analysis in R",
      "latent variable modeling with lavaan",
      "R package for psychometrics"
    ],
    "primary_use_cases": [
      "confirmatory factor analysis",
      "structural equation modeling"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "sem",
      "lavaan.survey"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "marginaleffects",
    "description": "Modern standard for interpreting regression results\u2014up to 1000\u00d7 faster than margins. Computes marginal effects, predictions, contrasts, and slopes for 100+ model classes. Published in JSS 2024.",
    "category": "Marginal Effects",
    "docs_url": "https://marginaleffects.com/",
    "github_url": "https://github.com/vincentarelbundock/marginaleffects",
    "url": "https://cran.r-project.org/package=marginaleffects",
    "install": "install.packages(\"marginaleffects\")",
    "tags": [
      "marginal-effects",
      "predictions",
      "contrasts",
      "interpretation",
      "slopes"
    ],
    "best_for": "Modern marginal effects interpretation\u20141000\u00d7 faster than margins with 100+ model support, JSS 2024",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "interpretation"
    ],
    "summary": "The marginaleffects package provides a modern standard for interpreting regression results, allowing users to compute marginal effects, predictions, contrasts, and slopes for over 100 model classes. It is designed for statisticians and data scientists who need efficient and accurate interpretation of regression outputs.",
    "use_cases": [
      "Interpreting regression results quickly",
      "Comparing model predictions",
      "Analyzing contrasts between groups"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for marginal effects",
      "how to compute predictions in R",
      "R package for regression interpretation",
      "marginal effects in R",
      "how to use contrasts in R",
      "R slopes analysis package"
    ],
    "primary_use_cases": [
      "computing marginal effects",
      "making predictions from regression models"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "JSS (2024)",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "python-louvain",
    "description": "Community detection in large networks using the Louvain algorithm, applicable to defense network analysis",
    "category": "Network Analysis",
    "docs_url": "https://python-louvain.readthedocs.io/",
    "github_url": "https://github.com/taynaud/python-louvain",
    "url": "https://python-louvain.readthedocs.io/",
    "install": "pip install python-louvain",
    "tags": [
      "community detection",
      "clustering",
      "networks",
      "Louvain"
    ],
    "best_for": "Identifying clusters in defense supply chains and alliance networks",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The python-louvain package facilitates community detection in large networks using the Louvain algorithm. It is particularly useful for analyzing defense networks.",
    "use_cases": [
      "Analyzing defense network structures",
      "Identifying communities in social networks"
    ],
    "audience": [
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for community detection",
      "how to analyze networks in python",
      "python louvain algorithm",
      "community detection in large networks",
      "clustering networks in python",
      "python package for clustering"
    ],
    "primary_use_cases": [
      "Community detection",
      "Network clustering"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "NetworkX",
      "igraph"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "mFilter",
    "description": "Implements time series filters for extracting trend and cyclical components. Includes Hodrick-Prescott, Baxter-King, Christiano-Fitzgerald, Butterworth, and trigonometric regression filters commonly used in macroeconomics and business cycle analysis.",
    "category": "Time Series Econometrics",
    "docs_url": "https://cran.r-project.org/web/packages/mFilter/mFilter.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=mFilter",
    "install": "install.packages(\"mFilter\")",
    "tags": [
      "HP-filter",
      "Baxter-King",
      "trend-extraction",
      "business-cycles",
      "detrending"
    ],
    "best_for": "Decomposing time series into trend and cyclical components for business cycle analysis",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "time-series"
    ],
    "summary": "mFilter implements various time series filters to extract trend and cyclical components, which are essential in macroeconomics and business cycle analysis. It is used by economists and data scientists working with time series data.",
    "use_cases": [
      "Analyzing economic trends",
      "Detrending time series data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for time series filtering",
      "how to extract trend components in R",
      "R package for Hodrick-Prescott filter",
      "Baxter-King filter implementation in R",
      "trigonometric regression filters in R",
      "business cycle analysis with R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "SDV (Synthetic Data Vault)",
    "description": "Comprehensive library for generating synthetic tabular, relational, and time series data using various models.",
    "category": "Synthetic Data Generation",
    "docs_url": "https://sdv.dev/",
    "github_url": "https://github.com/sdv-dev/SDV",
    "url": "https://github.com/sdv-dev/SDV",
    "install": "pip install sdv",
    "tags": [
      "synthetic data",
      "simulation"
    ],
    "best_for": "Privacy-preserving data, simulation, augmentation",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "SDV is a comprehensive library for generating synthetic tabular, relational, and time series data using various models. It is used by data scientists and researchers to create realistic datasets for testing and simulation purposes.",
    "use_cases": [
      "Generating synthetic datasets for machine learning",
      "Simulating data for testing algorithms"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for synthetic data generation",
      "how to generate synthetic data in python",
      "synthetic data generation techniques",
      "best practices for synthetic data",
      "using SDV for simulation",
      "creating synthetic datasets with python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "scikit-survival",
    "description": "Machine learning for survival analysis compatible with scikit-learn, including gradient boosted models, random survival forests, and Cox neural networks",
    "category": "Insurance & Actuarial",
    "docs_url": "https://scikit-survival.readthedocs.io/",
    "github_url": "https://github.com/sebp/scikit-survival",
    "url": "https://github.com/sebp/scikit-survival",
    "install": "pip install scikit-survival",
    "tags": [
      "survival-analysis",
      "machine-learning",
      "scikit-learn",
      "random-forests",
      "gradient-boosting"
    ],
    "best_for": "ML-based survival prediction, combining modern algorithms with censored data handling",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "scikit-learn"
    ],
    "topic_tags": [
      "survival-analysis",
      "machine-learning"
    ],
    "summary": "scikit-survival is a Python library designed for machine learning in survival analysis. It is compatible with scikit-learn and includes various models such as gradient boosted models and random survival forests, making it useful for data scientists and statisticians working in fields like insurance and actuarial science.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for survival analysis",
      "how to do survival analysis in python",
      "machine learning for survival analysis",
      "scikit-learn compatible survival analysis library",
      "gradient boosted models for survival analysis",
      "random survival forests in python"
    ],
    "use_cases": [
      "Analyzing patient survival times in medical research",
      "Predicting customer churn in insurance"
    ],
    "primary_use_cases": [
      "survival analysis using gradient boosted models",
      "predicting survival with random survival forests"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "scikit-learn"
    ],
    "related_packages": [
      "lifelines",
      "survival"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "scikit-survival",
    "description": "Survival analysis compatible with scikit-learn. Includes Cox proportional hazards, random survival forests, gradient boosting survival, and evaluation metrics (C-index, Brier score).",
    "category": "Healthcare Economics & Health-Tech",
    "docs_url": "https://scikit-survival.readthedocs.io/",
    "github_url": "https://github.com/sebp/scikit-survival",
    "url": "https://scikit-survival.readthedocs.io/",
    "install": "pip install scikit-survival",
    "tags": [
      "survival analysis",
      "machine learning",
      "clinical prediction"
    ],
    "best_for": "ML-based survival analysis with scikit-learn integration",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "scikit-learn"
    ],
    "topic_tags": [
      "survival-analysis",
      "machine-learning"
    ],
    "summary": "scikit-survival is a Python package designed for survival analysis that integrates seamlessly with scikit-learn. It is used by data scientists and researchers in healthcare to perform tasks such as clinical prediction and evaluation of survival models.",
    "use_cases": [
      "Analyzing patient survival data",
      "Predicting clinical outcomes based on survival analysis"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for survival analysis",
      "how to perform Cox proportional hazards in python",
      "scikit-learn compatible survival analysis tools",
      "best practices for clinical prediction in python",
      "using random survival forests in python",
      "evaluation metrics for survival analysis in python"
    ],
    "primary_use_cases": [
      "Cox proportional hazards modeling",
      "random survival forests"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "lifelines",
      "survival"
    ],
    "maintenance_status": "active",
    "framework_compatibility": [
      "scikit-learn"
    ],
    "model_score": 0.0001
  },
  {
    "name": "gamlss",
    "description": "Distributional regression where all parameters of a response distribution (location, scale, shape) can be modeled as functions of predictors, supporting 100+ distributions including highly skewed and kurtotic continuous and discrete distributions.",
    "category": "Generalized Additive Models",
    "docs_url": "https://www.gamlss.com/",
    "github_url": "https://github.com/gamlss-dev/gamlss",
    "url": "https://cran.r-project.org/package=gamlss",
    "install": "install.packages(\"gamlss\")",
    "tags": [
      "distributional-regression",
      "location-scale-shape",
      "flexible-distributions",
      "centile-estimation",
      "beyond-mean-modeling"
    ],
    "best_for": "Modeling non-normal responses where variance, skewness, or kurtosis depend on predictors, implementing Rigby & Stasinopoulos (2005)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The 'gamlss' package allows users to model all parameters of a response distribution as functions of predictors, supporting over 100 distributions. It is useful for statisticians and data scientists who need to analyze complex data distributions beyond traditional mean modeling.",
    "use_cases": [
      "Modeling highly skewed data distributions",
      "Estimating centiles for non-normal data"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for distributional regression",
      "how to model location scale shape in R",
      "R flexible distributions library",
      "gamlss usage examples",
      "R centile estimation package",
      "advanced regression modeling in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "ruspy",
    "description": "Python package for simulation and estimation of Rust (1987) bus engine replacement model. Implements the nested fixed point (NFXP) algorithm for dynamic discrete choice. The reference implementation for learning structural estimation.",
    "category": "Structural Econometrics & Estimation",
    "docs_url": "https://ruspy.readthedocs.io/",
    "github_url": "https://github.com/OpenSourceEconomics/ruspy",
    "url": "https://github.com/OpenSourceEconomics/ruspy",
    "install": "pip install ruspy",
    "tags": [
      "structural estimation",
      "dynamic discrete choice",
      "econometrics"
    ],
    "best_for": "Learning and implementing dynamic discrete choice models",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "structural estimation",
      "dynamic discrete choice",
      "econometrics"
    ],
    "summary": "ruspy is a Python package designed for the simulation and estimation of the Rust (1987) bus engine replacement model. It implements the nested fixed point (NFXP) algorithm for dynamic discrete choice, making it a valuable tool for those involved in structural estimation.",
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for structural estimation",
      "how to estimate dynamic discrete choice models in python",
      "NFXP algorithm implementation in python",
      "bus engine replacement model simulation python",
      "econometrics tools in python",
      "python package for Rust (1987) model"
    ],
    "use_cases": [
      "Estimating bus engine replacement decisions",
      "Simulating dynamic discrete choice models"
    ],
    "primary_use_cases": [
      "dynamic discrete choice modeling",
      "structural estimation"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "respy"
    ],
    "maintenance_status": "active",
    "implements_paper": "Rust (1987)",
    "model_score": 0.0001
  },
  {
    "name": "emmeans",
    "description": "Estimated Marginal Means (least-squares means) for factorial designs. Computes adjusted means and contrasts for balanced and unbalanced designs, with support for mixed models and Bayesian models.",
    "category": "Marginal Effects",
    "docs_url": "https://rvlenth.github.io/emmeans/",
    "github_url": "https://github.com/rvlenth/emmeans",
    "url": "https://cran.r-project.org/package=emmeans",
    "install": "install.packages(\"emmeans\")",
    "tags": [
      "marginal-means",
      "least-squares-means",
      "factorial-designs",
      "contrasts",
      "mixed-models"
    ],
    "best_for": "Estimated marginal means for factorial designs with interaction interpretation",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "marginal-effects",
      "mixed-models"
    ],
    "summary": "The emmeans package computes estimated marginal means, also known as least-squares means, for factorial designs. It is used by statisticians and data scientists to analyze and interpret the effects of factors in both balanced and unbalanced designs, including mixed and Bayesian models.",
    "use_cases": [
      "Analyzing the effects of different treatments in an experiment",
      "Comparing group means in a factorial design"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for estimated marginal means",
      "how to compute least-squares means in R",
      "R contrasts for factorial designs",
      "mixed models analysis in R",
      "bayesian models with emmeans",
      "marginal effects in R"
    ],
    "primary_use_cases": [
      "A/B test analysis",
      "contrasts in factorial designs"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "puncc",
    "description": "IRT Lab's library for predictive uncertainty with conformal prediction. Supports various conformal methods.",
    "category": "Conformal Prediction & Uncertainty",
    "docs_url": "https://github.com/deel-ai/puncc",
    "github_url": "https://github.com/deel-ai/puncc",
    "url": "https://github.com/deel-ai/puncc",
    "install": "pip install puncc",
    "tags": [
      "conformal prediction",
      "uncertainty",
      "calibration"
    ],
    "best_for": "Calibrated prediction sets",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "puncc is a library developed by IRT Lab for predictive uncertainty using conformal prediction methods. It is designed for users interested in applying various conformal methods for uncertainty quantification.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for conformal prediction",
      "how to implement uncertainty calibration in python",
      "predictive uncertainty library python",
      "conformal prediction methods in python",
      "IRT Lab conformal prediction library",
      "using puncc for uncertainty quantification"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "synthlearners",
    "description": "Fast synthetic control estimators for panel data problems. Optimized ATT estimation with multiple SC algorithms.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": null,
    "github_url": "https://github.com/apoorvalal/synthlearners",
    "url": "https://github.com/apoorvalal/synthlearners",
    "install": "pip install synthlearners",
    "tags": [
      "synthetic control",
      "causal inference",
      "panel data"
    ],
    "best_for": "Optimized synthetic control with multiple algorithm options",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "panel-data"
    ],
    "summary": "synthlearners provides fast synthetic control estimators specifically designed for panel data problems. It is used by researchers and practitioners in program evaluation to optimize average treatment effect estimation using multiple synthetic control algorithms.",
    "use_cases": [
      "Estimating treatment effects in policy evaluation",
      "Analyzing the impact of interventions in social sciences"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for synthetic control",
      "how to estimate ATT in python",
      "synthetic control methods in python",
      "panel data analysis with python",
      "causal inference tools in python",
      "fast synthetic control estimators python"
    ],
    "primary_use_cases": [
      "average treatment effect estimation",
      "program evaluation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "texreg",
    "description": "Converts coefficients, standard errors, significance stars, and fit statistics from statistical models into LaTeX, HTML, Word, or console output. Highly extensible with support for custom model types and confidence intervals.",
    "category": "Regression Output",
    "docs_url": "https://cran.r-project.org/web/packages/texreg/vignettes/texreg.pdf",
    "github_url": "https://github.com/leifeld/texreg",
    "url": "https://cran.r-project.org/package=texreg",
    "install": "install.packages(\"texreg\")",
    "tags": [
      "LaTeX-tables",
      "HTML-tables",
      "model-comparison",
      "Word-export",
      "extensible"
    ],
    "best_for": "Highly extensible regression tables with easy custom model type extensions, implementing Leifeld (2013, JSS)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The texreg package converts coefficients, standard errors, significance stars, and fit statistics from statistical models into various output formats including LaTeX, HTML, and Word. It is highly extensible and supports custom model types and confidence intervals, making it useful for statisticians and data scientists.",
    "use_cases": [
      "Generating LaTeX tables for academic papers",
      "Exporting regression results to Word for reports"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for converting model output to LaTeX",
      "how to export regression results to Word in R",
      "R library for HTML tables from statistical models",
      "texreg package documentation",
      "R model comparison tables",
      "how to create LaTeX tables from R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "SuperLearner",
    "description": "Implements the Super Learner algorithm for optimal ensemble prediction via cross-validation. Creates weighted combinations of multiple ML algorithms (XGBoost, Random Forest, glmnet, neural networks, SVM, BART) with guaranteed asymptotic optimality.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://cran.r-project.org/web/packages/SuperLearner/vignettes/Guide-to-SuperLearner.html",
    "github_url": "https://github.com/ecpolley/SuperLearner",
    "url": "https://cran.r-project.org/package=SuperLearner",
    "install": "install.packages(\"SuperLearner\")",
    "tags": [
      "ensemble-learning",
      "cross-validation",
      "stacking",
      "prediction",
      "model-selection"
    ],
    "best_for": "Building optimal prediction ensembles for nuisance parameter estimation (propensity scores, outcome models) in causal inference, implementing van der Laan, Polley & Hubbard (2007)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "ensemble-learning"
    ],
    "summary": "SuperLearner implements the Super Learner algorithm for optimal ensemble prediction through cross-validation. It creates weighted combinations of various machine learning algorithms, making it suitable for users looking to improve prediction accuracy.",
    "use_cases": [
      "Combining predictions from multiple machine learning models",
      "Improving prediction accuracy in complex datasets"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for ensemble learning",
      "how to implement Super Learner in R",
      "cross-validation techniques in R",
      "optimal ensemble prediction R package",
      "weighted combinations of ML algorithms R",
      "using SuperLearner for model selection"
    ],
    "primary_use_cases": [
      "model-selection"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "caret",
      "mlr"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "sensemakr",
    "description": "Suite of sensitivity analysis tools extending the traditional omitted variable bias framework, computing robustness values, bias-adjusted estimates, and sensitivity contour plots for OLS regression to assess how strong unmeasured confounders would need to be to overturn conclusions.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://carloscinelli.com/sensemakr/",
    "github_url": "https://github.com/carloscinelli/sensemakr",
    "url": "https://cran.r-project.org/package=sensemakr",
    "install": "install.packages(\"sensemakr\")",
    "tags": [
      "sensitivity-analysis",
      "omitted-variable-bias",
      "robustness-value",
      "causal-inference",
      "regression"
    ],
    "best_for": "Assessing how strong unmeasured confounders would need to be to overturn regression-based causal conclusions, implementing Cinelli & Hazlett (2020)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "sensitivity-analysis",
      "regression"
    ],
    "summary": "sensemakr is a suite of sensitivity analysis tools that extends the traditional omitted variable bias framework. It computes robustness values, bias-adjusted estimates, and sensitivity contour plots for OLS regression to evaluate the impact of unmeasured confounders on conclusions.",
    "use_cases": [
      "Assessing the robustness of regression results",
      "Evaluating the impact of unmeasured confounders on study conclusions"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for sensitivity analysis",
      "how to assess omitted variable bias in R",
      "tools for robustness analysis in regression",
      "sensitivity contour plots in R",
      "bias-adjusted estimates in R",
      "methods for causal inference in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "patchwork",
    "description": "Compose multiple ggplot2 plots into publication-ready multi-panel figures. Uses intuitive operators (+, |, /) for arrangement with automatic alignment and shared legends.",
    "category": "Visualization",
    "docs_url": "https://patchwork.data-imaginist.com/",
    "github_url": "https://github.com/thomasp85/patchwork",
    "url": "https://cran.r-project.org/package=patchwork",
    "install": "install.packages(\"patchwork\")",
    "tags": [
      "ggplot2",
      "multi-panel",
      "figure-composition",
      "visualization",
      "publication-ready"
    ],
    "best_for": "Composing multi-panel ggplot2 figures with intuitive + and | operators",
    "language": "R",
    "difficulty": "beginner|intermediate",
    "prerequisites": [],
    "topic_tags": [
      "visualization"
    ],
    "summary": "The patchwork package allows users to compose multiple ggplot2 plots into publication-ready multi-panel figures. It is particularly useful for R users who need to create visually appealing and aligned figures for academic or professional presentations.",
    "use_cases": [
      "Creating multi-panel figures for academic papers",
      "Arranging plots for presentations"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for multi-panel figures",
      "how to compose ggplot2 plots in R",
      "visualization tools for R",
      "create publication-ready figures in R",
      "patchwork ggplot2 tutorial",
      "R multi-panel figure examples"
    ],
    "api_complexity": "simple",
    "related_packages": [
      "ggplot2"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "heemod",
    "description": "Markov models for cost-effectiveness analysis in R. Define states, transitions, and costs/utilities with intuitive syntax. Includes DSA, PSA, and scenario analysis.",
    "category": "Healthcare Economics & Health-Tech",
    "docs_url": "https://pierucci.org/heemod/",
    "github_url": "https://github.com/pierucci/heemod",
    "url": "https://pierucci.org/heemod/",
    "install": "install.packages('heemod')",
    "tags": [
      "health economics",
      "Markov models",
      "cost-effectiveness",
      "R"
    ],
    "best_for": "Building Markov cost-effectiveness models with clean syntax",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "health economics",
      "cost-effectiveness",
      "Markov models"
    ],
    "summary": "heemod is an R package designed for cost-effectiveness analysis using Markov models. It allows users to define states, transitions, and costs/utilities with an intuitive syntax, making it accessible for those in healthcare economics.",
    "use_cases": [
      "Conducting cost-effectiveness analysis for healthcare interventions",
      "Modeling disease progression using Markov models"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for cost-effectiveness analysis",
      "how to model Markov processes in R",
      "health economics tools in R",
      "R library for healthcare decision modeling"
    ],
    "primary_use_cases": [
      "Cost-effectiveness analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "hesim",
      "BCEA",
      "dampack"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "survHE",
    "description": "Survival analysis for health economics in R. Fits multiple parametric distributions, extrapolates survival curves, and integrates with cost-effectiveness models.",
    "category": "Healthcare Economics & Health-Tech",
    "docs_url": "https://cran.r-project.org/web/packages/survHE/",
    "github_url": "https://github.com/giabaio/survHE",
    "url": "https://cran.r-project.org/web/packages/survHE/",
    "install": "install.packages('survHE')",
    "tags": [
      "survival analysis",
      "health economics",
      "extrapolation",
      "R"
    ],
    "best_for": "Survival extrapolation for health technology assessment",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "survival analysis",
      "health economics"
    ],
    "summary": "survHE is an R package designed for survival analysis in health economics. It allows users to fit multiple parametric distributions and extrapolate survival curves, making it useful for integrating with cost-effectiveness models.",
    "use_cases": [
      "Analyzing patient survival data",
      "Integrating survival analysis with economic models"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for survival analysis",
      "how to perform health economics analysis in R",
      "extrapolate survival curves in R",
      "cost-effectiveness models in R",
      "fit parametric distributions in R",
      "survival analysis tools for health economics"
    ],
    "primary_use_cases": [
      "extrapolating survival curves",
      "fitting parametric distributions"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "flexsurv",
      "survival",
      "hesim"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "MAPIE",
    "description": "Scikit-learn-contrib library for conformal prediction intervals. Provides model-agnostic uncertainty quantification for regression and classification.",
    "category": "Conformal Prediction & Uncertainty",
    "docs_url": "https://mapie.readthedocs.io/",
    "github_url": "https://github.com/scikit-learn-contrib/MAPIE",
    "url": "https://github.com/scikit-learn-contrib/MAPIE",
    "install": "pip install mapie",
    "tags": [
      "conformal prediction",
      "uncertainty",
      "intervals"
    ],
    "best_for": "Model-agnostic prediction intervals",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "conformal-prediction",
      "uncertainty-quantification"
    ],
    "summary": "MAPIE is a library that provides model-agnostic uncertainty quantification for regression and classification tasks through conformal prediction intervals. It is useful for data scientists and researchers who need to assess the uncertainty of their predictions.",
    "use_cases": [
      "Estimating prediction intervals for regression models",
      "Providing uncertainty quantification for classification tasks"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for conformal prediction",
      "how to quantify uncertainty in predictions using python",
      "scikit-learn conformal prediction intervals",
      "MAPIE library for uncertainty quantification",
      "predictive intervals in python",
      "model-agnostic uncertainty estimation python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "dcegm",
    "description": "JAX-compatible DC-EGM algorithm for discrete-continuous dynamic programming (Iskhakov et al. 2017).",
    "category": "Structural Econometrics & Estimation",
    "docs_url": null,
    "github_url": "https://github.com/OpenSourceEconomics/dcegm",
    "url": "https://github.com/OpenSourceEconomics/dcegm",
    "install": "pip install dcegm",
    "tags": [
      "structural",
      "dynamic programming",
      "JAX"
    ],
    "best_for": "Discrete-continuous choice models with EGM",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The dcegm package implements a JAX-compatible DC-EGM algorithm for solving discrete-continuous dynamic programming problems. It is useful for researchers and practitioners in structural econometrics and estimation.",
    "use_cases": [],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for dynamic programming",
      "how to implement DC-EGM in python",
      "JAX compatible algorithms for dynamic programming",
      "structural econometrics tools in python"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "JAX"
    ],
    "implements_paper": "Iskhakov et al. (2017)",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "LightFM",
    "description": "Hybrid recommendation library that handles cold-start by incorporating content features. Uses factorization machines to learn embeddings for users, items, and their features simultaneously.",
    "category": "MarTech & Customer Analytics",
    "docs_url": "https://making.lyst.com/lightfm/docs/home.html",
    "github_url": "https://github.com/lyst/lightfm",
    "url": "https://github.com/lyst/lightfm",
    "install": "pip install lightfm",
    "tags": [
      "recommendations",
      "hybrid",
      "cold-start",
      "factorization-machines"
    ],
    "best_for": "Building hybrid recommenders that work for new users/items using side information",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "LightFM is a hybrid recommendation library designed to handle cold-start problems by incorporating content features. It uses factorization machines to simultaneously learn embeddings for users, items, and their features, making it suitable for various recommendation scenarios.",
    "use_cases": [
      "Recommending items to new users",
      "Improving recommendation accuracy with content features"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for recommendations",
      "how to implement hybrid recommendation in python",
      "cold-start solutions in python",
      "factorization machines for recommendations",
      "LightFM documentation",
      "LightFM example usage",
      "recommendation systems in python",
      "how to use LightFM"
    ],
    "primary_use_cases": [
      "Recommendations"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "Surprise",
      "TensorFlow Recommenders"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "kep_solver",
    "description": "Kidney exchange optimization with hierarchical objectives. Production-ready for kidney paired donation.",
    "category": "Matching & Market Design",
    "docs_url": "https://kep-solver.readthedocs.io/en/latest/",
    "github_url": "https://gitlab.com/wpettersson/kep_solver",
    "url": "https://pypi.org/project/kep_solver/",
    "install": "pip install kep-solver",
    "tags": [
      "matching",
      "market design",
      "kidney exchange"
    ],
    "best_for": "Kidney exchange program optimization",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [],
    "summary": "The kep_solver package provides optimization solutions for kidney exchange programs, focusing on hierarchical objectives. It is designed for use in kidney paired donation scenarios, making it suitable for healthcare professionals and researchers in the field of organ transplantation.",
    "use_cases": [
      "Optimizing kidney paired donation matches",
      "Improving efficiency in organ transplantation programs"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for kidney exchange optimization",
      "how to optimize kidney paired donation in python",
      "matching algorithms for kidney exchange",
      "market design tools for healthcare",
      "hierarchical objectives in optimization",
      "python matching and market design library"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "ATbounds",
    "description": "Implements modern treatment effect bounds beyond basic Manski worst-case scenarios. Provides tighter bounds using monotonicity, mean independence, and other assumptions following Lee and Weidner (2021).",
    "category": "Causal Inference (Bounds)",
    "docs_url": "https://cran.r-project.org/web/packages/ATbounds/ATbounds.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=ATbounds",
    "install": "install.packages(\"ATbounds\")",
    "tags": [
      "partial-identification",
      "bounds",
      "treatment-effects",
      "Manski",
      "monotonicity"
    ],
    "best_for": "Modern treatment effect bounds with tighter identification under various assumptions, implementing Lee & Weidner (2021)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "ATbounds implements modern treatment effect bounds that go beyond basic Manski worst-case scenarios, providing tighter bounds using various assumptions. It is useful for researchers and practitioners in causal inference.",
    "use_cases": [
      "Estimating treatment effects under monotonicity assumptions",
      "Analyzing data with mean independence assumptions"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for treatment effect bounds",
      "how to implement causal inference in R",
      "bounds for treatment effects in R",
      "R library for partial identification",
      "Manski bounds implementation in R",
      "causal inference tools in R"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Lee and Weidner (2021)",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "PyFixest",
    "description": "Fast estimation of linear models with multiple high-dimensional fixed effects (like R's `fixest`). Supports OLS, IV, Poisson, robust/cluster SEs.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://github.com/py-econometrics/pyfixest",
    "github_url": null,
    "url": "https://github.com/py-econometrics/pyfixest",
    "install": "pip install pyfixest",
    "tags": [
      "panel data",
      "fixed effects"
    ],
    "best_for": "Longitudinal analysis, controlling for unobserved heterogeneity",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [],
    "summary": "PyFixest is a Python package designed for fast estimation of linear models with multiple high-dimensional fixed effects, similar to R's fixest. It is particularly useful for econometricians and data scientists working with panel data.",
    "use_cases": [
      "Estimating the impact of policy changes using panel data",
      "Analyzing consumer behavior across different demographics"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for fixed effects estimation",
      "how to estimate linear models with fixed effects in python",
      "fast estimation of panel data models in python",
      "python package for OLS with fixed effects",
      "how to use PyFixest",
      "linear models with high-dimensional fixed effects in python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "cowplot",
    "description": "Publication-ready ggplot2 themes and plot arrangement utilities. Provides clean themes, plot annotations, and functions for combining plots with shared axes.",
    "category": "Visualization",
    "docs_url": "https://wilkelab.org/cowplot/",
    "github_url": "https://github.com/wilkelab/cowplot",
    "url": "https://cran.r-project.org/package=cowplot",
    "install": "install.packages(\"cowplot\")",
    "tags": [
      "ggplot2",
      "themes",
      "publication-ready",
      "plot-arrangement",
      "annotations"
    ],
    "best_for": "Publication-ready ggplot2 themes and multi-plot arrangements with annotations",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The cowplot package provides publication-ready themes and utilities for ggplot2 in R. It is designed for users who need clean themes and functions for arranging plots with shared axes, making it suitable for researchers and data scientists creating visualizations.",
    "use_cases": [
      "Creating publication-ready visualizations",
      "Arranging multiple ggplot2 plots in a single figure"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for ggplot2 themes",
      "how to arrange plots in R",
      "publication-ready plots in R",
      "ggplot2 plot annotations",
      "clean themes for ggplot2",
      "combine ggplots with shared axes"
    ],
    "api_complexity": "simple",
    "related_packages": [
      "ggplot2"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "PyPSA",
    "description": "Python for Power System Analysis - the workhorse for large-scale power system optimization. Static power flow, linear OPF, capacity expansion, unit commitment, and storage modeling.",
    "category": "Energy & Utilities Economics",
    "docs_url": "https://pypsa.org/",
    "github_url": "https://github.com/PyPSA/PyPSA",
    "url": "https://pypsa.org/",
    "install": "pip install pypsa",
    "tags": [
      "power systems",
      "optimization",
      "capacity expansion"
    ],
    "best_for": "Large-scale power system modeling and capacity expansion",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "PyPSA is a Python library designed for large-scale power system optimization, enabling users to perform static power flow analysis, linear optimal power flow, capacity expansion, unit commitment, and storage modeling. It is primarily used by researchers and practitioners in the energy sector.",
    "use_cases": [
      "Optimizing power generation schedules",
      "Modeling the integration of renewable energy sources"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for power system optimization",
      "how to perform static power flow in python",
      "capacity expansion modeling in python",
      "unit commitment analysis in python",
      "storage modeling with PyPSA",
      "linear OPF in python"
    ],
    "primary_use_cases": [
      "Power system modeling"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "pandapower",
      "GenX",
      "PowerModels.jl"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "PyPSA",
    "description": "Python for Power System Analysis - open-source toolbox for simulating and optimizing power systems",
    "category": "Energy Systems Modeling",
    "docs_url": "https://pypsa.readthedocs.io/",
    "github_url": "https://github.com/PyPSA/PyPSA",
    "url": "https://pypsa.org/",
    "install": "pip install pypsa",
    "tags": [
      "power systems",
      "optimization",
      "grid modeling",
      "renewable integration"
    ],
    "best_for": "Modeling power system operations and planning with renewable energy",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "PyPSA is an open-source toolbox designed for simulating and optimizing power systems. It is primarily used by researchers and practitioners in the field of energy systems modeling.",
    "use_cases": [
      "Simulating power system operations",
      "Optimizing renewable energy integration"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for power system analysis",
      "how to optimize power systems in python",
      "renewable integration modeling in python",
      "grid modeling tools in python",
      "open-source toolbox for energy systems",
      "power systems optimization library python"
    ],
    "primary_use_cases": [
      "Power system optimization",
      "Grid planning"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "pandapower",
      "Pyomo"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "panelhetero",
    "description": "Heterogeneity analysis across units in panel data. Detects and characterizes unit-level variation.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://github.com/tkhdyanagi/panelhetero",
    "github_url": "https://github.com/tkhdyanagi/panelhetero",
    "url": "https://github.com/tkhdyanagi/panelhetero",
    "install": "pip install panelhetero",
    "tags": [
      "panel data",
      "heterogeneity",
      "unit effects"
    ],
    "best_for": "Unit heterogeneity in panels",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "panel data",
      "heterogeneity"
    ],
    "summary": "The panelhetero package facilitates heterogeneity analysis across units in panel data, allowing users to detect and characterize unit-level variation. It is primarily used by data scientists and researchers working with panel data to understand variations among different units.",
    "use_cases": [
      "Analyzing economic data across different regions",
      "Studying the impact of policy changes over time on various units"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for panel data analysis",
      "how to analyze heterogeneity in panel data using python",
      "detecting unit-level variation in panel data python",
      "characterizing unit effects in panel data",
      "panel data heterogeneity analysis python",
      "tools for panel data analysis in python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "here",
    "description": "Simple path construction from project root. Uses heuristics to find project root (RStudio, .git, .here) enabling portable paths that work across different machines and working directories.",
    "category": "Reproducibility",
    "docs_url": "https://here.r-lib.org/",
    "github_url": "https://github.com/r-lib/here",
    "url": "https://cran.r-project.org/package=here",
    "install": "install.packages(\"here\")",
    "tags": [
      "paths",
      "project-management",
      "reproducibility",
      "portability",
      "working-directory"
    ],
    "best_for": "Portable file paths from project root for reproducible scripts",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The 'here' package simplifies path construction from the project root using heuristics to locate the root in various environments. It is useful for R users who need to create portable paths that function across different machines and working directories.",
    "use_cases": [
      "Creating paths for data files in R projects",
      "Ensuring reproducibility in R scripts"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "R package for path construction",
      "how to create portable paths in R",
      "R project root detection",
      "R package for reproducibility",
      "how to manage paths in R",
      "R paths across machines"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "worldfootballR",
    "description": "R package for scraping FBref, Transfermarkt, and Understat soccer data including xG, player values, and match statistics",
    "category": "Sports Analytics",
    "docs_url": "https://jaseziv.github.io/worldfootballR/",
    "github_url": "https://github.com/JaseZiv/worldfootballR",
    "url": "https://github.com/JaseZiv/worldfootballR",
    "install": "devtools::install_github(\"JaseZiv/worldfootballR\")",
    "tags": [
      "soccer",
      "football",
      "sports-analytics",
      "R",
      "xG",
      "Transfermarkt"
    ],
    "best_for": "Soccer analytics in R, player valuation, and cross-league analysis",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The worldfootballR package is designed for scraping soccer data from FBref, Transfermarkt, and Understat, providing insights such as expected goals (xG), player values, and match statistics. It is useful for analysts and enthusiasts in the sports analytics field.",
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "R package for soccer data",
      "how to scrape FBref data in R",
      "Transfermarkt data analysis in R",
      "R package for xG statistics",
      "soccer match statistics R",
      "football data scraping R"
    ],
    "use_cases": [
      "Analyzing player performance metrics",
      "Comparing team statistics over a season"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "NeuralForecast",
    "description": "Deep learning models (N-BEATS, N-HiTS, Transformers, RNNs) for time series forecasting, built on PyTorch Lightning.",
    "category": "Time Series Forecasting",
    "docs_url": "https://nixtla.github.io/neuralforecast/",
    "github_url": "https://github.com/Nixtla/neuralforecast",
    "url": "https://github.com/Nixtla/neuralforecast",
    "install": "pip install neuralforecast",
    "tags": [
      "forecasting",
      "time series",
      "machine learning"
    ],
    "best_for": "Prediction, demand forecasting, trend analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "time-series",
      "machine-learning"
    ],
    "summary": "NeuralForecast provides deep learning models for time series forecasting, utilizing architectures such as N-BEATS, N-HiTS, Transformers, and RNNs. It is designed for data scientists and researchers looking to implement advanced forecasting techniques using PyTorch Lightning.",
    "use_cases": [
      "Forecasting sales data",
      "Predicting stock prices",
      "Weather forecasting"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for time series forecasting",
      "how to use deep learning for forecasting in python",
      "N-BEATS implementation in python",
      "time series prediction with PyTorch",
      "machine learning models for forecasting",
      "RNNs for time series analysis",
      "Transformers in time series forecasting"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "PyTorch Lightning"
    ],
    "related_packages": [
      "Prophet",
      "statsmodels"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "sna",
    "description": "Social network analysis tools including network visualization, centrality measures, and statistical models for network data. Part of the statnet suite for network regression and exponential random graph models.",
    "category": "Network Analysis",
    "docs_url": "https://cran.r-project.org/web/packages/sna/sna.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=sna",
    "install": "install.packages(\"sna\")",
    "tags": [
      "social-networks",
      "network-regression",
      "statnet",
      "ERGM",
      "centrality"
    ],
    "best_for": "Social network analysis and network regression as part of the statnet suite",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The 'sna' package provides tools for social network analysis, including visualization, centrality measures, and statistical models for analyzing network data. It is part of the statnet suite, which is commonly used by researchers and practitioners in the field of network analysis.",
    "use_cases": [
      "Analyzing social media interactions",
      "Studying collaboration networks in research"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for social network analysis",
      "how to visualize networks in R",
      "centrality measures in R",
      "statnet suite for network regression",
      "exponential random graph models in R",
      "R tools for network data analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "statnet"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "cregg",
    "description": "Tidy interface for conjoint analysis with visualization. Provides functions for calculating and plotting marginal means and AMCEs with ggplot2-based output for publication-ready figures.",
    "category": "Conjoint Analysis",
    "docs_url": "https://thomasleeper.com/cregg/",
    "github_url": "https://github.com/leeper/cregg",
    "url": "https://cran.r-project.org/package=cregg",
    "install": "install.packages(\"cregg\")",
    "tags": [
      "conjoint",
      "visualization",
      "marginal-means",
      "ggplot2",
      "survey-experiments"
    ],
    "best_for": "Tidy conjoint analysis with ggplot2 visualization for marginal means and AMCEs",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The cregg package provides a tidy interface for conducting conjoint analysis and visualizing the results. It is designed for users who need to calculate and plot marginal means and Average Marginal Component Effects (AMCEs) using ggplot2 for publication-ready figures.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "R package for conjoint analysis",
      "how to visualize marginal means in R",
      "conjoint analysis with ggplot2",
      "R library for survey experiments",
      "calculate AMCEs in R",
      "tidy interface for conjoint analysis"
    ],
    "primary_use_cases": [
      "calculating marginal means",
      "plotting AMCEs"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "crepes",
    "description": "Lightweight library for conformal regressors and predictive systems. Simple API for calibrated prediction intervals.",
    "category": "Conformal Prediction & Uncertainty",
    "docs_url": "https://github.com/henrikbostrom/crepes",
    "github_url": "https://github.com/henrikbostrom/crepes",
    "url": "https://github.com/henrikbostrom/crepes",
    "install": "pip install crepes",
    "tags": [
      "conformal prediction",
      "regression",
      "intervals"
    ],
    "best_for": "Simple conformal regressors",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "Crepes is a lightweight library designed for conformal regressors and predictive systems, providing a simple API for generating calibrated prediction intervals. It is useful for data scientists and statisticians who need reliable uncertainty quantification in their predictions.",
    "use_cases": [
      "Generating calibrated prediction intervals for regression models",
      "Implementing conformal prediction methods in data analysis"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for conformal prediction",
      "how to create prediction intervals in python",
      "lightweight library for regression in python",
      "calibrated prediction intervals python",
      "conformal regressors in python",
      "predictive systems library python"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "mgcv",
    "description": "The definitive GAM implementation providing generalized additive (mixed) models with automatic smoothness estimation via REML/GCV/ML, supporting thin plate splines, tensor products, multiple distributions, and scalable fitting for large datasets.",
    "category": "Generalized Additive Models",
    "docs_url": "https://cran.r-project.org/web/packages/mgcv/mgcv.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=mgcv",
    "install": "install.packages(\"mgcv\")",
    "tags": [
      "GAM",
      "splines",
      "smoothing",
      "penalized-regression",
      "mixed-models"
    ],
    "best_for": "Flexible nonparametric regression with automatic smoothing parameter selection, implementing Wood (2017)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The mgcv package provides a comprehensive implementation of generalized additive models (GAMs) and mixed models in R, allowing users to estimate smooth functions and fit complex models to data. It is widely used by statisticians and data scientists for tasks involving non-linear relationships and large datasets.",
    "use_cases": [
      "Modeling non-linear relationships in data",
      "Analyzing large datasets with smooth functions"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for generalized additive models",
      "how to fit GAMs in R",
      "R package for smoothness estimation",
      "using mgcv for mixed models",
      "automated smoothness estimation in R",
      "GAM implementation in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "ivreg",
    "description": "Modern implementation of two-stage least squares (2SLS) instrumental variables regression with comprehensive diagnostics including hat values, studentized residuals, and component-plus-residual plots. Successor to AER's ivreg() function with superior diagnostic tools.",
    "category": "Instrumental Variables",
    "docs_url": "https://zeileis.github.io/ivreg/",
    "github_url": "https://github.com/zeileis/ivreg",
    "url": "https://cran.r-project.org/package=ivreg",
    "install": "install.packages(\"ivreg\")",
    "tags": [
      "instrumental-variables",
      "2SLS",
      "IV-regression",
      "endogeneity",
      "diagnostics"
    ],
    "best_for": "Modern 2SLS instrumental variables regression with comprehensive diagnostic tools",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "diagnostics"
    ],
    "summary": "The ivreg package provides a modern implementation of two-stage least squares (2SLS) instrumental variables regression, offering comprehensive diagnostics such as hat values and studentized residuals. It is designed for users needing advanced regression analysis tools, particularly in the context of endogeneity.",
    "use_cases": [
      "Analyzing the impact of policy changes using instrumental variables",
      "Estimating causal relationships in economics",
      "Conducting regression analysis with endogeneity concerns"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for instrumental variables regression",
      "how to perform 2SLS in R",
      "diagnostics for IV regression in R",
      "R library for endogeneity correction",
      "hat values in R ivreg",
      "studentized residuals in R",
      "component-plus-residual plots in R"
    ],
    "primary_use_cases": [
      "instrumental variables regression",
      "diagnostic analysis for regression models"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "AER"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "glmmTMB",
    "description": "Fit generalized linear mixed models with extensions including zero-inflation, hurdle models, heteroscedasticity, and autocorrelation using Template Model Builder (TMB) with automatic differentiation and Laplace approximation.",
    "category": "Mixed Effects",
    "docs_url": "https://glmmtmb.github.io/glmmTMB/",
    "github_url": "https://github.com/glmmTMB/glmmTMB",
    "url": "https://cran.r-project.org/package=glmmTMB",
    "install": "install.packages(\"glmmTMB\")",
    "tags": [
      "GLMM",
      "zero-inflation",
      "negative-binomial",
      "TMB",
      "overdispersion"
    ],
    "best_for": "Zero-inflated, overdispersed, or complex GLMMs beyond lme4 capabilities, implementing Brooks et al. (2017)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "glmmTMB is an R package that fits generalized linear mixed models with various extensions such as zero-inflation and hurdle models. It is used by statisticians and data scientists for modeling complex data structures.",
    "use_cases": [
      "Modeling count data with excess zeros",
      "Analyzing longitudinal data with random effects"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for generalized linear mixed models",
      "how to fit mixed effects models in R",
      "zero-inflation models in R",
      "hurdle models R package",
      "TMB for mixed models",
      "R package for overdispersion handling",
      "fit autocorrelation in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "stm",
    "description": "Structural Topic Models incorporating document-level metadata as covariates affecting topic prevalence and content. Enables studying how topics vary across groups or time with uncertainty quantification.",
    "category": "Text Analysis",
    "docs_url": "https://www.structuraltopicmodel.com/",
    "github_url": "https://github.com/bstewart/stm",
    "url": "https://cran.r-project.org/package=stm",
    "install": "install.packages(\"stm\")",
    "tags": [
      "topic-models",
      "text-analysis",
      "covariates",
      "LDA",
      "document-metadata"
    ],
    "best_for": "Structural topic models with document metadata affecting topic prevalence and content",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "topic-models",
      "text-analysis",
      "covariates"
    ],
    "summary": "The stm package allows users to incorporate document-level metadata as covariates in Structural Topic Models, enabling the analysis of how topics vary across different groups or over time. It is particularly useful for researchers and data scientists interested in text analysis and topic modeling.",
    "use_cases": [
      "Analyzing how topics change over time",
      "Studying the impact of document metadata on topic prevalence"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for topic modeling",
      "how to analyze text with metadata in R",
      "R package for structural topic models",
      "how to use covariates in topic models R",
      "text analysis with R",
      "R package for LDA with metadata"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "fable",
    "description": "A tidyverse-native forecasting framework providing ETS, ARIMA, and other models for tidy time series (tsibble objects). Enables fitting multiple models across many time series simultaneously with a consistent formula-based interface.",
    "category": "Time Series Forecasting",
    "docs_url": "https://fable.tidyverts.org/",
    "github_url": "https://github.com/tidyverts/fable",
    "url": "https://cran.r-project.org/package=fable",
    "install": "install.packages(\"fable\")",
    "tags": [
      "time-series",
      "tidyverse",
      "ARIMA",
      "ETS",
      "tsibble"
    ],
    "best_for": "Tidy forecasting workflows handling many related time series with tidyverse-consistent syntax",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "time-series"
    ],
    "summary": "Fable is a forecasting framework designed for tidy time series data, allowing users to fit various models like ETS and ARIMA across multiple time series simultaneously. It is primarily used by data scientists and statisticians working with time series analysis in R.",
    "use_cases": [
      "Forecasting sales data over time",
      "Analyzing seasonal trends in web traffic"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for time series forecasting",
      "how to use ARIMA in R",
      "time series analysis with tidyverse",
      "forecasting with ETS in R",
      "fitting multiple models in R",
      "tsibble objects in R"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "tidyverse"
    ],
    "related_packages": [
      "forecast",
      "tsibble"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "Linfa",
    "description": "Rust ML toolkit inspired by scikit-learn with GLMs, clustering (K-Means), PCA, SVM, and regularization (Lasso/Ridge).",
    "category": "Tree & Ensemble Methods for Prediction",
    "docs_url": "https://rust-ml.github.io/linfa/",
    "github_url": "https://github.com/rust-ml/linfa",
    "url": "https://crates.io/crates/linfa",
    "install": "cargo add linfa",
    "tags": [
      "rust",
      "machine learning",
      "clustering",
      "PCA",
      "SVM"
    ],
    "best_for": "scikit-learn style ML in Rust",
    "language": "Rust",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Linfa is a Rust machine learning toolkit that provides a variety of algorithms including generalized linear models, clustering methods like K-Means, principal component analysis, support vector machines, and regularization techniques such as Lasso and Ridge. It is designed for users familiar with machine learning concepts who are looking to implement these techniques in Rust.",
    "use_cases": [
      "Building machine learning models in Rust",
      "Performing clustering analysis on datasets"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "rust library for machine learning",
      "how to do clustering in rust",
      "rust implementation of PCA",
      "using SVM in rust",
      "rust toolkit for GLMs",
      "K-Means algorithm in rust"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "SmartCore",
    "description": "Rust ML library with regression, classification, clustering, matrix decomposition (SVD, PCA), and model selection tools.",
    "category": "Tree & Ensemble Methods for Prediction",
    "docs_url": "https://docs.rs/smartcore",
    "github_url": "https://github.com/smartcorelib/smartcore",
    "url": "https://crates.io/crates/smartcore",
    "install": "cargo add smartcore",
    "tags": [
      "rust",
      "machine learning",
      "regression",
      "classification"
    ],
    "best_for": "Comprehensive ML algorithms in pure Rust",
    "language": "Rust",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "SmartCore is a Rust machine learning library that provides tools for regression, classification, clustering, matrix decomposition, and model selection. It is designed for data scientists and developers looking to implement machine learning algorithms in Rust.",
    "use_cases": [
      "Building predictive models",
      "Data analysis using machine learning techniques"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "Rust library for machine learning",
      "how to do regression in Rust",
      "Rust classification library",
      "machine learning clustering in Rust",
      "matrix decomposition in Rust",
      "model selection tools in Rust"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "hockeyR",
    "description": "R package for NHL play-by-play data with built-in expected goals models and player tracking statistics",
    "category": "Sports Analytics",
    "docs_url": "https://hockeyr.netlify.app/",
    "github_url": "https://github.com/danmorse314/hockeyR",
    "url": "https://github.com/danmorse314/hockeyR",
    "install": "devtools::install_github(\"danmorse314/hockeyR\")",
    "tags": [
      "hockey",
      "sports-analytics",
      "R",
      "NHL",
      "xG"
    ],
    "best_for": "Hockey analytics in R, expected goals modeling, and player evaluation",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The hockeyR package provides tools for analyzing NHL play-by-play data, including expected goals models and player tracking statistics. It is useful for sports analysts and enthusiasts interested in hockey analytics.",
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "R package for NHL play-by-play data",
      "how to analyze hockey statistics in R",
      "expected goals models in R",
      "R tools for sports analytics",
      "NHL player tracking statistics R",
      "hockey analytics R package"
    ],
    "use_cases": [
      "Analyzing player performance in NHL games",
      "Evaluating team strategies using expected goals"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "lifecontingencies",
    "description": "R package for life insurance mathematics including life tables, annuities, and insurance present value calculations following actuarial notation",
    "category": "Insurance & Actuarial",
    "docs_url": "https://cran.r-project.org/web/packages/lifecontingencies/vignettes/",
    "github_url": "https://github.com/spedygiorgio/lifecontingencies",
    "url": "https://cran.r-project.org/package=lifecontingencies",
    "install": "install.packages(\"lifecontingencies\")",
    "tags": [
      "life-insurance",
      "actuarial",
      "annuities",
      "life-tables",
      "present-values"
    ],
    "best_for": "Life insurance pricing in R using standard actuarial notation and methods",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The lifecontingencies package provides tools for life insurance mathematics, including the calculation of life tables, annuities, and present values based on actuarial notation. It is primarily used by actuaries and professionals in the insurance industry.",
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "R package for life insurance mathematics",
      "how to calculate life tables in R",
      "R annuities calculations",
      "insurance present value calculations R",
      "actuarial notation R package",
      "life insurance R tools"
    ],
    "use_cases": [
      "Calculating life insurance premiums",
      "Estimating retirement annuities"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "extRemes",
    "description": "Comprehensive toolkit for extreme value analysis with diagnostic plots, return level estimation, and non-stationary models for climate-related risks",
    "category": "Insurance & Actuarial",
    "docs_url": "https://cran.r-project.org/web/packages/extRemes/vignettes/",
    "github_url": "https://github.com/lbelzile/extRemes",
    "url": "https://cran.r-project.org/package=extRemes",
    "install": "install.packages(\"extRemes\")",
    "tags": [
      "extreme-values",
      "return-levels",
      "climate-risk",
      "non-stationary",
      "catastrophe"
    ],
    "best_for": "Climate risk analysis, return period estimation, and catastrophe loss modeling",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "extreme-values",
      "climate-risk"
    ],
    "summary": "extRemes is a comprehensive toolkit designed for extreme value analysis, providing diagnostic plots, return level estimation, and non-stationary models specifically for climate-related risks. It is primarily used by statisticians and data scientists working in fields related to insurance and actuarial science.",
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for extreme value analysis",
      "how to estimate return levels in R",
      "R package for climate risk analysis",
      "non-stationary models in R",
      "extreme value toolkit for insurance",
      "diagnostic plots for climate-related risks"
    ],
    "use_cases": [
      "Estimating return levels for insurance policies",
      "Analyzing extreme weather events for risk assessment"
    ],
    "primary_use_cases": [
      "return level estimation",
      "non-stationary modeling"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "RecBole",
    "description": "Comprehensive recommendation library with 100+ algorithms spanning general, sequential, context-aware, and knowledge-based approaches. Built on PyTorch with unified data loading and evaluation.",
    "category": "MarTech & Customer Analytics",
    "docs_url": "https://recbole.io/",
    "github_url": "https://github.com/RUCAIBox/RecBole",
    "url": "https://recbole.io/",
    "install": "pip install recbole",
    "tags": [
      "recommendations",
      "deep-learning",
      "sequential",
      "benchmark"
    ],
    "best_for": "Research and benchmarking across recommendation paradigms",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "RecBole is a comprehensive recommendation library that offers over 100 algorithms for various recommendation approaches, including general, sequential, context-aware, and knowledge-based methods. It is built on PyTorch and provides unified data loading and evaluation, making it suitable for data scientists and researchers in the field of recommendation systems.",
    "use_cases": [
      "Building a recommendation system for e-commerce",
      "Evaluating different recommendation algorithms on benchmark datasets"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for recommendations",
      "how to implement recommendation algorithms in python",
      "RecBole tutorial",
      "best practices for using RecBole",
      "deep learning recommendations python",
      "sequential recommendation systems in python"
    ],
    "primary_use_cases": [
      "Recommendations"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "Surprise",
      "LightFM",
      "Implicit"
    ],
    "maintenance_status": "active",
    "framework_compatibility": [
      "PyTorch"
    ],
    "model_score": 0.0001
  },
  {
    "name": "eventstudyr",
    "description": "Implements event study best practices from Freyaldenhoven et al. (2021) including sup-t confidence bands for uniform inference and formal pre-trend testing. Provides robust methods for dynamic treatment effect estimation.",
    "category": "Causal Inference (Event Study)",
    "docs_url": "https://cran.r-project.org/web/packages/eventstudyr/eventstudyr.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=eventstudyr",
    "install": "install.packages(\"eventstudyr\")",
    "tags": [
      "event-study",
      "pre-trends",
      "sup-t-bands",
      "uniform-inference",
      "dynamic-effects"
    ],
    "best_for": "Event study best practices with sup-t confidence bands and formal pre-trend testing, implementing Freyaldenhoven et al. (2021)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "event-study"
    ],
    "summary": "The eventstudyr package implements event study best practices, including sup-t confidence bands for uniform inference and formal pre-trend testing. It is designed for researchers and practitioners interested in dynamic treatment effect estimation.",
    "use_cases": [
      "Analyzing the impact of a policy change on stock prices",
      "Evaluating the effect of a new marketing campaign on sales"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for event study analysis",
      "how to implement pre-trend testing in R",
      "event study methods in R",
      "dynamic treatment effects R package",
      "sup-t confidence bands R",
      "uniform inference in R"
    ],
    "primary_use_cases": [
      "dynamic treatment effect estimation"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Freyaldenhoven et al. (2021)",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "TorchCP",
    "description": "PyTorch-native conformal prediction for DNNs, GNNs, and LLMs with GPU acceleration.",
    "category": "Conformal Prediction & Uncertainty",
    "docs_url": "https://torchcp.readthedocs.io/",
    "github_url": "https://github.com/ml-stat-Sustech/TorchCP",
    "url": "https://github.com/ml-stat-Sustech/TorchCP",
    "install": "pip install torchcp",
    "tags": [
      "conformal prediction",
      "PyTorch",
      "deep learning"
    ],
    "best_for": "Conformal prediction for neural networks",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "TorchCP is a PyTorch-native library designed for conformal prediction in deep neural networks, graph neural networks, and large language models, providing GPU acceleration. It is useful for practitioners in machine learning who need to quantify uncertainty in their models.",
    "use_cases": [],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for conformal prediction",
      "how to do conformal prediction in PyTorch",
      "PyTorch GPU acceleration for deep learning",
      "conformal prediction for DNNs",
      "using TorchCP for uncertainty quantification",
      "TorchCP examples",
      "installing TorchCP",
      "TorchCP documentation"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "PyTorch"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "Econ Project Templates",
    "description": "Cookiecutter templates for reproducible economics research projects. Standardized project structure.",
    "category": "Inference & Reporting Tools",
    "docs_url": "https://econ-project-templates.readthedocs.io/",
    "github_url": "https://github.com/OpenSourceEconomics/econ-project-templates",
    "url": "https://github.com/OpenSourceEconomics/econ-project-templates",
    "install": "",
    "tags": [
      "reproducibility",
      "templates",
      "workflow"
    ],
    "best_for": "Starting reproducible economics projects",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Econ Project Templates provides cookiecutter templates designed for reproducible economics research projects, offering a standardized project structure. It is useful for researchers in economics looking to streamline their project setup.",
    "use_cases": [],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for reproducible economics research",
      "how to create a standardized project structure in python",
      "cookiecutter templates for economics",
      "economics research project setup in python"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "Nalgebra",
    "description": "General-purpose linear algebra library for Rust with dense and sparse matrices, widely used in graphics and physics.",
    "category": "Numerical Optimization & Computational Tools",
    "docs_url": "https://www.nalgebra.org/",
    "github_url": "https://github.com/dimforge/nalgebra",
    "url": "https://crates.io/crates/nalgebra",
    "install": "cargo add nalgebra",
    "tags": [
      "rust",
      "linear algebra",
      "matrix",
      "sparse"
    ],
    "best_for": "General-purpose linear algebra in Rust",
    "language": "Rust",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Nalgebra is a general-purpose linear algebra library for Rust that supports both dense and sparse matrices. It is widely used in fields such as graphics and physics for various computational tasks.",
    "use_cases": [
      "Performing matrix calculations in graphics applications",
      "Solving linear equations in physics simulations"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "rust library for linear algebra",
      "how to perform matrix operations in rust",
      "sparse matrix library in rust",
      "rust linear algebra for graphics",
      "physics simulations in rust",
      "rust numerical optimization library"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "SyntheticControlMethods",
    "description": "Implementation of synthetic control methods for comparative case studies when panel data is available.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": null,
    "github_url": "https://github.com/OscarEngelbrektson/SyntheticControlMethods",
    "url": "https://github.com/OscarEngelbrektson/SyntheticControlMethods",
    "install": "pip install SyntheticControlMethods",
    "tags": [
      "DiD",
      "synthetic control",
      "RDD"
    ],
    "best_for": "Policy evaluation, natural experiments, quasi-experiments",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "program-evaluation"
    ],
    "summary": "SyntheticControlMethods provides an implementation of synthetic control methods for comparative case studies using panel data. It is useful for researchers and practitioners in program evaluation and causal inference.",
    "use_cases": [
      "Evaluating the impact of a policy change in a specific region",
      "Comparing economic outcomes between treated and control groups"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for synthetic control methods",
      "how to implement synthetic control in python",
      "comparative case studies in python",
      "panel data analysis python",
      "program evaluation methods in python",
      "synthetic control methods tutorial"
    ],
    "primary_use_cases": [
      "comparative case studies",
      "program evaluation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "quanteda",
    "description": "Comprehensive framework for quantitative text analysis. Provides fast text preprocessing, document-feature matrices, dictionary analysis, and integration with topic models. Standard for political science text analysis.",
    "category": "Text Analysis",
    "docs_url": "https://quanteda.io/",
    "github_url": "https://github.com/quanteda/quanteda",
    "url": "https://cran.r-project.org/package=quanteda",
    "install": "install.packages(\"quanteda\")",
    "tags": [
      "text-analysis",
      "NLP",
      "document-term-matrix",
      "text-preprocessing",
      "political-science"
    ],
    "best_for": "Comprehensive quantitative text analysis with fast preprocessing and document-feature matrices",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Quanteda is a comprehensive framework for quantitative text analysis that provides fast text preprocessing, document-feature matrices, and dictionary analysis. It is widely used in political science for text analysis.",
    "use_cases": [
      "Analyzing political speeches",
      "Conducting sentiment analysis on social media data"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for text analysis",
      "how to preprocess text in R",
      "document-feature matrices in R",
      "NLP tools for political science",
      "dictionary analysis in R",
      "quantitative text analysis framework"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "MLForecast",
    "description": "Scalable time series forecasting using machine learning models (e.g., LightGBM, XGBoost) as regressors.",
    "category": "Time Series Forecasting",
    "docs_url": "https://nixtla.github.io/mlforecast/",
    "github_url": "https://github.com/Nixtla/mlforecast",
    "url": "https://github.com/Nixtla/mlforecast",
    "install": "pip install mlforecast",
    "tags": [
      "forecasting",
      "time series",
      "machine learning"
    ],
    "best_for": "Prediction, demand forecasting, trend analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "time-series",
      "machine-learning"
    ],
    "summary": "MLForecast is a package designed for scalable time series forecasting using machine learning models such as LightGBM and XGBoost as regressors. It is useful for data scientists and analysts working with time series data who want to leverage machine learning techniques for forecasting.",
    "use_cases": [
      "Forecasting sales data",
      "Predicting stock prices"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for time series forecasting",
      "how to forecast time series with machine learning in python",
      "MLForecast documentation",
      "time series forecasting with LightGBM",
      "XGBoost for time series prediction",
      "scalable time series forecasting python"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "Prophet",
      "statsmodels"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "nba_api",
    "description": "Full NBA Stats API wrapper with 127+ endpoints for accessing shot charts, player tracking, play-by-play, and historical data",
    "category": "Sports Analytics",
    "docs_url": "https://github.com/swar/nba_api/blob/master/docs/table_of_contents.md",
    "github_url": "https://github.com/swar/nba_api",
    "url": "https://github.com/swar/nba_api",
    "install": "pip install nba_api",
    "tags": [
      "basketball",
      "sports-analytics",
      "NBA",
      "shot-charts"
    ],
    "best_for": "Basketball analytics, player performance analysis, and shot chart visualization",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The nba_api is a Python wrapper for accessing a wide range of NBA statistics through its 127+ endpoints. It is used by data analysts and sports enthusiasts to analyze player performance and game statistics.",
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for NBA stats",
      "how to access shot charts in python",
      "NBA player tracking API",
      "python wrapper for NBA data",
      "how to analyze basketball statistics in python",
      "NBA play-by-play data in python"
    ],
    "use_cases": [
      "Accessing historical NBA game data",
      "Analyzing player performance metrics"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "Lahman",
    "description": "R package providing the complete Lahman Baseball Database as native R data frames for seamless analysis",
    "category": "Sports Analytics",
    "docs_url": "https://cran.r-project.org/web/packages/Lahman/index.html",
    "github_url": "https://github.com/cdalzell/Lahman",
    "url": "https://cran.r-project.org/package=Lahman",
    "install": "install.packages(\"Lahman\")",
    "tags": [
      "baseball",
      "sports-analytics",
      "R",
      "sabermetrics",
      "historical"
    ],
    "best_for": "Baseball analytics in R, historical trend analysis, and teaching sabermetrics",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Lahman package provides the complete Lahman Baseball Database as native R data frames, allowing users to perform seamless analysis of baseball statistics. It is primarily used by analysts and researchers in the field of sports analytics.",
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "R package for baseball analysis",
      "how to analyze baseball statistics in R",
      "Lahman Baseball Database R package",
      "R sports analytics tools",
      "baseball data analysis in R",
      "R sabermetrics package"
    ],
    "use_cases": [
      "Analyzing historical baseball performance",
      "Conducting sabermetric analysis"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "hoopR",
    "description": "R package for accessing NBA Stats API plus ESPN and KenPom data for comprehensive basketball analytics",
    "category": "Sports Analytics",
    "docs_url": "https://hoopr.sportsdataverse.org/",
    "github_url": "https://github.com/sportsdataverse/hoopR",
    "url": "https://github.com/sportsdataverse/hoopR",
    "install": "install.packages(\"hoopR\")",
    "tags": [
      "basketball",
      "sports-analytics",
      "R",
      "NBA",
      "college-basketball"
    ],
    "best_for": "Basketball analytics in R, combining NBA and college basketball data",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "hoopR is an R package designed for accessing NBA Stats API along with ESPN and KenPom data, enabling users to perform comprehensive basketball analytics. It is primarily used by analysts and enthusiasts interested in basketball statistics.",
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "R package for NBA stats",
      "how to analyze basketball data in R",
      "ESPN data analysis in R",
      "KenPom data access R package",
      "basketball analytics R",
      "sports analytics R package"
    ],
    "use_cases": [
      "Analyzing player statistics",
      "Comparing team performance"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "spacetrack",
    "description": "Python client for the Space-Track.org API to access satellite catalog and TLE data",
    "category": "Space & Orbital Analysis",
    "docs_url": "https://spacetrack.readthedocs.io/",
    "github_url": "https://github.com/python-astraea/spacetrack",
    "url": "https://spacetrack.readthedocs.io/",
    "install": "pip install spacetrack",
    "tags": [
      "Space-Track",
      "satellites",
      "API",
      "TLE"
    ],
    "best_for": "Programmatic access to Space-Track.org satellite data",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "spacetrack is a Python client that allows users to access satellite catalog and TLE data through the Space-Track.org API. It is useful for anyone interested in satellite data, including researchers and developers in the space industry.",
    "use_cases": [],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for satellite data",
      "how to access TLE data in python",
      "Space-Track API python client",
      "retrieve satellite catalog using python",
      "python client for Space-Track",
      "how to use Space-Track API in python"
    ],
    "primary_use_cases": [
      "Data access",
      "Satellite research"
    ],
    "api_complexity": "simple",
    "related_packages": [
      "sgp4",
      "Skyfield"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "ivmodel",
    "description": "Specialized package for weak instrument diagnostics implementing Anderson-Rubin tests, k-class estimators (LIML, Fuller), and sensitivity analysis following Jiang et al. (2015). Essential when instrument strength is questionable.",
    "category": "Instrumental Variables",
    "docs_url": "https://cran.r-project.org/web/packages/ivmodel/ivmodel.pdf",
    "github_url": "https://github.com/hyunseungkang/ivmodel",
    "url": "https://cran.r-project.org/package=ivmodel",
    "install": "install.packages(\"ivmodel\")",
    "tags": [
      "instrumental-variables",
      "weak-instruments",
      "Anderson-Rubin",
      "LIML",
      "sensitivity-analysis"
    ],
    "best_for": "Weak instrument diagnostics with Anderson-Rubin tests and k-class estimators (LIML, Fuller)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The ivmodel package provides tools for weak instrument diagnostics, including Anderson-Rubin tests and k-class estimators like LIML and Fuller. It is essential for researchers dealing with questionable instrument strength in their econometric models.",
    "use_cases": [
      "Evaluating instrument strength in econometric models",
      "Conducting sensitivity analysis for instrumental variable estimates"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for weak instrument diagnostics",
      "how to perform Anderson-Rubin tests in R",
      "R LIML estimator package",
      "sensitivity analysis in R",
      "diagnosing weak instruments R",
      "ivmodel package usage",
      "econometrics package for R"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Jiang et al. (2015)",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "nvdlib",
    "description": "Python wrapper for the NIST National Vulnerability Database (NVD) API for automated vulnerability intelligence",
    "category": "Cybersecurity",
    "docs_url": "https://nvdlib.com/",
    "github_url": "https://github.com/vehemont/nvdlib",
    "url": "https://nvdlib.com/",
    "install": "pip install nvdlib",
    "tags": [
      "vulnerabilities",
      "CVE",
      "NVD",
      "security research"
    ],
    "best_for": "Programmatic access to vulnerability data for security economics research",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "nvdlib is a Python wrapper for the NIST National Vulnerability Database (NVD) API that facilitates automated vulnerability intelligence. It is used by cybersecurity professionals and researchers to access and analyze vulnerability data.",
    "use_cases": [
      "Automating vulnerability data retrieval",
      "Analyzing CVE data for security research"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for NVD API",
      "how to use nvdlib for vulnerability intelligence"
    ],
    "primary_use_cases": [
      "Vulnerability data access",
      "Security research"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "requests",
      "pandas"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "Implicit",
    "description": "GPU-accelerated library for collaborative filtering on implicit feedback data. Implements ALS, BPR, and logistic matrix factorization with CUDA support for scale.",
    "category": "MarTech & Customer Analytics",
    "docs_url": "https://benfred.github.io/implicit/",
    "github_url": "https://github.com/benfred/implicit",
    "url": "https://github.com/benfred/implicit",
    "install": "pip install implicit",
    "tags": [
      "recommendations",
      "implicit-feedback",
      "GPU",
      "ALS"
    ],
    "best_for": "Large-scale recommendation with click/purchase data (no explicit ratings)",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "Implicit is a GPU-accelerated library designed for collaborative filtering on implicit feedback data. It is used by data scientists and machine learning practitioners who need efficient algorithms for recommendations.",
    "use_cases": [
      "Building recommendation systems",
      "Analyzing user behavior based on implicit feedback"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for collaborative filtering",
      "how to implement ALS in python",
      "GPU recommendations library",
      "implicit feedback analysis in python",
      "BPR matrix factorization python",
      "logistic matrix factorization with CUDA"
    ],
    "primary_use_cases": [
      "Recommendations"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "Surprise",
      "LightFM"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "recommenderlab",
    "description": "R infrastructure for developing and evaluating recommender systems. Provides UBCF, IBCF, SVD, popular/random baselines with unified evaluation framework.",
    "category": "MarTech & Customer Analytics",
    "docs_url": "https://cran.r-project.org/package=recommenderlab",
    "github_url": "https://github.com/mhahsler/recommenderlab",
    "url": "https://github.com/mhahsler/recommenderlab",
    "install": "install.packages('recommenderlab')",
    "tags": [
      "recommendations",
      "R",
      "collaborative-filtering",
      "evaluation"
    ],
    "best_for": "Building and evaluating recommendations in R",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "recommenderlab provides an R infrastructure for developing and evaluating recommender systems. It is used by data scientists and researchers to implement various collaborative filtering techniques and evaluate their performance.",
    "use_cases": [
      "Building a movie recommendation system",
      "Evaluating different recommendation algorithms"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R library for recommender systems",
      "how to evaluate recommender systems in R",
      "R collaborative filtering package",
      "best practices for recommender systems in R",
      "R recommendations library",
      "how to use recommenderlab",
      "R package for UBCF",
      "R package for IBCF"
    ],
    "primary_use_cases": [
      "Recommendations"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "Surprise",
      "recosystem"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "gridstatus",
    "description": "Unified Python interface for U.S. electricity grid data from all major ISOs",
    "category": "Data Access",
    "docs_url": "https://docs.gridstatus.io/",
    "github_url": "https://github.com/gridstatus/gridstatus",
    "url": "https://www.gridstatus.io/",
    "install": "pip install gridstatus",
    "tags": [
      "ISO",
      "electricity markets",
      "real-time data",
      "unified API"
    ],
    "best_for": "Accessing standardized data across multiple U.S. electricity markets",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "gridstatus provides a unified Python interface for accessing U.S. electricity grid data from all major Independent System Operators (ISOs). It is useful for developers and analysts working with real-time electricity market data.",
    "use_cases": [
      "Accessing real-time electricity grid data",
      "Analyzing electricity market trends"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for U.S. electricity grid data",
      "how to access ISO electricity data in python",
      "real-time electricity market data python",
      "unified API for electricity data python"
    ],
    "primary_use_cases": [
      "Market data access",
      "Grid monitoring"
    ],
    "api_complexity": "simple",
    "related_packages": [
      "pandas",
      "requests"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "spdep",
    "description": "The foundational R package for spatial weights matrix creation and spatial autocorrelation testing. Provides functions for creating spatial weights from polygon contiguities and point patterns, computing global statistics (Moran's I, Geary's C), local indicators (LISA), and Lagrange multiplier tests.",
    "category": "Spatial Econometrics",
    "docs_url": "https://r-spatial.github.io/spdep/",
    "github_url": "https://github.com/r-spatial/spdep",
    "url": "https://cran.r-project.org/package=spdep",
    "install": "install.packages(\"spdep\")",
    "tags": [
      "spatial-weights",
      "autocorrelation",
      "morans-i",
      "neighborhood-analysis",
      "spatial-statistics"
    ],
    "best_for": "Creating spatial weights matrices and testing for spatial autocorrelation in cross-sectional data, implementing Bivand & Wong (2018)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "spatial-econometrics",
      "spatial-statistics"
    ],
    "summary": "The spdep package is designed for creating spatial weights matrices and conducting spatial autocorrelation tests in R. It is commonly used by researchers and analysts in spatial econometrics to analyze spatial data patterns.",
    "use_cases": [
      "Analyzing spatial patterns in real estate data",
      "Testing for spatial autocorrelation in environmental studies"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "R package for spatial weights",
      "how to test spatial autocorrelation in R",
      "spatial analysis tools in R",
      "create spatial weights matrix R",
      "Moran's I calculation in R",
      "LISA analysis in R"
    ],
    "primary_use_cases": [
      "spatial weights matrix creation",
      "spatial autocorrelation testing"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "urca",
    "description": "Implements unit root and cointegration tests commonly used in applied econometric analysis. Includes Augmented Dickey-Fuller, Phillips-Perron, KPSS, Elliott-Rothenberg-Stock, and Zivot-Andrews tests, plus Johansen's cointegration procedure for multivariate series.",
    "category": "Time Series Econometrics",
    "docs_url": "https://cran.r-project.org/web/packages/urca/urca.pdf",
    "github_url": "https://github.com/bpfaff/urca",
    "url": "https://cran.r-project.org/package=urca",
    "install": "install.packages(\"urca\")",
    "tags": [
      "unit-root",
      "cointegration",
      "ADF-test",
      "KPSS",
      "Johansen"
    ],
    "best_for": "Testing stationarity and finding cointegrating relationships in non-stationary time series, implementing Pfaff (2008)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "time-series",
      "econometrics"
    ],
    "summary": "The 'urca' package implements unit root and cointegration tests that are commonly used in applied econometric analysis. It is designed for researchers and practitioners who need to perform statistical tests on time series data.",
    "use_cases": [
      "Testing for unit roots in economic time series",
      "Analyzing cointegration between multiple economic indicators"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for unit root tests",
      "how to perform cointegration tests in R",
      "R ADF test implementation",
      "time series analysis in R",
      "Johansen cointegration procedure R",
      "KPSS test R package"
    ],
    "primary_use_cases": [
      "unit root testing",
      "cointegration analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "tseries",
      "forecast"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "randomizr",
    "description": "Proper randomization procedures for experiments with known assignment probabilities. Implements simple, complete, block, and cluster randomization with exact probability calculations for IPW estimation.",
    "category": "Experimental Design",
    "docs_url": "https://declaredesign.org/r/randomizr/",
    "github_url": "https://github.com/DeclareDesign/randomizr",
    "url": "https://cran.r-project.org/package=randomizr",
    "install": "install.packages(\"randomizr\")",
    "tags": [
      "randomization",
      "block-randomization",
      "cluster-randomization",
      "assignment-probability",
      "experiments"
    ],
    "best_for": "Proper experimental randomization with exact assignment probabilities for IPW",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "experimental-design"
    ],
    "summary": "The randomizr package provides proper randomization procedures for experiments with known assignment probabilities. It is used by researchers and practitioners conducting experiments that require randomization techniques.",
    "use_cases": [
      "Conducting randomized controlled trials",
      "Implementing block randomization in experiments"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "R package for randomization",
      "how to perform block randomization in R",
      "cluster randomization techniques in R",
      "randomization procedures for experiments in R"
    ],
    "primary_use_cases": [
      "block randomization",
      "cluster randomization"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "fixest",
    "description": "Fast and comprehensive package for estimating econometric models with multiple high-dimensional fixed effects, including OLS, GLM, Poisson, and negative binomial models. Features native support for clustered standard errors (up to four-way), instrumental variables, and modern difference-in-differences estimators including Sun-Abraham for staggered treatments.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://lrberge.github.io/fixest/",
    "github_url": "https://github.com/lrberge/fixest",
    "url": "https://cran.r-project.org/package=fixest",
    "install": "install.packages(\"fixest\")",
    "tags": [
      "fixed-effects",
      "panel-data",
      "clustered-standard-errors",
      "difference-in-differences",
      "instrumental-variables"
    ],
    "best_for": "Fast, production-ready estimation of linear/GLM models with multiple high-dimensional fixed effects and publication-quality regression tables via etable()",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "panel-data"
    ],
    "summary": "The fixest package provides a fast and comprehensive solution for estimating econometric models with multiple high-dimensional fixed effects. It is particularly useful for researchers and data scientists working with panel data and fixed effects models.",
    "use_cases": [
      "Estimating models with multiple fixed effects",
      "Conducting difference-in-differences analysis"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for estimating econometric models",
      "how to use fixest for fixed effects",
      "difference-in-differences in R",
      "clustered standard errors in R",
      "instrumental variables in R",
      "Poisson regression with fixest"
    ],
    "primary_use_cases": [
      "estimating econometric models",
      "difference-in-differences analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "tidymodels",
    "description": "Modern framework for modeling and machine learning using tidyverse principles. Meta-package including parsnip (model specification), recipes (preprocessing), workflows, tune (hyperparameter tuning), and yardstick (metrics). Successor to caret.",
    "category": "Machine Learning",
    "docs_url": "https://www.tidymodels.org/",
    "github_url": "https://github.com/tidymodels/tidymodels",
    "url": "https://cran.r-project.org/package=tidymodels",
    "install": "install.packages(\"tidymodels\")",
    "tags": [
      "machine-learning",
      "tidyverse",
      "modeling-framework",
      "hyperparameter-tuning",
      "preprocessing"
    ],
    "best_for": "Modern tidyverse-native ML framework with reproducible workflows\u2014successor to caret",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "tidymodels is a modern framework for modeling and machine learning that adheres to tidyverse principles. It is designed for data scientists and analysts who want to streamline their modeling processes using a cohesive set of tools.",
    "use_cases": [
      "Building predictive models",
      "Performing hyperparameter tuning",
      "Preprocessing data for analysis"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for machine learning",
      "how to do hyperparameter tuning in R",
      "tidyverse modeling framework",
      "best practices for preprocessing in R",
      "R package for model specification",
      "how to use tidymodels for machine learning"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "caret"
    ],
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "modelsummary",
    "description": "Creates publication-quality tables summarizing multiple statistical models side-by-side, plus coefficient plots, data summaries, and correlation matrices. Supports 100+ model types via broom/parameters with output to HTML, LaTeX, Word, PDF, PNG, and Excel.",
    "category": "Regression Output",
    "docs_url": "https://modelsummary.com/",
    "github_url": "https://github.com/vincentarelbundock/modelsummary",
    "url": "https://cran.r-project.org/package=modelsummary",
    "install": "install.packages(\"modelsummary\")",
    "tags": [
      "regression-tables",
      "model-summary",
      "coefficient-plots",
      "publication-tables",
      "tidyverse"
    ],
    "best_for": "Modern, flexible regression tables with extensive customization\u2014the successor to stargazer, implementing Arel-Bundock (2022, JSS)",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The modelsummary package creates publication-quality tables that summarize multiple statistical models side-by-side, along with coefficient plots, data summaries, and correlation matrices. It is primarily used by statisticians and data scientists who need to present model results clearly and effectively.",
    "use_cases": [
      "Generating tables for academic publications",
      "Creating visual summaries of model coefficients"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R library for regression tables",
      "how to create model summary tables in R",
      "R package for coefficient plots",
      "best R packages for statistical model summaries",
      "how to export tables to LaTeX in R",
      "R tools for data summaries"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "targets",
    "description": "Make-like pipeline toolkit for R. Declares dependencies between pipeline steps, skips up-to-date targets, and supports parallel execution. Standard for reproducible research workflows.",
    "category": "Reproducibility",
    "docs_url": "https://docs.ropensci.org/targets/",
    "github_url": "https://github.com/ropensci/targets",
    "url": "https://cran.r-project.org/package=targets",
    "install": "install.packages(\"targets\")",
    "tags": [
      "pipelines",
      "reproducibility",
      "make",
      "dependency-tracking",
      "parallel"
    ],
    "best_for": "Make-like reproducible pipelines with automatic dependency tracking and parallel execution",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The 'targets' package is a Make-like pipeline toolkit for R that helps users declare dependencies between pipeline steps, skip up-to-date targets, and supports parallel execution. It is designed for reproducible research workflows, making it useful for researchers and data scientists.",
    "use_cases": [
      "Building reproducible research workflows",
      "Managing complex data analysis pipelines"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R library for pipeline management",
      "how to create reproducible workflows in R",
      "R package for dependency tracking",
      "parallel execution in R",
      "make-like tools for R",
      "R package for data pipelines"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "pwr",
    "description": "Provides basic power calculations using effect sizes and notation from Cohen (1988). Supports t-tests, chi-squared tests, one-way ANOVA, correlation tests, proportion tests, and general linear models with analytical (closed-form) solutions.",
    "category": "Power Analysis",
    "docs_url": "https://cran.r-project.org/web/packages/pwr/pwr.pdf",
    "github_url": "https://github.com/heliosdrm/pwr",
    "url": "https://cran.r-project.org/package=pwr",
    "install": "install.packages(\"pwr\")",
    "tags": [
      "power-analysis",
      "sample-size",
      "effect-size",
      "Cohen-d",
      "t-test"
    ],
    "best_for": "Basic power calculations for standard statistical tests following Cohen's conventions from Cohen (1988)",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The 'pwr' package provides basic power calculations for various statistical tests using effect sizes and notation from Cohen (1988). It is useful for researchers and statisticians who need to determine sample sizes or power for t-tests, ANOVA, and other statistical analyses.",
    "use_cases": [
      "Determining sample size for a t-test",
      "Calculating power for a one-way ANOVA"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "R package for power analysis",
      "how to calculate sample size in R",
      "R power calculations for t-tests",
      "effect size calculations in R",
      "Cohen's d in R",
      "ANOVA power analysis R",
      "R package for statistical tests"
    ],
    "primary_use_cases": [
      "sample size determination",
      "power analysis for statistical tests"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "pycinc",
    "description": "Changes\u2011in\u2011Changes (CiC) estimator for distributional treatment effects (Athey\u00a0&\u00a0Imbens\u202f2006).",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": "https://pypi.org/project/pycinc/",
    "github_url": null,
    "url": "https://pypi.org/project/pycinc/",
    "install": "pip install pycinc",
    "tags": [
      "DiD",
      "synthetic control",
      "RDD",
      "causal inference"
    ],
    "best_for": "Policy evaluation, natural experiments, quasi-experiments",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "program-evaluation",
      "treatment-effects"
    ],
    "summary": "The pycinc package provides a Changes-in-Changes (CiC) estimator for analyzing distributional treatment effects, as proposed by Athey & Imbens in 2006. It is primarily used by researchers and practitioners in program evaluation and causal inference.",
    "use_cases": [
      "Estimating treatment effects in social programs",
      "Analyzing the impact of policy changes"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for distributional treatment effects",
      "how to estimate treatment effects in python",
      "python causal inference package",
      "Changes-in-Changes estimator python",
      "program evaluation methods in python",
      "synthetic control methods python"
    ],
    "primary_use_cases": [
      "distributional treatment effect estimation"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Athey & Imbens (2006)",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "splm",
    "description": "Maximum likelihood and GMM estimation for spatial panel data models. Implements fixed and random effects specifications with spatial lag and/or spatial error components, including the Kapoor-Kelejian-Prucha (2007) GM estimator. Provides diagnostic tests for spatial autocorrelation in panel settings.",
    "category": "Spatial Econometrics",
    "docs_url": "https://cran.r-project.org/web/packages/splm/splm.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=splm",
    "install": "install.packages(\"splm\")",
    "tags": [
      "spatial-panel",
      "panel-data",
      "fixed-effects",
      "random-effects",
      "GMM"
    ],
    "best_for": "Estimating spatial econometric models with panel (longitudinal) data structures, implementing Millo & Piras (2012)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "spatial-econometrics",
      "panel-data"
    ],
    "summary": "The 'splm' package provides maximum likelihood and GMM estimation for spatial panel data models, allowing users to implement fixed and random effects specifications. It is primarily used by researchers and practitioners in spatial econometrics for analyzing spatially correlated data.",
    "use_cases": [
      "Estimating the impact of spatially correlated variables in economic models",
      "Conducting diagnostic tests for spatial autocorrelation in panel datasets"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for spatial panel data",
      "how to estimate spatial models in R",
      "GMM estimation for panel data in R",
      "spatial econometrics tools in R",
      "fixed effects models in R",
      "random effects models for spatial data"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Kapoor-Kelejian-Prucha (2007)",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "EValue",
    "description": "Conducts sensitivity analyses for unmeasured confounding, selection bias, and measurement error in observational studies and meta-analyses. Computes E-values representing the minimum strength of association unmeasured confounders would need to fully explain away an observed effect.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://louisahsmith.github.io/evalue/",
    "github_url": "https://github.com/mayamathur/evalue_package",
    "url": "https://cran.r-project.org/package=EValue",
    "install": "install.packages(\"EValue\")",
    "tags": [
      "E-value",
      "unmeasured-confounding",
      "sensitivity-analysis",
      "selection-bias",
      "meta-analysis"
    ],
    "best_for": "Quantifying the minimum confounding strength on the risk ratio scale needed to explain away observed treatment-outcome associations, implementing VanderWeele & Ding (2017)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "EValue conducts sensitivity analyses for unmeasured confounding, selection bias, and measurement error in observational studies and meta-analyses. It computes E-values that represent the minimum strength of association unmeasured confounders would need to fully explain away an observed effect.",
    "use_cases": [
      "Assessing the impact of unmeasured confounding in observational studies",
      "Evaluating selection bias in meta-analyses"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for sensitivity analysis",
      "how to compute E-values in R",
      "unmeasured confounding analysis R",
      "selection bias analysis in R",
      "meta-analysis tools in R",
      "E-value computation in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "Greeners",
    "description": "Comprehensive Rust econometrics library with OLS, IV, panel data estimators, fixed effects, DiD, and heteroskedasticity-robust standard errors (HC0-HC3).",
    "category": "Structural Econometrics & Estimation",
    "docs_url": "https://docs.rs/greeners",
    "github_url": "https://github.com/sheep-farm/Greeners",
    "url": "https://crates.io/crates/greeners",
    "install": "cargo add greeners",
    "tags": [
      "rust",
      "econometrics",
      "IV",
      "panel data",
      "robust SE"
    ],
    "best_for": "Academic econometrics in Rust: IV, DiD, robust SEs",
    "language": "Rust",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "econometrics",
      "panel-data",
      "robust-statistics"
    ],
    "summary": "Greeners is a comprehensive Rust library for econometrics that provides various estimators including OLS, IV, and panel data methods. It is designed for researchers and practitioners in the field of econometrics who require robust statistical tools.",
    "use_cases": [
      "Estimating treatment effects using IV",
      "Analyzing panel data with fixed effects"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "Rust library for econometrics",
      "how to perform OLS in Rust",
      "IV estimation in Rust",
      "panel data analysis with Rust",
      "robust standard errors in Rust",
      "econometrics tools in Rust"
    ],
    "primary_use_cases": [
      "OLS estimation",
      "IV estimation",
      "panel data analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0001
  },
  {
    "name": "tmle",
    "description": "Implements targeted maximum likelihood estimation for point treatment effects with binary or continuous outcomes. Estimates ATE, ATT, ATC, and supports marginal structural models. Integrates SuperLearner for data-adaptive nuisance parameter estimation.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://cran.r-project.org/web/packages/tmle/tmle.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=tmle",
    "install": "install.packages(\"tmle\")",
    "tags": [
      "TMLE",
      "causal-inference",
      "ATE",
      "doubly-robust",
      "propensity-score"
    ],
    "best_for": "Estimating point treatment effects (ATE/ATT/ATC) in observational studies with binary treatments, implementing Gruber & van der Laan (2012)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The tmle package implements targeted maximum likelihood estimation for point treatment effects, supporting both binary and continuous outcomes. It is used by statisticians and data scientists working on causal inference problems.",
    "use_cases": [
      "Estimating average treatment effects in clinical trials",
      "Analyzing the impact of a new policy on health outcomes"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for targeted maximum likelihood estimation",
      "how to estimate ATE in R",
      "R package for causal inference",
      "tmle R documentation",
      "how to use SuperLearner in R",
      "tmle for binary outcomes",
      "tmle for continuous outcomes"
    ],
    "primary_use_cases": [
      "estimating average treatment effects (ATE)",
      "supporting marginal structural models"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "simr",
    "description": "Calculates power for generalized linear mixed models (GLMMs) using Monte Carlo simulation. Designed to work with lme4 models; supports LMMs and GLMMs with crossed random effects, non-normal responses, and complex variance structures where analytical solutions are unavailable.",
    "category": "Power Analysis",
    "docs_url": "https://cran.r-project.org/web/packages/simr/vignettes/fromscratch.html",
    "github_url": "https://github.com/pitakakariki/simr",
    "url": "https://cran.r-project.org/package=simr",
    "install": "install.packages(\"simr\")",
    "tags": [
      "power-analysis",
      "mixed-models",
      "simulation",
      "lme4",
      "GLMM"
    ],
    "best_for": "Power analysis for hierarchical/multilevel models via simulation when analytical solutions don't exist, implementing Green & MacLeod (2016, MEE)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The simr package calculates power for generalized linear mixed models (GLMMs) using Monte Carlo simulation. It is designed for researchers and statisticians who need to assess the power of their GLMM analyses, particularly in complex scenarios where analytical solutions are not available.",
    "use_cases": [
      "Assessing power for experimental designs with mixed models",
      "Evaluating sample size requirements for GLMMs"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for power analysis",
      "how to calculate power for GLMM in R",
      "Monte Carlo simulation for mixed models",
      "lme4 power analysis in R",
      "simulating power for generalized linear mixed models",
      "R simulation for mixed models"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "alpaca",
    "description": "Fits generalized linear models (Poisson, negative binomial, logit, probit, Gamma) with high-dimensional k-way fixed effects. Partials out factors during log-likelihood optimization and provides robust/multi-way clustered standard errors, fixed effects recovery, and analytical bias corrections for binary choice models.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://cran.r-project.org/web/packages/alpaca/vignettes/howto.html",
    "github_url": "https://github.com/amrei-stammann/alpaca",
    "url": "https://cran.r-project.org/package=alpaca",
    "install": "install.packages(\"alpaca\")",
    "tags": [
      "glm",
      "fixed-effects",
      "poisson-regression",
      "negative-binomial",
      "gravity-models"
    ],
    "best_for": "Nonlinear panel models (Poisson, logit, probit, negative binomial) with multiple high-dimensional fixed effects, especially structural gravity models for international trade",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The alpaca package fits generalized linear models with high-dimensional k-way fixed effects, optimizing log-likelihood while providing robust standard errors and bias corrections for binary choice models. It is used by data scientists and researchers working with panel data and fixed effects models.",
    "use_cases": [
      "Analyzing panel data with fixed effects",
      "Estimating models with high-dimensional covariates"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for generalized linear models",
      "how to fit Poisson regression in R",
      "R fixed effects model library",
      "robust standard errors in R",
      "negative binomial regression R package",
      "R package for binary choice models"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "didhetero",
    "description": "Doubly robust estimation for group-time conditional average treatment effects. UCB for heterogeneous DiD.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": "https://github.com/tkhdyanagi/didhetero",
    "github_url": "https://github.com/tkhdyanagi/didhetero",
    "url": "https://github.com/tkhdyanagi/didhetero",
    "install": "pip install didhetero",
    "tags": [
      "DiD",
      "heterogeneous effects",
      "doubly robust"
    ],
    "best_for": "Heterogeneous treatment effects in DiD",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "program-evaluation"
    ],
    "summary": "The didhetero package provides a framework for doubly robust estimation of group-time conditional average treatment effects, specifically focusing on heterogeneous effects in difference-in-differences analyses. It is primarily used by researchers and practitioners in program evaluation and causal inference.",
    "use_cases": [
      "Estimating treatment effects in policy evaluations",
      "Analyzing the impact of interventions over time"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for doubly robust estimation",
      "how to estimate heterogeneous treatment effects in python",
      "difference-in-differences analysis in python",
      "program evaluation methods in python",
      "python library for causal inference",
      "how to implement DiD in python"
    ],
    "primary_use_cases": [
      "doubly robust estimation",
      "group-time conditional average treatment effects"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "Surprise",
    "description": "A Python scikit for building and analyzing recommender systems with explicit ratings. Implements SVD, SVD++, NMF, k-NN, and other classic collaborative filtering algorithms. The go-to library for Netflix Prize-style recommendations.",
    "category": "Recommender Systems",
    "docs_url": "https://surpriselib.com/",
    "github_url": "https://github.com/NicolasHug/Surprise",
    "url": "https://github.com/NicolasHug/Surprise",
    "install": "pip install scikit-surprise",
    "tags": [
      "recommender systems",
      "collaborative filtering",
      "matrix factorization"
    ],
    "best_for": "Building and evaluating recommender systems with explicit ratings",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "Surprise is a Python scikit designed for building and analyzing recommender systems using explicit ratings. It is widely used by data scientists and researchers interested in collaborative filtering techniques.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for recommender systems",
      "how to build a recommender system in python",
      "Surprise library documentation",
      "collaborative filtering in python",
      "SVD implementation in python",
      "NMF for recommendations python"
    ],
    "use_cases": [
      "Building a movie recommendation system",
      "Analyzing user preferences for e-commerce products"
    ],
    "primary_use_cases": [
      "recommendation systems",
      "collaborative filtering"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "scikit-surprise"
    ],
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "mstate",
    "description": "Multi-state models in R. Handles competing risks, illness-death models, and complex disease progressions. Estimation, prediction, and visualization.",
    "category": "Healthcare Economics & Health-Tech",
    "docs_url": "https://cran.r-project.org/web/packages/mstate/",
    "github_url": "https://github.com/hputter/mstate",
    "url": "https://cran.r-project.org/web/packages/mstate/",
    "install": "install.packages('mstate')",
    "tags": [
      "multi-state models",
      "competing risks",
      "survival",
      "R"
    ],
    "best_for": "Multi-state and competing risks survival models",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The mstate package provides tools for analyzing multi-state models in R, focusing on competing risks and illness-death models. It is useful for researchers and practitioners in healthcare economics and health technology who need to estimate, predict, and visualize complex disease progressions.",
    "use_cases": [
      "Analyzing patient transitions between health states",
      "Estimating survival probabilities in competing risks scenarios"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "R package for multi-state models",
      "how to analyze competing risks in R",
      "visualizing illness-death models in R",
      "R tools for survival analysis",
      "multi-state modeling in healthcare",
      "estimating disease progression in R"
    ],
    "primary_use_cases": [
      "Multi-state modeling"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "survival",
      "flexsurv",
      "msm"
    ],
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "Surprise",
    "description": "Scikit-learn-style library for building and analyzing recommender systems. Implements SVD, SVD++, NMF, KNN, and baseline algorithms with built-in cross-validation and hyperparameter search.",
    "category": "MarTech & Customer Analytics",
    "docs_url": "https://surpriselib.com/",
    "github_url": "https://github.com/NicolasHug/Surprise",
    "url": "https://surpriselib.com/",
    "install": "pip install scikit-surprise",
    "tags": [
      "recommendations",
      "collaborative-filtering",
      "matrix-factorization"
    ],
    "best_for": "Learning and prototyping recommendation algorithms with a familiar sklearn API",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "Surprise is a Scikit-learn-style library designed for building and analyzing recommender systems. It is used by data scientists and developers looking to implement various recommendation algorithms with ease.",
    "use_cases": [
      "Building a movie recommendation system",
      "Analyzing user preferences for products"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for recommender systems",
      "how to build a recommendation engine in python",
      "collaborative filtering in python",
      "matrix factorization with Surprise",
      "using SVD for recommendations",
      "hyperparameter tuning for recommender systems"
    ],
    "primary_use_cases": [
      "Recommendations"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "Surprise",
      "LightFM"
    ],
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "mediation",
    "description": "Estimates Average Causal Mediation Effects (ACME) with sensitivity analysis for unmeasured confounding. Implements Tingley et al. (2014 JSS) methods for understanding causal mechanisms.",
    "category": "Causal Inference (Mediation)",
    "docs_url": "https://cran.r-project.org/web/packages/mediation/mediation.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=mediation",
    "install": "install.packages(\"mediation\")",
    "tags": [
      "mediation",
      "ACME",
      "causal-mechanisms",
      "sensitivity-analysis",
      "indirect-effects"
    ],
    "best_for": "Average Causal Mediation Effects with sensitivity analysis, implementing Tingley et al. (2014 JSS)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The mediation package estimates Average Causal Mediation Effects (ACME) with sensitivity analysis for unmeasured confounding. It is used by researchers and practitioners interested in understanding causal mechanisms in their data.",
    "use_cases": [],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for causal mediation analysis",
      "how to estimate ACME in R",
      "sensitivity analysis for mediation effects in R",
      "understanding causal mechanisms R package"
    ],
    "primary_use_cases": [
      "estimating average causal mediation effects",
      "sensitivity analysis for unmeasured confounding"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Tingley et al. (2014)",
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "lme4",
    "description": "Fit linear and generalized linear mixed-effects models using S4 classes with Eigen C++ library for efficient computation, supporting arbitrarily nested and crossed random effects structures for hierarchical and longitudinal data.",
    "category": "Mixed Effects",
    "docs_url": "https://cran.r-project.org/web/packages/lme4/vignettes/",
    "github_url": "https://github.com/lme4/lme4",
    "url": "https://cran.r-project.org/package=lme4",
    "install": "install.packages(\"lme4\")",
    "tags": [
      "linear-mixed-models",
      "GLMM",
      "random-effects",
      "hierarchical-models",
      "repeated-measures"
    ],
    "best_for": "Standard linear and generalized linear mixed-effects modeling with crossed/nested random effects, implementing Bates et al. (2015)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The lme4 package is designed to fit linear and generalized linear mixed-effects models using S4 classes. It is utilized by statisticians and data scientists for analyzing hierarchical and longitudinal data with complex random effects structures.",
    "use_cases": [
      "Analyzing longitudinal data",
      "Modeling hierarchical data structures"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for linear mixed models",
      "how to fit generalized linear mixed models in R",
      "lme4 documentation",
      "lme4 examples",
      "R mixed effects modeling",
      "lme4 random effects structures"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "nlme"
    ],
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "nlme",
    "description": "Fit Gaussian linear and nonlinear mixed-effects models with flexible correlation structures, variance functions for heteroscedasticity, and nested random effects. Ships with base R and offers more variance-covariance flexibility than lme4.",
    "category": "Mixed Effects",
    "docs_url": "https://cran.r-project.org/web/packages/nlme/nlme.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=nlme",
    "install": "install.packages(\"nlme\")",
    "tags": [
      "nonlinear-mixed-models",
      "autocorrelation",
      "heteroscedasticity",
      "repeated-measures",
      "longitudinal"
    ],
    "best_for": "Models requiring custom correlation structures, variance functions, or nonlinear mixed effects, implementing Pinheiro & Bates (2000)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "mixed-effects-models",
      "longitudinal-data"
    ],
    "summary": "The nlme package is designed to fit Gaussian linear and nonlinear mixed-effects models, accommodating flexible correlation structures and variance functions for heteroscedasticity. It is widely used by statisticians and data scientists working with complex hierarchical data.",
    "use_cases": [
      "Modeling repeated measures data",
      "Analyzing longitudinal studies"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for mixed-effects models",
      "how to fit nonlinear mixed models in R",
      "R nlme package documentation",
      "Gaussian mixed-effects models in R",
      "nlme vs lme4 in R",
      "how to handle heteroscedasticity in R"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "lme4"
    ],
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "PowerModels.jl",
    "description": "Power network optimization in Julia. Supports AC/DC optimal power flow, transmission expansion, and custom formulations with strong mathematical rigor.",
    "category": "Energy & Utilities Economics",
    "docs_url": "https://lanl-ansi.github.io/PowerModels.jl/stable/",
    "github_url": "https://github.com/lanl-ansi/PowerModels.jl",
    "url": "https://lanl-ansi.github.io/PowerModels.jl/stable/",
    "install": "Julia package",
    "tags": [
      "power flow",
      "Julia",
      "OPF",
      "optimization"
    ],
    "best_for": "Research-grade power flow and OPF with custom formulations",
    "language": "Julia",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "PowerModels.jl is a package for power network optimization in Julia, supporting both AC and DC optimal power flow, as well as transmission expansion and custom formulations. It is designed for users who require strong mathematical rigor in their optimization tasks.",
    "use_cases": [
      "Optimizing power flow in electrical grids",
      "Modeling transmission expansion scenarios"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "Julia library for power flow optimization",
      "how to perform AC optimal power flow in Julia",
      "transmission expansion modeling in Julia",
      "power network optimization tools in Julia"
    ],
    "primary_use_cases": [
      "Power flow optimization"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "GenX",
      "JuMP",
      "PyPSA"
    ],
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "PowerModels.jl",
    "description": "Julia/JuMP package for power flow and optimal power flow problems",
    "category": "Energy Systems Modeling",
    "docs_url": "https://lanl-ansi.github.io/PowerModels.jl/",
    "github_url": "https://github.com/lanl-ansi/PowerModels.jl",
    "url": "https://lanl-ansi.github.io/PowerModels.jl/",
    "install": "using Pkg; Pkg.add(\"PowerModels\")",
    "tags": [
      "power flow",
      "optimal power flow",
      "Julia",
      "JuMP"
    ],
    "best_for": "Research on power flow formulations and optimization algorithms",
    "language": "Julia",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "energy systems",
      "optimization"
    ],
    "summary": "PowerModels.jl is a Julia/JuMP package designed for solving power flow and optimal power flow problems. It is primarily used by researchers and practitioners in the field of energy systems modeling.",
    "use_cases": [
      "Solving power flow problems in electrical grids",
      "Optimizing power generation and distribution"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "Julia package for power flow",
      "how to solve optimal power flow in Julia",
      "energy systems modeling in Julia",
      "JuMP package for power flow problems",
      "PowerModels.jl usage examples",
      "Julia optimization for power systems"
    ],
    "primary_use_cases": [
      "power flow analysis",
      "optimal power flow optimization"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "GenX",
      "JuMP"
    ],
    "maintenance_status": "active",
    "framework_compatibility": [
      "JuMP"
    ],
    "model_score": 0.0
  },
  {
    "name": "fect",
    "description": "Fixed Effects Counterfactual Estimators (v2.0+) incorporating gsynth functionality. Supports treatment switching on/off with carryover effects, matrix completion methods, and Rambachan & Roth sensitivity analysis for parallel trends violations.",
    "category": "Causal Inference (DiD)",
    "docs_url": "https://yiqingxu.org/packages/fect/",
    "github_url": "https://github.com/xuyiqing/fect",
    "url": "https://cran.r-project.org/package=fect",
    "install": "install.packages(\"fect\")",
    "tags": [
      "counterfactual",
      "matrix-completion",
      "interactive-fixed-effects",
      "sensitivity-analysis",
      "carryover"
    ],
    "best_for": "Counterfactual estimation with interactive fixed effects, treatment switching, and sensitivity analysis",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The fect package provides Fixed Effects Counterfactual Estimators that incorporate gsynth functionality, allowing for treatment switching and carryover effects. It is used by researchers and practitioners in causal inference, particularly in the context of DiD analyses.",
    "use_cases": [
      "Estimating treatment effects in observational studies",
      "Analyzing panel data with fixed effects"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for fixed effects counterfactual estimators",
      "how to perform sensitivity analysis in R",
      "R treatment switching analysis",
      "matrix completion methods in R",
      "R package for carryover effects",
      "R gsynth functionality"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "didimputation",
    "description": "Implements the imputation-based DiD estimator that first estimates Y(0) counterfactuals from untreated observations using two-way fixed effects, then imputes treatment effects for treated units. Avoids negative weighting problems of conventional TWFE under heterogeneous treatment effects.",
    "category": "Causal Inference (DiD)",
    "docs_url": "https://github.com/kylebutts/didimputation",
    "github_url": "https://github.com/kylebutts/didimputation",
    "url": "https://cran.r-project.org/package=didimputation",
    "install": "install.packages(\"didimputation\")",
    "tags": [
      "imputation",
      "two-way-fixed-effects",
      "event-study",
      "counterfactual",
      "robust-estimation"
    ],
    "best_for": "Event-study designs where imputation-based correction for TWFE bias is preferred, implementing Borusyak, Jaravel & Spiess (2024)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "counterfactual",
      "robust-estimation"
    ],
    "summary": "The didimputation package implements an imputation-based DiD estimator that estimates Y(0) counterfactuals from untreated observations using two-way fixed effects. It is designed for researchers and practitioners who need to analyze treatment effects while avoiding negative weighting issues in heterogeneous treatment scenarios.",
    "use_cases": [
      "Estimating treatment effects in policy evaluation",
      "Analyzing the impact of interventions in social sciences"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for DiD estimation",
      "how to impute treatment effects in R",
      "two-way fixed effects in R",
      "counterfactual analysis in R",
      "robust estimation methods in R",
      "event study analysis in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "ShiftShareSE",
    "description": "Implements correct standard errors for Bartik/shift-share instrumental variables designs following Ad\u00e3o, Koles\u00e1r, and Morales (2019 QJE). Standard clustered SEs are typically incorrect for shift-share\u2014this package provides econometrically valid inference.",
    "category": "Instrumental Variables",
    "docs_url": "https://cran.r-project.org/web/packages/ShiftShareSE/ShiftShareSE.pdf",
    "github_url": "https://github.com/kolesarm/ShiftShareSE",
    "url": "https://cran.r-project.org/package=ShiftShareSE",
    "install": "install.packages(\"ShiftShareSE\")",
    "tags": [
      "shift-share",
      "Bartik",
      "instrumental-variables",
      "standard-errors",
      "regional-economics"
    ],
    "best_for": "Correct standard errors for Bartik/shift-share IV designs, implementing Ad\u00e3o, Koles\u00e1r & Morales (2019 QJE)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "econometrics"
    ],
    "summary": "ShiftShareSE implements correct standard errors for Bartik/shift-share instrumental variables designs, providing econometrically valid inference. It is primarily used by researchers and practitioners in regional economics.",
    "use_cases": [
      "Estimating the impact of regional economic policies",
      "Analyzing labor market effects using shift-share designs"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for shift-share analysis",
      "how to implement Bartik IV in R",
      "standard errors for shift-share designs in R",
      "econometric inference with R package",
      "Bartik instrumental variables R",
      "shift-share standard errors R package"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Ad\u00e3o, Koles\u00e1r, and Morales (2019)",
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "sf",
    "description": "The modern standard for spatial vector data in R, implementing Simple Features access (ISO 19125). Represents spatial data as data frames with geometry list-columns, enabling seamless tidyverse integration. Interfaces with GDAL (I/O), GEOS (geometry operations), PROJ (projections), and s2 (spherical geometry).",
    "category": "Spatial Econometrics",
    "docs_url": "https://r-spatial.github.io/sf/",
    "github_url": "https://github.com/r-spatial/sf",
    "url": "https://cran.r-project.org/package=sf",
    "install": "install.packages(\"sf\")",
    "tags": [
      "simple-features",
      "spatial-data",
      "vector-data",
      "tidyverse",
      "GDAL-GEOS-PROJ"
    ],
    "best_for": "Reading, writing, manipulating, and visualizing spatial vector data; foundation for all spatial workflows, implementing Pebesma (2018)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "spatial-data",
      "vector-data"
    ],
    "summary": "The 'sf' package provides a modern standard for handling spatial vector data in R, utilizing Simple Features for seamless integration with the tidyverse. It is widely used by data scientists and researchers working with spatial data analysis.",
    "use_cases": [
      "Analyzing geographical data",
      "Visualizing spatial relationships"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for spatial vector data",
      "how to use simple features in R",
      "spatial data analysis in R",
      "tidyverse integration with spatial data",
      "R sf package documentation",
      "best practices for spatial data in R"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "tidyverse"
    ],
    "related_packages": [
      "sp",
      "raster"
    ],
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "sf",
    "description": "R package for simple features geospatial data handling, useful for military installation mapping and conflict geography",
    "category": "Geospatial",
    "docs_url": "https://r-spatial.github.io/sf/",
    "github_url": "https://github.com/r-spatial/sf",
    "url": "https://r-spatial.github.io/sf/",
    "install": "install.packages('sf')",
    "tags": [
      "geospatial",
      "GIS",
      "mapping",
      "spatial analysis"
    ],
    "best_for": "Mapping military installations and analyzing conflict geography",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The 'sf' package is an R library designed for handling simple features geospatial data, making it particularly useful for tasks such as military installation mapping and analyzing conflict geography. It is utilized by researchers and practitioners in fields that require geospatial analysis.",
    "use_cases": [
      "Mapping military installations",
      "Analyzing conflict geography"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "R package for geospatial data",
      "how to map military installations in R",
      "R GIS tools",
      "spatial analysis in R",
      "geospatial analysis with sf",
      "R package for conflict geography"
    ],
    "primary_use_cases": [
      "Geospatial analysis",
      "Mapping"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "tmap",
      "leaflet"
    ],
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "WebPower",
    "description": "Comprehensive collection of tools for basic and advanced statistical power analysis including correlation, t-test, ANOVA, regression, mediation analysis, structural equation modeling (SEM), and multilevel models. Features both R package and web interface.",
    "category": "Power Analysis",
    "docs_url": "https://webpower.psychstat.org/",
    "github_url": "https://github.com/johnnyzhz/WebPower",
    "url": "https://cran.r-project.org/package=WebPower",
    "install": "install.packages(\"WebPower\")",
    "tags": [
      "power-analysis",
      "SEM",
      "mediation",
      "multilevel-models",
      "cluster-randomized-trials"
    ],
    "best_for": "Advanced power analysis for SEM, mediation, and cluster randomized trials, implementing Zhang & Yuan (2018)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "power-analysis"
    ],
    "summary": "WebPower is a comprehensive collection of tools for both basic and advanced statistical power analysis. It is used by researchers and statisticians to conduct various analyses including correlation, t-tests, ANOVA, regression, and more.",
    "use_cases": [
      "Conducting power analysis for a research study",
      "Performing mediation analysis in psychological research"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for power analysis",
      "how to conduct mediation analysis in R",
      "statistical power analysis tools",
      "WebPower R package",
      "ANOVA power analysis in R",
      "structural equation modeling tools in R"
    ],
    "primary_use_cases": [
      "statistical power analysis",
      "mediation analysis",
      "structural equation modeling"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "glmnet",
    "description": "Efficient procedures for fitting regularized generalized linear models via penalized maximum likelihood. Implements LASSO, ridge regression, and elastic net with extremely fast coordinate descent algorithms. Foundation for high-dimensional regression and causal ML.",
    "category": "Machine Learning",
    "docs_url": "https://glmnet.stanford.edu/",
    "github_url": null,
    "url": "https://cran.r-project.org/package=glmnet",
    "install": "install.packages(\"glmnet\")",
    "tags": [
      "LASSO",
      "ridge",
      "elastic-net",
      "regularization",
      "high-dimensional"
    ],
    "best_for": "LASSO, ridge, and elastic net regularization\u2014foundation for high-dimensional regression and causal ML",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "glmnet provides efficient procedures for fitting regularized generalized linear models through penalized maximum likelihood. It is widely used by data scientists and statisticians for high-dimensional regression and causal machine learning applications.",
    "use_cases": [
      "Fitting LASSO models for feature selection",
      "Applying ridge regression for multicollinearity issues"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for LASSO",
      "how to perform ridge regression in R",
      "glmnet tutorial",
      "R elastic net example",
      "high-dimensional regression in R",
      "causal ML with glmnet"
    ],
    "primary_use_cases": [
      "high-dimensional regression",
      "causal ML"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "caret",
      "glm",
      "lme4"
    ],
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "gtsummary",
    "description": "Creates publication-ready analytical and summary tables (Table 1 demographics, regression results, survival analyses) with one line of code. Auto-detects variable types, calculates appropriate statistics, and formats regression models with reference rows and appropriate headers.",
    "category": "Regression Output",
    "docs_url": "https://www.danieldsjoberg.com/gtsummary/",
    "github_url": "https://github.com/ddsjoberg/gtsummary",
    "url": "https://cran.r-project.org/package=gtsummary",
    "install": "install.packages(\"gtsummary\")",
    "tags": [
      "summary-tables",
      "Table1",
      "clinical-tables",
      "regression-tables",
      "reproducible-research"
    ],
    "best_for": "Table 1 demographics and regression summary tables for medical/scientific publications, implementing Sjoberg et al. (2021, R Journal)",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "gtsummary creates publication-ready analytical and summary tables with minimal code. It is commonly used by researchers and data scientists for generating demographic and regression result tables.",
    "use_cases": [
      "Generating Table 1 for clinical trials",
      "Creating regression result tables for research papers"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for summary tables",
      "how to create regression tables in R",
      "R library for publication-ready tables",
      "best R package for clinical tables",
      "gtsummary documentation",
      "R summary tables for demographics"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "gt",
    "description": "Build display tables from tabular data using a cohesive grammar of table parts (header, stub, body, footer). Enables progressive construction of publication-quality tables with extensive formatting, footnotes, and cell styling. Outputs to HTML, LaTeX, and RTF.",
    "category": "Regression Output",
    "docs_url": "https://gt.rstudio.com/",
    "github_url": "https://github.com/rstudio/gt",
    "url": "https://cran.r-project.org/package=gt",
    "install": "install.packages(\"gt\")",
    "tags": [
      "grammar-of-tables",
      "display-tables",
      "HTML-tables",
      "Posit",
      "formatting"
    ],
    "best_for": "Publication-ready display tables with precise formatting control and multiple output formats",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The 'gt' package allows users to build display tables from tabular data using a cohesive grammar of table parts. It is used by individuals looking to create publication-quality tables with extensive formatting options.",
    "use_cases": [
      "Creating formatted tables for academic publications",
      "Generating HTML reports with tables"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "R library for display tables",
      "how to format tables in R",
      "create HTML tables in R",
      "R package for publication-quality tables",
      "build tables from data frames in R",
      "R grammar of tables package"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "dynlm",
    "description": "Provides an interface for fitting dynamic linear regression models with extended formula syntax. Supports convenient lag operators L(), differencing d(), trend(), season(), and harmonic components while preserving time series attributes.",
    "category": "Time Series Econometrics",
    "docs_url": "https://cran.r-project.org/web/packages/dynlm/dynlm.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=dynlm",
    "install": "install.packages(\"dynlm\")",
    "tags": [
      "dynamic-regression",
      "lag-operator",
      "time-series-regression",
      "distributed-lags",
      "formula-syntax"
    ],
    "best_for": "Time series regression with easy specification of lags, differences, and seasonal patterns using formula syntax",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "time-series"
    ],
    "summary": "The dynlm package provides an interface for fitting dynamic linear regression models using an extended formula syntax. It is useful for users who need to apply lag operators and other time series features in their regression analyses.",
    "use_cases": [
      "Fitting dynamic linear regression models",
      "Analyzing time series data with lagged variables"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for dynamic linear regression",
      "how to fit time series regression in R",
      "R dynlm package examples",
      "dynamic regression models in R",
      "using lag operators in R",
      "time series econometrics R package"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "renv",
    "description": "Project-local R dependency management. Creates reproducible environments by recording package versions in a lockfile, isolating project libraries, and enabling version restore.",
    "category": "Reproducibility",
    "docs_url": "https://rstudio.github.io/renv/",
    "github_url": "https://github.com/rstudio/renv",
    "url": "https://cran.r-project.org/package=renv",
    "install": "install.packages(\"renv\")",
    "tags": [
      "reproducibility",
      "package-management",
      "dependency-isolation",
      "lockfile",
      "environments"
    ],
    "best_for": "Project-local package management for reproducible R environments",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The renv package provides project-local R dependency management, allowing users to create reproducible environments by recording package versions in a lockfile. It is useful for R developers who need to isolate project libraries and enable version restoration.",
    "use_cases": [
      "Managing dependencies for R projects",
      "Creating reproducible research environments"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "R package for dependency management",
      "how to create reproducible environments in R",
      "R lockfile management",
      "isolate R project libraries",
      "R package version control",
      "how to restore R package versions"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "broom",
    "description": "Converts messy output from 100+ statistical model types into consistent tidy tibbles using three verbs: tidy() for coefficient-level statistics, glance() for model-level summaries (R\u00b2, AIC), and augment() for fitted values and residuals.",
    "category": "Regression Output",
    "docs_url": "https://broom.tidymodels.org/",
    "github_url": "https://github.com/tidymodels/broom",
    "url": "https://cran.r-project.org/package=broom",
    "install": "install.packages(\"broom\")",
    "tags": [
      "tidy-data",
      "tidymodels",
      "statistical-models",
      "tidyverse",
      "modeling"
    ],
    "best_for": "Converting R statistical model output into consistent tidy data frames for analysis pipelines, based on Wickham (2014, JSS)",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The broom package converts messy output from over 100 statistical model types into consistent tidy tibbles. It is primarily used by data scientists and statisticians to streamline the process of extracting and summarizing model results.",
    "use_cases": [
      "Extracting coefficient-level statistics from regression models",
      "Generating model-level summaries like R\u00b2 and AIC"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for converting model output",
      "how to tidy statistical model results in R",
      "R tidyverse package for model summaries",
      "broom package documentation",
      "R tidy data for regression models",
      "how to use broom for model diagnostics"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "pybaseball",
    "description": "Python library for pulling baseball data from Statcast, FanGraphs, Baseball Reference, and the Lahman database with easy-to-use functions",
    "category": "Sports Analytics",
    "docs_url": "https://github.com/jldbc/pybaseball#readme",
    "github_url": "https://github.com/jldbc/pybaseball",
    "url": "https://github.com/jldbc/pybaseball",
    "install": "pip install pybaseball",
    "tags": [
      "baseball",
      "sports-analytics",
      "Statcast",
      "sabermetrics"
    ],
    "best_for": "Baseball analytics, Statcast data access, and sabermetric research",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [],
    "summary": "pybaseball is a Python library designed for pulling baseball data from various sources like Statcast and FanGraphs. It is useful for analysts and enthusiasts looking to perform sports analytics.",
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for baseball data",
      "how to analyze baseball statistics in python",
      "Statcast data analysis python",
      "baseball analytics with python",
      "using FanGraphs data in python",
      "Lahman database python library"
    ],
    "use_cases": [
      "Analyzing player performance statistics",
      "Visualizing baseball data trends"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "Ndarray",
    "description": "N-dimensional array library for Rust\u2014the NumPy equivalent with slicing, broadcasting, and BLAS/LAPACK integration.",
    "category": "Numerical Optimization & Computational Tools",
    "docs_url": "https://docs.rs/ndarray",
    "github_url": "https://github.com/rust-ndarray/ndarray",
    "url": "https://crates.io/crates/ndarray",
    "install": "cargo add ndarray",
    "tags": [
      "rust",
      "arrays",
      "numpy",
      "scientific computing"
    ],
    "best_for": "NumPy-style N-dimensional arrays in Rust",
    "language": "Rust",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Ndarray is an N-dimensional array library for Rust, designed to provide functionality similar to NumPy, including features like slicing, broadcasting, and integration with BLAS/LAPACK. It is useful for developers and researchers in scientific computing who require efficient array manipulation in Rust.",
    "use_cases": [
      "Data analysis in scientific computing",
      "Machine learning model development"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "rust library for n-dimensional arrays",
      "how to use ndarray in rust",
      "scientific computing with rust",
      "rust equivalent of numpy",
      "array manipulation in rust",
      "broadcasting in rust arrays"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "systemfit",
    "description": "Simultaneous systems estimation implementing Seemingly Unrelated Regression (SUR), two-stage least squares (2SLS), and three-stage least squares (3SLS). Critical for demand systems and structural macro models.",
    "category": "Instrumental Variables",
    "docs_url": "https://cran.r-project.org/web/packages/systemfit/systemfit.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=systemfit",
    "install": "install.packages(\"systemfit\")",
    "tags": [
      "SUR",
      "2SLS",
      "3SLS",
      "systems-estimation",
      "demand-systems"
    ],
    "best_for": "Simultaneous equation systems: SUR, 2SLS, and 3SLS estimation for demand systems and structural models",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "instrumental-variables",
      "econometrics"
    ],
    "summary": "The systemfit package allows for simultaneous estimation of systems of equations using methods such as Seemingly Unrelated Regression (SUR), two-stage least squares (2SLS), and three-stage least squares (3SLS). It is particularly useful for economists and researchers working on demand systems and structural macroeconomic models.",
    "use_cases": [
      "Estimating demand systems for consumer goods",
      "Analyzing structural macroeconomic models",
      "Conducting policy impact assessments"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for simultaneous systems estimation",
      "how to perform SUR in R",
      "R 2SLS implementation",
      "best practices for demand systems in R",
      "three-stage least squares R tutorial",
      "econometric modeling with R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "car",
    "description": "Functions accompanying 'An R Companion to Applied Regression.' Provides advanced regression diagnostics including variance inflation factors (VIF), Type II/III ANOVA, influence measures, linear hypothesis testing, power transformations (Box-Cox), and comprehensive diagnostic plots.",
    "category": "Model Diagnostics",
    "docs_url": "https://www.john-fox.ca/Companion/index.html",
    "github_url": null,
    "url": "https://cran.r-project.org/package=car",
    "install": "install.packages(\"car\")",
    "tags": [
      "regression-diagnostics",
      "VIF",
      "ANOVA",
      "hypothesis-testing",
      "influence-diagnostics"
    ],
    "best_for": "Classical regression diagnostics: VIF for multicollinearity, Type II/III ANOVA, linear hypothesis tests, from Fox & Weisberg (2019)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The 'car' package provides functions for advanced regression diagnostics, including variance inflation factors, ANOVA, and influence measures. It is primarily used by statisticians and data scientists for performing thorough regression analysis.",
    "use_cases": [
      "Assessing multicollinearity in regression models",
      "Evaluating the influence of data points on regression results"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for regression diagnostics",
      "how to calculate VIF in R",
      "ANOVA analysis in R",
      "influence measures in regression R",
      "Box-Cox transformation R",
      "diagnostic plots in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "bife",
    "description": "Estimates fixed effects binary choice models (logit and probit) with potentially many individual fixed effects using a pseudo-demeaning algorithm. Addresses the incidental parameters problem through analytical bias correction based on Fern\u00e1ndez-Val (2009) and computes average partial effects.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://cran.r-project.org/web/packages/bife/vignettes/howto.html",
    "github_url": "https://github.com/amrei-stammann/bife",
    "url": "https://cran.r-project.org/package=bife",
    "install": "install.packages(\"bife\")",
    "tags": [
      "binary-choice",
      "fixed-effects",
      "logit-probit",
      "bias-correction",
      "panel-data"
    ],
    "best_for": "Fast estimation of fixed effects logit/probit models on large panel data with analytical bias correction for the incidental parameters problem",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The 'bife' package estimates fixed effects binary choice models, specifically logit and probit models, using a pseudo-demeaning algorithm. It is designed for researchers and analysts dealing with panel data who need to address the incidental parameters problem and compute average partial effects.",
    "use_cases": [
      "Estimating binary choice models with individual fixed effects",
      "Conducting panel data analysis with bias correction"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for fixed effects binary choice models",
      "how to estimate logit models with fixed effects in R",
      "R package for bias correction in binary choice models",
      "panel data analysis in R",
      "average partial effects in fixed effects models",
      "how to use bife package in R"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Fern\u00e1ndez-Val (2009)",
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "plm",
    "description": "Comprehensive econometrics package for linear panel models providing fixed effects (within), random effects, between, first-difference, Hausman-Taylor, and nested random effects estimators. Includes GMM, FGLS, and extensive diagnostic tests for serial correlation, cross-sectional dependence, and panel unit roots.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://cran.r-project.org/web/packages/plm/vignettes/",
    "github_url": "https://github.com/ycroissant/plm",
    "url": "https://cran.r-project.org/package=plm",
    "install": "install.packages(\"plm\")",
    "tags": [
      "panel-data",
      "econometrics",
      "fixed-effects",
      "random-effects",
      "hausman-test"
    ],
    "best_for": "Comprehensive panel data analysis requiring within/between/random effects estimation, Hausman tests, and extensive diagnostic testing",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "panel-data",
      "econometrics"
    ],
    "summary": "The plm package is a comprehensive econometrics tool designed for linear panel models, offering various estimators such as fixed effects and random effects. It is primarily used by researchers and analysts in economics and social sciences to analyze panel data.",
    "use_cases": [
      "Estimating fixed effects models for economic data",
      "Conducting diagnostic tests for panel data"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for panel data analysis",
      "how to perform fixed effects in R",
      "GMM estimation in R",
      "Hausman-Taylor estimator in R",
      "diagnostic tests for panel data in R",
      "random effects model R package"
    ],
    "primary_use_cases": [
      "fixed effects estimation",
      "random effects estimation"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "plm"
    ],
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "lmtest",
    "description": "Collection of tests for diagnostic checking in linear regression models. Provides the essential coeftest() function for testing coefficients with alternative variance-covariance matrices (pairs with sandwich), plus Breusch-Pagan, Durbin-Watson, and RESET tests.",
    "category": "Robust Standard Errors",
    "docs_url": "https://cran.r-project.org/web/packages/lmtest/vignettes/lmtest-intro.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=lmtest",
    "install": "install.packages(\"lmtest\")",
    "tags": [
      "regression-diagnostics",
      "heteroskedasticity-test",
      "Breusch-Pagan",
      "Durbin-Watson",
      "serial-correlation"
    ],
    "best_for": "Testing coefficient significance with robust SEs and diagnostic tests for regression assumptions, implementing Zeileis & Hothorn (2002)",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The lmtest package provides a collection of tests for diagnostic checking in linear regression models, including essential functions for testing coefficients and various statistical tests. It is primarily used by data scientists and statisticians working with regression analysis.",
    "use_cases": [
      "Testing for heteroskedasticity in regression models",
      "Performing serial correlation tests on residuals"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for regression diagnostics",
      "how to test coefficients in R",
      "Breusch-Pagan test in R",
      "Durbin-Watson test R package",
      "lmtest package documentation",
      "R linear regression model checks"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "strucchange",
    "description": "Testing, monitoring, and dating structural changes in linear regression models. Implements the generalized fluctuation test framework (CUSUM, MOSUM, recursive estimates) and F-test framework (Chow test, supF, aveF, expF) with breakpoint estimation and confidence intervals.",
    "category": "Time Series Econometrics",
    "docs_url": "https://cran.r-project.org/web/packages/strucchange/vignettes/strucchange-intro.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=strucchange",
    "install": "install.packages(\"strucchange\")",
    "tags": [
      "structural-break",
      "CUSUM",
      "Chow-test",
      "breakpoints",
      "parameter-stability"
    ],
    "best_for": "Detecting and dating parameter instability and structural breaks in regression relationships, implementing Zeileis et al. (2002)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "time-series"
    ],
    "summary": "The strucchange package is designed for testing, monitoring, and dating structural changes in linear regression models. It is used by econometricians and data scientists who need to analyze structural breaks in time series data.",
    "use_cases": [
      "Analyzing economic data for structural breaks",
      "Monitoring changes in time series data over time"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for structural change analysis",
      "how to test for structural breaks in R",
      "monitoring structural changes in regression models R",
      "CUSUM test in R",
      "Chow test implementation in R",
      "breakpoint estimation in R"
    ],
    "primary_use_cases": [
      "testing for structural changes",
      "breakpoint estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "Scikit-learn Ens.",
    "description": "(`RandomForestClassifier`/`Regressor`) Widely-used, versatile implementation of Random Forests. Easy API and parallel processing support.",
    "category": "Tree & Ensemble Methods for Prediction",
    "docs_url": "https://scikit-learn.org/stable/modules/ensemble.html#random-forests",
    "github_url": "https://github.com/scikit-learn/scikit-learn",
    "url": "https://github.com/scikit-learn/scikit-learn",
    "install": "pip install scikit-learn",
    "tags": [
      "regression",
      "linear models",
      "machine learning",
      "prediction"
    ],
    "best_for": "OLS regression, basic econometrics, data manipulation",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-pandas",
      "scikit-learn-basics"
    ],
    "topic_tags": [],
    "summary": "Scikit-learn Ens. provides a versatile implementation of Random Forests, suitable for both classification and regression tasks. It is widely used by data scientists and machine learning practitioners for its easy-to-use API and support for parallel processing.",
    "use_cases": [
      "Predicting customer churn",
      "Classifying images based on features"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for Random Forests",
      "how to use RandomForestClassifier in python",
      "scikit-learn tutorial",
      "machine learning regression in python",
      "ensemble methods in python",
      "predictive modeling with scikit-learn"
    ],
    "primary_use_cases": [
      "Building regression models",
      "Model selection"
    ],
    "api_complexity": "simple",
    "related_packages": [
      "XGBoost",
      "LightGBM"
    ],
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "xgboost",
    "description": "Extreme Gradient Boosting implementing state-of-the-art gradient boosted decision trees. Highly efficient, scalable, and portable with interfaces to R, Python, and other languages. Essential for prediction in double ML workflows.",
    "category": "Machine Learning",
    "docs_url": "https://xgboost.readthedocs.io/",
    "github_url": "https://github.com/dmlc/xgboost",
    "url": "https://cran.r-project.org/package=xgboost",
    "install": "install.packages(\"xgboost\")",
    "tags": [
      "gradient-boosting",
      "XGBoost",
      "prediction",
      "machine-learning",
      "ensemble"
    ],
    "best_for": "State-of-the-art gradient boosting for prediction in causal ML and double ML workflows",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn-basics"
    ],
    "topic_tags": [],
    "summary": "XGBoost is an implementation of extreme gradient boosting that provides highly efficient and scalable gradient boosted decision trees. It is widely used for prediction tasks in various machine learning workflows, especially in double ML scenarios.",
    "use_cases": [
      "Predicting outcomes in machine learning competitions",
      "Building models for regression tasks"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for gradient boosting",
      "how to use XGBoost in R",
      "XGBoost prediction examples",
      "best practices for using XGBoost",
      "XGBoost vs other machine learning algorithms",
      "installing XGBoost in Python"
    ],
    "primary_use_cases": [
      "prediction in double ML workflows"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "R",
      "Python"
    ],
    "related_packages": [
      "LightGBM",
      "CatBoost"
    ],
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "scarfmatch",
    "description": "Matching with couples using Scarf's algorithm. Essential for NRMP-style medical residency matching.",
    "category": "Matching & Market Design",
    "docs_url": null,
    "github_url": "https://github.com/dwtang/scarf",
    "url": "https://pypi.org/project/scarfmatch/",
    "install": "pip install scarfmatch",
    "tags": [
      "matching",
      "market design",
      "couples"
    ],
    "best_for": "Residency matching with couples",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [],
    "summary": "Scarfmatch implements Scarf's algorithm for matching couples, which is essential for NRMP-style medical residency matching. It is designed for users involved in market design and matching processes.",
    "use_cases": [
      "Matching couples for medical residency",
      "Designing matching algorithms for market scenarios"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for matching couples",
      "how to use Scarf's algorithm in python",
      "NRMP-style matching in python",
      "market design algorithms in python"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "performance",
    "description": "Utilities for computing indices of model quality and goodness of fit, including R\u00b2, RMSE, ICC, AIC/BIC. Provides functions to check models for overdispersion, zero-inflation, multicollinearity (VIF), convergence, and singularity. Supports mixed effects and Bayesian models.",
    "category": "Model Diagnostics",
    "docs_url": "https://easystats.github.io/performance/",
    "github_url": "https://github.com/easystats/performance",
    "url": "https://cran.r-project.org/package=performance",
    "install": "install.packages(\"performance\")",
    "tags": [
      "model-diagnostics",
      "R-squared",
      "assumption-checking",
      "VIF",
      "goodness-of-fit"
    ],
    "best_for": "Comprehensive model quality assessment, especially the check_model() visual diagnostic panel, implementing L\u00fcdecke et al. (2021, JOSS)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "model-diagnostics",
      "goodness-of-fit"
    ],
    "summary": "The 'performance' package provides utilities for assessing model quality and goodness of fit through various indices such as R\u00b2 and RMSE. It is useful for statisticians and data scientists working with mixed effects and Bayesian models.",
    "use_cases": [
      "Evaluating the fit of a mixed effects model",
      "Checking for zero-inflation in count data models"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for model diagnostics",
      "how to compute R-squared in R",
      "functions for checking overdispersion in R",
      "R utilities for goodness of fit",
      "how to assess multicollinearity in R",
      "R package for Bayesian model evaluation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "see",
    "description": "Visualization toolbox for the easystats ecosystem built on ggplot2. Provides publication-ready plotting methods for model parameters, predictions, and performance diagnostics from all easystats packages via simple plot() calls.",
    "category": "Model Diagnostics",
    "docs_url": "https://easystats.github.io/see/",
    "github_url": "https://github.com/easystats/see",
    "url": "https://cran.r-project.org/package=see",
    "install": "install.packages(\"see\")",
    "tags": [
      "visualization",
      "ggplot2",
      "diagnostic-plots",
      "publication-ready",
      "easystats"
    ],
    "best_for": "Publication-ready visualizations of model diagnostics with a simple plot() interface, implementing L\u00fcdecke et al. (2021, JOSS)",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The 'see' package is a visualization toolbox designed for the easystats ecosystem, leveraging ggplot2 to create publication-ready plots. It is useful for users looking to visualize model parameters, predictions, and performance diagnostics easily.",
    "use_cases": [
      "Creating publication-ready plots for model outputs",
      "Visualizing predictions from statistical models"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "visualization toolbox for easystats",
      "ggplot2 diagnostic plots",
      "how to visualize model parameters in R",
      "publication-ready plots in R",
      "easystats visualization methods",
      "R package for performance diagnostics"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "panelr",
    "description": "Automates within-between (hybrid) model specification for panel/longitudinal data, combining fixed effects robustness to time-invariant confounding with random effects ability to estimate time-invariant coefficients. Uses lme4 for multilevel estimation with optional Bayesian (brms) and GEE (geepack) backends.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://panelr.jacob-long.com/",
    "github_url": "https://github.com/jacob-long/panelr",
    "url": "https://cran.r-project.org/package=panelr",
    "install": "install.packages(\"panelr\")",
    "tags": [
      "hybrid-models",
      "within-between",
      "panel-data",
      "longitudinal-analysis",
      "bell-jones"
    ],
    "best_for": "Researchers needing fixed effects-equivalent estimates while retaining time-invariant predictors and random slopes",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "panel-data",
      "longitudinal-analysis",
      "fixed-effects",
      "random-effects"
    ],
    "summary": "The panelr package automates the specification of hybrid models for panel and longitudinal data, leveraging fixed effects for robustness against time-invariant confounding and random effects for estimating time-invariant coefficients. It is primarily used by data scientists and researchers working with complex longitudinal datasets.",
    "use_cases": [
      "Analyzing the impact of policy changes over time",
      "Evaluating the effectiveness of a treatment across different groups"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for hybrid models",
      "how to analyze panel data in R",
      "longitudinal data analysis in R",
      "fixed effects vs random effects in R",
      "automate panel model specification R",
      "R lme4 package usage",
      "Bayesian modeling in R"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "lme4",
      "brms",
      "geepack"
    ],
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "sandwich",
    "description": "Object-oriented software for model-robust covariance matrix estimators including heteroscedasticity-consistent (HC0-HC5), heteroscedasticity- and autocorrelation-consistent (HAC/Newey-West), clustered, panel, and bootstrap covariances. Works with lm, glm, fixest, survival models, and many others.",
    "category": "Robust Standard Errors",
    "docs_url": "https://sandwich.R-Forge.R-project.org/",
    "github_url": null,
    "url": "https://cran.r-project.org/package=sandwich",
    "install": "install.packages(\"sandwich\")",
    "tags": [
      "robust-standard-errors",
      "heteroskedasticity-consistent",
      "HAC-covariance",
      "cluster-robust",
      "Newey-West"
    ],
    "best_for": "Computing robust standard errors for cross-sectional, time series, clustered, or panel data, implementing Zeileis (2004, 2006, 2020, JSS)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The 'sandwich' package provides object-oriented software for model-robust covariance matrix estimators, including various types of heteroscedasticity-consistent and clustered covariances. It is used by statisticians and data scientists working with linear and generalized linear models, among others.",
    "use_cases": [
      "Estimating robust standard errors for regression models",
      "Calculating clustered covariances in panel data analysis"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for robust standard errors",
      "how to calculate HAC covariance in R",
      "R library for heteroskedasticity-consistent estimators",
      "cluster-robust covariance in R",
      "bootstrap covariances in R",
      "R package for panel data covariance estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "spatialreg",
    "description": "Comprehensive package for spatial regression model estimation, split from spdep in 2019. Provides maximum likelihood, two-stage least squares, and GMM estimation for spatial lag (SAR), spatial error (SEM), and combined (SARAR/SAC) models, plus Spatial Durbin and SLX variants with impact calculations.",
    "category": "Spatial Econometrics",
    "docs_url": "https://r-spatial.github.io/spatialreg/",
    "github_url": "https://github.com/r-spatial/spatialreg",
    "url": "https://cran.r-project.org/package=spatialreg",
    "install": "install.packages(\"spatialreg\")",
    "tags": [
      "spatial-regression",
      "maximum-likelihood",
      "spatial-lag",
      "spatial-error",
      "GMM"
    ],
    "best_for": "Estimating cross-sectional spatial regression models (SAR, SEM, SAC, SDM) with maximum likelihood or GMM, implementing Bivand & Piras (2015)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "spatial-econometrics"
    ],
    "summary": "The spatialreg package provides tools for estimating spatial regression models, including maximum likelihood and GMM estimation methods. It is used by researchers and practitioners in spatial econometrics to analyze spatial data and model spatial relationships.",
    "use_cases": [
      "Estimating spatial lag models for regional economic analysis",
      "Analyzing the impact of spatially correlated variables in urban studies"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for spatial regression",
      "how to estimate spatial lag model in R",
      "maximum likelihood estimation in spatial econometrics",
      "GMM for spatial error models",
      "spatial Durbin model in R",
      "spatial econometrics tools",
      "impact calculations in spatial models"
    ],
    "primary_use_cases": [
      "spatial lag model estimation",
      "spatial error model analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "spdep"
    ],
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "KFAS",
    "description": "State space modeling framework for exponential family time series with computationally efficient Kalman filtering, smoothing, forecasting, and simulation. Supports observations from Gaussian, Poisson, binomial, negative binomial, and gamma distributions.",
    "category": "Time Series Econometrics",
    "docs_url": "https://cran.r-project.org/web/packages/KFAS/KFAS.pdf",
    "github_url": "https://github.com/helske/KFAS",
    "url": "https://cran.r-project.org/package=KFAS",
    "install": "install.packages(\"KFAS\")",
    "tags": [
      "state-space",
      "kalman-filter",
      "time-series",
      "forecasting",
      "exponential-family"
    ],
    "best_for": "Multivariate time series modeling with non-Gaussian observations (e.g., count data with Poisson), implementing Helske (2017)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "time-series",
      "state-space",
      "forecasting"
    ],
    "summary": "KFAS is a state space modeling framework designed for exponential family time series analysis. It provides computationally efficient Kalman filtering, smoothing, forecasting, and simulation, making it suitable for users working with various distributions.",
    "use_cases": [
      "Modeling time series data with Gaussian observations",
      "Forecasting using Poisson distributed data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for state space modeling",
      "how to perform Kalman filtering in R",
      "time series forecasting with KFAS",
      "R library for exponential family time series",
      "state space modeling in R",
      "how to use KFAS for forecasting"
    ],
    "primary_use_cases": [
      "Kalman filtering",
      "time series forecasting"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "dlm",
    "description": "Maximum likelihood and Bayesian analysis of Normal linear state space models (Dynamic Linear Models). Features numerically stable SVD-based algorithms for Kalman filtering and smoothing, plus tools for MCMC-based Bayesian inference including forward filtering backward sampling (FFBS).",
    "category": "Time Series Econometrics",
    "docs_url": "https://cran.r-project.org/web/packages/dlm/vignettes/dlm.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=dlm",
    "install": "install.packages(\"dlm\")",
    "tags": [
      "state-space",
      "kalman-filter",
      "Bayesian",
      "time-series",
      "dynamic-linear-models"
    ],
    "best_for": "Bayesian analysis of linear Gaussian state space models with MCMC methods (Gibbs sampling), implementing Petris (2010)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "time-series",
      "bayesian"
    ],
    "summary": "The dlm package provides tools for maximum likelihood and Bayesian analysis of Normal linear state space models, specifically Dynamic Linear Models. It is used by statisticians and data scientists for Kalman filtering, smoothing, and MCMC-based Bayesian inference.",
    "use_cases": [
      "Kalman filtering for time series data",
      "Bayesian inference in dynamic models"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for dynamic linear models",
      "how to perform Kalman filtering in R",
      "Bayesian analysis with R",
      "state space models in R",
      "MCMC Bayesian inference R",
      "time series analysis R package"
    ],
    "primary_use_cases": [
      "Kalman filtering",
      "Bayesian inference"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "Augurs",
    "description": "Time series forecasting and analysis for Rust with ETS, MSTL decomposition, seasonality detection, outlier detection, and Prophet-style models.",
    "category": "Time Series Forecasting",
    "docs_url": "https://docs.augu.rs/",
    "github_url": "https://github.com/grafana/augurs",
    "url": "https://crates.io/crates/augurs",
    "install": "cargo add augurs",
    "tags": [
      "rust",
      "time series",
      "forecasting",
      "ETS",
      "MSTL"
    ],
    "best_for": "Time series forecasting and structural analysis in Rust",
    "language": "Rust",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "time-series"
    ],
    "summary": "Augurs is a Rust package designed for time series forecasting and analysis, featuring methods like ETS, MSTL decomposition, and Prophet-style models. It is suitable for users interested in advanced time series techniques and analysis.",
    "use_cases": [
      "Forecasting future values in time series data",
      "Detecting seasonality in datasets"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "Rust library for time series forecasting",
      "how to do time series analysis in Rust",
      "time series decomposition in Rust",
      "outlier detection in Rust",
      "seasonality detection in Rust",
      "Prophet-style models in Rust"
    ],
    "primary_use_cases": [
      "time series forecasting",
      "seasonality detection"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "forecast",
    "description": "The foundational R package for univariate time series forecasting. Provides methods for exponential smoothing via state space models (ETS), automatic ARIMA modeling with auto.arima(), TBATS for complex seasonality, and comprehensive model evaluation tools.",
    "category": "Time Series Forecasting",
    "docs_url": "https://pkg.robjhyndman.com/forecast/",
    "github_url": "https://github.com/robjhyndman/forecast",
    "url": "https://cran.r-project.org/package=forecast",
    "install": "install.packages(\"forecast\")",
    "tags": [
      "time-series",
      "ARIMA",
      "exponential-smoothing",
      "ETS",
      "auto.arima"
    ],
    "best_for": "Classical statistical forecasting for univariate time series with automatic model selection, implementing Hyndman & Khandakar (2008)",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "time-series"
    ],
    "summary": "The forecast package is designed for univariate time series forecasting in R. It provides various methods for modeling and evaluating time series data, making it suitable for statisticians and data scientists working with time series analysis.",
    "use_cases": [
      "Forecasting sales data",
      "Predicting stock prices",
      "Analyzing seasonal trends"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for time series forecasting",
      "how to use auto.arima in R",
      "exponential smoothing methods in R",
      "forecasting with ETS in R",
      "TBATS model in R",
      "time series analysis R package"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "forecastHybrid",
      "fable"
    ],
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "XGBoost",
    "description": "High-performance, optimized gradient boosting library (also supports RF). Known for speed, efficiency, and winning competitions.",
    "category": "Tree & Ensemble Methods for Prediction",
    "docs_url": "https://xgboost.readthedocs.io/",
    "github_url": "https://github.com/dmlc/xgboost",
    "url": "https://github.com/dmlc/xgboost",
    "install": "pip install xgboost",
    "tags": [
      "machine learning",
      "prediction"
    ],
    "best_for": "Random forests, gradient boosting, prediction tasks",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "XGBoost is a high-performance, optimized gradient boosting library that supports both gradient boosting and random forests. It is widely used in machine learning competitions for its speed and efficiency.",
    "use_cases": [
      "predictive modeling",
      "classification tasks"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for gradient boosting",
      "how to use XGBoost in python",
      "XGBoost tutorial",
      "XGBoost vs random forest",
      "best practices for XGBoost",
      "XGBoost parameter tuning"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "LightGBM",
      "CatBoost"
    ],
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "xgboost",
    "description": "Gradient boosting framework widely used as baseline for CTR prediction and attribution modeling",
    "category": "Machine Learning",
    "docs_url": "https://xgboost.readthedocs.io/",
    "github_url": "https://github.com/dmlc/xgboost",
    "url": "https://xgboost.readthedocs.io/",
    "install": "pip install xgboost",
    "tags": [
      "gradient boosting",
      "CTR",
      "baseline",
      "XGBoost"
    ],
    "best_for": "Fast, high-performance baseline models for CTR and conversion prediction",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "XGBoost is a gradient boosting framework that is widely used for tasks such as click-through rate (CTR) prediction and attribution modeling. It is popular among data scientists and machine learning practitioners for its efficiency and performance.",
    "use_cases": [
      "Predicting click-through rates for online ads",
      "Attribution modeling in marketing campaigns"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for gradient boosting",
      "how to do CTR prediction in python",
      "XGBoost tutorial",
      "XGBoost for attribution modeling",
      "implementing gradient boosting in python",
      "XGBoost examples",
      "using XGBoost for machine learning"
    ],
    "primary_use_cases": [
      "Classification",
      "CTR prediction"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "LightGBM",
      "CatBoost"
    ],
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "tsDyn",
    "description": "Implements nonlinear autoregressive time series models including threshold AR (TAR/SETAR), smooth transition AR (STAR, LSTAR), and multivariate extensions (TVAR, TVECM). Enables regime-switching dynamics analysis with parametric and non-parametric approaches.",
    "category": "Time Series Econometrics",
    "docs_url": "https://github.com/MatthieuStigler/tsDyn/wiki",
    "github_url": "https://github.com/MatthieuStigler/tsDyn",
    "url": "https://cran.r-project.org/package=tsDyn",
    "install": "install.packages(\"tsDyn\")",
    "tags": [
      "nonlinear",
      "SETAR",
      "LSTAR",
      "threshold-VAR",
      "regime-switching"
    ],
    "best_for": "Modeling regime-switching dynamics and threshold cointegration in univariate and multivariate series",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "time-series"
    ],
    "summary": "tsDyn implements nonlinear autoregressive time series models, enabling analysis of regime-switching dynamics through various approaches. It is used by researchers and practitioners in econometrics and time series analysis.",
    "use_cases": [
      "Analyzing economic time series data",
      "Forecasting with regime-switching models"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for nonlinear autoregressive models",
      "how to analyze regime-switching dynamics in R",
      "R package for threshold AR models",
      "time series econometrics in R",
      "R library for smooth transition AR",
      "how to implement TAR models in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "vars",
    "description": "Comprehensive package for Vector Autoregression (VAR), Structural VAR (SVAR), and Structural Vector Error Correction (SVEC) models. Provides estimation, lag selection, diagnostic testing, forecasting, Granger causality analysis, impulse response functions, and forecast error variance decomposition.",
    "category": "Time Series Econometrics",
    "docs_url": "https://cran.r-project.org/web/packages/vars/vars.pdf",
    "github_url": "https://github.com/bpfaff/vars",
    "url": "https://cran.r-project.org/package=vars",
    "install": "install.packages(\"vars\")",
    "tags": [
      "VAR",
      "SVAR",
      "impulse-response",
      "Granger-causality",
      "FEVD"
    ],
    "best_for": "Multivariate time series analysis with impulse response functions and variance decomposition, implementing Pfaff (2008)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "time-series"
    ],
    "summary": "The vars package provides tools for estimating Vector Autoregression (VAR) and related models, including diagnostic testing and forecasting. It is used by econometricians and data scientists interested in time series analysis.",
    "use_cases": [
      "Estimating VAR models for economic data",
      "Conducting Granger causality tests",
      "Forecasting future values based on historical data"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for Vector Autoregression",
      "how to perform Granger causality analysis in R",
      "forecasting with VAR models in R",
      "impulse response functions in R",
      "Structural VAR analysis in R",
      "time series econometrics R package"
    ],
    "primary_use_cases": [
      "Granger causality analysis",
      "forecasting"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "OasisLMF",
    "description": "Open-source catastrophe modeling platform used by major reinsurers, supporting custom hazard/vulnerability models with standardized data formats",
    "category": "Insurance & Actuarial",
    "docs_url": "https://oasislmf.github.io/",
    "github_url": "https://github.com/OasisLMF/OasisLMF",
    "url": "https://oasislmf.org/",
    "install": "pip install oasislmf",
    "tags": [
      "catastrophe-modeling",
      "reinsurance",
      "exposure-management",
      "loss-modeling",
      "open-source"
    ],
    "best_for": "Building and running catastrophe models for property insurance and reinsurance",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python"
    ],
    "topic_tags": [],
    "summary": "OasisLMF is an open-source catastrophe modeling platform that allows users to create custom hazard and vulnerability models using standardized data formats. It is primarily used by major reinsurers to manage exposure and assess losses.",
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for catastrophe modeling",
      "how to model reinsurance exposure in python",
      "open-source tools for loss modeling",
      "custom hazard models in python",
      "OasisLMF documentation",
      "catastrophe modeling platform for reinsurers"
    ],
    "use_cases": [
      "Modeling natural disasters for risk assessment",
      "Creating custom vulnerability models for specific regions"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "ranger",
    "description": "Fast implementation of random forests particularly suited for high-dimensional data. Provides survival forests, classification, and regression with efficient memory usage. Core backend for grf's causal forests.",
    "category": "Machine Learning",
    "docs_url": "https://cran.r-project.org/web/packages/ranger/ranger.pdf",
    "github_url": "https://github.com/imbs-hl/ranger",
    "url": "https://cran.r-project.org/package=ranger",
    "install": "install.packages(\"ranger\")",
    "tags": [
      "random-forests",
      "survival-forests",
      "high-dimensional",
      "fast",
      "causal-forests"
    ],
    "best_for": "Fast random forests for high-dimensional data\u2014backend for grf causal forests",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "machine-learning",
      "random-forests",
      "survival-analysis"
    ],
    "summary": "Ranger is a fast implementation of random forests that is particularly suited for high-dimensional data. It provides capabilities for survival forests, classification, and regression, making it useful for data scientists and statisticians working with complex datasets.",
    "use_cases": [
      "Predicting outcomes in high-dimensional datasets",
      "Conducting survival analysis with random forests"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for random forests",
      "how to implement survival forests in R",
      "fast random forests in R",
      "R package for high-dimensional data analysis"
    ],
    "primary_use_cases": [
      "causal forest estimation"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "randomForest",
      "grf"
    ],
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "Argmin",
    "description": "Numerical optimization framework for Rust with Newton, BFGS, L-BFGS, trust region, and derivative-free methods for MLE/GMM.",
    "category": "Optimization",
    "docs_url": "https://docs.rs/argmin",
    "github_url": "https://github.com/argmin-rs/argmin",
    "url": "https://crates.io/crates/argmin",
    "install": "cargo add argmin",
    "tags": [
      "rust",
      "optimization",
      "BFGS",
      "MLE",
      "GMM"
    ],
    "best_for": "Maximum Likelihood and GMM estimation in Rust",
    "language": "Rust",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Argmin is a numerical optimization framework designed for Rust programming language. It provides various optimization methods such as Newton, BFGS, L-BFGS, trust region, and derivative-free methods for maximum likelihood estimation (MLE) and generalized method of moments (GMM).",
    "use_cases": [],
    "audience": [],
    "synthetic_questions": [
      "Rust library for numerical optimization",
      "how to perform MLE in Rust",
      "Rust optimization framework",
      "BFGS method implementation in Rust",
      "derivative-free optimization in Rust",
      "trust region methods in Rust"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "lfe",
    "description": "Efficiently estimates linear models with multiple high-dimensional fixed effects using the Method of Alternating Projections. Designed for datasets with factors having thousands of levels (hundreds of thousands of dummy variables), with full support for 2SLS instrumental variables and multi-way clustered standard errors.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://cran.r-project.org/web/packages/lfe/lfe.pdf",
    "github_url": "https://github.com/r-econometrics/lfe",
    "url": "https://cran.r-project.org/package=lfe",
    "install": "install.packages(\"lfe\")",
    "tags": [
      "high-dimensional-fe",
      "worker-firm",
      "memory-efficient",
      "instrumental-variables",
      "clustered-se"
    ],
    "best_for": "AKM-style wage decompositions and matched employer-employee data with hundreds of thousands of worker/firm fixed effects",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The lfe package efficiently estimates linear models with multiple high-dimensional fixed effects using the Method of Alternating Projections. It is designed for datasets with factors having thousands of levels and supports 2SLS instrumental variables and multi-way clustered standard errors.",
    "use_cases": [],
    "audience": [],
    "synthetic_questions": [
      "R package for high-dimensional fixed effects",
      "how to estimate linear models with fixed effects in R",
      "R library for 2SLS instrumental variables",
      "memory efficient R package for panel data",
      "clustered standard errors in R",
      "efficient estimation of linear models in R",
      "R package for multi-way clustered standard errors"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "clubSandwich",
    "description": "Provides cluster-robust variance estimators with small-sample corrections, including bias-reduced linearization (BRL/CR2). Includes functions for hypothesis testing with Satterthwaite degrees of freedom and Hotelling's T\u00b2 approximation\u2014essential when the number of clusters is small.",
    "category": "Robust Standard Errors",
    "docs_url": "https://jepusto.github.io/clubSandwich/",
    "github_url": "https://github.com/jepusto/clubSandwich",
    "url": "https://cran.r-project.org/package=clubSandwich",
    "install": "install.packages(\"clubSandwich\")",
    "tags": [
      "cluster-robust",
      "small-sample-corrections",
      "bias-reduced-linearization",
      "fixed-effects",
      "meta-analysis"
    ],
    "best_for": "Cluster-robust inference when the number of clusters is small, especially in panel data and meta-analysis, implementing Pustejovsky & Tipton (2018, JBES)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "robust-statistics",
      "hypothesis-testing"
    ],
    "summary": "The clubSandwich package provides cluster-robust variance estimators with small-sample corrections, including bias-reduced linearization. It is particularly useful for hypothesis testing in scenarios with a limited number of clusters.",
    "use_cases": [
      "Analyzing data with a small number of clusters",
      "Conducting hypothesis tests with cluster-robust methods"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for cluster-robust variance estimators",
      "how to perform hypothesis testing with small samples in R",
      "R library for bias-reduced linearization",
      "cluster-robust standard errors in R",
      "functions for Satterthwaite degrees of freedom in R",
      "Hotelling's T\u00b2 approximation in R"
    ],
    "primary_use_cases": [
      "hypothesis testing with Satterthwaite degrees of freedom",
      "calculating bias-reduced linearization"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "text2vec",
    "description": "Efficient text vectorization with word embeddings (GloVe), topic models (LDA), and document similarity. Memory-efficient streaming API for large corpora with C++ backend.",
    "category": "Text Analysis",
    "docs_url": "https://text2vec.org/",
    "github_url": "https://github.com/dselivanov/text2vec",
    "url": "https://cran.r-project.org/package=text2vec",
    "install": "install.packages(\"text2vec\")",
    "tags": [
      "word-embeddings",
      "GloVe",
      "text-vectorization",
      "LDA",
      "document-similarity"
    ],
    "best_for": "Efficient word embeddings (GloVe) and text vectorization for large corpora",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "text2vec is an R package designed for efficient text vectorization using word embeddings like GloVe, topic models such as LDA, and document similarity. It is suitable for users dealing with large corpora and is particularly useful for data scientists and researchers in text analysis.",
    "use_cases": [
      "Analyzing large text corpora",
      "Building topic models for research"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for text vectorization",
      "how to use GloVe in R",
      "text similarity analysis in R",
      "topic modeling with LDA in R",
      "document similarity using R",
      "efficient text processing in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "sgp4",
    "description": "Implementation of the SGP4/SDP4 satellite propagation algorithms for processing TLE orbital data",
    "category": "Space & Orbital Analysis",
    "docs_url": "https://pypi.org/project/sgp4/",
    "github_url": "https://github.com/brandon-rhodes/python-sgp4",
    "url": "https://pypi.org/project/sgp4/",
    "install": "pip install sgp4",
    "tags": [
      "satellites",
      "TLE",
      "propagation",
      "orbital mechanics"
    ],
    "best_for": "Processing Space-Track TLE data for satellite position prediction",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The sgp4 package implements the SGP4/SDP4 satellite propagation algorithms, allowing users to process TLE (Two-Line Element) orbital data for satellites. It is used by individuals and organizations involved in space and orbital analysis.",
    "use_cases": [],
    "audience": [],
    "synthetic_questions": [
      "python library for satellite propagation",
      "how to process TLE data in python",
      "SGP4 algorithm implementation in python",
      "orbital mechanics library in python",
      "satellite tracking with python",
      "python TLE orbital analysis"
    ],
    "primary_use_cases": [
      "Satellite propagation",
      "TLE processing"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "Skyfield",
      "poliastro"
    ],
    "maintenance_status": "active",
    "model_score": 0.0
  },
  {
    "name": "EDSL",
    "description": "Expected Parrot Domain-Specific Language for designing and running LLM-powered surveys and experiments. Create AI agent personas with demographic traits for homo silicus research.",
    "category": "Agentic AI",
    "docs_url": "https://docs.expectedparrot.com",
    "github_url": "https://github.com/expectedparrot/edsl",
    "url": "https://www.expectedparrot.com/",
    "install": "pip install edsl",
    "tags": [
      "LLM",
      "surveys",
      "experiments",
      "homo-silicus",
      "synthetic-agents"
    ],
    "best_for": "LLM-based surveys and simulated economic agents",
    "language": "Python",
    "model_score": 0.0
  },
  {
    "name": "anthropic",
    "description": "Official Python SDK for Claude and Anthropic's API. Build AI applications with Claude models.",
    "category": "Agentic AI",
    "docs_url": "https://docs.anthropic.com/",
    "github_url": "https://github.com/anthropics/anthropic-sdk-python",
    "url": "https://www.anthropic.com/",
    "install": "pip install anthropic",
    "tags": [
      "LLM",
      "Claude",
      "API",
      "Anthropic"
    ],
    "best_for": "Building applications with Claude models",
    "language": "Python",
    "model_score": 0.0
  },
  {
    "name": "openai",
    "description": "Official Python SDK for OpenAI's API. Access GPT-4, o1, DALL-E, embeddings, and other OpenAI models.",
    "category": "Agentic AI",
    "docs_url": "https://platform.openai.com/docs/",
    "github_url": "https://github.com/openai/openai-python",
    "url": "https://platform.openai.com/",
    "install": "pip install openai",
    "tags": [
      "LLM",
      "GPT",
      "API",
      "OpenAI",
      "embeddings"
    ],
    "best_for": "Building applications with GPT and OpenAI models",
    "language": "Python",
    "model_score": 0.0
  },
  {
    "name": "NLTK",
    "description": "Natural Language Toolkit - comprehensive library for NLP research and education with 50+ corpora and lexical resources.",
    "category": "Natural Language Processing for Economics",
    "docs_url": "https://www.nltk.org/",
    "github_url": "https://github.com/nltk/nltk",
    "url": "https://www.nltk.org/",
    "install": "pip install nltk",
    "tags": [
      "NLP",
      "text-analysis",
      "corpora",
      "tokenization"
    ],
    "best_for": "NLP research, text preprocessing, educational use",
    "language": "Python",
    "model_score": 0.0
  },
  {
    "name": "sentence-transformers",
    "description": "Framework for state-of-the-art sentence, text and image embeddings. Powers semantic search and similarity applications.",
    "category": "Natural Language Processing for Economics",
    "docs_url": "https://www.sbert.net/",
    "github_url": "https://github.com/UKPLab/sentence-transformers",
    "url": "https://www.sbert.net/",
    "install": "pip install sentence-transformers",
    "tags": [
      "embeddings",
      "semantic-search",
      "NLP",
      "transformers"
    ],
    "best_for": "Sentence embeddings, semantic similarity, document search",
    "language": "Python",
    "model_score": 0.0
  },
  {
    "name": "TensorFlow",
    "description": "Google's end-to-end open-source machine learning platform. Build and deploy ML models at scale.",
    "category": "Machine Learning",
    "docs_url": "https://www.tensorflow.org/api_docs",
    "github_url": "https://github.com/tensorflow/tensorflow",
    "url": "https://www.tensorflow.org/",
    "install": "pip install tensorflow",
    "tags": [
      "deep-learning",
      "neural-networks",
      "machine-learning",
      "Google"
    ],
    "best_for": "Deep learning, neural networks, production ML systems",
    "language": "Python",
    "model_score": 0.0
  },
  {
    "name": "Elicit",
    "description": "AI research assistant that automates literature review across 126M+ academic papers. 99%+ accuracy in data extraction from research papers.",
    "category": "Research Tools",
    "docs_url": null,
    "github_url": null,
    "url": "https://elicit.com/",
    "install": null,
    "tags": [
      "literature-review",
      "research",
      "AI-assistant",
      "academic"
    ],
    "best_for": "Automated literature search and paper summarization",
    "language": "Web",
    "model_score": 0.0
  },
  {
    "name": "Consensus",
    "description": "AI-powered academic search engine providing evidence-based answers from peer-reviewed literature with economics specialty.",
    "category": "Research Tools",
    "docs_url": null,
    "github_url": null,
    "url": "https://consensus.app/",
    "install": null,
    "tags": [
      "literature-review",
      "research",
      "evidence-based",
      "academic"
    ],
    "best_for": "Finding research consensus on economic questions",
    "language": "Web",
    "model_score": 0.0
  },
  {
    "name": "Semantic Scholar API",
    "description": "AI-powered research tool with 200M+ papers indexed. Free API access for academic paper search and citation analysis.",
    "category": "Research Tools",
    "docs_url": "https://api.semanticscholar.org/",
    "github_url": null,
    "url": "https://www.semanticscholar.org/",
    "install": "pip install semanticscholar",
    "tags": [
      "literature-review",
      "API",
      "citations",
      "academic"
    ],
    "best_for": "Programmatic access to academic paper metadata and citations",
    "language": "Python",
    "model_score": 0.0
  },
  {
    "name": "Research Rabbit",
    "description": "Free tool for discovering academic papers through network visualization of paper connections and co-authorships.",
    "category": "Research Tools",
    "docs_url": null,
    "github_url": null,
    "url": "https://www.researchrabbit.ai/",
    "install": null,
    "tags": [
      "literature-review",
      "visualization",
      "discovery",
      "academic"
    ],
    "best_for": "Discovering related papers through citation networks",
    "language": "Web",
    "model_score": 0.0
  },
  {
    "name": "Connected Papers",
    "description": "Visual tool for exploring academic paper relationships. Creates visual graphs showing prior and derivative works.",
    "category": "Research Tools",
    "docs_url": null,
    "github_url": null,
    "url": "https://www.connectedpapers.com/",
    "install": null,
    "tags": [
      "literature-review",
      "visualization",
      "citations",
      "academic"
    ],
    "best_for": "Visualizing citation relationships between papers",
    "language": "Web",
    "model_score": 0.0
  },
  {
    "name": "Mesa",
    "description": "Leading open-source Python framework for agent-based modeling with spatial grids, agent schedulers, and Solara visualization. Mesa 3 (2025) requires Python 3.12+.",
    "category": "Simulation & Computational Economics",
    "docs_url": "https://mesa.readthedocs.io/",
    "github_url": "https://github.com/projectmesa/mesa",
    "url": "https://mesa.readthedocs.io/",
    "install": "pip install mesa",
    "tags": [
      "agent-based-modeling",
      "simulation",
      "ABM",
      "multi-agent",
      "complexity"
    ],
    "best_for": "Building and analyzing agent-based models of economic and social systems",
    "language": "Python",
    "model_score": 0.0
  },
  {
    "name": "AgentPy",
    "description": "Modern Python framework for agent-based modeling integrating model design with SALib sensitivity analysis and NetworkX network structures.",
    "category": "Simulation & Computational Economics",
    "docs_url": "https://agentpy.readthedocs.io/",
    "github_url": "https://github.com/JoelForamitti/agentpy",
    "url": "https://github.com/JoelForamitti/agentpy",
    "install": "pip install agentpy",
    "tags": [
      "agent-based-modeling",
      "simulation",
      "sensitivity-analysis",
      "networks"
    ],
    "best_for": "Agent-based models with integrated sensitivity analysis and network support",
    "language": "Python",
    "model_score": 0.0
  },
  {
    "name": "ABCE",
    "description": "Agent-Based Computational Economics library from Oxford INET. Automatically handles trade with physically consistent goods, includes built-in Firm/Household archetypes.",
    "category": "Simulation & Computational Economics",
    "docs_url": "https://abce.readthedocs.io/",
    "github_url": "https://github.com/AB-CE/abce",
    "url": "https://github.com/AB-CE/abce",
    "install": "pip install abce",
    "tags": [
      "agent-based-modeling",
      "economics",
      "trade",
      "macroeconomics",
      "Oxford-INET"
    ],
    "best_for": "Economic agent-based models with automatic trade handling and accounting",
    "language": "Python",
    "model_score": 0.0
  },
  {
    "name": "pyNetLogo",
    "description": "Python-NetLogo interface enabling SALib sensitivity analysis integration and parallel NetLogo simulations. Published in JASSS (2018).",
    "category": "Simulation & Computational Economics",
    "docs_url": "https://pynetlogo.readthedocs.io/",
    "github_url": "https://github.com/quaquel/pyNetLogo",
    "url": "https://github.com/quaquel/pyNetLogo",
    "install": "pip install pynetlogo",
    "tags": [
      "NetLogo",
      "agent-based-modeling",
      "sensitivity-analysis",
      "simulation"
    ],
    "best_for": "Running NetLogo ABMs from Python with sensitivity analysis",
    "language": "Python",
    "model_score": 0.0
  },
  {
    "name": "NetLogoR",
    "description": "Pure R implementation of NetLogo framework\u2014no NetLogo installation required. Benefits from ggplot2 integration and R spatial objects.",
    "category": "Simulation & Computational Economics",
    "docs_url": "https://cran.r-project.org/web/packages/NetLogoR/",
    "github_url": "https://github.com/PredictiveEcology/NetLogoR",
    "url": "https://cran.r-project.org/web/packages/NetLogoR/",
    "install": "install.packages('NetLogoR')",
    "tags": [
      "NetLogo",
      "agent-based-modeling",
      "R",
      "spatial-modeling"
    ],
    "best_for": "Agent-based modeling in R with NetLogo-style syntax",
    "language": "R",
    "model_score": 0.0
  },
  {
    "name": "RNetLogo",
    "description": "Embeds NetLogo into R for statistical analysis integration. Enables running NetLogo models and analyzing results in R environment.",
    "category": "Simulation & Computational Economics",
    "docs_url": "https://cran.r-project.org/package=RNetLogo",
    "github_url": null,
    "url": "https://cran.r-project.org/package=RNetLogo",
    "install": "install.packages('RNetLogo')",
    "tags": [
      "NetLogo",
      "agent-based-modeling",
      "R",
      "integration"
    ],
    "best_for": "Integrating NetLogo simulations with R statistical analysis",
    "language": "R",
    "model_score": 0.0
  },
  {
    "name": "nlrx",
    "description": "rOpenSci package for NetLogo simulation via XML with BehaviorSpace support. Enables systematic NetLogo experiments from R.",
    "category": "Simulation & Computational Economics",
    "docs_url": "https://docs.ropensci.org/nlrx/",
    "github_url": "https://github.com/ropensci/nlrx",
    "url": "https://docs.ropensci.org/nlrx/",
    "install": "install.packages('nlrx')",
    "tags": [
      "NetLogo",
      "agent-based-modeling",
      "R",
      "experiment-design",
      "rOpenSci"
    ],
    "best_for": "Designing and running systematic NetLogo experiments from R",
    "language": "R",
    "model_score": 0.0
  },
  {
    "name": "Gymnasium",
    "description": "Farama Foundation's successor to OpenAI Gym. Standard single-agent reinforcement learning API for environment development and benchmarking.",
    "category": "Simulation & Computational Economics",
    "docs_url": "https://gymnasium.farama.org/",
    "github_url": "https://github.com/Farama-Foundation/Gymnasium",
    "url": "https://gymnasium.farama.org/",
    "install": "pip install gymnasium",
    "tags": [
      "reinforcement-learning",
      "environments",
      "RL",
      "OpenAI-Gym",
      "benchmarking"
    ],
    "best_for": "Building and testing reinforcement learning environments",
    "language": "Python",
    "model_score": 0.0
  },
  {
    "name": "Stable-Baselines3",
    "description": "Reliable PyTorch implementations of A2C, DDPG, DQN, PPO, SAC, TD3 RL algorithms. Published in JMLR 2021.",
    "category": "Simulation & Computational Economics",
    "docs_url": "https://stable-baselines3.readthedocs.io/",
    "github_url": "https://github.com/DLR-RM/stable-baselines3",
    "url": "https://stable-baselines3.readthedocs.io/",
    "install": "pip install stable-baselines3",
    "tags": [
      "reinforcement-learning",
      "PyTorch",
      "PPO",
      "DQN",
      "SAC",
      "algorithms"
    ],
    "best_for": "Training RL agents with reliable, well-tested implementations",
    "language": "Python",
    "model_score": 0.0
  },
  {
    "name": "RLlib",
    "description": "Industry-grade scalable reinforcement learning library from Ray. Native multi-agent support for distributed training at scale.",
    "category": "Simulation & Computational Economics",
    "docs_url": "https://docs.ray.io/en/latest/rllib/",
    "github_url": "https://github.com/ray-project/ray",
    "url": "https://docs.ray.io/en/latest/rllib/",
    "install": "pip install 'ray[rllib]'",
    "tags": [
      "reinforcement-learning",
      "distributed",
      "multi-agent",
      "scalable",
      "Ray"
    ],
    "best_for": "Scalable multi-agent RL training on clusters",
    "language": "Python",
    "model_score": 0.0
  },
  {
    "name": "CleanRL",
    "description": "Single-file RL algorithm implementations (~340 lines each) for educational purposes and research. Published in JMLR 2022.",
    "category": "Simulation & Computational Economics",
    "docs_url": "https://docs.cleanrl.dev/",
    "github_url": "https://github.com/vwxyzjn/cleanrl",
    "url": "https://github.com/vwxyzjn/cleanrl",
    "install": "pip install cleanrl",
    "tags": [
      "reinforcement-learning",
      "educational",
      "single-file",
      "reproducible"
    ],
    "best_for": "Learning RL algorithms through readable single-file implementations",
    "language": "Python",
    "model_score": 0.0
  },
  {
    "name": "TorchRL",
    "description": "Official PyTorch reinforcement learning library with TensorDict abstraction for modular RL development.",
    "category": "Simulation & Computational Economics",
    "docs_url": "https://pytorch.org/rl/",
    "github_url": "https://github.com/pytorch/rl",
    "url": "https://pytorch.org/rl/",
    "install": "pip install torchrl",
    "tags": [
      "reinforcement-learning",
      "PyTorch",
      "modular",
      "TensorDict"
    ],
    "best_for": "Building modular RL systems with PyTorch ecosystem",
    "language": "Python",
    "model_score": 0.0
  },
  {
    "name": "PettingZoo",
    "description": "Multi-agent version of Gymnasium with Agent-Environment-Cycle (AEC) model. Includes card games, MPE, and cooperative environments. NeurIPS 2021.",
    "category": "Simulation & Computational Economics",
    "docs_url": "https://pettingzoo.farama.org/",
    "github_url": "https://github.com/Farama-Foundation/PettingZoo",
    "url": "https://pettingzoo.farama.org/",
    "install": "pip install pettingzoo",
    "tags": [
      "multi-agent-RL",
      "environments",
      "games",
      "cooperative",
      "competitive"
    ],
    "best_for": "Multi-agent reinforcement learning environments and research",
    "language": "Python",
    "model_score": 0.0
  },
  {
    "name": "SuperSuit",
    "description": "Wrapper library for PettingZoo preprocessing including frame stacking, normalization, and action masking.",
    "category": "Simulation & Computational Economics",
    "docs_url": null,
    "github_url": "https://github.com/Farama-Foundation/SuperSuit",
    "url": "https://github.com/Farama-Foundation/SuperSuit",
    "install": "pip install supersuit",
    "tags": [
      "multi-agent-RL",
      "preprocessing",
      "wrappers",
      "PettingZoo"
    ],
    "best_for": "Preprocessing and wrapping PettingZoo environments",
    "language": "Python",
    "model_score": 0.0
  },
  {
    "name": "MO-Gymnasium",
    "description": "Multi-objective reinforcement learning environments for Pareto-optimal policy learning with conflicting objectives.",
    "category": "Simulation & Computational Economics",
    "docs_url": "https://mo-gymnasium.farama.org/",
    "github_url": "https://github.com/Farama-Foundation/MO-Gymnasium",
    "url": "https://mo-gymnasium.farama.org/",
    "install": "pip install mo-gymnasium",
    "tags": [
      "multi-objective",
      "reinforcement-learning",
      "Pareto",
      "trade-offs"
    ],
    "best_for": "Multi-objective RL with trade-offs between competing objectives",
    "language": "Python",
    "model_score": 0.0
  },
  {
    "name": "AI Economist",
    "description": "Salesforce's two-level RL environment for tax policy design. Published in Science Advances 2022. Includes COVID-19 economic simulation.",
    "category": "Simulation & Computational Economics",
    "docs_url": null,
    "github_url": "https://github.com/salesforce/ai-economist",
    "url": "https://github.com/salesforce/ai-economist",
    "install": "pip install ai-economist",
    "tags": [
      "economic-simulation",
      "tax-policy",
      "multi-agent",
      "mechanism-design",
      "Salesforce"
    ],
    "best_for": "Designing and testing economic policies with RL agents",
    "language": "Python",
    "model_score": 0.0
  },
  {
    "name": "FinRL",
    "description": "First open-source deep reinforcement learning framework for quantitative finance. Train-test-trade pipeline for NASDAQ, DJIA, S&P 500.",
    "category": "Simulation & Computational Economics",
    "docs_url": "https://finrl.readthedocs.io/",
    "github_url": "https://github.com/AI4Finance-Foundation/FinRL",
    "url": "https://github.com/AI4Finance-Foundation/FinRL",
    "install": "pip install finrl",
    "tags": [
      "finance",
      "trading",
      "reinforcement-learning",
      "quantitative",
      "portfolio"
    ],
    "best_for": "Training RL agents for stock trading and portfolio management",
    "language": "Python",
    "model_score": 0.0
  },
  {
    "name": "OR-Gym",
    "description": "Operations research environments for RL including knapsack, bin packing, supply chain, newsvendor, and portfolio optimization.",
    "category": "Simulation & Computational Economics",
    "docs_url": null,
    "github_url": "https://github.com/hubbs5/or-gym",
    "url": "https://github.com/hubbs5/or-gym",
    "install": "pip install or-gym",
    "tags": [
      "operations-research",
      "inventory",
      "supply-chain",
      "optimization",
      "newsvendor"
    ],
    "best_for": "RL for operations research and supply chain optimization",
    "language": "Python",
    "model_score": 0.0
  },
  {
    "name": "mbt_gym",
    "description": "Model-based trading environments for market-making and optimal execution RL. Implements Avellaneda-Stoikov and Cartea-Jaimungal models.",
    "category": "Simulation & Computational Economics",
    "docs_url": null,
    "github_url": "https://github.com/JJJerome/mbt_gym",
    "url": "https://github.com/JJJerome/mbt_gym",
    "install": "pip install mbt-gym",
    "tags": [
      "market-making",
      "trading",
      "high-frequency",
      "optimal-execution",
      "reinforcement-learning"
    ],
    "best_for": "RL for market-making and optimal execution strategies",
    "language": "Python",
    "model_score": 0.0
  },
  {
    "name": "ABIDES",
    "description": "JPMorgan's agent-based interactive discrete event simulation for market microstructure research. NASDAQ-like exchange with multiple agent types.",
    "category": "Simulation & Computational Economics",
    "docs_url": null,
    "github_url": "https://github.com/jpmorganchase/abides-jpmc-public",
    "url": "https://github.com/jpmorganchase/abides-jpmc-public",
    "install": "pip install abides-jpmc",
    "tags": [
      "market-simulation",
      "order-book",
      "agent-based",
      "microstructure",
      "JPMorgan"
    ],
    "best_for": "Simulating limit order book markets with heterogeneous agents",
    "language": "Python",
    "model_score": 0.0
  },
  {
    "name": "AuctionGym",
    "description": "Amazon's ad auction simulator for first/second-price auctions with RL bidding agents. Best Paper at AdKDD 2022.",
    "category": "Simulation & Computational Economics",
    "docs_url": null,
    "github_url": "https://github.com/amzn/auction-gym",
    "url": "https://github.com/amzn/auction-gym",
    "install": null,
    "tags": [
      "auction-simulation",
      "mechanism-design",
      "advertising",
      "bidding",
      "Amazon"
    ],
    "best_for": "Simulating and training RL agents for ad auction bidding",
    "language": "Python",
    "model_score": 0.0
  },
  {
    "name": "matchingR",
    "description": "R/C++ implementation of Gale-Shapley and Irving's algorithms for stable matching. Tested with 30,000+ participants.",
    "category": "Matching & Market Design",
    "docs_url": "https://cran.r-project.org/web/packages/matchingR/",
    "github_url": "https://github.com/jtilly/matchingR",
    "url": "https://cran.r-project.org/web/packages/matchingR/",
    "install": "install.packages('matchingR')",
    "tags": [
      "matching",
      "Gale-Shapley",
      "stable-matching",
      "market-design"
    ],
    "best_for": "Computing stable matchings for two-sided markets",
    "language": "R",
    "model_score": 0.0
  },
  {
    "name": "gale-shapley",
    "description": "Python O(n\u00b2) implementation of Gale-Shapley algorithm for stable matching with simulation capabilities.",
    "category": "Matching & Market Design",
    "docs_url": null,
    "github_url": "https://github.com/oedokumaci/gale-shapley",
    "url": "https://github.com/oedokumaci/gale-shapley",
    "install": "pip install gale-shapley",
    "tags": [
      "matching",
      "Gale-Shapley",
      "stable-matching",
      "algorithm"
    ],
    "best_for": "Stable matching simulations in Python",
    "language": "Python",
    "model_score": 0.0
  },
  {
    "name": "CTGAN",
    "description": "GAN-based tabular data synthesizer using Variational GMM for mode-specific normalization. Published at NeurIPS 2019. Core component of SDV ecosystem.",
    "category": "Synthetic Data Generation",
    "docs_url": "https://docs.sdv.dev/ctgan/",
    "github_url": "https://github.com/sdv-dev/CTGAN",
    "url": "https://github.com/sdv-dev/CTGAN",
    "install": "pip install ctgan",
    "tags": [
      "synthetic-data",
      "GAN",
      "tabular",
      "privacy",
      "deep-learning"
    ],
    "best_for": "Generating realistic synthetic tabular data using GANs",
    "language": "Python",
    "model_score": 0.0
  },
  {
    "name": "DeepEcho",
    "description": "Time series synthetic data generation using deep learning. Part of the SDV ecosystem for sequential data.",
    "category": "Synthetic Data Generation",
    "docs_url": "https://docs.sdv.dev/deepecho/",
    "github_url": "https://github.com/sdv-dev/DeepEcho",
    "url": "https://github.com/sdv-dev/DeepEcho",
    "install": "pip install deepecho",
    "tags": [
      "synthetic-data",
      "time-series",
      "sequential",
      "deep-learning"
    ],
    "best_for": "Generating synthetic time series and sequential data",
    "language": "Python",
    "model_score": 0.0
  },
  {
    "name": "Faker",
    "description": "Comprehensive fake data generator for 50+ locales including names, addresses, financial data, and more. Most popular Python library for test data generation.",
    "category": "Synthetic Data Generation",
    "docs_url": "https://faker.readthedocs.io/",
    "github_url": "https://github.com/joke2k/faker",
    "url": "https://faker.readthedocs.io/",
    "install": "pip install faker",
    "tags": [
      "synthetic-data",
      "test-data",
      "fake-data",
      "localization",
      "testing"
    ],
    "best_for": "Generating realistic fake data for testing and development",
    "language": "Python",
    "model_score": 0.0
  },
  {
    "name": "Mimesis",
    "description": "High-performance fake data generator\u2014faster than Faker. Provides data for multiple domains and 35+ locales.",
    "category": "Synthetic Data Generation",
    "docs_url": "https://mimesis.name/en/master/",
    "github_url": "https://github.com/lk-geimfari/mimesis",
    "url": "https://mimesis.name/",
    "install": "pip install mimesis",
    "tags": [
      "synthetic-data",
      "fake-data",
      "high-performance",
      "localization"
    ],
    "best_for": "Fast generation of realistic fake data at scale",
    "language": "Python",
    "model_score": 0.0
  },
  {
    "name": "Gretel Synthetics",
    "description": "Open-source synthetic data library with DGAN for time series, ACTGAN, and differential privacy support from Gretel.ai.",
    "category": "Synthetic Data Generation",
    "docs_url": "https://docs.gretel.ai/",
    "github_url": "https://github.com/gretelai/gretel-synthetics",
    "url": "https://github.com/gretelai/gretel-synthetics",
    "install": "pip install gretel-synthetics",
    "tags": [
      "synthetic-data",
      "differential-privacy",
      "time-series",
      "ACTGAN"
    ],
    "best_for": "Privacy-preserving synthetic data generation",
    "language": "Python",
    "model_score": 0.0
  },
  {
    "name": "DataSynthesizer",
    "description": "Privacy-preserving synthetic data using Bayesian networks with differential privacy. From University of Washington DataResponsibly project.",
    "category": "Synthetic Data Generation",
    "docs_url": null,
    "github_url": "https://github.com/DataResponsibly/DataSynthesizer",
    "url": "https://github.com/DataResponsibly/DataSynthesizer",
    "install": "pip install DataSynthesizer",
    "tags": [
      "synthetic-data",
      "differential-privacy",
      "Bayesian-networks",
      "privacy"
    ],
    "best_for": "Generating differentially private synthetic datasets",
    "language": "Python",
    "model_score": 0.0
  },
  {
    "name": "sdcMicro",
    "description": "Statistical Disclosure Control for microdata used by World Bank and census agencies. Comprehensive anonymization toolkit.",
    "category": "Synthetic Data Generation",
    "docs_url": "https://cran.r-project.org/web/packages/sdcMicro/vignettes/sdc_guidelines.pdf",
    "github_url": "https://github.com/sdcTools/sdcMicro",
    "url": "https://cran.r-project.org/web/packages/sdcMicro/",
    "install": "install.packages('sdcMicro')",
    "tags": [
      "statistical-disclosure-control",
      "privacy",
      "anonymization",
      "census"
    ],
    "best_for": "Statistical disclosure control and microdata anonymization",
    "language": "R",
    "model_score": 0.0
  },
  {
    "name": "simPop",
    "description": "Synthetic population simulation for EU-SILC style survey data. Creates realistic household and individual-level synthetic populations.",
    "category": "Synthetic Data Generation",
    "docs_url": "https://cran.r-project.org/web/packages/simPop/vignettes/simPop.html",
    "github_url": null,
    "url": "https://cran.r-project.org/web/packages/simPop/",
    "install": "install.packages('simPop')",
    "tags": [
      "synthetic-population",
      "survey-data",
      "microsimulation",
      "EU-SILC"
    ],
    "best_for": "Creating synthetic populations for survey microsimulation",
    "language": "R",
    "model_score": 0.0
  },
  {
    "name": "simChef",
    "description": "DGP (Data Generating Process) framework for systematic simulation studies. Enables reproducible computational experiments.",
    "category": "Power Simulation & Design of Experiments",
    "docs_url": "https://yu-group.github.io/simChef/",
    "github_url": "https://github.com/Yu-Group/simChef",
    "url": "https://yu-group.github.io/simChef/",
    "install": "install.packages('simChef')",
    "tags": [
      "simulation",
      "DGP",
      "experiments",
      "reproducibility",
      "statistics"
    ],
    "best_for": "Designing and running systematic simulation studies",
    "language": "R",
    "model_score": 0.0
  },
  {
    "name": "Superpower",
    "description": "Simulation-based power analysis for factorial ANOVA designs (up to 3 factors). Includes Shiny app for interactive power analysis.",
    "category": "Power Simulation & Design of Experiments",
    "docs_url": "https://aaroncaldwell.us/SuperpowerBook/",
    "github_url": "https://github.com/arcaldwell49/Superpower",
    "url": "https://cran.r-project.org/web/packages/Superpower/",
    "install": "install.packages('Superpower')",
    "tags": [
      "power-analysis",
      "ANOVA",
      "simulation",
      "factorial-design"
    ],
    "best_for": "Power analysis for complex factorial ANOVA designs",
    "language": "R",
    "model_score": 0.0
  },
  {
    "name": "mlpwr",
    "description": "Machine learning-based power analysis using surrogate models. Efficient sample size planning for complex study designs.",
    "category": "Power Simulation & Design of Experiments",
    "docs_url": "https://cran.r-project.org/web/packages/mlpwr/vignettes/mlpwr.html",
    "github_url": null,
    "url": "https://cran.r-project.org/web/packages/mlpwr/",
    "install": "install.packages('mlpwr')",
    "tags": [
      "power-analysis",
      "machine-learning",
      "sample-size",
      "simulation"
    ],
    "best_for": "ML-based power analysis for complex designs",
    "language": "R",
    "model_score": 0.0
  },
  {
    "name": "recombinator",
    "description": "Block bootstrap methods including Moving Block, Circular Block, Stationary, and Tapered Block Bootstrap for time series.",
    "category": "Bootstrap & Inference",
    "docs_url": null,
    "github_url": "https://github.com/InvestmentSystems/recombinator",
    "url": "https://pypi.org/project/recombinator/",
    "install": "pip install recombinator",
    "tags": [
      "bootstrap",
      "time-series",
      "block-bootstrap",
      "resampling"
    ],
    "best_for": "Block bootstrap for dependent time series data",
    "language": "Python",
    "model_score": 0.0
  },
  {
    "name": "bootUR",
    "description": "Bootstrap unit root tests with sieve and wild bootstrap methods for time series stationarity testing.",
    "category": "Bootstrap & Inference",
    "docs_url": "https://cran.r-project.org/web/packages/bootUR/vignettes/bootUR.html",
    "github_url": null,
    "url": "https://cran.r-project.org/web/packages/bootUR/",
    "install": "install.packages('bootUR')",
    "tags": [
      "bootstrap",
      "unit-root",
      "time-series",
      "stationarity"
    ],
    "best_for": "Bootstrap unit root testing for time series",
    "language": "R",
    "model_score": 0.0
  }
]