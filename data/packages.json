[
  {
    "name": "BayesianBandits",
    "description": "Lightweight microframework for Bayesian bandits (Thompson Sampling) with support for contextual/restless/delayed rewards.",
    "category": "Adaptive Experimentation & Bandits",
    "docs_url": "https://bayesianbandits.readthedocs.io/en/latest/",
    "github_url": "https://github.com/bayesianbandits/bayesianbandits",
    "url": "https://github.com/bayesianbandits/bayesianbandits",
    "install": "pip install bayesianbandits",
    "tags": [
      "A/B testing",
      "experimentation",
      "Bayesian"
    ],
    "best_for": "Online A/B testing, multi-armed bandits, adaptive allocation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "bayesian"
    ],
    "summary": "BayesianBandits is a lightweight microframework designed for implementing Bayesian bandits using Thompson Sampling. It is useful for researchers and practitioners in the field of adaptive experimentation, particularly those interested in A/B testing and contextual bandits.",
    "use_cases": [
      "Optimizing online marketing campaigns",
      "Personalizing content recommendations"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for Bayesian bandits",
      "how to implement Thompson Sampling in python",
      "A/B testing framework in python",
      "contextual bandits python library"
    ],
    "primary_use_cases": [
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "PyMC3",
      "TensorFlow Probability"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "ContextualBandits",
    "description": "Implements a wide range of contextual bandit algorithms (linear, tree-based, neural) and off-policy evaluation methods.",
    "category": "Adaptive Experimentation & Bandits",
    "docs_url": "https://contextual-bandits.readthedocs.io/",
    "github_url": "https://github.com/david-cortes/contextualbandits",
    "url": "https://github.com/david-cortes/contextualbandits",
    "install": "pip install contextualbandits",
    "tags": [
      "A/B testing",
      "experimentation",
      "machine learning"
    ],
    "best_for": "Online A/B testing, multi-armed bandits, adaptive allocation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "machine learning"
    ],
    "summary": "ContextualBandits implements a variety of contextual bandit algorithms and off-policy evaluation methods. It is useful for researchers and practitioners in the fields of machine learning and experimentation.",
    "use_cases": [
      "personalized recommendations",
      "dynamic pricing strategies"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for contextual bandits",
      "how to implement A/B testing in python",
      "contextual bandit algorithms in python",
      "off-policy evaluation methods python",
      "experimentation tools in python",
      "machine learning library for bandits"
    ],
    "primary_use_cases": [
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "MABWiser",
    "description": "Production-ready, scikit-learn style library for contextual & stochastic bandits with parallelism and simulation tools.",
    "category": "Adaptive Experimentation & Bandits",
    "docs_url": "https://fidelity.github.io/mabwiser/",
    "github_url": "https://github.com/fidelity/mabwiser",
    "url": "https://github.com/fidelity/mabwiser",
    "install": "pip install mabwiser",
    "tags": [
      "A/B testing",
      "experimentation"
    ],
    "best_for": "Online A/B testing, multi-armed bandits, adaptive allocation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "adaptive experimentation",
      "bandit algorithms"
    ],
    "summary": "MABWiser is a production-ready library designed for contextual and stochastic bandits, offering tools for parallelism and simulation. It is suitable for data scientists and researchers involved in experimentation and A/B testing.",
    "use_cases": [
      "Running A/B tests with contextual bandits",
      "Simulating bandit algorithms for research"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for contextual bandits",
      "how to perform A/B testing in python",
      "stochastic bandits library python",
      "parallelism in experimentation python",
      "MABWiser documentation",
      "scikit-learn style bandits library"
    ],
    "primary_use_cases": [
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "scikit-learn",
      "bandit",
      "PyTorch-Bandits"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "Open Bandit Pipeline (OBP)",
    "description": "Framework for **offline evaluation (OPE)** of bandit policies using logged data. Implements IPS, DR, DM estimators.",
    "category": "Adaptive Experimentation & Bandits",
    "docs_url": "https://zr-obp.readthedocs.io/en/latest/",
    "github_url": "https://github.com/st-tech/zr-obp",
    "url": "https://github.com/st-tech/zr-obp",
    "install": "pip install obp",
    "tags": [
      "A/B testing",
      "experimentation"
    ],
    "best_for": "Online A/B testing, multi-armed bandits, adaptive allocation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "bandit-algorithms"
    ],
    "summary": "Open Bandit Pipeline (OBP) is a framework designed for offline evaluation of bandit policies using logged data. It is particularly useful for researchers and practitioners in the field of adaptive experimentation who need to assess the performance of various bandit algorithms.",
    "use_cases": [
      "Evaluating the effectiveness of different bandit algorithms",
      "Analyzing logged data from previous experiments"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for offline evaluation of bandit policies",
      "how to implement IPS estimator in python",
      "bandit algorithms evaluation in python",
      "offline policy evaluation with logged data",
      "using Open Bandit Pipeline for A/B testing",
      "how to analyze bandit policies in python"
    ],
    "primary_use_cases": [
      "offline evaluation of bandit policies",
      "implementing IPS, DR, DM estimators"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "PyXAB",
    "description": "Library for advanced bandit problems: X-armed bandits (continuous/structured action spaces) and online optimization.",
    "category": "Adaptive Experimentation & Bandits",
    "docs_url": "https://pyxab.readthedocs.io/en/latest/",
    "github_url": "https://github.com/WilliamLwj/PyXAB",
    "url": "https://github.com/WilliamLwj/PyXAB",
    "install": "pip install pyxab",
    "tags": [
      "A/B testing",
      "experimentation"
    ],
    "best_for": "Online A/B testing, multi-armed bandits, adaptive allocation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "PyXAB is a library designed for tackling advanced bandit problems, including X-armed bandits with continuous and structured action spaces. It is useful for researchers and practitioners in the field of adaptive experimentation and online optimization.",
    "use_cases": [],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for bandit problems",
      "how to implement X-armed bandits in python",
      "online optimization library python",
      "advanced experimentation library python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "SMPyBandits",
    "description": "Comprehensive research framework for single/multi-player MAB algorithms (stochastic, adversarial, contextual).",
    "category": "Adaptive Experimentation & Bandits",
    "docs_url": "https://smpybandits.github.io/",
    "github_url": "https://github.com/SMPyBandits/SMPyBandits",
    "url": "https://github.com/SMPyBandits/SMPyBandits",
    "install": "pip install SMPyBandits",
    "tags": [
      "A/B testing",
      "experimentation"
    ],
    "best_for": "Online A/B testing, multi-armed bandits, adaptive allocation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "SMPyBandits is a comprehensive research framework designed for implementing single and multi-player Multi-Armed Bandit (MAB) algorithms, including stochastic, adversarial, and contextual settings. It is primarily used by researchers and practitioners in the field of adaptive experimentation and bandit algorithms.",
    "use_cases": [],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for multi-armed bandits",
      "how to implement A/B testing in python",
      "MAB algorithms in python",
      "contextual bandits framework python",
      "stochastic bandits library python",
      "adversarial bandits implementation python"
    ],
    "primary_use_cases": [
      "A/B test analysis",
      "multi-player MAB algorithms"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "abracadabra",
    "description": "Sequential testing with always-valid inference. Supports continuous monitoring of A/B tests.",
    "category": "Adaptive Experimentation & Bandits",
    "docs_url": "https://pypi.org/project/abracadabra/",
    "github_url": "https://web.archive.org/web/20210601/https://github.com/quizlet/abracadabra",
    "url": "https://pypi.org/project/abracadabra/",
    "install": "pip install abracadabra",
    "tags": [
      "sequential testing",
      "A/B testing",
      "always-valid"
    ],
    "best_for": "Continuous experiment monitoring",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "adaptive-experimentation"
    ],
    "summary": "Abracadabra is a Python package designed for sequential testing with always-valid inference, allowing for continuous monitoring of A/B tests. It is useful for data scientists and researchers involved in experimental design and analysis.",
    "use_cases": [
      "Monitoring A/B tests in real-time",
      "Conducting sequential analysis for marketing experiments"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for sequential testing",
      "how to do A/B testing in python",
      "continuous monitoring of A/B tests",
      "always-valid inference in experiments"
    ],
    "primary_use_cases": [
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "crewai",
    "description": "Framework for orchestrating role-playing autonomous AI agents. Multi-agent collaboration made intuitive.",
    "category": "Agentic AI",
    "docs_url": "https://docs.crewai.com/",
    "github_url": "https://github.com/crewAIInc/crewAI",
    "url": "https://www.crewai.com/",
    "install": "pip install crewai",
    "tags": [
      "agents",
      "multi-agent",
      "orchestration",
      "roles"
    ],
    "best_for": "Role-based multi-agent teams",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "crewai is a framework designed for orchestrating role-playing autonomous AI agents, making multi-agent collaboration intuitive. It is suitable for developers and researchers interested in agent-based systems.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for multi-agent collaboration",
      "how to orchestrate AI agents in python",
      "framework for role-playing AI agents",
      "autonomous AI agents in python",
      "python multi-agent orchestration",
      "collaborative AI agents framework"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "langchain",
    "description": "Framework for developing LLM-powered applications. Chains, tools, memory, and retrieval.",
    "category": "Agentic AI",
    "docs_url": "https://python.langchain.com/",
    "github_url": "https://github.com/langchain-ai/langchain",
    "url": "https://python.langchain.com/",
    "install": "pip install langchain",
    "tags": [
      "LLM",
      "chains",
      "tools",
      "RAG"
    ],
    "best_for": "LLM framework \u2014 chains, tools, memory",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Langchain is a framework designed for developing applications powered by large language models (LLMs). It is used by developers and data scientists looking to create sophisticated AI-driven solutions.",
    "use_cases": [
      "Building conversational agents",
      "Creating data retrieval systems"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for LLM applications",
      "how to build chains in python",
      "tools for memory in langchain",
      "RAG in langchain",
      "developing AI applications with langchain",
      "langchain framework documentation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "langgraph",
    "description": "Framework for building stateful, multi-actor LLM applications. Graph-based agent workflows with persistence.",
    "category": "Agentic AI",
    "docs_url": "https://langchain-ai.github.io/langgraph/",
    "github_url": "https://github.com/langchain-ai/langgraph",
    "url": "https://langchain-ai.github.io/langgraph/",
    "install": "pip install langgraph",
    "tags": [
      "agents",
      "LLM",
      "workflows",
      "multi-agent"
    ],
    "best_for": "Production agent workflows with state management",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Langgraph is a framework designed for building stateful, multi-actor LLM applications. It enables users to create graph-based agent workflows with persistence, making it suitable for developers working on complex AI systems.",
    "use_cases": [
      "Developing chatbots with multiple interacting agents",
      "Creating AI-driven applications that require state management"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for building LLM applications",
      "how to create multi-agent workflows in python",
      "stateful LLM application framework",
      "graph-based agent workflows in python",
      "langgraph documentation",
      "examples of langgraph usage"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "openai-agents",
    "description": "OpenAI's lightweight, production-ready SDK for building agentic AI applications. Fast prototyping.",
    "category": "Agentic AI",
    "docs_url": "https://openai.github.io/openai-agents-python/",
    "github_url": "https://github.com/openai/openai-agents-python",
    "url": "https://openai.github.io/openai-agents-python/",
    "install": "pip install openai-agents",
    "tags": [
      "agents",
      "OpenAI",
      "tools",
      "lightweight"
    ],
    "best_for": "OpenAI's lightweight SDK \u2014 fast prototyping",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "OpenAI's openai-agents is a lightweight SDK designed for building agentic AI applications, enabling fast prototyping. It is suitable for developers looking to create AI-driven solutions efficiently.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for agentic AI",
      "how to build AI applications in python",
      "OpenAI SDK for agents",
      "lightweight AI tools in python",
      "fast prototyping AI applications",
      "using openai-agents for AI development"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active"
  },
  {
    "name": "bsts",
    "description": "Bayesian Structural Time Series providing the foundation for CausalImpact. Supports spike-and-slab variable selection, multiple state components (trend, seasonality, regression), and non-Gaussian outcomes. Developed at Google.",
    "category": "Bayesian Causal Inference",
    "docs_url": "https://cran.r-project.org/web/packages/bsts/bsts.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=bsts",
    "install": "install.packages(\"bsts\")",
    "tags": [
      "Bayesian",
      "structural-time-series",
      "spike-and-slab",
      "state-space",
      "Google"
    ],
    "best_for": "Bayesian structural time series with spike-and-slab selection\u2014foundation for CausalImpact",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "time-series",
      "bayesian"
    ],
    "summary": "The bsts package provides a framework for Bayesian Structural Time Series modeling, which is foundational for causal impact analysis. It is particularly useful for users interested in understanding the effects of interventions over time.",
    "use_cases": [
      "Analyzing the impact of marketing campaigns on sales",
      "Forecasting future trends based on historical data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "bayesian structural time series R",
      "how to perform causal impact analysis in R",
      "R package for spike-and-slab variable selection",
      "time series modeling with bsts",
      "bayesian analysis for time series",
      "structural time series in R"
    ],
    "primary_use_cases": [
      "causal impact analysis",
      "time series forecasting"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "Bambi",
    "description": "High-level interface for building Bayesian GLMMs, built on top of PyMC. Uses formula syntax similar to R's `lme4`.",
    "category": "Bayesian Econometrics",
    "docs_url": "https://bambinos.github.io/bambi/",
    "github_url": "https://github.com/bambinos/bambi",
    "url": "https://github.com/bambinos/bambi",
    "install": "pip install bambi",
    "tags": [
      "Bayesian",
      "inference"
    ],
    "best_for": "Uncertainty quantification, prior-informed inference, probabilistic modeling",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "bayesian"
    ],
    "summary": "Bambi provides a high-level interface for building Bayesian Generalized Linear Mixed Models (GLMMs) using a formula syntax similar to R's `lme4`. It is designed for users who want to perform Bayesian inference in a user-friendly manner.",
    "use_cases": [
      "Modeling hierarchical data",
      "Analyzing longitudinal data"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for Bayesian GLMMs",
      "how to build Bayesian models in python",
      "Bambi package for Bayesian inference",
      "Bayesian econometrics in python",
      "using PyMC for GLMMs",
      "high-level Bayesian modeling in python"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "PyMC"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "LightweightMMM",
    "description": "Bayesian Marketing Mix Modeling (see Marketing Mix Models section).",
    "category": "Bayesian Econometrics",
    "docs_url": null,
    "github_url": "https://github.com/google/lightweight_mmm",
    "url": "https://github.com/google/lightweight_mmm",
    "install": "pip install lightweight_mmm",
    "tags": [
      "Bayesian",
      "inference"
    ],
    "best_for": "Uncertainty quantification, prior-informed inference, probabilistic modeling",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "bayesian",
      "marketing-mix-modeling"
    ],
    "summary": "LightweightMMM is a package designed for Bayesian Marketing Mix Modeling, allowing users to analyze the effectiveness of marketing strategies. It is primarily used by data scientists and marketing analysts looking to optimize their marketing spend.",
    "use_cases": [
      "Evaluating marketing campaign effectiveness",
      "Optimizing advertising spend"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for Bayesian Marketing Mix Modeling",
      "how to perform marketing mix analysis in python",
      "Bayesian inference for marketing",
      "LightweightMMM usage examples",
      "best practices for marketing mix models in python",
      "how to analyze marketing effectiveness with python"
    ],
    "primary_use_cases": [
      "marketing mix modeling"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "PyMC3",
      "Stan"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "NumPyro",
    "description": "Probabilistic programming library built on JAX for scalable Bayesian inference, often faster than PyMC.",
    "category": "Bayesian Econometrics",
    "docs_url": "https://num.pyro.ai/",
    "github_url": "https://github.com/pyro-ppl/numpyro",
    "url": "https://github.com/pyro-ppl/numpyro",
    "install": "pip install numpyro",
    "tags": [
      "Bayesian",
      "inference"
    ],
    "best_for": "Uncertainty quantification, prior-informed inference, probabilistic modeling",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "bayesian"
    ],
    "summary": "NumPyro is a probabilistic programming library built on JAX that facilitates scalable Bayesian inference. It is designed for users looking for efficient and faster alternatives to traditional Bayesian frameworks like PyMC.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for probabilistic programming",
      "how to do Bayesian inference in python",
      "NumPyro tutorial",
      "scalable Bayesian inference python",
      "JAX probabilistic programming",
      "compare NumPyro and PyMC"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "JAX"
    ],
    "related_packages": [
      "PyMC"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "PyMC",
    "description": "Flexible probabilistic programming library for Bayesian modeling and inference using MCMC algorithms (NUTS).",
    "category": "Bayesian Econometrics",
    "docs_url": "https://www.pymc.io/",
    "github_url": "https://github.com/pymc-devs/pymc",
    "url": "https://github.com/pymc-devs/pymc",
    "install": "pip install pymc",
    "tags": [
      "Bayesian",
      "inference"
    ],
    "best_for": "Uncertainty quantification, prior-informed inference, probabilistic modeling",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "bayesian"
    ],
    "summary": "PyMC is a flexible probabilistic programming library designed for Bayesian modeling and inference using Markov Chain Monte Carlo (MCMC) algorithms, specifically the No-U-Turn Sampler (NUTS). It is used by data scientists and statisticians for building complex models and conducting inference.",
    "use_cases": [
      "modeling complex data distributions",
      "conducting Bayesian inference"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for Bayesian modeling",
      "how to perform inference in Python",
      "Bayesian inference with PyMC",
      "MCMC algorithms in Python",
      "using NUTS in PyMC",
      "probabilistic programming in Python"
    ],
    "primary_use_cases": [
      "Bayesian modeling",
      "inference using MCMC"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "Stan",
      "TensorFlow Probability"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "bayesplot",
    "description": "Extensive library of ggplot2-based plotting functions for posterior analysis, MCMC diagnostics, and prior/posterior predictive checks supporting the applied Bayesian workflow for any MCMC-fitted model.",
    "category": "Bayesian Inference",
    "docs_url": "https://mc-stan.org/bayesplot/",
    "github_url": "https://github.com/stan-dev/bayesplot",
    "url": "https://cran.r-project.org/package=bayesplot",
    "install": "install.packages(\"bayesplot\")",
    "tags": [
      "visualization",
      "MCMC-diagnostics",
      "posterior-predictive-checks",
      "ggplot2",
      "Bayesian"
    ],
    "best_for": "Diagnostic plots and posterior visualization for MCMC-based Bayesian models, implementing Gabry et al. (2019)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "bayesian"
    ],
    "summary": "bayesplot is an extensive library of ggplot2-based plotting functions designed for posterior analysis, MCMC diagnostics, and prior/posterior predictive checks. It is used by statisticians and data scientists who work with MCMC-fitted models in Bayesian analysis.",
    "use_cases": [
      "Visualizing MCMC diagnostics",
      "Creating posterior predictive checks",
      "Analyzing Bayesian model outputs"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for MCMC diagnostics",
      "how to visualize posterior distributions in R",
      "bayesian plotting functions in R",
      "ggplot2 for Bayesian analysis",
      "posterior predictive checks in R",
      "R package for MCMC analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "brms",
    "description": "High-level interface for fitting Bayesian generalized multilevel models using Stan, with lme4-style formula syntax supporting linear, count, survival, ordinal, zero-inflated, hurdle, and mixture models with flexible prior specification.",
    "category": "Bayesian Inference",
    "docs_url": "https://paul-buerkner.github.io/brms/",
    "github_url": "https://github.com/paul-buerkner/brms",
    "url": "https://cran.r-project.org/package=brms",
    "install": "install.packages(\"brms\")",
    "tags": [
      "Bayesian",
      "multilevel-models",
      "Stan",
      "regression",
      "distributional-regression"
    ],
    "best_for": "Complex hierarchical Bayesian regression with familiar R formula syntax, implementing B\u00fcrkner (2017)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "bayesian"
    ],
    "summary": "The brms package provides a high-level interface for fitting Bayesian generalized multilevel models using Stan. It is particularly useful for statisticians and data scientists who need to specify complex models with flexible prior distributions.",
    "use_cases": [
      "Fitting linear mixed models",
      "Conducting Bayesian regression analyses"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for Bayesian multilevel models",
      "how to fit a Bayesian model in R",
      "Stan interface for R",
      "Bayesian regression in R",
      "multilevel modeling with brms",
      "brms package documentation"
    ],
    "primary_use_cases": [
      "Fitting Bayesian generalized multilevel models",
      "Specifying flexible prior distributions"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "rstan",
      "lme4"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "rstan",
    "description": "Core R interface to the Stan probabilistic programming language, providing full Bayesian inference via NUTS/HMC, approximate inference via ADVI, and penalized maximum likelihood via L-BFGS for custom Bayesian models.",
    "category": "Bayesian Inference",
    "docs_url": "https://mc-stan.org/rstan/",
    "github_url": "https://github.com/stan-dev/rstan",
    "url": "https://cran.r-project.org/package=rstan",
    "install": "install.packages(\"rstan\")",
    "tags": [
      "Stan",
      "MCMC",
      "HMC",
      "probabilistic-programming",
      "Bayesian"
    ],
    "best_for": "Custom Bayesian models requiring direct Stan language access for maximum flexibility, implementing Carpenter et al. (2017)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "bayesian"
    ],
    "summary": "rstan is the core R interface to the Stan probabilistic programming language, enabling full Bayesian inference and approximate inference for custom Bayesian models. It is used by statisticians and data scientists who require advanced modeling techniques.",
    "use_cases": [
      "Bayesian data analysis",
      "Statistical modeling of complex data"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for Bayesian inference",
      "how to perform MCMC in R",
      "Stan interface for R",
      "Bayesian modeling in R",
      "R probabilistic programming",
      "using HMC with R",
      "L-BFGS in R for Bayesian models"
    ],
    "primary_use_cases": [
      "Bayesian data analysis",
      "Statistical modeling"
    ],
    "api_complexity": "advanced",
    "related_packages": [
      "brms",
      "rstanarm"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "rstanarm",
    "description": "Pre-compiled Bayesian regression models using Stan that mimic familiar R functions (lm, glm, lmer) with customary formula syntax, weakly informative default priors, and zero model compilation time.",
    "category": "Bayesian Inference",
    "docs_url": "https://mc-stan.org/rstanarm/",
    "github_url": "https://github.com/stan-dev/rstanarm",
    "url": "https://cran.r-project.org/package=rstanarm",
    "install": "install.packages(\"rstanarm\")",
    "tags": [
      "Bayesian",
      "Stan",
      "regression",
      "mixed-effects",
      "pre-compiled"
    ],
    "best_for": "Applied Bayesian regression with minimal learning curve for lm/glm/lmer users",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "bayesian"
    ],
    "summary": "rstanarm provides pre-compiled Bayesian regression models that mimic familiar R functions, allowing users to perform regression analysis with minimal setup time. It is primarily used by statisticians and data scientists who require Bayesian modeling capabilities.",
    "use_cases": [
      "Performing Bayesian regression analysis",
      "Conducting mixed-effects modeling"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for Bayesian regression",
      "how to use rstanarm for mixed-effects models",
      "Bayesian modeling in R",
      "pre-compiled Bayesian models in R",
      "rstanarm documentation",
      "install rstanarm package",
      "examples of rstanarm usage"
    ],
    "primary_use_cases": [
      "Bayesian regression analysis",
      "mixed-effects modeling"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "rstan",
      "brms"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "boot",
    "description": "Classic bootstrap methods implementing the approaches described in Davison & Hinkley (1997). Provides functions for both parametric and nonparametric resampling with various confidence interval methods.",
    "category": "Bootstrap & Inference",
    "docs_url": "https://cran.r-project.org/web/packages/boot/boot.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=boot",
    "install": "install.packages(\"boot\")",
    "tags": [
      "bootstrap",
      "resampling",
      "confidence-intervals",
      "nonparametric",
      "parametric"
    ],
    "best_for": "Classic bootstrap methods from Davison & Hinkley (1997) for general resampling inference",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "bootstrap",
      "resampling"
    ],
    "summary": "The 'boot' package provides classic bootstrap methods for statistical inference as described in Davison & Hinkley (1997). It is used by statisticians and data scientists for both parametric and nonparametric resampling to create confidence intervals.",
    "use_cases": [
      "Estimating confidence intervals for a sample mean",
      "Conducting hypothesis tests using bootstrap methods"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for bootstrap methods",
      "how to perform resampling in R",
      "confidence intervals in R",
      "nonparametric bootstrap in R",
      "parametric bootstrap techniques R",
      "statistical inference R package"
    ],
    "primary_use_cases": [
      "confidence interval estimation",
      "hypothesis testing"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Davison & Hinkley (1997)",
    "maintenance_status": "active"
  },
  {
    "name": "fwildclusterboot",
    "description": "Fast wild cluster bootstrap implementation following Roodman et al. (2019)\u2014up to 1000\u00d7 faster than alternatives. Critical for panel data with few clusters. Integrates with fixest and lfe for efficient inference.",
    "category": "Bootstrap & Inference",
    "docs_url": "https://s3alfisc.github.io/fwildclusterboot/",
    "github_url": "https://github.com/s3alfisc/fwildclusterboot",
    "url": "https://cran.r-project.org/package=fwildclusterboot",
    "install": "install.packages(\"fwildclusterboot\")",
    "tags": [
      "wild-bootstrap",
      "cluster-robust",
      "few-clusters",
      "panel-data",
      "fixest"
    ],
    "best_for": "Fast wild cluster bootstrap for panel data with few clusters, implementing Roodman et al. (2019)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "fwildclusterboot is a fast wild cluster bootstrap implementation that significantly speeds up analysis for panel data with few clusters. It is particularly useful for researchers and data scientists working with econometric models.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for wild bootstrap",
      "how to perform cluster-robust inference in R",
      "fast bootstrap methods for panel data",
      "wild cluster bootstrap implementation R",
      "R package for efficient inference with fixest",
      "bootstrap methods for few clusters in R"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "fixest",
      "lfe"
    ],
    "implements_paper": "Roodman et al. (2019)",
    "maintenance_status": "active"
  },
  {
    "name": "rsample",
    "description": "Modern tidyverse-compatible resampling infrastructure. Provides functions for creating resamples (bootstrap, cross-validation, time series splits) that integrate seamlessly with tidymodels workflows.",
    "category": "Bootstrap & Inference",
    "docs_url": "https://rsample.tidymodels.org/",
    "github_url": "https://github.com/tidymodels/rsample",
    "url": "https://cran.r-project.org/package=rsample",
    "install": "install.packages(\"rsample\")",
    "tags": [
      "resampling",
      "cross-validation",
      "bootstrap",
      "tidymodels",
      "time-series-cv"
    ],
    "best_for": "Tidyverse-native resampling for bootstrap, cross-validation, and time series splits",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "resampling",
      "cross-validation",
      "bootstrap",
      "time-series"
    ],
    "summary": "The rsample package provides modern resampling infrastructure that is compatible with the tidyverse. It offers functions for creating various types of resamples, such as bootstrap and cross-validation, which integrate seamlessly with tidymodels workflows.",
    "use_cases": [
      "Creating bootstrap samples for model training",
      "Performing time series cross-validation for predictive modeling"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R library for resampling",
      "how to perform cross-validation in R",
      "bootstrap methods in R",
      "tidymodels resampling functions",
      "time series cross-validation in R",
      "R package for model evaluation"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "tidymodels"
    ],
    "related_packages": [
      "boot",
      "caret"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "bunching",
    "description": "Implements Kleven-Waseem style bunching estimation for kink and notch designs. Calculates parametric elasticities from bunching at tax thresholds with publication-ready output.",
    "category": "Bunching Estimation",
    "docs_url": "https://cran.r-project.org/web/packages/bunching/bunching.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=bunching",
    "install": "install.packages(\"bunching\")",
    "tags": [
      "bunching",
      "kink-design",
      "notch-design",
      "tax-research",
      "elasticity"
    ],
    "best_for": "Kleven-Waseem bunching estimation at kinks and notches for tax research",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "bunching",
      "tax-research",
      "elasticity"
    ],
    "summary": "The 'bunching' package implements Kleven-Waseem style bunching estimation for kink and notch designs. It is used by researchers and analysts in tax policy to calculate parametric elasticities from bunching at tax thresholds.",
    "use_cases": [
      "Estimating tax elasticities at specific income thresholds",
      "Analyzing the effects of tax policy changes on taxpayer behavior"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for bunching estimation",
      "how to calculate elasticities in R",
      "Kleven-Waseem bunching method in R",
      "R tax threshold analysis package",
      "bunching analysis for tax research",
      "notch design estimation in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "bnlearn",
    "description": "Bayesian network structure learning, parameter estimation, and inference. Implements constraint-based (PC, GS), score-based (HC, TABU), and hybrid algorithms for DAG learning with discrete and continuous data.",
    "category": "Causal Discovery",
    "docs_url": "https://www.bnlearn.com/",
    "github_url": null,
    "url": "https://cran.r-project.org/package=bnlearn",
    "install": "install.packages(\"bnlearn\")",
    "tags": [
      "Bayesian-networks",
      "structure-learning",
      "parameter-estimation",
      "probabilistic-graphical-models",
      "inference"
    ],
    "best_for": "Bayesian network learning and inference with constraint-based and score-based algorithms",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "bayesian"
    ],
    "summary": "bnlearn is a package for Bayesian network structure learning, parameter estimation, and inference. It is used by data scientists and researchers interested in causal discovery and probabilistic graphical models.",
    "use_cases": [
      "Learning the structure of a Bayesian network from data",
      "Estimating parameters of a Bayesian network"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for Bayesian networks",
      "how to perform structure learning in R",
      "Bayesian network inference in R",
      "parameter estimation for Bayesian networks R",
      "constraint-based algorithms for DAG learning R",
      "score-based learning in R"
    ],
    "primary_use_cases": [
      "Bayesian network structure learning",
      "parameter estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "dagitty",
    "description": "Analysis of structural causal models represented as DAGs. Computes adjustment sets, identifies instrumental variables, tests conditional independencies, and finds minimal sufficient adjustment sets for causal identification.",
    "category": "Causal Discovery",
    "docs_url": "http://www.dagitty.net/",
    "github_url": "https://github.com/jtextor/dagitty",
    "url": "https://cran.r-project.org/package=dagitty",
    "install": "install.packages(\"dagitty\")",
    "tags": [
      "DAG",
      "causal-graphs",
      "adjustment-sets",
      "d-separation",
      "instrumental-variables"
    ],
    "best_for": "DAG-based causal analysis with adjustment set computation and d-separation testing",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "Dagitty is a tool for analyzing structural causal models represented as directed acyclic graphs (DAGs). It is used by researchers and practitioners in the field of causal inference to compute adjustment sets and identify instrumental variables.",
    "use_cases": [
      "Identifying instrumental variables for causal analysis",
      "Testing conditional independencies in datasets"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for causal graphs",
      "how to compute adjustment sets in R",
      "DAG analysis in R",
      "R library for instrumental variables",
      "conditional independence testing in R",
      "R package for d-separation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "ggdag",
    "description": "Visualize and analyze causal DAGs using ggplot2. Provides tidy interface to dagitty with publication-quality DAG plots, path highlighting, and adjustment set visualization.",
    "category": "Causal Discovery",
    "docs_url": "https://r-causal.github.io/ggdag/",
    "github_url": "https://github.com/malcolmbarrett/ggdag",
    "url": "https://cran.r-project.org/package=ggdag",
    "install": "install.packages(\"ggdag\")",
    "tags": [
      "DAG",
      "visualization",
      "ggplot2",
      "causal-diagrams",
      "adjustment-sets"
    ],
    "best_for": "Publication-quality DAG visualization using ggplot2 with dagitty integration",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The ggdag package allows users to visualize and analyze causal Directed Acyclic Graphs (DAGs) using ggplot2. It is particularly useful for researchers and practitioners in causal discovery who need publication-quality DAG plots and path highlighting.",
    "use_cases": [
      "Creating publication-quality causal DAGs",
      "Highlighting paths in causal diagrams"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for causal DAG visualization",
      "how to create DAGs in R",
      "ggplot2 DAG plotting",
      "analyze causal diagrams in R",
      "visualize adjustment sets R",
      "path highlighting in causal analysis R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "pcalg",
    "description": "Causal structure learning from observational data using the PC algorithm and variants. Estimates Markov equivalence class of DAGs from conditional independence tests with intervention support.",
    "category": "Causal Discovery",
    "docs_url": "https://cran.r-project.org/web/packages/pcalg/pcalg.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=pcalg",
    "install": "install.packages(\"pcalg\")",
    "tags": [
      "causal-discovery",
      "PC-algorithm",
      "structure-learning",
      "DAG",
      "conditional-independence"
    ],
    "best_for": "Causal structure learning from observational data using PC algorithm",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The pcalg package provides tools for causal structure learning from observational data using the PC algorithm and its variants. It is primarily used by researchers and practitioners in the field of causal inference.",
    "use_cases": [],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for causal structure learning",
      "how to use PC algorithm in R",
      "conditional independence tests in R",
      "DAG estimation in R",
      "causal discovery with pcalg",
      "intervention support in causal analysis"
    ],
    "primary_use_cases": [
      "causal structure learning",
      "Markov equivalence class estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "Ananke",
    "description": "Causal inference using graphical models (DAGs), including identification theory and effect estimation.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": "https://ananke.readthedocs.io/",
    "github_url": "[https://github.com/py-why/Ananke](https://github.com/ghosthamlet/ananke",
    "url": "[https://github.com/py-why/Ananke](https://github.com/ghosthamlet/ananke",
    "install": "pip install ananke-causal",
    "tags": [
      "causal inference",
      "graphs"
    ],
    "best_for": "Learning causal structure from data, DAG estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "graphs"
    ],
    "summary": "Ananke is a Python package designed for causal inference using graphical models, specifically directed acyclic graphs (DAGs). It focuses on identification theory and effect estimation, making it useful for researchers and data scientists working in causal analysis.",
    "use_cases": [
      "Estimating causal effects in observational studies",
      "Conducting A/B tests using causal models"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to estimate effects using DAGs in python",
      "causal discovery tools in python",
      "graphical models for causal analysis python",
      "python package for identification theory",
      "DAGs in causal inference python"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "DoWhy",
      "CausalInference"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "Benchpress",
    "description": "Benchmarking 41+ structure learning algorithms for causal discovery. Standardized evaluation framework.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": "https://benchpressdocs.readthedocs.io/",
    "github_url": "https://github.com/felixleopoldo/benchpress",
    "url": "https://github.com/felixleopoldo/benchpress",
    "install": "pip install benchpress",
    "tags": [
      "causal discovery",
      "benchmarking",
      "structure learning"
    ],
    "best_for": "Comparing causal discovery algorithms",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "graphical-models"
    ],
    "summary": "Benchpress is a benchmarking tool for evaluating over 41 structure learning algorithms used in causal discovery. It provides a standardized evaluation framework for researchers and practitioners in the field.",
    "use_cases": [
      "Comparing different structure learning algorithms",
      "Evaluating causal discovery methods in research"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for causal discovery",
      "how to benchmark structure learning algorithms in python",
      "evaluate structure learning algorithms python",
      "benchmarking causal discovery methods python",
      "python causal inference library",
      "structure learning algorithms evaluation python"
    ],
    "primary_use_cases": [
      "benchmarking structure learning algorithms",
      "evaluating causal discovery methods"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "Causal Discovery Toolbox (CDT)",
    "description": "Implements algorithms for causal discovery (recovering causal graph structure) from observational data.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": "https://fentechsolutions.github.io/CausalDiscoveryToolbox/html/index.html",
    "github_url": "https://github.com/FenTechSolutions/CausalDiscoveryToolbox",
    "url": "https://github.com/FenTechSolutions/CausalDiscoveryToolbox",
    "install": "pip install cdt",
    "tags": [
      "causal inference",
      "graphs"
    ],
    "best_for": "Learning causal structure from data, DAG estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "graphs"
    ],
    "summary": "The Causal Discovery Toolbox (CDT) implements algorithms for recovering causal graph structures from observational data. It is used by researchers and practitioners in fields such as statistics, data science, and economics to understand causal relationships.",
    "use_cases": [
      "Analyzing causal relationships in observational studies",
      "Building causal models for decision-making"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for causal discovery",
      "how to recover causal graphs in python",
      "causal inference tools in python",
      "causal discovery algorithms python",
      "graphs in causal inference python",
      "observational data causal analysis python"
    ],
    "primary_use_cases": [
      "causal graph structure recovery"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "CausalNex",
    "description": "Uses Bayesian Networks for causal reasoning, combining ML with expert knowledge to model relationships.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": null,
    "github_url": "https://github.com/mckinsey/causalnex",
    "url": "https://github.com/mckinsey/causalnex",
    "install": "pip install causalnex",
    "tags": [
      "causal inference",
      "graphs",
      "Bayesian"
    ],
    "best_for": "Learning causal structure from data, DAG estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "bayesian"
    ],
    "summary": "CausalNex is a Python package that utilizes Bayesian Networks for causal reasoning, effectively combining machine learning with expert knowledge to model complex relationships. It is used by data scientists and researchers interested in understanding causal relationships in their data.",
    "use_cases": [
      "Analyzing the impact of marketing strategies on sales",
      "Understanding the causal relationships in healthcare data"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to model relationships with Bayesian networks in python",
      "CausalNex documentation",
      "causal discovery tools in python",
      "using Bayesian networks for causal reasoning",
      "best practices for causal inference in python"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "pgmpy",
      "DoWhy"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "LiNGAM",
    "description": "Specialized package for learning non-Gaussian linear causal models, implementing various versions of the LiNGAM algorithm including ICA-based methods.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": "https://lingam.readthedocs.io/",
    "github_url": "https://github.com/cdt15/lingam",
    "url": "https://github.com/cdt15/lingam",
    "install": "pip install lingam",
    "tags": [
      "causal inference",
      "graphs"
    ],
    "best_for": "Learning causal structure from data, DAG estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "graphs"
    ],
    "summary": "LiNGAM is a specialized package for learning non-Gaussian linear causal models, implementing various versions of the LiNGAM algorithm including ICA-based methods. It is used by researchers and practitioners interested in causal discovery and graphical models.",
    "use_cases": [
      "Analyzing causal relationships in observational data",
      "Developing models for causal inference in social sciences"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to learn non-Gaussian linear causal models in python",
      "LiNGAM algorithm implementation in python",
      "causal discovery tools in python",
      "graphs in causal inference python",
      "ICA-based methods for causal models python"
    ],
    "primary_use_cases": [
      "causal forest estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "MCD",
    "description": "Mixture of Causal Graphs discovery for heterogeneous time series (ICML 2024). Finds time-varying causal structures.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": null,
    "github_url": "https://github.com/Rose-STL-Lab/MCD",
    "url": "https://pypi.org/project/MCD/",
    "install": "pip install mcd",
    "tags": [
      "causal discovery",
      "time series",
      "heterogeneous"
    ],
    "best_for": "Time-varying causal structure discovery",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "time-series",
      "graphical-models"
    ],
    "summary": "MCD is a Python package designed for discovering time-varying causal structures in heterogeneous time series data. It is useful for researchers and practitioners in the fields of causal discovery and graphical models.",
    "use_cases": [
      "Analyzing causal relationships in economic data",
      "Studying the impact of interventions over time"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for causal discovery",
      "how to analyze time series with causal graphs",
      "MCD package for heterogeneous time series",
      "discovering causal structures in Python",
      "time-varying causal analysis library",
      "causal graphs in Python"
    ],
    "primary_use_cases": [
      "causal forest estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "SDCI",
    "description": "State-dependent causal inference for conditionally stationary processes (ICML 2025). Handles regime-switching causal graphs.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": null,
    "github_url": "https://github.com/charlio23/SDCI",
    "url": "https://pypi.org/project/SDCI/",
    "install": "pip install sdci",
    "tags": [
      "causal discovery",
      "time series",
      "regime switching"
    ],
    "best_for": "State-dependent causal discovery",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "time-series",
      "regime-switching"
    ],
    "summary": "SDCI is a Python package designed for state-dependent causal inference in conditionally stationary processes. It is particularly useful for researchers and practitioners working with regime-switching causal graphs.",
    "use_cases": [
      "Analyzing causal relationships in time series data",
      "Studying the effects of regime changes on causal inference"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for causal discovery",
      "how to perform regime switching analysis in python",
      "time series causal inference python",
      "SDCI package usage",
      "regime-switching causal graphs in python",
      "causal inference for time series"
    ],
    "primary_use_cases": [
      "causal forest estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "Tigramite",
    "description": "Specialized package for causal inference in time series data implementing PCMCI, PCMCIplus, LPCMCI algorithms with conditional independence tests.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": "https://jakobrunge.github.io/tigramite/",
    "github_url": "https://github.com/jakobrunge/tigramite",
    "url": "https://github.com/jakobrunge/tigramite",
    "install": "pip install tigramite",
    "tags": [
      "causal inference",
      "graphs"
    ],
    "best_for": "Learning causal structure from data, DAG estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "time-series"
    ],
    "summary": "Tigramite is a specialized package for causal inference in time series data that implements PCMCI, PCMCIplus, and LPCMCI algorithms along with conditional independence tests. It is used by researchers and data scientists working on causal discovery in temporal datasets.",
    "use_cases": [
      "Analyzing causal relationships in economic time series",
      "Evaluating the impact of interventions over time"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to perform causal discovery in time series",
      "Tigramite package usage",
      "conditional independence tests in Python",
      "time series causal inference library",
      "implementing PCMCI in Python"
    ],
    "primary_use_cases": [
      "causal forest estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "causal-learn",
    "description": "Comprehensive Python package serving as Python translation and extension of Java-based Tetrad toolkit for causal discovery algorithms.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": "https://causal-learn.readthedocs.io/",
    "github_url": "https://github.com/py-why/causal-learn",
    "url": "https://github.com/py-why/causal-learn",
    "install": "pip install causal-learn",
    "tags": [
      "causal inference",
      "graphs"
    ],
    "best_for": "Learning causal structure from data, DAG estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "graphs"
    ],
    "summary": "Causal-learn is a comprehensive Python package that serves as a translation and extension of the Java-based Tetrad toolkit for causal discovery algorithms. It is used by researchers and practitioners in the field of causal inference to analyze and model causal relationships.",
    "use_cases": [
      "Analyzing causal relationships in datasets",
      "Developing causal models for research"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for causal discovery",
      "how to perform causal inference in python",
      "causal graphs in python",
      "Tetrad toolkit in python",
      "causal analysis with python",
      "causal-learn package usage"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "Tetrad",
      "DoWhy"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "causal-llm-bfs",
    "description": "LLM + BFS hybrid for efficient causal graph discovery. Uses language models to guide structure search.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": "https://github.com/superkaiba/causal-llm-bfs",
    "github_url": "https://github.com/superkaiba/causal-llm-bfs",
    "url": "https://github.com/superkaiba/causal-llm-bfs",
    "install": "pip install causal-llm-bfs",
    "tags": [
      "causal discovery",
      "LLM",
      "graphs"
    ],
    "best_for": "LLM-assisted causal discovery",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "graphs"
    ],
    "summary": "The causal-llm-bfs package combines language models with breadth-first search techniques to enhance the discovery of causal graphs. It is useful for researchers and practitioners in causal inference and data science.",
    "use_cases": [
      "Discovering causal relationships in datasets",
      "Guiding structure search in complex data environments"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for causal discovery",
      "how to use LLM for graph discovery",
      "efficient causal graph discovery in python",
      "BFS algorithm for causal graphs",
      "causal inference with language models",
      "graphical models in python"
    ],
    "primary_use_cases": [
      "causal graph discovery"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "gCastle",
    "description": "Huawei Noah's Ark Lab end-to-end causal structure learning toolchain emphasizing gradient-based methods with GPU acceleration (NOTEARS, GOLEM).",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": "https://gcastle.readthedocs.io/",
    "github_url": "https://github.com/huawei-noah/trustworthyAI",
    "url": "https://github.com/huawei-noah/trustworthyAI",
    "install": "pip install gcastle",
    "tags": [
      "causal inference",
      "graphs"
    ],
    "best_for": "Learning causal structure from data, DAG estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "graphs"
    ],
    "summary": "gCastle is an end-to-end causal structure learning toolchain developed by Huawei Noah's Ark Lab, focusing on gradient-based methods with GPU acceleration. It is used by researchers and practitioners in the field of causal inference and graphical models.",
    "use_cases": [
      "learning causal structures from data",
      "analyzing causal relationships in complex systems"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to do causal structure learning in python",
      "gradient-based methods for graphs in python",
      "GPU acceleration for causal inference python",
      "causal discovery tools in python",
      "Huawei Noah's Ark Lab causal tools"
    ],
    "primary_use_cases": [
      "causal structure learning",
      "gradient-based causal inference"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "py-tetrad",
    "description": "Python interface to Tetrad Java library using JPype, providing direct access to Tetrad's causal discovery algorithms with efficient data translation.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": null,
    "github_url": "https://github.com/cmu-phil/py-tetrad",
    "url": "https://github.com/cmu-phil/py-tetrad",
    "install": "Available on GitHub (installation via git clone)",
    "tags": [
      "causal inference",
      "graphs"
    ],
    "best_for": "Learning causal structure from data, DAG estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "JPype"
    ],
    "topic_tags": [
      "causal-inference",
      "graphs"
    ],
    "summary": "py-tetrad is a Python interface to the Tetrad Java library that provides direct access to Tetrad's causal discovery algorithms. It is used by data scientists and researchers interested in causal inference and graphical models.",
    "use_cases": [
      "Causal analysis in research studies",
      "Graphical model construction for data analysis"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for causal discovery",
      "how to use Tetrad in Python",
      "causal inference with py-tetrad",
      "graphs in causal analysis Python",
      "implementing causal algorithms in Python",
      "Tetrad Java library interface Python"
    ],
    "primary_use_cases": [
      "causal discovery",
      "graphical model estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "CATENets",
    "description": "JAX-accelerated neural network CATE estimators implementing SNet, FlexTENet, TARNet, CFRNet, and DragonNet architectures.",
    "category": "Causal Inference & Matching",
    "docs_url": null,
    "github_url": "https://github.com/AliciaCurth/CATENets",
    "url": "https://github.com/AliciaCurth/CATENets",
    "install": "pip install catenets",
    "tags": [
      "causal inference",
      "deep learning",
      "JAX"
    ],
    "best_for": "GPU-accelerated neural CATE estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "deep-learning"
    ],
    "summary": "CATENets is a library for JAX-accelerated neural network CATE estimators that implements various architectures such as SNet, FlexTENet, TARNet, CFRNet, and DragonNet. It is designed for researchers and practitioners in causal inference seeking advanced deep learning solutions.",
    "use_cases": [],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to estimate CATE in python",
      "deep learning for causal inference",
      "JAX neural network estimators",
      "CATE estimators in python",
      "using JAX for deep learning",
      "neural networks for causal analysis",
      "implementing TARNet in python"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "JAX"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "CausalInference",
    "description": "Implements classical causal inference methods like propensity score matching, inverse probability weighting, stratification.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://causalinferenceinpython.org",
    "github_url": "https://github.com/laurencium/causalinference",
    "url": "https://github.com/laurencium/causalinference",
    "install": "pip install CausalInference",
    "tags": [
      "causal inference",
      "matching"
    ],
    "best_for": "Estimating treatment effects, propensity score matching, observational studies",
    "language": "Python",
    "difficulty": "beginner|intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "matching"
    ],
    "summary": "CausalInference implements classical causal inference methods such as propensity score matching, inverse probability weighting, and stratification. It is used by data scientists and researchers to analyze causal relationships in data.",
    "use_cases": [
      "Analyzing treatment effects in observational studies",
      "Evaluating the impact of interventions in healthcare"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to do propensity score matching in python",
      "inverse probability weighting python package",
      "stratification methods in python",
      "matching techniques in causal analysis",
      "causal inference tools for data science"
    ],
    "primary_use_cases": [
      "propensity score matching",
      "inverse probability weighting"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "causalml",
      "DoWhy"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "CausalLib",
    "description": "IBM-developed package that provides a scikit-learn-inspired API for causal inference with meta-algorithms supporting arbitrary machine learning models.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://causallib.readthedocs.io/",
    "github_url": "https://github.com/IBM/causallib",
    "url": "https://github.com/IBM/causallib",
    "install": "pip install causallib",
    "tags": [
      "causal inference",
      "matching"
    ],
    "best_for": "Estimating treatment effects, propensity score matching, observational studies",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "matching"
    ],
    "summary": "CausalLib is an IBM-developed package that provides a scikit-learn-inspired API for causal inference. It is designed for users who need to apply causal inference techniques using various machine learning models.",
    "use_cases": [
      "Estimating causal effects in observational studies",
      "Conducting A/B tests using machine learning models"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to do matching in python",
      "causal inference with machine learning",
      "scikit-learn inspired causal inference library",
      "using CausalLib for A/B testing",
      "meta-algorithms for causal inference in python"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "DoWhy",
      "EconML"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "CausalML",
    "description": "Focuses on uplift modeling and heterogeneous treatment effect estimation using machine learning techniques.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://causalml.readthedocs.io/",
    "github_url": "https://github.com/uber/causalml",
    "url": "https://github.com/uber/causalml",
    "install": "pip install causalml",
    "tags": [
      "causal inference",
      "matching"
    ],
    "best_for": "Estimating treatment effects, propensity score matching, observational studies",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "matching"
    ],
    "summary": "CausalML focuses on uplift modeling and heterogeneous treatment effect estimation using machine learning techniques. It is used by data scientists and researchers interested in causal inference.",
    "use_cases": [
      "Estimating the effect of marketing campaigns",
      "Personalizing treatment recommendations"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for uplift modeling",
      "how to estimate treatment effects in python",
      "machine learning for causal inference",
      "CausalML documentation",
      "best practices for matching in python",
      "python library for heterogeneous treatment effects"
    ],
    "primary_use_cases": [
      "uplift modeling",
      "heterogeneous treatment effect estimation"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "DoWhy",
      "EconML"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "CausalMatch",
    "description": "Implements Propensity Score Matching (PSM) and Coarsened Exact Matching (CEM) with ML flexibility for propensity score estimation.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://github.com/bytedance/CausalMatch",
    "github_url": null,
    "url": "https://github.com/bytedance/CausalMatch",
    "install": "pip install causalmatch",
    "tags": [
      "causal inference",
      "matching"
    ],
    "best_for": "Estimating treatment effects, propensity score matching, observational studies",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "matching"
    ],
    "summary": "CausalMatch implements Propensity Score Matching (PSM) and Coarsened Exact Matching (CEM) with machine learning flexibility for propensity score estimation. It is useful for researchers and data scientists interested in causal inference methodologies.",
    "use_cases": [
      "Estimating treatment effects in observational studies",
      "Conducting A/B tests with matched samples"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for propensity score matching",
      "how to perform causal inference in python",
      "CausalMatch documentation",
      "examples of Coarsened Exact Matching in python",
      "best practices for matching in causal analysis",
      "python library for causal inference"
    ],
    "primary_use_cases": [
      "propensity score matching",
      "coarsened exact matching"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "CausalPlayground",
    "description": "Python library for causal research that addresses the scarcity of real-world datasets with known causal relations. Provides fine-grained control over structural causal models.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://causal-playground.readthedocs.io/",
    "github_url": "https://github.com/sa-and/CausalPlayground",
    "url": "https://github.com/sa-and/CausalPlayground",
    "install": "pip install causal-playground",
    "tags": [
      "causal inference",
      "matching"
    ],
    "best_for": "Estimating treatment effects, propensity score matching, observational studies",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "CausalPlayground is a Python library designed for causal research, focusing on the use of real-world datasets with known causal relations. It is suitable for researchers and data scientists interested in structural causal models.",
    "use_cases": [
      "analyzing causal relationships in observational data",
      "designing experiments for A/B testing"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for causal research",
      "how to use structural causal models in python"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "PyMC Marketing",
    "description": "Developed by PyMC Labs, focuses specifically on causal inference in quasi-experimental settings. Specializes in scenarios where randomization is impossible or expensive.",
    "category": "Marketing Mix Models (MMM) & Business Analytics",
    "docs_url": "https://www.pymc-marketing.io/",
    "github_url": "https://github.com/pymc-labs/pymc-marketing",
    "url": "https://github.com/pymc-labs/pymc-marketing",
    "install": "pip install pymc-marketing",
    "tags": [
      "causal inference",
      "matching",
      "marketing",
      "analytics",
      "Bayesian"
    ],
    "best_for": "Estimating treatment effects, propensity score matching, observational studies",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "matching",
      "bayesian",
      "marketing",
      "analytics"
    ],
    "summary": "PyMC Marketing is a collection of Bayesian marketing models built with PyMC, designed for tasks such as Marketing Mix Modeling (MMM), Customer Lifetime Value (CLV) estimation, and attribution analysis. It is useful for data scientists and marketers looking to apply Bayesian methods to their marketing strategies.",
    "use_cases": [
      "Analyzing the impact of a policy change",
      "Evaluating the effectiveness of a marketing campaign",
      "Estimating the impact of marketing channels",
      "Analyzing customer lifetime value",
      "Attributing sales to different marketing efforts"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to do causal analysis in python",
      "matching techniques in python",
      "quasi-experimental methods python",
      "CausalPy documentation",
      "causal inference tools in python",
      "python library for Bayesian marketing models",
      "how to perform MMM in python",
      "customer lifetime value estimation with PyMC",
      "attribution modeling in python",
      "bayesian analytics for marketing",
      "marketing mix models in python"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis",
      "Marketing Mix Modeling",
      "Customer Lifetime Value estimation",
      "Attribution analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "DoWhy",
      "EconML"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "DoWhy",
    "description": "End-to-end framework for causal inference based on causal graphs (DAGs) and potential outcomes. Covers identification, estimation, refutation.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://www.pywhy.org/dowhy/",
    "github_url": "https://github.com/py-why/dowhy",
    "url": "https://github.com/py-why/dowhy",
    "install": "pip install dowhy",
    "tags": [
      "causal inference",
      "matching"
    ],
    "best_for": "Estimating treatment effects, propensity score matching, observational studies",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "DoWhy is an end-to-end framework for causal inference that utilizes causal graphs (DAGs) and potential outcomes. It is designed for researchers and practitioners looking to identify, estimate, and refute causal relationships.",
    "use_cases": [
      "Estimating causal effects from observational data",
      "Conducting A/B tests using causal inference methods"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to perform causal analysis in python",
      "DoWhy package usage",
      "causal graphs in python",
      "potential outcomes framework python",
      "causal inference tools python"
    ],
    "primary_use_cases": [
      "causal effect estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "EconML",
      "CausalML"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "KECENI",
    "description": "Doubly robust, non-parametric estimation of node-wise counterfactual means under network interference (arXiv 2024).",
    "category": "Causal Inference & Matching",
    "docs_url": null,
    "github_url": "https://github.com/HeejongBong/KECENI",
    "url": "https://pypi.org/project/KECENI/",
    "install": "pip install keceni",
    "tags": [
      "networks",
      "spillovers",
      "causal inference"
    ],
    "best_for": "Network interference with node heterogeneity",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "networks"
    ],
    "summary": "KECENI provides a method for doubly robust, non-parametric estimation of counterfactual means in the presence of network interference. It is particularly useful for researchers and practitioners in causal inference who are dealing with complex network data.",
    "use_cases": [
      "Estimating treatment effects in networked populations",
      "Analyzing spillover effects in social networks"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to estimate counterfactual means in python",
      "network interference estimation python",
      "doubly robust estimation in python",
      "spillover effects analysis python",
      "non-parametric causal inference python"
    ],
    "primary_use_cases": [
      "causal forest estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "NetworkCausalTree",
    "description": "Estimates both direct treatment effects and spillover effects under clustered network interference (Bargagli-Stoffi et al. 2025).",
    "category": "Causal Inference & Matching",
    "docs_url": null,
    "github_url": "https://github.com/fbargaglistoffi/NetworkCausalTree",
    "url": "https://github.com/fbargaglistoffi/NetworkCausalTree",
    "install": "pip install networkcausaltree",
    "tags": [
      "causal inference",
      "networks",
      "spillovers"
    ],
    "best_for": "Treatment effects with network interference",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "networks",
      "spillovers"
    ],
    "summary": "NetworkCausalTree estimates both direct treatment effects and spillover effects in the context of clustered network interference. It is useful for researchers and practitioners in causal inference who are dealing with network data.",
    "use_cases": [
      "Estimating treatment effects in social networks",
      "Analyzing spillover effects in public health interventions"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to estimate spillover effects in python",
      "network interference analysis in python",
      "direct treatment effects estimation python",
      "causal inference with networks",
      "spillover effects in clustered networks"
    ],
    "primary_use_cases": [
      "causal forest estimation"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Bargagli-Stoffi et al. (2025)",
    "maintenance_status": "active"
  },
  {
    "name": "PySensemakr",
    "description": "Implements Cinelli-Hazlett framework for assessing robustness to unobserved confounding. Computes confounder strength needed to invalidate results.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://github.com/carloscinelli/PySensemakr",
    "github_url": "https://github.com/carloscinelli/PySensemakr",
    "url": "https://github.com/carloscinelli/PySensemakr",
    "install": "pip install pysensemakr",
    "tags": [
      "causal inference",
      "sensitivity analysis",
      "robustness"
    ],
    "best_for": "Sensitivity analysis with publication-ready contour plots",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "sensitivity-analysis",
      "robustness"
    ],
    "summary": "PySensemakr implements the Cinelli-Hazlett framework to assess robustness against unobserved confounding in causal inference studies. It is useful for researchers and data scientists who need to evaluate the strength of confounders that could potentially invalidate their results.",
    "use_cases": [
      "Assessing the robustness of causal estimates",
      "Evaluating the impact of unobserved confounding on study results"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to perform sensitivity analysis in python",
      "robustness assessment in python",
      "Cinelli-Hazlett framework implementation",
      "confounder strength analysis python",
      "evaluate unobserved confounding python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "aipyw",
    "description": "Minimal, fast AIPW (Augmented Inverse Probability Weighting) implementation for discrete treatments. Sklearn-compatible with cross-fitting.",
    "category": "Causal Inference & Matching",
    "docs_url": null,
    "github_url": "https://github.com/apoorvalal/aipyw",
    "url": "https://github.com/apoorvalal/aipyw",
    "install": "pip install aipyw",
    "tags": [
      "causal inference",
      "AIPW",
      "treatment effects"
    ],
    "best_for": "Fast AIPW estimation with sklearn models",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "Aipyw is a minimal and fast implementation of Augmented Inverse Probability Weighting (AIPW) for discrete treatments. It is designed to be compatible with Scikit-learn and supports cross-fitting, making it suitable for users interested in causal inference.",
    "use_cases": [
      "Estimating treatment effects in clinical trials",
      "Analyzing A/B test results"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for AIPW",
      "how to do causal inference in python",
      "AIPW implementation in python",
      "fast AIPW for discrete treatments",
      "cross-fitting in causal inference python",
      "scikit-learn compatible AIPW"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "scikit-learn"
    ],
    "related_packages": [
      "causalml",
      "econml"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "causal-curve",
    "description": "Continuous treatment dose-response curve estimation. GPS and TMLE methods for continuous treatments.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://causal-curve.readthedocs.io/",
    "github_url": "https://github.com/ronikobrosly/causal-curve",
    "url": "https://github.com/ronikobrosly/causal-curve",
    "install": "pip install causal-curve",
    "tags": [
      "dose-response",
      "continuous treatment",
      "GPS"
    ],
    "best_for": "Dose-response curves for continuous treatments",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "dose-response"
    ],
    "summary": "The causal-curve package provides methods for estimating continuous treatment dose-response curves using Generalized Propensity Score (GPS) and Targeted Maximum Likelihood Estimation (TMLE). It is useful for researchers and practitioners in causal inference who are working with continuous treatments.",
    "use_cases": [
      "Estimating treatment effects in clinical trials",
      "Analyzing the impact of dosage levels in drug studies"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for dose-response estimation",
      "how to estimate continuous treatment effects in python",
      "GPS methods in python",
      "TMLE methods for continuous treatments",
      "causal inference tools in python",
      "continuous treatment analysis python"
    ],
    "primary_use_cases": [
      "causal forest estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "fastmatch",
    "description": "Fast k-nearest-neighbor matching for large datasets using Facebook's FAISS library.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://github.com/py-econometrics/fastmatch",
    "github_url": null,
    "url": "https://github.com/py-econometrics/fastmatch",
    "install": "pip install fastmatch",
    "tags": [
      "causal inference",
      "matching"
    ],
    "best_for": "Estimating treatment effects, propensity score matching, observational studies",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "matching"
    ],
    "summary": "Fastmatch is a Python package designed for efficient k-nearest-neighbor matching on large datasets, leveraging Facebook's FAISS library. It is particularly useful for researchers and data scientists working in causal inference and matching scenarios.",
    "use_cases": [
      "Matching large datasets for causal analysis",
      "Conducting A/B tests with efficient neighbor matching"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for k-nearest-neighbor matching",
      "how to perform causal inference in python",
      "fastmatch package documentation",
      "efficient matching for large datasets in python",
      "using FAISS for matching in python",
      "best practices for causal inference with python"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "mcf (Modified Causal Forest)",
    "description": "Comprehensive Python implementation for heterogeneous treatment effect estimation. Handles binary/multiple discrete treatments with optimal policy learning via Policy Trees.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://mcfpy.github.io/mcf/",
    "github_url": "https://github.com/MCFpy/mcf",
    "url": "https://github.com/MCFpy/mcf",
    "install": "pip install mcf",
    "tags": [
      "causal inference",
      "treatment effects",
      "policy learning"
    ],
    "best_for": "CATE estimation with policy tree optimization",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "policy-learning"
    ],
    "summary": "The mcf package provides a comprehensive Python implementation for estimating heterogeneous treatment effects. It is designed for researchers and practitioners interested in causal inference and optimal policy learning.",
    "use_cases": [
      "Estimating treatment effects in clinical trials",
      "Optimizing marketing strategies based on treatment outcomes"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to estimate treatment effects in python",
      "policy learning with python",
      "modified causal forest implementation",
      "A/B testing in python",
      "python library for heterogeneous treatment effects"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "policy learning"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "pydtr",
    "description": "Dynamic treatment regimes using Iterative Q-Learning. Scikit-learn compatible for multi-stage optimal treatment sequencing.",
    "category": "Causal Inference & Matching",
    "docs_url": null,
    "github_url": "https://github.com/fullflu/pydtr",
    "url": "https://pypi.org/project/pydtr/",
    "install": "pip install pydtr",
    "tags": [
      "dynamic treatment",
      "reinforcement learning",
      "causal inference"
    ],
    "best_for": "Multi-stage dynamic treatment regimes",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "reinforcement-learning"
    ],
    "summary": "pydtr is a Python package designed for implementing dynamic treatment regimes using Iterative Q-Learning. It is compatible with Scikit-learn and is used for multi-stage optimal treatment sequencing in causal inference contexts.",
    "use_cases": [
      "optimizing treatment strategies in clinical trials",
      "analyzing multi-stage decision processes"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for dynamic treatment regimes",
      "how to implement iterative Q-learning in python",
      "scikit-learn compatible treatment sequencing",
      "dynamic treatment using reinforcement learning in python"
    ],
    "primary_use_cases": [
      "dynamic treatment optimization",
      "multi-stage treatment sequencing"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "scikit-learn"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "pyregadj",
    "description": "Regression and ML adjustments to treatment effects in RCTs. Implements List et al. (2024) methods.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://github.com/vyasenov/pyregadj",
    "github_url": "https://github.com/vyasenov/pyregadj",
    "url": "https://github.com/vyasenov/pyregadj",
    "install": "pip install pyregadj",
    "tags": [
      "RCT",
      "regression adjustment",
      "treatment effects"
    ],
    "best_for": "Covariate adjustment in experiments",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "machine-learning"
    ],
    "summary": "pyregadj provides methods for regression and machine learning adjustments to treatment effects in randomized controlled trials (RCTs). It is useful for researchers and data scientists working in causal inference.",
    "use_cases": [
      "Adjusting treatment effects in clinical trials",
      "Analyzing data from randomized controlled trials"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for regression adjustment",
      "how to analyze treatment effects in RCTs with python",
      "machine learning adjustments for causal inference in python",
      "pyregadj documentation",
      "RCT analysis tools in python",
      "methods for treatment effects in python"
    ],
    "primary_use_cases": [
      "regression adjustment",
      "treatment effect estimation"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "List et al. (2024)",
    "maintenance_status": "active"
  },
  {
    "name": "scikit-uplift",
    "description": "Focuses on uplift modeling and estimating heterogeneous treatment effects using various ML-based methods.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://scikit-uplift.readthedocs.io/en/latest/",
    "github_url": "https://github.com/maks-sh/scikit-uplift",
    "url": "https://github.com/maks-sh/scikit-uplift",
    "install": "pip install scikit-uplift",
    "tags": [
      "causal inference",
      "matching"
    ],
    "best_for": "Estimating treatment effects, propensity score matching, observational studies",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "matching"
    ],
    "summary": "scikit-uplift focuses on uplift modeling and estimating heterogeneous treatment effects using various ML-based methods. It is used by data scientists and researchers interested in causal inference and treatment effect estimation.",
    "use_cases": [
      "Estimating the effect of marketing campaigns",
      "Personalizing treatment recommendations"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for uplift modeling",
      "how to estimate treatment effects in python",
      "causal inference library in python",
      "matching methods in python",
      "scikit-uplift documentation",
      "uplift modeling techniques python"
    ],
    "primary_use_cases": [
      "uplift modeling",
      "heterogeneous treatment effect estimation"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "causalml",
      "econml"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "y0",
    "description": "Causal inference framework providing tools for causal graph manipulation and effect identification.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://y0.readthedocs.io/",
    "github_url": "https://github.com/y0-causal-inference/y0",
    "url": "https://github.com/y0-causal-inference/y0",
    "install": "pip install y0",
    "tags": [
      "causal inference",
      "graphs",
      "identification"
    ],
    "best_for": "Causal graph manipulation and do-calculus",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "graphs"
    ],
    "summary": "y0 is a causal inference framework that provides tools for manipulating causal graphs and identifying effects. It is used by data scientists and researchers interested in causal analysis.",
    "use_cases": [
      "Analyzing the impact of interventions",
      "Understanding causal relationships in data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to manipulate causal graphs in python",
      "tools for effect identification in python",
      "causal inference framework python",
      "python causal graphs library",
      "identifying effects with python"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "ATbounds",
    "description": "Implements modern treatment effect bounds beyond basic Manski worst-case scenarios. Provides tighter bounds using monotonicity, mean independence, and other assumptions following Lee and Weidner (2021).",
    "category": "Causal Inference (Bounds)",
    "docs_url": "https://cran.r-project.org/web/packages/ATbounds/ATbounds.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=ATbounds",
    "install": "install.packages(\"ATbounds\")",
    "tags": [
      "partial-identification",
      "bounds",
      "treatment-effects",
      "Manski",
      "monotonicity"
    ],
    "best_for": "Modern treatment effect bounds with tighter identification under various assumptions, implementing Lee & Weidner (2021)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "ATbounds implements modern treatment effect bounds that go beyond basic Manski worst-case scenarios, providing tighter bounds using various assumptions. It is useful for researchers and practitioners in causal inference.",
    "use_cases": [
      "Estimating treatment effects under monotonicity assumptions",
      "Analyzing data with mean independence assumptions"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for treatment effect bounds",
      "how to implement causal inference in R",
      "bounds for treatment effects in R",
      "R library for partial identification",
      "Manski bounds implementation in R",
      "causal inference tools in R"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Lee and Weidner (2021)",
    "maintenance_status": "active"
  },
  {
    "name": "CausalGPS",
    "description": "Machine learning-based generalized propensity score estimation for continuous treatments. Uses SuperLearner ensemble methods for flexible estimation of dose-response curves.",
    "category": "Causal Inference (Continuous Treatment)",
    "docs_url": "https://cran.r-project.org/web/packages/CausalGPS/CausalGPS.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=CausalGPS",
    "install": "install.packages(\"CausalGPS\")",
    "tags": [
      "GPS",
      "continuous-treatment",
      "machine-learning",
      "SuperLearner",
      "dose-response"
    ],
    "best_for": "ML-based generalized propensity scores for continuous treatment dose-response estimation",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "machine-learning"
    ],
    "summary": "CausalGPS is a machine learning package designed for generalized propensity score estimation for continuous treatments. It employs SuperLearner ensemble methods to provide flexible estimation of dose-response curves, making it useful for researchers and practitioners in causal inference.",
    "use_cases": [
      "Estimating treatment effects in observational studies",
      "Analyzing dose-response relationships in clinical trials"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for generalized propensity score",
      "how to estimate dose-response curves in R",
      "machine learning for continuous treatments R",
      "CausalGPS documentation",
      "SuperLearner for causal inference",
      "R library for dose-response analysis"
    ],
    "primary_use_cases": [
      "generalized propensity score estimation",
      "dose-response curve estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "DRDID",
    "description": "Implements locally efficient doubly robust DiD estimators that combine inverse probability weighting and outcome regression for improved statistical properties. Handles both panel data and repeated cross-sections in the canonical 2x2 DiD setting with covariates, providing robustness against model misspecification.",
    "category": "Causal Inference (DiD)",
    "docs_url": "https://psantanna.com/DRDID/",
    "github_url": "https://github.com/pedrohcgs/DRDID",
    "url": "https://cran.r-project.org/package=DRDID",
    "install": "install.packages(\"DRDID\")",
    "tags": [
      "doubly-robust",
      "difference-in-differences",
      "inverse-probability-weighting",
      "ATT",
      "covariates"
    ],
    "best_for": "Two-period DiD with covariates requiring robust estimation against model misspecification, implementing Sant'Anna & Zhao (2020)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "DRDID implements locally efficient doubly robust DiD estimators that combine inverse probability weighting and outcome regression for improved statistical properties. It is used by researchers and practitioners working with panel data and repeated cross-sections in causal inference.",
    "use_cases": [
      "Estimating treatment effects in policy evaluations",
      "Analyzing the impact of interventions in social sciences"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for doubly robust DiD",
      "how to implement difference-in-differences in R",
      "R library for causal inference with covariates",
      "inverse probability weighting in R",
      "outcome regression for DiD in R",
      "statistical properties of DiD estimators in R"
    ],
    "primary_use_cases": [
      "causal inference analysis",
      "treatment effect estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "HonestDiD",
    "description": "Constructs robust confidence intervals for DiD and event-study designs under violations of parallel trends. Allows researchers to conduct sensitivity analysis by relaxing the parallel trends assumption using smoothness or relative magnitude restrictions on pre-trend violations.",
    "category": "Causal Inference (DiD)",
    "docs_url": "https://github.com/asheshrambachan/HonestDiD",
    "github_url": "https://github.com/asheshrambachan/HonestDiD",
    "url": "https://cran.r-project.org/package=HonestDiD",
    "install": "install.packages(\"HonestDiD\")",
    "tags": [
      "sensitivity-analysis",
      "parallel-trends",
      "robust-inference",
      "confidence-intervals",
      "event-study"
    ],
    "best_for": "Assessing how treatment effect conclusions change under plausible parallel trends violations, implementing Rambachan & Roth (2023)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "HonestDiD constructs robust confidence intervals for Difference-in-Differences (DiD) and event-study designs, addressing violations of parallel trends. It is primarily used by researchers conducting sensitivity analysis in causal inference.",
    "use_cases": [
      "Analyzing the impact of policy changes using DiD",
      "Conducting sensitivity analysis for pre-trend violations"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for causal inference",
      "how to conduct sensitivity analysis in R",
      "robust confidence intervals for DiD",
      "event-study analysis in R",
      "parallel trends violation analysis",
      "R library for event-study designs"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "bacondecomp",
    "description": "Performs Goodman-Bacon decomposition showing how two-way fixed effects (TWFE) estimates are weighted averages of all possible 2\u00d72 DiD comparisons. Essential for diagnosing negative weights problems in staggered adoption designs.",
    "category": "Causal Inference (DiD)",
    "docs_url": "https://cran.r-project.org/web/packages/bacondecomp/bacondecomp.pdf",
    "github_url": "https://github.com/evanjflack/bacondecomp",
    "url": "https://cran.r-project.org/package=bacondecomp",
    "install": "install.packages(\"bacondecomp\")",
    "tags": [
      "DiD",
      "TWFE",
      "Goodman-Bacon",
      "decomposition",
      "staggered-adoption"
    ],
    "best_for": "Goodman-Bacon decomposition for diagnosing negative weights in TWFE staggered DiD designs",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "decomposition"
    ],
    "summary": "The bacondecomp package performs Goodman-Bacon decomposition, which helps in understanding how two-way fixed effects estimates are weighted averages of all possible 2\u00d72 DiD comparisons. It is essential for diagnosing negative weights problems in staggered adoption designs.",
    "use_cases": [
      "Analyzing staggered adoption designs",
      "Diagnosing negative weights in DiD estimates"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for Goodman-Bacon decomposition",
      "how to diagnose negative weights in staggered adoption",
      "two-way fixed effects in R",
      "R library for causal inference",
      "Goodman-Bacon decomposition in R",
      "staggered adoption analysis in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "did",
    "description": "Implements group-time average treatment effects (ATT(g,t)) for staggered DiD designs with multiple periods and variation in treatment timing. Provides flexible aggregation into event-study plots or overall treatment effect estimates, addressing the well-documented negative weighting issues with conventional TWFE under staggered adoption.",
    "category": "Causal Inference (DiD)",
    "docs_url": "https://bcallaway11.github.io/did/",
    "github_url": "https://github.com/bcallaway11/did",
    "url": "https://cran.r-project.org/package=did",
    "install": "install.packages(\"did\")",
    "tags": [
      "difference-in-differences",
      "staggered-adoption",
      "event-study",
      "treatment-effects",
      "panel-data"
    ],
    "best_for": "Staggered rollout designs where different units adopt treatment at different times, implementing the Callaway & Sant'Anna (2021) estimator",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The 'did' package implements group-time average treatment effects for staggered DiD designs, allowing for flexible aggregation into event-study plots and overall treatment effect estimates. It is primarily used by researchers and practitioners in causal inference to address issues with conventional methods under staggered adoption.",
    "use_cases": [
      "Analyzing treatment effects in policy evaluations",
      "Creating event-study plots for staggered adoption scenarios"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for group-time average treatment effects",
      "how to create event-study plots in R",
      "difference-in-differences analysis in R",
      "staggered adoption treatment effects R",
      "R library for causal inference",
      "R package for panel data treatment effects"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "didimputation",
    "description": "Implements the imputation-based DiD estimator that first estimates Y(0) counterfactuals from untreated observations using two-way fixed effects, then imputes treatment effects for treated units. Avoids negative weighting problems of conventional TWFE under heterogeneous treatment effects.",
    "category": "Causal Inference (DiD)",
    "docs_url": "https://github.com/kylebutts/didimputation",
    "github_url": "https://github.com/kylebutts/didimputation",
    "url": "https://cran.r-project.org/package=didimputation",
    "install": "install.packages(\"didimputation\")",
    "tags": [
      "imputation",
      "two-way-fixed-effects",
      "event-study",
      "counterfactual",
      "robust-estimation"
    ],
    "best_for": "Event-study designs where imputation-based correction for TWFE bias is preferred, implementing Borusyak, Jaravel & Spiess (2024)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "counterfactual",
      "robust-estimation"
    ],
    "summary": "The didimputation package implements an imputation-based DiD estimator that estimates Y(0) counterfactuals from untreated observations using two-way fixed effects. It is designed for researchers and practitioners who need to analyze treatment effects while avoiding negative weighting issues in heterogeneous treatment scenarios.",
    "use_cases": [
      "Estimating treatment effects in policy evaluation",
      "Analyzing the impact of interventions in social sciences"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for DiD estimation",
      "how to impute treatment effects in R",
      "two-way fixed effects in R",
      "counterfactual analysis in R",
      "robust estimation methods in R",
      "event study analysis in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "fastdid",
    "description": "High-performance implementation of Callaway & Sant'Anna estimators optimized for large datasets with millions of observations. Reduces computation time from hours to seconds while supporting time-varying covariates and multiple events per unit.",
    "category": "Causal Inference (DiD)",
    "docs_url": "https://tsailintung.github.io/fastdid",
    "github_url": "https://github.com/TsaiLintung/fastdid",
    "url": "https://cran.r-project.org/package=fastdid",
    "install": "install.packages(\"fastdid\")",
    "tags": [
      "high-performance",
      "large-scale",
      "staggered-DiD",
      "time-varying-covariates",
      "fast-computation"
    ],
    "best_for": "Large-scale applications where standard did package is computationally prohibitive, with support for time-varying covariates",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "fastdid is a high-performance implementation of Callaway & Sant'Anna estimators optimized for large datasets. It is designed for researchers and practitioners who need to conduct causal inference analysis efficiently.",
    "use_cases": [
      "Estimating treatment effects in large observational studies",
      "Analyzing the impact of policy changes over time"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for causal inference",
      "how to implement Callaway & Sant'Anna estimators in R",
      "fast computation for DiD analysis in R",
      "large-scale causal inference R package",
      "time-varying covariates in R",
      "high-performance DiD estimators R"
    ],
    "primary_use_cases": [
      "causal forest estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "fect",
    "description": "Fixed Effects Counterfactual Estimators (v2.0+) incorporating gsynth functionality. Supports treatment switching on/off with carryover effects, matrix completion methods, and Rambachan & Roth sensitivity analysis for parallel trends violations.",
    "category": "Causal Inference (DiD)",
    "docs_url": "https://yiqingxu.org/packages/fect/",
    "github_url": "https://github.com/xuyiqing/fect",
    "url": "https://cran.r-project.org/package=fect",
    "install": "install.packages(\"fect\")",
    "tags": [
      "counterfactual",
      "matrix-completion",
      "interactive-fixed-effects",
      "sensitivity-analysis",
      "carryover"
    ],
    "best_for": "Counterfactual estimation with interactive fixed effects, treatment switching, and sensitivity analysis",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The fect package provides Fixed Effects Counterfactual Estimators that incorporate gsynth functionality, allowing for treatment switching and carryover effects. It is used by researchers and practitioners in causal inference, particularly in the context of DiD analyses.",
    "use_cases": [
      "Estimating treatment effects in observational studies",
      "Analyzing panel data with fixed effects"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for fixed effects counterfactual estimators",
      "how to perform sensitivity analysis in R",
      "R treatment switching analysis",
      "matrix completion methods in R",
      "R package for carryover effects",
      "R gsynth functionality"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "staggered",
    "description": "Provides the efficient estimator for randomized staggered rollout designs, offering optimal weighting schemes for treatment effect estimation. Also implements Callaway & Sant'Anna and Sun & Abraham estimators with design-based Fisher inference for randomized experiments.",
    "category": "Causal Inference (DiD)",
    "docs_url": "https://cran.r-project.org/web/packages/staggered/readme/README.html",
    "github_url": "https://github.com/jonathandroth/staggered",
    "url": "https://cran.r-project.org/package=staggered",
    "install": "install.packages(\"staggered\")",
    "tags": [
      "staggered-rollout",
      "randomized-experiments",
      "efficient-estimation",
      "event-study",
      "fisher-inference"
    ],
    "best_for": "Randomized experiments with staggered treatment timing where efficiency gains matter, implementing Roth & Sant'Anna (2023)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The staggered package provides efficient estimators for randomized staggered rollout designs, implementing optimal weighting schemes for treatment effect estimation. It is useful for researchers and practitioners conducting randomized experiments.",
    "use_cases": [
      "Estimating treatment effects in randomized staggered rollout designs",
      "Conducting event studies with optimal weighting schemes"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for staggered rollout designs",
      "how to estimate treatment effects in R",
      "efficient estimation for randomized experiments in R",
      "Callaway & Sant'Anna estimator R package",
      "Sun & Abraham estimator in R",
      "Fisher inference for randomized experiments R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "DTRreg",
    "description": "Dynamic treatment regime estimation via G-estimation for sequential treatment decisions. Implements methods for finding optimal treatment rules that adapt over time based on patient characteristics.",
    "category": "Causal Inference (Dynamic Treatment)",
    "docs_url": "https://cran.r-project.org/web/packages/DTRreg/DTRreg.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=DTRreg",
    "install": "install.packages(\"DTRreg\")",
    "tags": [
      "dynamic-treatment",
      "G-estimation",
      "sequential-decisions",
      "optimal-treatment",
      "personalization"
    ],
    "best_for": "Dynamic treatment regime estimation via G-estimation for sequential treatment decisions",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "DTRreg is a package for estimating dynamic treatment regimes using G-estimation for sequential treatment decisions. It is used by researchers and practitioners in causal inference to find optimal treatment rules that adapt over time based on patient characteristics.",
    "use_cases": [
      "Estimating optimal treatment strategies in clinical trials",
      "Adapting treatment plans based on patient responses"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for dynamic treatment regimes",
      "how to perform G-estimation in R",
      "sequential treatment decisions in R",
      "optimal treatment rules R package",
      "personalization in treatment decisions R",
      "dynamic treatment estimation R"
    ],
    "primary_use_cases": [
      "dynamic treatment estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "DynTxRegime",
    "description": "Comprehensive package for dynamic treatment regimes implementing Q-learning, value search, and outcome-weighted learning methods. Accompanies the textbook 'Dynamic Treatment Regimes' (Tsiatis et al., 2020).",
    "category": "Causal Inference (Dynamic Treatment)",
    "docs_url": "https://cran.r-project.org/web/packages/DynTxRegime/DynTxRegime.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=DynTxRegime",
    "install": "install.packages(\"DynTxRegime\")",
    "tags": [
      "dynamic-treatment",
      "Q-learning",
      "value-search",
      "reinforcement-learning",
      "personalized-medicine"
    ],
    "best_for": "Comprehensive dynamic treatment regimes with Q-learning and value search, from Tsiatis et al. (2020) textbook",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "DynTxRegime is a comprehensive R package designed for implementing dynamic treatment regimes using Q-learning, value search, and outcome-weighted learning methods. It is particularly useful for researchers and practitioners in the field of personalized medicine.",
    "use_cases": [],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for dynamic treatment regimes",
      "how to implement Q-learning in R",
      "outcome-weighted learning methods in R",
      "personalized medicine R package",
      "value search in dynamic treatment",
      "causal inference R tools"
    ],
    "primary_use_cases": [
      "dynamic treatment regimes",
      "Q-learning implementation"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Tsiatis et al. (2020)",
    "maintenance_status": "active"
  },
  {
    "name": "eventstudyr",
    "description": "Implements event study best practices from Freyaldenhoven et al. (2021) including sup-t confidence bands for uniform inference and formal pre-trend testing. Provides robust methods for dynamic treatment effect estimation.",
    "category": "Causal Inference (Event Study)",
    "docs_url": "https://cran.r-project.org/web/packages/eventstudyr/eventstudyr.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=eventstudyr",
    "install": "install.packages(\"eventstudyr\")",
    "tags": [
      "event-study",
      "pre-trends",
      "sup-t-bands",
      "uniform-inference",
      "dynamic-effects"
    ],
    "best_for": "Event study best practices with sup-t confidence bands and formal pre-trend testing, implementing Freyaldenhoven et al. (2021)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "event-study"
    ],
    "summary": "The eventstudyr package implements event study best practices, including sup-t confidence bands for uniform inference and formal pre-trend testing. It is designed for researchers and practitioners interested in dynamic treatment effect estimation.",
    "use_cases": [
      "Analyzing the impact of a policy change on stock prices",
      "Evaluating the effect of a new marketing campaign on sales"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for event study analysis",
      "how to implement pre-trend testing in R",
      "event study methods in R",
      "dynamic treatment effects R package",
      "sup-t confidence bands R",
      "uniform inference in R"
    ],
    "primary_use_cases": [
      "dynamic treatment effect estimation"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Freyaldenhoven et al. (2021)",
    "maintenance_status": "active"
  },
  {
    "name": "fixes",
    "description": "Streamlined event study workflows with simple run_es() and plot_es() functions built on fixest. New 2025 package providing convenient wrappers for common event study specifications.",
    "category": "Causal Inference (Event Study)",
    "docs_url": "https://cran.r-project.org/web/packages/fixes/fixes.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=fixes",
    "install": "install.packages(\"fixes\")",
    "tags": [
      "event-study",
      "fixest",
      "DiD",
      "streamlined",
      "visualization"
    ],
    "best_for": "Streamlined event study workflows with simple run_es() and plot_es() functions on fixest",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "event-study"
    ],
    "summary": "The 'fixes' package streamlines event study workflows by providing simple functions like run_es() and plot_es() built on the fixest framework. It is designed for users looking for convenient wrappers for common event study specifications.",
    "use_cases": [
      "Analyzing the impact of a policy change on stock prices",
      "Visualizing event study results for academic research"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "R package for event study",
      "how to conduct event studies in R",
      "fixest event study tutorial",
      "visualization of event studies in R",
      "streamlined event study workflows R",
      "event study analysis with fixes package"
    ],
    "api_complexity": "simple",
    "framework_compatibility": [
      "fixest"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "inferference",
    "description": "Computes inverse probability weighted (IPW) causal effects under partial interference following Tchetgen Tchetgen and VanderWeele (2012). Handles spillover effects within groups while maintaining independence across groups.",
    "category": "Causal Inference (Interference)",
    "docs_url": "https://cran.r-project.org/web/packages/inferference/inferference.pdf",
    "github_url": "https://github.com/bsaul/inferference",
    "url": "https://cran.r-project.org/package=inferference",
    "install": "install.packages(\"inferference\")",
    "tags": [
      "interference",
      "spillovers",
      "IPW",
      "partial-interference",
      "SUTVA-violations"
    ],
    "best_for": "IPW causal effects under partial interference with within-group spillovers, implementing Tchetgen Tchetgen & VanderWeele (2012)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The 'inferference' package computes inverse probability weighted (IPW) causal effects under partial interference, addressing spillover effects within groups while maintaining independence across groups. It is useful for researchers and practitioners in causal inference who are dealing with complex group interactions.",
    "use_cases": [],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R library for causal inference",
      "how to compute IPW causal effects in R",
      "spillover effects analysis in R",
      "partial interference methods in R",
      "Tchetgen Tchetgen and VanderWeele causal effects",
      "R package for handling SUTVA violations"
    ],
    "primary_use_cases": [
      "causal effect estimation under partial interference",
      "analyzing spillover effects"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Tchetgen Tchetgen and VanderWeele (2012)",
    "maintenance_status": "active"
  },
  {
    "name": "latenetwork",
    "description": "Handles both noncompliance AND network interference of unknown form following Hoshino and Yanagi (2023 JASA). Provides valid inference when treatment effects spill over through network connections.",
    "category": "Causal Inference (Interference)",
    "docs_url": "https://cran.r-project.org/web/packages/latenetwork/latenetwork.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=latenetwork",
    "install": "install.packages(\"latenetwork\")",
    "tags": [
      "network-interference",
      "noncompliance",
      "LATE",
      "spillovers",
      "IV"
    ],
    "best_for": "LATE estimation with network interference and noncompliance, implementing Hoshino & Yanagi (2023 JASA)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "network-interference"
    ],
    "summary": "The latenetwork package handles noncompliance and network interference of unknown form, providing valid inference when treatment effects spill over through network connections. It is useful for researchers and practitioners in causal inference who need to account for these complexities in their analyses.",
    "use_cases": [
      "Analyzing treatment effects in social networks",
      "Estimating causal effects in experimental designs with noncompliance"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for network interference",
      "how to handle noncompliance in R",
      "R causal inference package",
      "spillover effects analysis in R",
      "network effects in treatment studies",
      "Hoshino and Yanagi causal inference R package"
    ],
    "primary_use_cases": [
      "valid inference in causal studies",
      "analyzing spillover effects"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Hoshino and Yanagi (2023)",
    "maintenance_status": "active"
  },
  {
    "name": "EValue",
    "description": "Conducts sensitivity analyses for unmeasured confounding, selection bias, and measurement error in observational studies and meta-analyses. Computes E-values representing the minimum strength of association unmeasured confounders would need to fully explain away an observed effect.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://louisahsmith.github.io/evalue/",
    "github_url": "https://github.com/mayamathur/evalue_package",
    "url": "https://cran.r-project.org/package=EValue",
    "install": "install.packages(\"EValue\")",
    "tags": [
      "E-value",
      "unmeasured-confounding",
      "sensitivity-analysis",
      "selection-bias",
      "meta-analysis"
    ],
    "best_for": "Quantifying the minimum confounding strength on the risk ratio scale needed to explain away observed treatment-outcome associations, implementing VanderWeele & Ding (2017)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "EValue conducts sensitivity analyses for unmeasured confounding, selection bias, and measurement error in observational studies and meta-analyses. It computes E-values that represent the minimum strength of association unmeasured confounders would need to fully explain away an observed effect.",
    "use_cases": [
      "Assessing the impact of unmeasured confounding in observational studies",
      "Evaluating selection bias in meta-analyses"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for sensitivity analysis",
      "how to compute E-values in R",
      "unmeasured confounding analysis R",
      "selection bias analysis in R",
      "meta-analysis tools in R",
      "E-value computation in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "SuperLearner",
    "description": "Implements the Super Learner algorithm for optimal ensemble prediction via cross-validation. Creates weighted combinations of multiple ML algorithms (XGBoost, Random Forest, glmnet, neural networks, SVM, BART) with guaranteed asymptotic optimality.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://cran.r-project.org/web/packages/SuperLearner/vignettes/Guide-to-SuperLearner.html",
    "github_url": "https://github.com/ecpolley/SuperLearner",
    "url": "https://cran.r-project.org/package=SuperLearner",
    "install": "install.packages(\"SuperLearner\")",
    "tags": [
      "ensemble-learning",
      "cross-validation",
      "stacking",
      "prediction",
      "model-selection"
    ],
    "best_for": "Building optimal prediction ensembles for nuisance parameter estimation (propensity scores, outcome models) in causal inference, implementing van der Laan, Polley & Hubbard (2007)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "ensemble-learning"
    ],
    "summary": "SuperLearner implements the Super Learner algorithm for optimal ensemble prediction through cross-validation. It creates weighted combinations of various machine learning algorithms, making it suitable for users looking to improve prediction accuracy.",
    "use_cases": [
      "Combining predictions from multiple machine learning models",
      "Improving prediction accuracy in complex datasets"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for ensemble learning",
      "how to implement Super Learner in R",
      "cross-validation techniques in R",
      "optimal ensemble prediction R package",
      "weighted combinations of ML algorithms R",
      "using SuperLearner for model selection"
    ],
    "primary_use_cases": [
      "model-selection"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "caret",
      "mlr"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "causalToolbox",
    "description": "Implements meta-learner algorithms (S-learner, T-learner, X-learner) for heterogeneous treatment effect estimation using flexible base learners including honest Random Forests and BART for personalized CATE estimation.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://rdrr.io/github/soerenkuenzel/causalToolbox/",
    "github_url": "https://github.com/forestry-labs/causalToolbox",
    "url": "https://github.com/forestry-labs/causalToolbox",
    "install": "devtools::install_github(\"forestry-labs/causalToolbox\")",
    "tags": [
      "metalearners",
      "X-learner",
      "T-learner",
      "S-learner",
      "CATE"
    ],
    "best_for": "Comparing and benchmarking different CATE meta-learner strategies (S/T/X-learner) with BART or RF base learners, implementing K\u00fcnzel et al. (2019)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "causalToolbox implements meta-learner algorithms for estimating heterogeneous treatment effects using flexible base learners. It is useful for researchers and practitioners in causal inference looking to personalize treatment effect estimation.",
    "use_cases": [],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for causal inference",
      "how to estimate treatment effects in R",
      "R meta-learner algorithms",
      "personalized CATE estimation in R",
      "using Random Forests for causal inference",
      "BART for treatment effect estimation"
    ],
    "primary_use_cases": [
      "heterogeneous treatment effect estimation",
      "personalized CATE estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "causalweight",
    "description": "Semiparametric causal inference methods based on inverse probability weighting and double machine learning for average treatment effects, causal mediation analysis (direct/indirect effects), and dynamic treatment evaluation. Supports LATE estimation with instrumental variables.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://cran.r-project.org/web/packages/causalweight/causalweight.pdf",
    "github_url": "https://github.com/hbodory/causalweight",
    "url": "https://cran.r-project.org/package=causalweight",
    "install": "install.packages(\"causalweight\")",
    "tags": [
      "inverse-probability-weighting",
      "causal-mediation",
      "double-machine-learning",
      "LATE",
      "instrumental-variables"
    ],
    "best_for": "Mediation analysis and LATE estimation using weighting-based approaches with flexible nuisance estimation, implementing Huber (2014) and Fr\u00f6lich & Huber (2017)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The causalweight package provides semiparametric causal inference methods using inverse probability weighting and double machine learning techniques. It is designed for researchers and practitioners interested in estimating average treatment effects, causal mediation analysis, and dynamic treatment evaluation.",
    "use_cases": [
      "Estimating average treatment effects in clinical trials",
      "Conducting causal mediation analysis in social sciences"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R library for causal inference",
      "how to perform causal mediation analysis in R",
      "R package for double machine learning",
      "LATE estimation in R",
      "inverse probability weighting in R",
      "instrumental variables in R"
    ],
    "primary_use_cases": [
      "average treatment effects estimation",
      "causal mediation analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "ddml",
    "description": "Streamlined double/debiased machine learning estimation with emphasis on (short-)stacking to combine multiple base learners, increasing robustness to unknown data generating processes. Designed as a complement to DoubleML with simpler syntax.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://thomaswiemann.com/ddml/",
    "github_url": "https://github.com/thomaswiemann/ddml",
    "url": "https://cran.r-project.org/package=ddml",
    "install": "install.packages(\"ddml\")",
    "tags": [
      "double-machine-learning",
      "stacking",
      "model-averaging",
      "treatment-effects",
      "causal-inference"
    ],
    "best_for": "Quick, robust DML estimation using short-stacking to ensemble multiple ML learners without extensive tuning, implementing Ahrens, Hansen, Schaffer & Wiemann (2024)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "ddml is a package designed for streamlined double/debiased machine learning estimation, focusing on stacking to enhance robustness against unknown data generating processes. It is particularly useful for researchers and practitioners in causal inference who seek a simpler syntax compared to DoubleML.",
    "use_cases": [
      "Estimating treatment effects in observational studies",
      "Combining multiple machine learning models for improved predictions"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for double machine learning",
      "how to perform causal inference in R",
      "stacking models in R",
      "treatment effects estimation in R",
      "double machine learning R tutorial",
      "robust machine learning methods R"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "treatment effect estimation"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "DoubleML"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "grf",
    "description": "Forest-based statistical estimation and inference for heterogeneous treatment effects, supporting multiple treatment arms, instrumental variables, survival outcomes, and quantile regression\u2014all with honest estimation and valid confidence intervals. The most widely-used R package for CATE estimation.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://grf-labs.github.io/grf/",
    "github_url": "https://github.com/grf-labs/grf",
    "url": "https://cran.r-project.org/package=grf",
    "install": "install.packages(\"grf\")",
    "tags": [
      "causal-forest",
      "heterogeneous-treatment-effects",
      "CATE",
      "machine-learning",
      "econometrics"
    ],
    "best_for": "Estimating individual-level treatment effects (CATE) with valid statistical inference in RCTs or observational studies, implementing Athey, Tibshirani & Wager (2019)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "econometrics"
    ],
    "summary": "The 'grf' package provides forest-based statistical estimation and inference for heterogeneous treatment effects, making it suitable for various complex scenarios such as multiple treatment arms and instrumental variables. It is widely used by researchers and practitioners in fields like econometrics and machine learning for causal analysis.",
    "use_cases": [
      "Estimating causal effects in clinical trials",
      "Analyzing treatment effects in observational studies"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for causal forest",
      "how to estimate treatment effects in R",
      "R package for heterogeneous treatment effects",
      "using grf for CATE estimation",
      "forest-based inference in R",
      "R package for quantile regression"
    ],
    "primary_use_cases": [
      "causal forest estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "hdm",
    "description": "High-dimensional statistical methods featuring heteroscedasticity-robust LASSO with theoretically-grounded penalty selection, post-double-selection inference, and treatment effect estimation under sparsity assumptions for high-dimensional controls.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://cran.r-project.org/web/packages/hdm/vignettes/hdm.html",
    "github_url": "https://github.com/MartinSpindler/hdm",
    "url": "https://cran.r-project.org/package=hdm",
    "install": "install.packages(\"hdm\")",
    "tags": [
      "lasso",
      "post-double-selection",
      "high-dimensional",
      "instrumental-variables",
      "sparsity"
    ],
    "best_for": "Post-double-selection LASSO inference and treatment effect estimation when the true model is sparse, implementing Belloni, Chernozhukov & Hansen (2014)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "high-dimensional"
    ],
    "summary": "The hdm package provides high-dimensional statistical methods that focus on heteroscedasticity-robust LASSO, enabling users to perform post-double-selection inference and treatment effect estimation under sparsity assumptions. It is primarily used by researchers and practitioners in causal inference and statistics.",
    "use_cases": [
      "Estimating treatment effects in high-dimensional settings",
      "Conducting post-double-selection inference for causal analysis"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for high-dimensional LASSO",
      "how to perform treatment effect estimation in R",
      "heteroscedasticity-robust methods in R",
      "post-double-selection inference R package",
      "sparsity assumptions in causal inference",
      "instrumental variables in high-dimensional settings"
    ],
    "primary_use_cases": [
      "treatment effect estimation",
      "post-double-selection inference"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "ltmle",
    "description": "Targeted maximum likelihood estimation for treatment/censoring-specific mean outcomes with time-varying treatments and confounders. Supports longitudinal settings, marginal structural models, and dynamic treatment regimes alongside IPTW and G-computation.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://joshuaschwab.github.io/ltmle/",
    "github_url": "https://github.com/joshuaschwab/ltmle",
    "url": "https://cran.r-project.org/package=ltmle",
    "install": "install.packages(\"ltmle\")",
    "tags": [
      "TMLE",
      "longitudinal",
      "time-varying-treatment",
      "dynamic-regimes",
      "MSM"
    ],
    "best_for": "Causal inference with time-varying treatments, time-varying confounders, and right-censored longitudinal data, implementing Lendle et al. (2017)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "longitudinal-data"
    ],
    "summary": "The ltmle package provides targeted maximum likelihood estimation for treatment and censoring-specific mean outcomes in longitudinal settings. It is particularly useful for researchers dealing with time-varying treatments and confounders in causal inference.",
    "use_cases": [
      "Estimating treatment effects in clinical trials",
      "Analyzing longitudinal health data"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for targeted maximum likelihood estimation",
      "how to use ltmle for longitudinal data analysis",
      "time-varying treatment analysis in R",
      "causal inference with ltmle",
      "dynamic treatment regimes in R",
      "marginal structural models R package"
    ],
    "primary_use_cases": [
      "causal forest estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "sensemakr",
    "description": "Suite of sensitivity analysis tools extending the traditional omitted variable bias framework, computing robustness values, bias-adjusted estimates, and sensitivity contour plots for OLS regression to assess how strong unmeasured confounders would need to be to overturn conclusions.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://carloscinelli.com/sensemakr/",
    "github_url": "https://github.com/carloscinelli/sensemakr",
    "url": "https://cran.r-project.org/package=sensemakr",
    "install": "install.packages(\"sensemakr\")",
    "tags": [
      "sensitivity-analysis",
      "omitted-variable-bias",
      "robustness-value",
      "causal-inference",
      "regression"
    ],
    "best_for": "Assessing how strong unmeasured confounders would need to be to overturn regression-based causal conclusions, implementing Cinelli & Hazlett (2020)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "sensitivity-analysis",
      "regression"
    ],
    "summary": "sensemakr is a suite of sensitivity analysis tools that extends the traditional omitted variable bias framework. It computes robustness values, bias-adjusted estimates, and sensitivity contour plots for OLS regression to evaluate the impact of unmeasured confounders on conclusions.",
    "use_cases": [
      "Assessing the robustness of regression results",
      "Evaluating the impact of unmeasured confounders on study conclusions"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for sensitivity analysis",
      "how to assess omitted variable bias in R",
      "tools for robustness analysis in regression",
      "sensitivity contour plots in R",
      "bias-adjusted estimates in R",
      "methods for causal inference in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "tmle",
    "description": "Implements targeted maximum likelihood estimation for point treatment effects with binary or continuous outcomes. Estimates ATE, ATT, ATC, and supports marginal structural models. Integrates SuperLearner for data-adaptive nuisance parameter estimation.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://cran.r-project.org/web/packages/tmle/tmle.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=tmle",
    "install": "install.packages(\"tmle\")",
    "tags": [
      "TMLE",
      "causal-inference",
      "ATE",
      "doubly-robust",
      "propensity-score"
    ],
    "best_for": "Estimating point treatment effects (ATE/ATT/ATC) in observational studies with binary treatments, implementing Gruber & van der Laan (2012)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The tmle package implements targeted maximum likelihood estimation for point treatment effects, supporting both binary and continuous outcomes. It is used by statisticians and data scientists working on causal inference problems.",
    "use_cases": [
      "Estimating average treatment effects in clinical trials",
      "Analyzing the impact of a new policy on health outcomes"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for targeted maximum likelihood estimation",
      "how to estimate ATE in R",
      "R package for causal inference",
      "tmle R documentation",
      "how to use SuperLearner in R",
      "tmle for binary outcomes",
      "tmle for continuous outcomes"
    ],
    "primary_use_cases": [
      "estimating average treatment effects (ATE)",
      "supporting marginal structural models"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "tmle3",
    "description": "A modular, extensible framework for targeted minimum loss-based estimation supporting custom TMLE parameters through a unified interface. Part of the tlverse ecosystem, designed to be as general as the mathematical TMLE framework itself for complex analyses.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://tlverse.org/tmle3/",
    "github_url": "https://github.com/tlverse/tmle3",
    "url": "https://github.com/tlverse/tmle3",
    "install": "remotes::install_github(\"tlverse/tmle3\")",
    "tags": [
      "TMLE",
      "tlverse",
      "modular",
      "extensible",
      "stochastic-interventions"
    ],
    "best_for": "Complex TMLE analyses requiring custom parameters, mediation, stochastic interventions, or optimal treatment regimes",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "tmle3 is a modular and extensible framework designed for targeted minimum loss-based estimation, allowing users to support custom TMLE parameters through a unified interface. It is part of the tlverse ecosystem and is suitable for complex analyses in causal inference.",
    "use_cases": [],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for targeted minimum loss-based estimation",
      "how to use tmle3 for causal inference",
      "tmle3 examples in R",
      "tlverse ecosystem packages",
      "modular framework for TMLE in R",
      "tmle3 documentation"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "tlverse"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "CBPS",
    "description": "Implements Covariate Balancing Propensity Score, which estimates propensity scores by jointly optimizing treatment prediction and covariate balance via generalized method of moments (GMM). Supports binary, multi-valued, and continuous treatments, as well as longitudinal settings for marginal structural models.",
    "category": "Causal Inference (Matching)",
    "docs_url": "https://cran.r-project.org/web/packages/CBPS/CBPS.pdf",
    "github_url": "https://github.com/kosukeimai/CBPS",
    "url": "https://cran.r-project.org/package=CBPS",
    "install": "install.packages(\"CBPS\")",
    "tags": [
      "propensity-score",
      "covariate-balance",
      "GMM",
      "weighting",
      "treatment-effects"
    ],
    "best_for": "When propensity score model specification is uncertain and you want simultaneous balance optimization, implementing Imai & Ratkovic (2014)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "CBPS implements the Covariate Balancing Propensity Score method, which estimates propensity scores by optimizing treatment prediction and covariate balance using generalized method of moments (GMM). It is useful for researchers and practitioners in causal inference who need to analyze treatment effects.",
    "use_cases": [
      "Estimating treatment effects in observational studies",
      "Balancing covariates in experimental designs"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for covariate balancing",
      "how to estimate propensity scores in R",
      "R package for treatment effects analysis",
      "propensity score matching in R",
      "GMM for treatment prediction R",
      "longitudinal treatment effects R"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "MatchIt",
    "description": "Comprehensive matching package that selects matched samples of treated and control groups with similar covariate distributions. Provides a unified interface to multiple matching methods including nearest neighbor, optimal pair, optimal full, genetic, exact, coarsened exact (CEM), cardinality matching, and subclassification with propensity score estimation via GLM, GAM, random forest, and BART.",
    "category": "Causal Inference (Matching)",
    "docs_url": "https://kosukeimai.github.io/MatchIt/",
    "github_url": "https://github.com/kosukeimai/MatchIt",
    "url": "https://cran.r-project.org/package=MatchIt",
    "install": "install.packages(\"MatchIt\")",
    "tags": [
      "propensity-score-matching",
      "causal-inference",
      "observational-studies",
      "covariate-balance",
      "treatment-effects"
    ],
    "best_for": "Preprocessing observational data via matching to reduce confounding before estimating causal treatment effects, implementing Ho et al. (2007, 2011)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "MatchIt is a comprehensive matching package that selects matched samples of treated and control groups with similar covariate distributions. It provides a unified interface to multiple matching methods, making it useful for researchers conducting observational studies.",
    "use_cases": [
      "Selecting matched samples for treatment studies",
      "Improving covariate balance in observational research"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for propensity score matching",
      "how to perform matching in R",
      "matching methods in R",
      "causal inference with R",
      "observational studies R package",
      "covariate balance in R"
    ],
    "primary_use_cases": [
      "propensity score matching",
      "treatment effect estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "WeightIt",
    "description": "Unified interface for generating balancing weights for causal effect estimation in observational studies. Supports binary, multi-category, and continuous treatments for point and longitudinal/marginal structural models. Methods include inverse probability weighting (IPW), entropy balancing, covariate balancing propensity score (CBPS), energy balancing, stable balancing weights, BART, and SuperLearner.",
    "category": "Causal Inference (Matching)",
    "docs_url": "https://ngreifer.github.io/WeightIt/",
    "github_url": "https://github.com/ngreifer/WeightIt",
    "url": "https://cran.r-project.org/package=WeightIt",
    "install": "install.packages(\"WeightIt\")",
    "tags": [
      "propensity-score-weighting",
      "inverse-probability-weighting",
      "entropy-balancing",
      "CBPS",
      "marginal-structural-models"
    ],
    "best_for": "Generating balancing weights using modern weighting methods (IPW, entropy balancing, CBPS, etc.) for point or longitudinal treatments",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "WeightIt provides a unified interface for generating balancing weights used in causal effect estimation for observational studies. It is utilized by researchers and practitioners in fields such as statistics and data science to ensure balanced treatment groups in various types of analyses.",
    "use_cases": [
      "Estimating treatment effects in observational studies",
      "Creating balanced datasets for analysis"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for causal effect estimation",
      "how to generate balancing weights in R",
      "R library for inverse probability weighting",
      "entropy balancing in R",
      "propensity score weighting R",
      "marginal structural models in R"
    ],
    "primary_use_cases": [
      "causal effect estimation",
      "balancing weights generation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "cobalt",
    "description": "Generates standardized balance tables and plots for covariates after preprocessing via matching, weighting, or subclassification. Provides unified balance assessment across multiple R packages (MatchIt, WeightIt, twang, Matching, optmatch, CBPS, ebal, cem, sbw, designmatch). Supports multi-category, continuous, and longitudinal treatments with clustered and multiply imputed data.",
    "category": "Causal Inference (Matching)",
    "docs_url": "https://ngreifer.github.io/cobalt/",
    "github_url": "https://github.com/ngreifer/cobalt",
    "url": "https://cran.r-project.org/package=cobalt",
    "install": "install.packages(\"cobalt\")",
    "tags": [
      "covariate-balance",
      "balance-diagnostics",
      "love-plot",
      "standardized-mean-difference",
      "balance-tables"
    ],
    "best_for": "Assessing and visualizing covariate balance before/after matching or weighting to validate causal inference preprocessing",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The cobalt package generates standardized balance tables and plots for covariates after preprocessing via matching, weighting, or subclassification. It is used by researchers and practitioners in causal inference to assess balance across multiple R packages.",
    "use_cases": [
      "Assessing covariate balance after matching",
      "Visualizing balance diagnostics for treatment groups"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for covariate balance",
      "how to assess balance in causal inference R",
      "R balance diagnostics tools",
      "generate love plot in R",
      "standardized mean difference in R",
      "R package for matching and weighting"
    ],
    "primary_use_cases": [
      "balance assessment across multiple R packages",
      "generating balance tables and plots"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "MatchIt",
      "WeightIt",
      "twang",
      "Matching",
      "optmatch"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "ebal",
    "description": "Implements entropy balancing, a reweighting method that finds weights for control units such that specified covariate moment conditions (means, variances) are exactly satisfied while staying as close as possible to uniform weights by minimizing Kullback-Leibler divergence. Primarily designed for ATT estimation.",
    "category": "Causal Inference (Matching)",
    "docs_url": "https://cran.r-project.org/web/packages/ebal/ebal.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=ebal",
    "install": "install.packages(\"ebal\")",
    "tags": [
      "entropy-balancing",
      "reweighting",
      "covariate-balance",
      "observational-studies",
      "ATT"
    ],
    "best_for": "When you need exact covariate balance on specified moments (means, variances) with minimal weight dispersion, implementing Hainmueller (2012)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The ebal package implements entropy balancing, a reweighting method that finds weights for control units to satisfy specified covariate moment conditions while minimizing Kullback-Leibler divergence. It is primarily designed for Average Treatment Effect on the Treated (ATT) estimation.",
    "use_cases": [
      "Estimating treatment effects in observational studies",
      "Balancing covariates in experimental designs"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for entropy balancing",
      "how to perform reweighting in R",
      "R package for covariate balance",
      "entropy balancing for observational studies",
      "ATT estimation in R",
      "how to minimize Kullback-Leibler divergence in R",
      "R methods for causal inference",
      "reweighting methods in R"
    ],
    "primary_use_cases": [
      "ATT estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "optmatch",
    "description": "Distance-based bipartite matching using minimum cost network flow algorithms, oriented to matching treatment and control groups in observational studies. Provides optimal full matching and pair matching with support for propensity score distances, Mahalanobis distance, calipers, and exact matching constraints.",
    "category": "Causal Inference (Matching)",
    "docs_url": "https://markmfredrickson.github.io/optmatch",
    "github_url": "https://github.com/markmfredrickson/optmatch",
    "url": "https://cran.r-project.org/package=optmatch",
    "install": "install.packages(\"optmatch\")",
    "tags": [
      "optimal-matching",
      "propensity-score",
      "network-flow",
      "observational-studies",
      "full-matching"
    ],
    "best_for": "When you need mathematically optimal matching solutions that minimize total matched distance with flexible control:treatment ratios (full matching), implementing Hansen & Klopfer (2006)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The optmatch package provides distance-based bipartite matching using minimum cost network flow algorithms, specifically designed for matching treatment and control groups in observational studies. It is used by researchers and data scientists working in causal inference to achieve optimal matching.",
    "use_cases": [
      "Matching treatment and control groups in observational studies",
      "Conducting optimal full matching for causal analysis"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for optimal matching",
      "how to perform full matching in R",
      "distance-based matching in R",
      "matching treatment and control groups in R",
      "R library for network flow algorithms",
      "observational studies matching in R"
    ],
    "primary_use_cases": [
      "optimal full matching",
      "pair matching"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "CMAverse",
    "description": "Unified interface for six causal mediation approaches including traditional regression, inverse odds weighting, and g-formula. Supports multiple sequential mediators and exposure-mediator interactions.",
    "category": "Causal Inference (Mediation)",
    "docs_url": "https://bs1125.github.io/CMAverse/",
    "github_url": null,
    "url": "https://cran.r-project.org/package=CMAverse",
    "install": "install.packages(\"CMAverse\")",
    "tags": [
      "mediation",
      "g-formula",
      "multiple-mediators",
      "causal-mechanisms",
      "unified-interface"
    ],
    "best_for": "Unified causal mediation analysis with six approaches and multiple sequential mediators",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "CMAverse provides a unified interface for various causal mediation approaches, allowing users to analyze complex relationships involving multiple mediators and exposure-mediator interactions. It is useful for researchers and practitioners in causal inference who need to implement mediation analysis.",
    "use_cases": [
      "Analyzing the effect of a treatment on an outcome through multiple mediators",
      "Evaluating the impact of exposure-mediator interactions in a study"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for causal mediation analysis",
      "how to use CMAverse for mediation",
      "CMAverse examples",
      "causal mediation approaches in R",
      "R unified interface for mediation",
      "multiple mediators analysis in R",
      "g-formula in R",
      "inverse odds weighting in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "mediation",
    "description": "Estimates Average Causal Mediation Effects (ACME) with sensitivity analysis for unmeasured confounding. Implements Tingley et al. (2014 JSS) methods for understanding causal mechanisms.",
    "category": "Causal Inference (Mediation)",
    "docs_url": "https://cran.r-project.org/web/packages/mediation/mediation.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=mediation",
    "install": "install.packages(\"mediation\")",
    "tags": [
      "mediation",
      "ACME",
      "causal-mechanisms",
      "sensitivity-analysis",
      "indirect-effects"
    ],
    "best_for": "Average Causal Mediation Effects with sensitivity analysis, implementing Tingley et al. (2014 JSS)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The mediation package estimates Average Causal Mediation Effects (ACME) with sensitivity analysis for unmeasured confounding. It is used by researchers and practitioners interested in understanding causal mechanisms in their data.",
    "use_cases": [],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for causal mediation analysis",
      "how to estimate ACME in R",
      "sensitivity analysis for mediation effects in R",
      "understanding causal mechanisms R package"
    ],
    "primary_use_cases": [
      "estimating average causal mediation effects",
      "sensitivity analysis for unmeasured confounding"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Tingley et al. (2014)",
    "maintenance_status": "active"
  },
  {
    "name": "PStrata",
    "description": "Principal stratification analysis for noncompliance and truncation-by-death using both Bayesian (Stan) and frequentist estimation. Implements Liu and Li (2023) methods for causal inference with post-treatment complications.",
    "category": "Causal Inference (Principal Stratification)",
    "docs_url": "https://cran.r-project.org/web/packages/PStrata/PStrata.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=PStrata",
    "install": "install.packages(\"PStrata\")",
    "tags": [
      "principal-stratification",
      "noncompliance",
      "truncation-by-death",
      "Bayesian",
      "Stan"
    ],
    "best_for": "Principal stratification for noncompliance and truncation-by-death with Bayesian/frequentist estimation",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "bayesian"
    ],
    "summary": "PStrata is designed for principal stratification analysis, focusing on noncompliance and truncation-by-death. It utilizes both Bayesian and frequentist estimation methods, making it suitable for researchers dealing with causal inference in complex scenarios.",
    "use_cases": [
      "Analyzing noncompliance in clinical trials",
      "Estimating causal effects with truncation-by-death"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for principal stratification",
      "how to perform causal inference in R",
      "Bayesian analysis for noncompliance",
      "truncation-by-death analysis in R",
      "PStrata package usage",
      "methods for causal inference with post-treatment complications"
    ],
    "primary_use_cases": [
      "principal stratification analysis",
      "causal inference with post-treatment complications"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Liu and Li (2023)",
    "maintenance_status": "active"
  },
  {
    "name": "rddapp",
    "description": "Supports multi-assignment RDD with two running variables, power analysis for RDD designs, and includes a Shiny interface for interactive analysis. Handles both sharp and fuzzy designs with bandwidth selection.",
    "category": "Causal Inference (RDD)",
    "docs_url": "https://cran.r-project.org/web/packages/rddapp/rddapp.pdf",
    "github_url": "https://github.com/felixthoemmes/rddapp",
    "url": "https://cran.r-project.org/package=rddapp",
    "install": "install.packages(\"rddapp\")",
    "tags": [
      "RDD",
      "multi-assignment",
      "power-analysis",
      "Shiny",
      "fuzzy-RDD"
    ],
    "best_for": "Multi-assignment RDD with two running variables and power analysis with Shiny interface",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The rddapp package supports multi-assignment regression discontinuity designs (RDD) with two running variables and provides power analysis for RDD designs. It includes a Shiny interface for interactive analysis, making it useful for researchers and practitioners in causal inference.",
    "use_cases": [
      "Analyzing the impact of policy changes using RDD",
      "Conducting power analysis for experimental designs"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for multi-assignment RDD",
      "how to perform power analysis for RDD in R",
      "interactive analysis for RDD using Shiny",
      "fuzzy RDD analysis in R",
      "bandwidth selection for RDD",
      "causal inference tools in R"
    ],
    "primary_use_cases": [
      "multi-assignment RDD analysis",
      "power analysis for RDD designs"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "rddensity",
    "description": "Implements manipulation testing (density discontinuity testing) procedures using local polynomial density estimators to detect perfect self-selection around a cutoff. Provides rddensity() for hypothesis testing, rdbwdensity() for bandwidth selection, and rdplotdensity() for density plots with confidence bands.",
    "category": "Causal Inference (RDD)",
    "docs_url": "https://rdpackages.github.io/rddensity/",
    "github_url": "https://github.com/rdpackages/rddensity",
    "url": "https://cran.r-project.org/package=rddensity",
    "install": "install.packages(\"rddensity\")",
    "tags": [
      "manipulation-testing",
      "density-discontinuity",
      "McCrary-test",
      "falsification",
      "sorting"
    ],
    "best_for": "Testing RDD validity by detecting bunching/manipulation around the cutoff (McCrary-type tests), implementing Cattaneo, Jansson & Ma (2020)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "rddensity implements manipulation testing procedures using local polynomial density estimators to detect perfect self-selection around a cutoff. It is used primarily by researchers and practitioners in causal inference to test hypotheses and visualize density plots.",
    "use_cases": [
      "Testing for self-selection around a cutoff",
      "Visualizing density plots with confidence bands"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for manipulation testing",
      "how to perform density discontinuity testing in R",
      "R density plots with confidence bands",
      "local polynomial density estimators in R",
      "hypothesis testing for self-selection in R",
      "bandwidth selection in R for density tests"
    ],
    "primary_use_cases": [
      "density discontinuity testing",
      "hypothesis testing"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "rddtools",
    "description": "Regression discontinuity design toolkit with clustered inference for geographic discontinuities. Provides bandwidth selection, specification tests, and visualization tools.",
    "category": "Causal Inference (RDD)",
    "docs_url": "https://cran.r-project.org/web/packages/rddtools/rddtools.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=rddtools",
    "install": "install.packages(\"rddtools\")",
    "tags": [
      "RDD",
      "clustered-inference",
      "bandwidth-selection",
      "geographic-discontinuity",
      "visualization"
    ],
    "best_for": "RDD with clustered inference for geographic discontinuities",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The rddtools package provides a toolkit for regression discontinuity design, focusing on clustered inference for geographic discontinuities. It is useful for researchers and practitioners who need to perform causal inference analyses with a focus on bandwidth selection and visualization.",
    "use_cases": [
      "Analyzing the impact of policy changes at geographic boundaries",
      "Evaluating educational interventions using RDD"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for regression discontinuity design",
      "how to visualize geographic discontinuities in R",
      "bandwidth selection in RDD",
      "clustered inference in R",
      "specification tests for RDD",
      "tools for causal inference in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "rdlocrand",
    "description": "Provides tools for RD analysis under local randomization: rdrandinf() performs hypothesis testing using randomization inference, rdwinselect() selects a window around the cutoff where randomization likely holds, rdsensitivity() assesses sensitivity to different windows, and rdrbounds() constructs Rosenbaum bounds for unobserved confounders.",
    "category": "Causal Inference (RDD)",
    "docs_url": "https://rdpackages.github.io/rdlocrand/",
    "github_url": "https://github.com/rdpackages/rdlocrand",
    "url": "https://cran.r-project.org/package=rdlocrand",
    "install": "install.packages(\"rdlocrand\")",
    "tags": [
      "local-randomization",
      "randomization-inference",
      "finite-sample",
      "window-selection",
      "sensitivity-analysis"
    ],
    "best_for": "Finite-sample inference in RDD when local randomization assumption is plausible near the cutoff, implementing Cattaneo, Frandsen & Titiunik (2015)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The rdlocrand package provides tools for regression discontinuity analysis under local randomization. It is useful for researchers and practitioners who need to perform hypothesis testing and sensitivity analysis in causal inference settings.",
    "use_cases": [
      "Performing hypothesis testing using randomization inference",
      "Assessing sensitivity to different windows in regression discontinuity designs"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for randomization inference",
      "how to perform sensitivity analysis in R",
      "tools for regression discontinuity analysis in R",
      "R local randomization methods",
      "hypothesis testing with randomization in R",
      "window selection for RDD in R"
    ],
    "primary_use_cases": [
      "hypothesis testing",
      "sensitivity analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "rdmulti",
    "description": "Provides tools for RD designs with multiple cutoffs or scores: rdmc() estimates pooled and cutoff-specific effects in multi-cutoff designs, rdmcplot() draws RD plots for multi-cutoff designs, and rdms() estimates effects in cumulative cutoffs or multi-score (geographic/boundary) designs.",
    "category": "Causal Inference (RDD)",
    "docs_url": "https://rdpackages.github.io/rdmulti/",
    "github_url": "https://github.com/rdpackages/rdmulti",
    "url": "https://cran.r-project.org/package=rdmulti",
    "install": "install.packages(\"rdmulti\")",
    "tags": [
      "multiple-cutoffs",
      "multi-score",
      "geographic-RD",
      "pooled-effects",
      "extrapolation"
    ],
    "best_for": "RDD with multiple cutoffs (e.g., different thresholds across regions) or multiple running variables (geographic boundaries), implementing Cattaneo, Titiunik & Vazquez-Bare (2020)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The rdmulti package provides tools for regression discontinuity designs with multiple cutoffs or scores. It is useful for researchers and practitioners in causal inference who need to estimate effects in complex RD designs.",
    "use_cases": [
      "Estimating pooled effects in multi-cutoff designs",
      "Visualizing regression discontinuity plots for multiple cutoffs"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for multiple cutoffs",
      "how to estimate effects in multi-score designs",
      "tools for regression discontinuity designs in R",
      "rdmc() function usage",
      "creating RD plots with rdmcplot()",
      "cumulative cutoffs analysis in R",
      "geographic RD analysis tools"
    ],
    "primary_use_cases": [
      "estimating effects in multi-cutoff designs",
      "drawing RD plots for multi-cutoff designs"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "rdpower",
    "description": "Provides tools for power, sample size, and minimum detectable effects (MDE) calculations in RD designs using robust bias-corrected local polynomial inference: rdpower() calculates power, rdsampsi() calculates required sample size for desired power, and rdmde() computes minimum detectable effects.",
    "category": "Causal Inference (RDD)",
    "docs_url": "https://rdpackages.github.io/rdpower/",
    "github_url": "https://github.com/rdpackages/rdpower",
    "url": "https://cran.r-project.org/package=rdpower",
    "install": "install.packages(\"rdpower\")",
    "tags": [
      "power-analysis",
      "sample-size",
      "MDE",
      "study-design",
      "ex-ante-analysis"
    ],
    "best_for": "Planning RDD studies\u2014calculating required sample sizes, statistical power, or minimum detectable effects, implementing Cattaneo, Titiunik & Vazquez-Bare (2019)",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The rdpower package provides tools for power, sample size, and minimum detectable effects calculations in regression discontinuity designs. It is useful for researchers and practitioners working in causal inference who need to determine the necessary sample sizes and power for their studies.",
    "use_cases": [
      "Calculating power for a regression discontinuity design study",
      "Determining sample size needed for desired power in an RD analysis"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "R package for power analysis",
      "how to calculate sample size in R",
      "minimum detectable effects in R",
      "tools for study design in R",
      "power calculations for RD designs",
      "rdpower package documentation",
      "R package for ex-ante analysis"
    ],
    "primary_use_cases": [
      "power calculation",
      "sample size determination",
      "minimum detectable effects computation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "SCtools",
    "description": "Automates placebo tests and multi-treated-unit ATT calculations for synthetic control. Provides utilities for generating in-space and in-time placebos with visualization.",
    "category": "Causal Inference (Synthetic Control)",
    "docs_url": "https://cran.r-project.org/web/packages/SCtools/SCtools.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=SCtools",
    "install": "install.packages(\"SCtools\")",
    "tags": [
      "synthetic-control",
      "placebo-tests",
      "multi-unit",
      "ATT",
      "visualization"
    ],
    "best_for": "Automated placebo tests and multi-treated-unit ATT calculations for synthetic control",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "visualization"
    ],
    "summary": "SCtools automates placebo tests and multi-treated-unit ATT calculations for synthetic control. It provides utilities for generating in-space and in-time placebos with visualization, making it useful for researchers in causal inference.",
    "use_cases": [
      "Conducting placebo tests for causal inference studies",
      "Calculating ATT for multiple treated units"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for synthetic control",
      "how to perform placebo tests in R",
      "multi-treated-unit ATT calculations in R",
      "visualization tools for synthetic control",
      "automate placebo tests in R",
      "R utilities for synthetic control"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "Synth",
    "description": "The original synthetic control method implementation for comparative case studies. Constructs a weighted combination of comparison units to create a synthetic counterfactual for estimating effects of interventions on a single treated unit, as used in seminal studies of California tobacco program and German reunification.",
    "category": "Causal Inference (Synthetic Control)",
    "docs_url": "https://web.stanford.edu/~jhain/",
    "github_url": "https://github.com/cran/Synth",
    "url": "https://cran.r-project.org/package=Synth",
    "install": "install.packages(\"Synth\")",
    "tags": [
      "synthetic-control",
      "comparative-case-studies",
      "counterfactual",
      "policy-evaluation",
      "single-unit-treatment"
    ],
    "best_for": "Classic single-treated-unit policy evaluations, implementing Abadie, Diamond & Hainmueller (2010, 2011, 2015)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "comparative-case-studies",
      "policy-evaluation"
    ],
    "summary": "Synth is an implementation of the synthetic control method for comparative case studies. It constructs a synthetic counterfactual to estimate the effects of interventions on a single treated unit, making it useful for researchers in policy evaluation.",
    "use_cases": [
      "Evaluating the impact of the California tobacco program",
      "Analyzing the effects of German reunification"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R library for synthetic control",
      "how to perform comparative case studies in R",
      "synthetic counterfactual analysis R",
      "policy evaluation methods in R",
      "single unit treatment effects R",
      "synthetic control method tutorial"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "augsynth",
    "description": "Implements the Augmented Synthetic Control Method, which uses an outcome model (ridge regression by default) to correct for bias when pre-treatment fit is imperfect. Uniquely supports staggered adoption across multiple treated units via multisynth() function.",
    "category": "Causal Inference (Synthetic Control)",
    "docs_url": "https://github.com/ebenmichael/augsynth/blob/master/vignettes/singlesynth-vignette.md",
    "github_url": "https://github.com/ebenmichael/augsynth",
    "url": "https://github.com/ebenmichael/augsynth",
    "install": "devtools::install_github(\"ebenmichael/augsynth\")",
    "tags": [
      "augmented-synthetic-control",
      "bias-correction",
      "staggered-adoption",
      "ridge-regression",
      "imperfect-fit"
    ],
    "best_for": "SC applications with imperfect pre-treatment fit or staggered adoption across units, implementing Ben-Michael, Feller & Rothstein (2021)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "AugSynth implements the Augmented Synthetic Control Method to correct for bias in pre-treatment fit using an outcome model. It is particularly useful for researchers dealing with staggered adoption across multiple treated units.",
    "use_cases": [
      "Analyzing the impact of policy changes across different regions",
      "Evaluating treatment effects in staggered adoption scenarios"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for augmented synthetic control",
      "how to correct bias in synthetic control in R",
      "R library for staggered adoption analysis",
      "implementing ridge regression in synthetic control",
      "R synthetic control method for causal inference",
      "using multisynth function in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "gsynth",
    "description": "Implements generalized synthetic control with interactive fixed effects, extending SCM to multiple treated units with variable treatment timing. Uses factor models to impute counterfactuals, handling unbalanced panels and complex treatment patterns with latent factor structures.",
    "category": "Causal Inference (Synthetic Control)",
    "docs_url": "https://yiqingxu.org/packages/gsynth/",
    "github_url": "https://github.com/xuyiqing/gsynth",
    "url": "https://cran.r-project.org/package=gsynth",
    "install": "install.packages(\"gsynth\")",
    "tags": [
      "generalized-synthetic-control",
      "interactive-fixed-effects",
      "factor-models",
      "multiple-treated-units",
      "unbalanced-panels"
    ],
    "best_for": "Multiple treated units with staggered treatment timing and latent factor structures, implementing Xu (2017)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The gsynth package implements generalized synthetic control methods with interactive fixed effects, allowing for the analysis of multiple treated units with varying treatment timings. It is primarily used by researchers and practitioners in causal inference to handle complex treatment patterns in unbalanced panel data.",
    "use_cases": [
      "Analyzing the impact of policy changes across multiple regions",
      "Evaluating treatment effects in economic studies"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for generalized synthetic control",
      "how to implement interactive fixed effects in R",
      "R library for causal inference with unbalanced panels",
      "generalized synthetic control methods in R",
      "factor models for counterfactuals in R",
      "R package for multiple treated units analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "microsynth",
    "description": "Extends synthetic control method to micro-level data with many units. Implements permutation inference and handles high-dimensional settings where traditional SCM struggles.",
    "category": "Causal Inference (Synthetic Control)",
    "docs_url": "https://cran.r-project.org/web/packages/microsynth/microsynth.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=microsynth",
    "install": "install.packages(\"microsynth\")",
    "tags": [
      "synthetic-control",
      "micro-data",
      "permutation-inference",
      "high-dimensional",
      "many-units"
    ],
    "best_for": "Synthetic control for micro-level data with many units and permutation inference",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "high-dimensional"
    ],
    "summary": "Microsynth extends the synthetic control method to micro-level data with many units, allowing for permutation inference and handling high-dimensional settings. It is useful for researchers and practitioners in causal inference who work with complex datasets.",
    "use_cases": [
      "Analyzing treatment effects in observational studies",
      "Evaluating policy impacts on micro-level data"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for synthetic control",
      "how to perform permutation inference in R",
      "high-dimensional causal inference in R",
      "synthetic control for micro-data",
      "R library for many units analysis",
      "advanced synthetic control methods in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "pensynth",
    "description": "Implements penalized synthetic control method from Abadie & L'Hour (2021). Adds regularization to improve pre-treatment fit and reduce interpolation bias in sparse donor pools.",
    "category": "Causal Inference (Synthetic Control)",
    "docs_url": "https://cran.r-project.org/web/packages/pensynth/pensynth.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=pensynth",
    "install": "install.packages(\"pensynth\")",
    "tags": [
      "synthetic-control",
      "penalized",
      "regularization",
      "interpolation-bias",
      "sparse-donors"
    ],
    "best_for": "Penalized synthetic control with regularization, implementing Abadie & L'Hour (2021)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "synthetic-control"
    ],
    "summary": "The pensynth package implements a penalized synthetic control method to enhance pre-treatment fit and mitigate interpolation bias in sparse donor pools. It is primarily used by researchers and practitioners in causal inference who require improved estimation techniques.",
    "use_cases": [
      "Evaluating treatment effects in observational studies",
      "Improving causal inference in economic research"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R library for synthetic control",
      "how to implement penalized synthetic control in R",
      "synthetic control method for sparse donors",
      "penalized synthetic control example",
      "R package for interpolation bias in causal inference",
      "best practices for synthetic control in R"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Abadie & L'Hour (2021)",
    "maintenance_status": "active"
  },
  {
    "name": "scpi",
    "description": "Provides rigorous prediction intervals for synthetic control methods following Cattaneo et al. (2021, 2025). Supports staggered adoption designs with valid uncertainty quantification.",
    "category": "Causal Inference (Synthetic Control)",
    "docs_url": "https://nppackages.github.io/scpi/",
    "github_url": null,
    "url": "https://cran.r-project.org/package=scpi",
    "install": "install.packages(\"scpi\")",
    "tags": [
      "synthetic-control",
      "prediction-intervals",
      "uncertainty-quantification",
      "staggered-adoption",
      "inference"
    ],
    "best_for": "Rigorous prediction intervals for synthetic control, implementing Cattaneo et al. (2021, 2025)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "uncertainty-quantification"
    ],
    "summary": "The scpi package provides rigorous prediction intervals for synthetic control methods, allowing users to perform valid uncertainty quantification in staggered adoption designs. It is particularly useful for researchers and practitioners in causal inference.",
    "use_cases": [],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R library for synthetic control methods",
      "how to create prediction intervals in R",
      "uncertainty quantification in R",
      "staggered adoption designs in R",
      "Cattaneo 2021 synthetic control",
      "Cattaneo 2025 prediction intervals"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Cattaneo et al. (2021)",
    "maintenance_status": "active"
  },
  {
    "name": "synthdid",
    "description": "Implements synthetic difference-in-differences, a hybrid method combining insights from both DiD and synthetic control that reweights and matches pre-treatment trends. Provides improved robustness properties compared to either method alone by combining their strengths.",
    "category": "Causal Inference (Synthetic Control)",
    "docs_url": "https://synth-inference.github.io/synthdid/",
    "github_url": "https://github.com/synth-inference/synthdid",
    "url": "https://cran.r-project.org/package=synthdid",
    "install": "install.packages(\"synthdid\")",
    "tags": [
      "synthetic-control",
      "difference-in-differences",
      "hybrid-estimator",
      "panel-data",
      "robust-estimation"
    ],
    "best_for": "Settings where neither pure DiD nor pure SC is ideal, implementing Arkhangelsky, Athey, Hirshberg, Imbens & Wager (2021)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "synthetic-control"
    ],
    "summary": "The synthdid package implements synthetic difference-in-differences, a method that combines insights from both difference-in-differences and synthetic control. It is used by researchers and practitioners in causal inference to improve robustness in estimating treatment effects.",
    "use_cases": [],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for synthetic difference-in-differences",
      "how to implement synthetic control in R",
      "difference-in-differences method in R",
      "hybrid estimator for causal inference R",
      "robust estimation techniques in R",
      "panel data analysis in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "tidysynth",
    "description": "Brings synthetic control method into the tidyverse with cleaner syntax and built-in placebo inference. Provides pipe-friendly workflows for SCM estimation and visualization.",
    "category": "Causal Inference (Synthetic Control)",
    "docs_url": "https://cran.r-project.org/web/packages/tidysynth/tidysynth.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=tidysynth",
    "install": "install.packages(\"tidysynth\")",
    "tags": [
      "synthetic-control",
      "tidyverse",
      "placebo-inference",
      "causal-inference",
      "policy-evaluation"
    ],
    "best_for": "Tidyverse-friendly synthetic control method with clean syntax and built-in placebo inference",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "policy-evaluation"
    ],
    "summary": "tidysynth brings the synthetic control method into the tidyverse, providing a cleaner syntax and built-in placebo inference. It offers pipe-friendly workflows for estimating and visualizing synthetic control models, making it accessible for users in the data science community.",
    "use_cases": [
      "Evaluating the impact of a policy intervention",
      "Comparing treatment and control groups in observational studies"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for synthetic control",
      "how to use synthetic control in R",
      "tidyverse package for causal inference",
      "visualizing synthetic control results in R",
      "placebo inference in R",
      "synthetic control method tidyverse"
    ],
    "primary_use_cases": [
      "synthetic control estimation",
      "visualization of causal effects"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "tidyverse"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "MAPIE",
    "description": "Scikit-learn-contrib library for conformal prediction intervals. Provides model-agnostic uncertainty quantification for regression and classification.",
    "category": "Conformal Prediction & Uncertainty",
    "docs_url": "https://mapie.readthedocs.io/",
    "github_url": "https://github.com/scikit-learn-contrib/MAPIE",
    "url": "https://github.com/scikit-learn-contrib/MAPIE",
    "install": "pip install mapie",
    "tags": [
      "conformal prediction",
      "uncertainty",
      "intervals"
    ],
    "best_for": "Model-agnostic prediction intervals",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "conformal-prediction",
      "uncertainty-quantification"
    ],
    "summary": "MAPIE is a library that provides model-agnostic uncertainty quantification for regression and classification tasks through conformal prediction intervals. It is useful for data scientists and researchers who need to assess the uncertainty of their predictions.",
    "use_cases": [
      "Estimating prediction intervals for regression models",
      "Providing uncertainty quantification for classification tasks"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for conformal prediction",
      "how to quantify uncertainty in predictions using python",
      "scikit-learn conformal prediction intervals",
      "MAPIE library for uncertainty quantification",
      "predictive intervals in python",
      "model-agnostic uncertainty estimation python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "TorchCP",
    "description": "PyTorch-native conformal prediction for DNNs, GNNs, and LLMs with GPU acceleration.",
    "category": "Conformal Prediction & Uncertainty",
    "docs_url": "https://torchcp.readthedocs.io/",
    "github_url": "https://github.com/ml-stat-Sustech/TorchCP",
    "url": "https://github.com/ml-stat-Sustech/TorchCP",
    "install": "pip install torchcp",
    "tags": [
      "conformal prediction",
      "PyTorch",
      "deep learning"
    ],
    "best_for": "Conformal prediction for neural networks",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "TorchCP is a PyTorch-native library designed for conformal prediction in deep neural networks, graph neural networks, and large language models, providing GPU acceleration. It is useful for practitioners in machine learning who need to quantify uncertainty in their models.",
    "use_cases": [],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for conformal prediction",
      "how to do conformal prediction in PyTorch",
      "PyTorch GPU acceleration for deep learning",
      "conformal prediction for DNNs",
      "using TorchCP for uncertainty quantification",
      "TorchCP examples",
      "installing TorchCP",
      "TorchCP documentation"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "PyTorch"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "crepes",
    "description": "Lightweight library for conformal regressors and predictive systems. Simple API for calibrated prediction intervals.",
    "category": "Conformal Prediction & Uncertainty",
    "docs_url": "https://github.com/henrikbostrom/crepes",
    "github_url": "https://github.com/henrikbostrom/crepes",
    "url": "https://github.com/henrikbostrom/crepes",
    "install": "pip install crepes",
    "tags": [
      "conformal prediction",
      "regression",
      "intervals"
    ],
    "best_for": "Simple conformal regressors",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "Crepes is a lightweight library designed for conformal regressors and predictive systems, providing a simple API for generating calibrated prediction intervals. It is useful for data scientists and statisticians who need reliable uncertainty quantification in their predictions.",
    "use_cases": [
      "Generating calibrated prediction intervals for regression models",
      "Implementing conformal prediction methods in data analysis"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for conformal prediction",
      "how to create prediction intervals in python",
      "lightweight library for regression in python",
      "calibrated prediction intervals python",
      "conformal regressors in python",
      "predictive systems library python"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active"
  },
  {
    "name": "fortuna",
    "description": "AWS library for uncertainty quantification in deep learning. Bayesian and conformal methods.",
    "category": "Conformal Prediction & Uncertainty",
    "docs_url": "https://aws-fortuna.readthedocs.io/",
    "github_url": "https://github.com/awslabs/fortuna",
    "url": "https://github.com/awslabs/fortuna",
    "install": "pip install fortuna",
    "tags": [
      "uncertainty",
      "Bayesian",
      "deep learning"
    ],
    "best_for": "Deep learning uncertainty quantification",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "bayesian",
      "uncertainty"
    ],
    "summary": "Fortuna is an AWS library designed for uncertainty quantification in deep learning, utilizing Bayesian and conformal methods. It is primarily used by data scientists and researchers working on uncertainty in machine learning models.",
    "use_cases": [
      "Quantifying uncertainty in deep learning models",
      "Implementing Bayesian methods for model predictions"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for uncertainty quantification",
      "how to implement Bayesian methods in python",
      "AWS library for deep learning uncertainty",
      "conformal prediction in python",
      "deep learning uncertainty quantification",
      "Bayesian deep learning library"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "puncc",
    "description": "IRT Lab's library for predictive uncertainty with conformal prediction. Supports various conformal methods.",
    "category": "Conformal Prediction & Uncertainty",
    "docs_url": "https://github.com/deel-ai/puncc",
    "github_url": "https://github.com/deel-ai/puncc",
    "url": "https://github.com/deel-ai/puncc",
    "install": "pip install puncc",
    "tags": [
      "conformal prediction",
      "uncertainty",
      "calibration"
    ],
    "best_for": "Calibrated prediction sets",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "puncc is a library developed by IRT Lab for predictive uncertainty using conformal prediction methods. It is designed for users interested in applying various conformal methods for uncertainty quantification.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for conformal prediction",
      "how to implement uncertainty calibration in python",
      "predictive uncertainty library python",
      "conformal prediction methods in python",
      "IRT Lab conformal prediction library",
      "using puncc for uncertainty quantification"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "cjoint",
    "description": "Estimates Average Marginal Component Effects (AMCEs) for conjoint experiments following Hainmueller, Hopkins & Yamamoto (2014). Handles multi-dimensional preferences with clustered standard errors.",
    "category": "Conjoint Analysis",
    "docs_url": "https://cran.r-project.org/web/packages/cjoint/cjoint.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=cjoint",
    "install": "install.packages(\"cjoint\")",
    "tags": [
      "conjoint",
      "AMCE",
      "survey-experiments",
      "preferences",
      "political-science"
    ],
    "best_for": "AMCE estimation for conjoint experiments, implementing Hainmueller, Hopkins & Yamamoto (2014)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "conjoint-analysis",
      "survey-experiments",
      "political-science"
    ],
    "summary": "The cjoint package estimates Average Marginal Component Effects (AMCEs) for conjoint experiments, allowing researchers to analyze multi-dimensional preferences with clustered standard errors. It is primarily used by social scientists conducting survey experiments.",
    "use_cases": [
      "Analyzing voter preferences in political surveys",
      "Evaluating consumer choices in market research"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R library for conjoint analysis",
      "how to estimate AMCEs in R",
      "R package for survey experiments",
      "multi-dimensional preferences analysis in R",
      "clustered standard errors in R",
      "political science conjoint analysis R"
    ],
    "primary_use_cases": [
      "estimating average marginal component effects",
      "analyzing multi-dimensional preferences"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Hainmueller, Hopkins & Yamamoto (2014)",
    "maintenance_status": "active"
  },
  {
    "name": "cregg",
    "description": "Tidy interface for conjoint analysis with visualization. Provides functions for calculating and plotting marginal means and AMCEs with ggplot2-based output for publication-ready figures.",
    "category": "Conjoint Analysis",
    "docs_url": "https://thomasleeper.com/cregg/",
    "github_url": "https://github.com/leeper/cregg",
    "url": "https://cran.r-project.org/package=cregg",
    "install": "install.packages(\"cregg\")",
    "tags": [
      "conjoint",
      "visualization",
      "marginal-means",
      "ggplot2",
      "survey-experiments"
    ],
    "best_for": "Tidy conjoint analysis with ggplot2 visualization for marginal means and AMCEs",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The cregg package provides a tidy interface for conducting conjoint analysis and visualizing the results. It is designed for users who need to calculate and plot marginal means and Average Marginal Component Effects (AMCEs) using ggplot2 for publication-ready figures.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "R package for conjoint analysis",
      "how to visualize marginal means in R",
      "conjoint analysis with ggplot2",
      "R library for survey experiments",
      "calculate AMCEs in R",
      "tidy interface for conjoint analysis"
    ],
    "primary_use_cases": [
      "calculating marginal means",
      "plotting AMCEs"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "H2O Sparkling Water",
    "description": "H2O's distributed ML engine on Spark with GLM/GAM that provides p-values, confidence intervals, and Tweedie/Gamma distributions.",
    "category": "Core Libraries & Linear Models",
    "docs_url": "https://docs.h2o.ai/sparkling-water/3.3/latest-stable/doc/index.html",
    "github_url": "https://github.com/h2oai/sparkling-water",
    "url": "https://github.com/h2oai/sparkling-water",
    "install": "pip install h2o_pysparkling_3.4",
    "tags": [
      "spark",
      "GLM",
      "GAM",
      "distributed",
      "p-values"
    ],
    "best_for": "Econometric inference (p-values, CIs) at Spark scale",
    "language": "Spark",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "H2O Sparkling Water is a distributed machine learning engine that integrates with Spark, providing functionalities for Generalized Linear Models (GLM) and Generalized Additive Models (GAM). It is used by data scientists and machine learning practitioners who require scalable solutions for statistical modeling and inference.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "H2O Sparkling Water library",
      "how to use GLM in Spark",
      "distributed machine learning with Spark",
      "H2O Sparkling Water examples",
      "GAM implementation in Spark",
      "p-values in machine learning",
      "Tweedie distribution in Spark",
      "H2O machine learning engine"
    ],
    "primary_use_cases": [
      "statistical modeling",
      "machine learning inference"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "Spark"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "Linregress",
    "description": "Simple linear regression for Rust with R-style formula syntax, standard errors, t-stats, and p-values.",
    "category": "Core Libraries & Linear Models",
    "docs_url": "https://docs.rs/linregress",
    "github_url": "https://github.com/n1m3/linregress",
    "url": "https://crates.io/crates/linregress",
    "install": "cargo add linregress",
    "tags": [
      "rust",
      "regression",
      "OLS",
      "statistics"
    ],
    "best_for": "Simple no-frills OLS regression in Rust",
    "language": "Rust",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "statistics"
    ],
    "summary": "Linregress is a Rust library that provides simple linear regression capabilities using R-style formula syntax. It is useful for statisticians and data scientists looking to perform regression analysis in Rust.",
    "use_cases": [
      "Performing linear regression analysis on datasets",
      "Calculating standard errors and p-values for regression models"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "rust library for linear regression",
      "how to perform OLS in Rust",
      "statistics package in Rust",
      "Rust regression analysis",
      "simple linear regression Rust",
      "Rust library for statistics"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active"
  },
  {
    "name": "Polars",
    "description": "Blazingly fast DataFrame library for Rust and Python with SQL-like syntax, lazy evaluation, and excellent time series handling.",
    "category": "Core Libraries & Linear Models",
    "docs_url": "https://pola.rs/",
    "github_url": "https://github.com/pola-rs/polars",
    "url": "https://crates.io/crates/polars",
    "install": "cargo add polars",
    "tags": [
      "rust",
      "dataframe",
      "data manipulation",
      "performance"
    ],
    "best_for": "High-performance data manipulation (pandas alternative)",
    "language": "Rust",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "data manipulation",
      "time-series"
    ],
    "summary": "Polars is a high-performance DataFrame library designed for Rust and Python, offering SQL-like syntax and lazy evaluation. It is particularly useful for data manipulation and handling time series data efficiently.",
    "use_cases": [
      "Data analysis with large datasets",
      "Time series data processing"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for data manipulation",
      "how to handle time series in python",
      "fast dataframe library for python",
      "rust dataframe library",
      "sql-like syntax for data analysis",
      "lazy evaluation in data processing",
      "performance data manipulation library"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "pandas",
      "dask"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "Scikit-learn Ens.",
    "description": "(`RandomForestClassifier`/`Regressor`) Widely-used, versatile implementation of Random Forests. Easy API and parallel processing support.",
    "category": "Tree & Ensemble Methods for Prediction",
    "docs_url": "https://scikit-learn.org/stable/modules/ensemble.html#random-forests",
    "github_url": "https://github.com/scikit-learn/scikit-learn",
    "url": "https://github.com/scikit-learn/scikit-learn",
    "install": "pip install scikit-learn",
    "tags": [
      "regression",
      "linear models",
      "machine learning",
      "prediction"
    ],
    "best_for": "OLS regression, basic econometrics, data manipulation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn-basics"
    ],
    "topic_tags": [],
    "summary": "Scikit-learn is a foundational machine learning library in Python that provides tools for regression models, model selection, cross-validation, and evaluation metrics. It is widely used by data scientists and machine learning practitioners for building and evaluating predictive models.",
    "use_cases": [
      "Building regression models",
      "Evaluating model performance",
      "Predicting outcomes based on historical data",
      "Classifying data into categories using ensemble methods"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for regression",
      "how to do model selection in python",
      "cross-validation in scikit-learn",
      "evaluation metrics for machine learning in python",
      "implementing linear models in python",
      "scikit-learn documentation",
      "best practices for using scikit-learn",
      "python library for Random Forests",
      "how to use RandomForestClassifier in python",
      "machine learning prediction with scikit-learn",
      "parallel processing in scikit-learn",
      "Random Forests implementation in python",
      "predictive modeling with RandomForestRegressor"
    ],
    "primary_use_cases": [
      "Building regression models",
      "Model selection"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "statsmodels",
      "TensorFlow",
      "PyTorch",
      "RandomForest",
      "XGBoost"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "Statsmodels",
    "description": "Comprehensive library for estimating statistical models (OLS, GLM, etc.), conducting tests, and data exploration. Core tool.",
    "category": "Core Libraries & Linear Models",
    "docs_url": "https://www.statsmodels.org/",
    "github_url": "https://github.com/statsmodels/statsmodels",
    "url": "https://github.com/statsmodels/statsmodels",
    "install": "pip install statsmodels",
    "tags": [
      "regression",
      "linear models"
    ],
    "best_for": "OLS regression, basic econometrics, data manipulation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "linear models"
    ],
    "summary": "Statsmodels is a comprehensive library for estimating statistical models such as OLS and GLM, conducting statistical tests, and exploring data. It is widely used by data scientists and statisticians for various statistical analyses.",
    "use_cases": [
      "Estimating linear regression models",
      "Conducting hypothesis tests on statistical data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for statistical models",
      "how to conduct regression analysis in python",
      "python library for data exploration",
      "how to perform OLS in python",
      "python GLM examples",
      "best python libraries for regression"
    ],
    "primary_use_cases": [
      "linear regression analysis",
      "generalized linear model fitting"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "scikit-learn",
      "stats",
      "pandas"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "appelpy",
    "description": "Applied Econometrics Library bridging Stata-like syntax with Python. Built on statsmodels with convenient API.",
    "category": "Core Libraries & Linear Models",
    "docs_url": "https://appelpy.readthedocs.io/",
    "github_url": "https://github.com/mfarragher/appelpy",
    "url": "https://github.com/mfarragher/appelpy",
    "install": "pip install appelpy",
    "tags": [
      "regression",
      "linear models",
      "Stata"
    ],
    "best_for": "Stata-like econometrics workflow in Python",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "statsmodels"
    ],
    "topic_tags": [
      "regression",
      "linear models"
    ],
    "summary": "Appelpy is an Applied Econometrics Library that bridges Stata-like syntax with Python, providing a convenient API built on statsmodels. It is designed for users who are familiar with econometric modeling and want to leverage Python's capabilities.",
    "use_cases": [
      "Conducting regression analysis",
      "Performing econometric modeling"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for applied econometrics",
      "how to perform regression in python",
      "Stata-like syntax in python",
      "linear models in python",
      "applying econometrics with python",
      "python statsmodels tutorial"
    ],
    "primary_use_cases": [
      "regression analysis",
      "econometric modeling"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "statsmodels",
      "pandas"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "collapse",
    "description": "High-performance data transformation package designed by an economist. Provides fast grouped operations, time series functions, and panel data tools with 10-100\u00d7 speedups over dplyr on large data.",
    "category": "Data Workflow",
    "docs_url": "https://sebkrantz.github.io/collapse/",
    "github_url": "https://github.com/SebKrantz/collapse",
    "url": "https://cran.r-project.org/package=collapse",
    "install": "install.packages(\"collapse\")",
    "tags": [
      "data-transformation",
      "high-performance",
      "panel-data",
      "time-series",
      "grouped-operations"
    ],
    "best_for": "High-performance data transformation optimized for economists\u201410-100\u00d7 faster than dplyr",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "data-transformation",
      "time-series",
      "panel-data"
    ],
    "summary": "The 'collapse' package is a high-performance data transformation tool designed for economists, providing fast grouped operations, time series functions, and panel data tools. It is particularly useful for users dealing with large datasets who require efficient data manipulation.",
    "use_cases": [
      "Transforming large datasets efficiently",
      "Conducting time series analysis",
      "Performing grouped operations on panel data"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for high-performance data transformation",
      "how to perform grouped operations in R",
      "R time series functions",
      "efficient panel data tools in R",
      "data manipulation package for economists",
      "fast data transformation in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "data.table",
    "description": "Extension of data.frame providing fast aggregation of large data (100GB+), ordered joins, and memory-efficient operations. Uses reference semantics for in-place modification with concise syntax [:=, .SD, by=].",
    "category": "Data Workflow",
    "docs_url": "https://rdatatable.gitlab.io/data.table/",
    "github_url": "https://github.com/Rdatatable/data.table",
    "url": "https://cran.r-project.org/package=data.table",
    "install": "install.packages(\"data.table\")",
    "tags": [
      "data-manipulation",
      "fast",
      "large-data",
      "reference-semantics",
      "aggregation"
    ],
    "best_for": "Fast operations on large datasets (100GB+) with memory-efficient reference semantics",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "data.table is an extension of data.frame in R that provides fast aggregation of large datasets, ordered joins, and memory-efficient operations. It is commonly used by data scientists and statisticians working with large data sets.",
    "use_cases": [
      "Aggregating large datasets for analysis",
      "Performing ordered joins on large data tables"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for fast data manipulation",
      "how to aggregate large data in R",
      "R package for memory-efficient data operations",
      "data.table vs data.frame in R",
      "efficient joins in R",
      "R data manipulation tools"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "dplyr",
      "data.frame"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "haven",
    "description": "Import and export Stata, SPSS, and SAS data files preserving variable labels and value labels. Handles .dta, .sav, .sas7bdat, and .xpt formats with labelled vectors for metadata.",
    "category": "Data Workflow",
    "docs_url": "https://haven.tidyverse.org/",
    "github_url": "https://github.com/tidyverse/haven",
    "url": "https://cran.r-project.org/package=haven",
    "install": "install.packages(\"haven\")",
    "tags": [
      "Stata",
      "SPSS",
      "SAS",
      "data-import",
      "labelled-data"
    ],
    "best_for": "Reading and writing Stata, SPSS, and SAS files with preserved labels",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The haven package allows users to import and export data files from Stata, SPSS, and SAS while preserving variable and value labels. It is useful for data analysts and researchers who work with these statistical software formats.",
    "use_cases": [
      "Importing .dta files from Stata",
      "Exporting .sav files to SPSS"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for importing Stata data",
      "how to export SPSS files in R",
      "R haven package documentation",
      "import SAS data into R",
      "R data import tools",
      "preserve variable labels in R",
      "R package for labelled data"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active"
  },
  {
    "name": "tidyverse",
    "description": "Meta-package installing core tidyverse packages: ggplot2 (visualization), dplyr (manipulation), tidyr (tidying), readr (import), purrr (functional programming), tibble (data frames), stringr (strings), and forcats (factors).",
    "category": "Data Workflow",
    "docs_url": "https://www.tidyverse.org/",
    "github_url": "https://github.com/tidyverse/tidyverse",
    "url": "https://cran.r-project.org/package=tidyverse",
    "install": "install.packages(\"tidyverse\")",
    "tags": [
      "tidyverse",
      "data-science",
      "dplyr",
      "ggplot2",
      "meta-package"
    ],
    "best_for": "Core tidyverse ecosystem for consistent data science workflows",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The tidyverse is a collection of R packages designed for data science, providing tools for data manipulation, visualization, and import. It is widely used by data scientists and statisticians to streamline their workflow.",
    "use_cases": [
      "Creating visualizations with ggplot2",
      "Data manipulation with dplyr"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "R library for data manipulation",
      "how to visualize data in R",
      "R package for data import",
      "functional programming in R",
      "R tools for data frames",
      "tidyverse package overview"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "ggplot2",
      "dplyr",
      "tidyr",
      "readr",
      "purrr",
      "tibble",
      "stringr",
      "forcats"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "AER",
    "description": "Companion package to 'Applied Econometrics with R' (Kleiber & Zeileis) plus datasets from Stock & Watson. Provides ivreg() for instrumental variables, tobit(), and econometric testing functions.",
    "category": "Datasets",
    "docs_url": "https://cran.r-project.org/web/packages/AER/AER.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=AER",
    "install": "install.packages(\"AER\")",
    "tags": [
      "datasets",
      "textbook",
      "instrumental-variables",
      "Stock-Watson",
      "Kleiber-Zeileis"
    ],
    "best_for": "Datasets and functions from 'Applied Econometrics with R' plus Stock & Watson data",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "econometrics",
      "instrumental-variables"
    ],
    "summary": "AER is a companion package to 'Applied Econometrics with R' that provides functions for instrumental variables and econometric testing. It is used by students and practitioners in econometrics for analysis and testing.",
    "use_cases": [
      "Performing instrumental variable regression",
      "Conducting tobit analysis",
      "Testing econometric hypotheses"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for instrumental variables",
      "how to perform econometric testing in R",
      "datasets for applied econometrics",
      "R package for tobit model",
      "AER R package documentation",
      "how to use ivreg in R",
      "Stock-Watson datasets in R"
    ],
    "primary_use_cases": [
      "instrumental variable regression",
      "tobit model analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "causaldata",
    "description": "Unified collection of datasets from three major causal inference textbooks: 'The Effect' (Huntington-Klein), 'Causal Inference: The Mixtape' (Cunningham), and 'Causal Inference: What If?' (Hern\u00e1n & Robins).",
    "category": "Datasets",
    "docs_url": "https://cran.r-project.org/web/packages/causaldata/causaldata.pdf",
    "github_url": "https://github.com/NickCH-K/causaldata",
    "url": "https://cran.r-project.org/package=causaldata",
    "install": "install.packages(\"causaldata\")",
    "tags": [
      "datasets",
      "causal-inference",
      "textbook",
      "The-Effect",
      "Mixtape"
    ],
    "best_for": "Datasets from The Effect, Causal Inference: The Mixtape, and What If? textbooks",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The causaldata package provides a unified collection of datasets from three major causal inference textbooks, making it easier for users to access and utilize these resources for causal analysis. It is primarily used by students and practitioners in the field of causal inference.",
    "use_cases": [],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R library for causal inference datasets",
      "how to use datasets from The Effect in R",
      "datasets for causal analysis in R",
      "R package for causal inference textbooks",
      "access causal inference datasets in R",
      "causaldata R package usage"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active"
  },
  {
    "name": "wooldridge",
    "description": "All 115 datasets from Wooldridge's 'Introductory Econometrics: A Modern Approach' (7th edition). Includes wage equations, crime data, housing prices, and classic econometrics teaching examples.",
    "category": "Datasets",
    "docs_url": "https://cran.r-project.org/web/packages/wooldridge/wooldridge.pdf",
    "github_url": "https://github.com/JustinMShea/wooldridge",
    "url": "https://cran.r-project.org/package=wooldridge",
    "install": "install.packages(\"wooldridge\")",
    "tags": [
      "datasets",
      "textbook",
      "teaching",
      "Wooldridge",
      "econometrics"
    ],
    "best_for": "115 datasets from Wooldridge's 'Introductory Econometrics' for teaching and examples",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "econometrics",
      "datasets",
      "teaching"
    ],
    "summary": "The wooldridge package provides access to all 115 datasets from Wooldridge's 'Introductory Econometrics: A Modern Approach' (7th edition). It is primarily used by students and educators in econometrics for teaching and learning purposes.",
    "use_cases": [
      "Teaching econometrics using real datasets",
      "Analyzing wage equations for educational purposes"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "R library for econometrics datasets",
      "how to access Wooldridge datasets in R",
      "datasets for teaching econometrics",
      "Wooldridge econometrics examples in R",
      "R package for wage equations",
      "crime data analysis in R",
      "housing prices datasets in R",
      "econometrics teaching resources in R"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active"
  },
  {
    "name": "FactorAnalyzer",
    "description": "Specialized library for Exploratory (EFA) and Confirmatory (CFA) Factor Analysis with rotation options for interpretability.",
    "category": "Dimensionality Reduction",
    "docs_url": "https://factor-analyzer.readthedocs.io/en/latest/",
    "github_url": "https://github.com/EducationalTestingService/factor_analyzer",
    "url": "https://github.com/EducationalTestingService/factor_analyzer",
    "install": "pip install factor_analyzer",
    "tags": [
      "machine learning",
      "dimensionality"
    ],
    "best_for": "Feature extraction, PCA, high-dimensional data",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "FactorAnalyzer is a specialized library designed for Exploratory and Confirmatory Factor Analysis, providing various rotation options for better interpretability. It is used by data scientists and researchers who need to analyze complex datasets and uncover latent structures.",
    "use_cases": [
      "Analyzing survey data to identify underlying factors",
      "Reducing dimensionality in large datasets"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for factor analysis",
      "how to perform EFA in python",
      "CFA library in python",
      "factor analysis with rotation options python",
      "exploratory factor analysis python",
      "confirmatory factor analysis python"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "factor_analyzer"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "openTSNE",
    "description": "Optimized, parallel implementation of t-distributed Stochastic Neighbor Embedding (t-SNE) for large datasets.",
    "category": "Dimensionality Reduction",
    "docs_url": "https://opentsne.readthedocs.io/en/stable/",
    "github_url": "https://github.com/pavlin-policar/openTSNE",
    "url": "https://github.com/pavlin-policar/openTSNE",
    "install": "pip install opentsne",
    "tags": [
      "machine learning",
      "dimensionality"
    ],
    "best_for": "Feature extraction, PCA, high-dimensional data",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "openTSNE is an optimized and parallel implementation of t-distributed Stochastic Neighbor Embedding (t-SNE) designed for handling large datasets. It is commonly used by data scientists and researchers for visualizing high-dimensional data in a lower-dimensional space.",
    "use_cases": [
      "Visualizing clusters in high-dimensional data",
      "Reducing dimensionality for machine learning preprocessing"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for dimensionality reduction",
      "how to use t-SNE in python",
      "visualizing high-dimensional data in python",
      "openTSNE tutorial",
      "parallel t-SNE implementation python",
      "large dataset t-SNE python"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "scikit-learn",
      "umap-learn"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "umap-learn",
    "description": "Fast and scalable implementation of Uniform Manifold Approximation and Projection (UMAP) for non-linear reduction.",
    "category": "Dimensionality Reduction",
    "docs_url": "https://umap-learn.readthedocs.io/en/latest/",
    "github_url": "https://github.com/lmcinnes/umap",
    "url": "https://github.com/lmcinnes/umap",
    "install": "pip install umap-learn",
    "tags": [
      "machine learning",
      "dimensionality"
    ],
    "best_for": "Feature extraction, PCA, high-dimensional data",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "umap-learn is a fast and scalable implementation of Uniform Manifold Approximation and Projection (UMAP) for non-linear dimensionality reduction. It is used by data scientists and machine learning practitioners to visualize high-dimensional data in lower dimensions.",
    "use_cases": [
      "Visualizing high-dimensional datasets",
      "Reducing dimensions for clustering algorithms"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for dimensionality reduction",
      "how to use UMAP in python",
      "fast UMAP implementation python",
      "scalable UMAP for machine learning",
      "visualize high-dimensional data python",
      "UMAP for non-linear reduction python"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "scikit-learn"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "Biogeme",
    "description": "Maximum likelihood estimation of parametric models, with strong support for complex discrete choice models.",
    "category": "Discrete Choice Models",
    "docs_url": "https://biogeme.epfl.ch/index.html",
    "github_url": "https://github.com/michelbierlaire/biogeme",
    "url": "https://github.com/michelbierlaire/biogeme",
    "install": "pip install biogeme",
    "tags": [
      "discrete choice",
      "logit"
    ],
    "best_for": "Logit/probit models, consumer choice, demand estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "discrete-choice",
      "statistical-modeling"
    ],
    "summary": "Biogeme is a Python package designed for maximum likelihood estimation of parametric models, particularly excelling in complex discrete choice models. It is used by researchers and practitioners in fields such as transportation, marketing, and economics to analyze choice behavior.",
    "use_cases": [
      "Estimating consumer preferences in marketing",
      "Analyzing transportation mode choice"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for discrete choice models",
      "how to estimate parametric models in python",
      "maximum likelihood estimation in python",
      "discrete choice analysis with Biogeme",
      "Biogeme tutorial",
      "complex discrete choice models in python"
    ],
    "primary_use_cases": [
      "maximum likelihood estimation",
      "discrete choice modeling"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "choicepy",
      "pylogit"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "PyBLP",
    "description": "Tools for estimating demand for differentiated products using the Berry-Levinsohn-Pakes (BLP) method.",
    "category": "Discrete Choice Models",
    "docs_url": "https://pyblp.readthedocs.io/",
    "github_url": "https://github.com/jeffgortmaker/pyblp",
    "url": "https://github.com/jeffgortmaker/pyblp",
    "install": "pip install pyblp",
    "tags": [
      "discrete choice",
      "logit"
    ],
    "best_for": "Logit/probit models, consumer choice, demand estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "PyBLP is a Python package designed for estimating demand for differentiated products using the Berry-Levinsohn-Pakes (BLP) method. It is primarily used by researchers and practitioners in economics and data science who are interested in discrete choice modeling.",
    "use_cases": [
      "Estimating demand for consumer products",
      "Analyzing market competition"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for estimating demand",
      "how to use BLP method in python",
      "tools for discrete choice modeling in python",
      "python package for differentiated products demand estimation",
      "how to estimate demand using PyBLP",
      "BLP method implementation in python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "PyLogit",
    "description": "Flexible implementation of conditional/multinomial logit models with utilities for data preparation.",
    "category": "Discrete Choice Models",
    "docs_url": null,
    "github_url": "https://github.com/timothyb0912/pylogit",
    "url": "https://github.com/timothyb0912/pylogit",
    "install": "pip install pylogit",
    "tags": [
      "discrete choice",
      "logit"
    ],
    "best_for": "Logit/probit models, consumer choice, demand estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "discrete choice"
    ],
    "summary": "PyLogit is a flexible implementation of conditional and multinomial logit models, providing utilities for data preparation. It is primarily used by data scientists and researchers working on discrete choice modeling.",
    "use_cases": [
      "Estimating consumer preferences",
      "Analyzing survey data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for discrete choice models",
      "how to implement multinomial logit in python",
      "PyLogit documentation",
      "conditional logit model python",
      "data preparation for logit models",
      "discrete choice analysis python",
      "logit model examples in python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "XLogit",
    "description": "Fast estimation of Multinomial Logit and Mixed Logit models, optimized for performance.",
    "category": "Discrete Choice Models",
    "docs_url": "https://xlogit.readthedocs.io/",
    "github_url": "https://github.com/arteagac/xlogit",
    "url": "https://github.com/arteagac/xlogit",
    "install": "pip install xlogit",
    "tags": [
      "discrete choice",
      "logit"
    ],
    "best_for": "Logit/probit models, consumer choice, demand estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "discrete choice"
    ],
    "summary": "XLogit is a Python package designed for the fast estimation of Multinomial Logit and Mixed Logit models, focusing on performance optimization. It is primarily used by data scientists and researchers working in the field of discrete choice modeling.",
    "use_cases": [
      "Estimating consumer choice behavior",
      "Analyzing survey data for preference modeling"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for Multinomial Logit",
      "how to estimate Mixed Logit models in python",
      "XLogit package documentation",
      "discrete choice modeling in python",
      "fast estimation of logit models python",
      "performance optimization in logit models"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "torch-choice",
    "description": "PyTorch framework for flexible estimation of complex discrete choice models, leveraging GPU acceleration.",
    "category": "Discrete Choice Models",
    "docs_url": "https://gsbdbi.github.io/torch-choice/",
    "github_url": "https://github.com/gsbDBI/torch-choice",
    "url": "https://github.com/gsbDBI/torch-choice",
    "install": "pip install torch-choice",
    "tags": [
      "discrete choice",
      "logit"
    ],
    "best_for": "Logit/probit models, consumer choice, demand estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "torch-choice is a PyTorch framework designed for flexible estimation of complex discrete choice models, utilizing GPU acceleration. It is primarily used by data scientists and researchers working on discrete choice analysis.",
    "use_cases": [
      "Estimating consumer preferences in marketing",
      "Modeling transportation choices"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for discrete choice models",
      "how to estimate complex discrete choice models in python",
      "torch-choice usage examples",
      "PyTorch discrete choice modeling",
      "GPU acceleration for discrete choice models",
      "discrete choice analysis in python"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "PyTorch"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "DoubleML",
    "description": "Implements the double/debiased ML framework (Chernozhukov et al.) for estimating causal parameters (ATE, LATE, POM) with ML nuisances.",
    "category": "Double/Debiased Machine Learning (DML)",
    "docs_url": "https://docs.doubleml.org/",
    "github_url": "https://github.com/DoubleML/doubleml-for-py",
    "url": "https://github.com/DoubleML/doubleml-for-py",
    "install": "pip install DoubleML",
    "tags": [
      "machine learning",
      "causal inference"
    ],
    "best_for": "High-dimensional controls, ML-based causal inference",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "DoubleML implements the double/debiased ML framework for estimating causal parameters such as ATE, LATE, and POM using machine learning nuisances. It is used by data scientists and researchers focused on causal inference.",
    "use_cases": [
      "Estimating average treatment effects",
      "Conducting A/B test analysis"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to estimate ATE in python",
      "double ML framework python",
      "machine learning for causal parameters",
      "using DoubleML for LATE estimation",
      "python package for debiased ML"
    ],
    "primary_use_cases": [
      "causal forest estimation"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Chernozhukov et al. (2018)",
    "related_packages": [
      "causalml",
      "econml"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "Doubly-Debiased-Lasso",
    "description": "High-dimensional inference under hidden confounding. Doubly debiased Lasso for valid inference.",
    "category": "Double/Debiased Machine Learning (DML)",
    "docs_url": "https://github.com/zijguo/Doubly-Debiased-Lasso",
    "github_url": "https://github.com/zijguo/Doubly-Debiased-Lasso",
    "url": "https://github.com/zijguo/Doubly-Debiased-Lasso",
    "install": "Install from GitHub",
    "tags": [
      "high-dimensional",
      "Lasso",
      "debiased"
    ],
    "best_for": "High-dim inference with confounding",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "high-dimensional"
    ],
    "summary": "Doubly-Debiased-Lasso is a Python package designed for high-dimensional inference under hidden confounding. It is primarily used by data scientists and researchers who need valid inference methods in causal analysis.",
    "use_cases": [
      "Estimating causal effects in high-dimensional settings",
      "Conducting valid inference in the presence of confounding variables"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for high-dimensional inference",
      "how to use Lasso for causal inference in python",
      "Doubly Debiased Lasso tutorial",
      "valid inference methods in python",
      "high-dimensional data analysis python",
      "causal inference library python"
    ],
    "primary_use_cases": [
      "causal effect estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "EconML",
    "description": "Microsoft toolkit for estimating heterogeneous treatment effects using DML, causal forests, meta-learners, and orthogonal ML methods.",
    "category": "Double/Debiased Machine Learning (DML)",
    "docs_url": "https://econml.azurewebsites.net/",
    "github_url": "https://github.com/py-why/EconML",
    "url": "https://github.com/py-why/EconML",
    "install": "pip install econml",
    "tags": [
      "machine learning",
      "causal inference"
    ],
    "best_for": "High-dimensional controls, ML-based causal inference",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "EconML is a Microsoft toolkit designed for estimating heterogeneous treatment effects using advanced machine learning methods. It is primarily used by data scientists and researchers in the field of causal inference.",
    "use_cases": [
      "Estimating treatment effects in clinical trials",
      "Analyzing A/B test results",
      "Evaluating policy impacts using observational data"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for estimating treatment effects",
      "how to use causal forests in python",
      "EconML documentation",
      "machine learning for causal inference in python",
      "DML methods in python",
      "meta-learners for treatment effects"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "causalml",
      "dowhy"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "SynapseML",
    "description": "Microsoft's distributed ML library with native Double ML (DoubleMLEstimator) for heterogeneous treatment effects at scale.",
    "category": "Double/Debiased Machine Learning (DML)",
    "docs_url": "https://microsoft.github.io/SynapseML/",
    "github_url": "https://github.com/microsoft/SynapseML",
    "url": "https://github.com/microsoft/SynapseML",
    "install": "pip install synapseml",
    "tags": [
      "spark",
      "causal inference",
      "double ML",
      "distributed"
    ],
    "best_for": "Causal inference at 100M+ rows on Spark clusters",
    "language": "Spark",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "distributed-machine-learning"
    ],
    "summary": "SynapseML is a distributed machine learning library developed by Microsoft that provides native support for Double ML, specifically designed for estimating heterogeneous treatment effects at scale. It is used by data scientists and researchers working in causal inference and machine learning.",
    "use_cases": [
      "Estimating treatment effects in clinical trials",
      "Analyzing the impact of marketing campaigns"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for distributed machine learning",
      "how to perform causal inference in Spark",
      "Double ML implementation in Python",
      "Microsoft ML library for treatment effects",
      "using SynapseML for A/B testing",
      "causal inference with Spark library",
      "distributed ML for heterogeneous treatment effects"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "Spark"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "pydoublelasso",
    "description": "Double\u2011post\u00a0Lasso estimator for high\u2011dimensional treatment effects (Belloni\u2011Chernozhukov\u2011Hansen\u202f2014).",
    "category": "Double/Debiased Machine Learning (DML)",
    "docs_url": "https://pypi.org/project/pydoublelasso/",
    "github_url": null,
    "url": "https://pypi.org/project/pydoublelasso/",
    "install": "pip install pydoublelasso",
    "tags": [
      "machine learning",
      "causal inference"
    ],
    "best_for": "High-dimensional controls, ML-based causal inference",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "machine-learning"
    ],
    "summary": "The pydoublelasso package provides a Double-post Lasso estimator for analyzing high-dimensional treatment effects, based on the methodology established by Belloni, Chernozhukov, and Hansen in 2014. It is primarily used by researchers and practitioners in causal inference and econometrics.",
    "use_cases": [
      "Estimating treatment effects in observational studies",
      "Analyzing high-dimensional data for causal inference"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for double post lasso",
      "how to estimate treatment effects in python",
      "pydoublelasso documentation",
      "double lasso estimator python",
      "causal inference python package",
      "high-dimensional treatment effects in python"
    ],
    "primary_use_cases": [
      "high-dimensional treatment effect estimation"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Belloni-Chernozhukov-Hansen (2014)",
    "maintenance_status": "active"
  },
  {
    "name": "pyhtelasso",
    "description": "Debiased\u2011Lasso detector of heterogeneous treatment effects in randomized experiments.",
    "category": "Double/Debiased Machine Learning (DML)",
    "docs_url": "https://pypi.org/project/pyhtelasso/",
    "github_url": null,
    "url": "https://pypi.org/project/pyhtelasso/",
    "install": "pip install pyhtelasso",
    "tags": [
      "machine learning",
      "causal inference"
    ],
    "best_for": "High-dimensional controls, ML-based causal inference",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "pyhtelasso is a Python package designed for detecting heterogeneous treatment effects in randomized experiments using a debiased Lasso approach. It is useful for researchers and practitioners in the field of causal inference and machine learning.",
    "use_cases": [
      "Analyzing treatment effects in clinical trials",
      "Evaluating policy interventions in social sciences"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for debiased Lasso",
      "how to detect heterogeneous treatment effects in python",
      "causal inference tools in python",
      "randomized experiments analysis python",
      "machine learning for treatment effects",
      "debiased Lasso implementation python"
    ],
    "primary_use_cases": [
      "causal forest estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "DeclareDesign",
    "description": "Ex ante experimental design declaration and diagnosis. Enables researchers to formally describe their research design, diagnose statistical properties via simulation, and improve designs before data collection.",
    "category": "Experimental Design",
    "docs_url": "https://declaredesign.org/",
    "github_url": "https://github.com/DeclareDesign/DeclareDesign",
    "url": "https://cran.r-project.org/package=DeclareDesign",
    "install": "install.packages(\"DeclareDesign\")",
    "tags": [
      "experimental-design",
      "pre-registration",
      "power-analysis",
      "simulation",
      "design-diagnosis"
    ],
    "best_for": "Ex ante experimental design declaration and diagnosis via simulation",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "experimental-design"
    ],
    "summary": "DeclareDesign is a package that allows researchers to formally describe their research design and diagnose its statistical properties through simulation. It is primarily used by researchers looking to improve their experimental designs before data collection.",
    "use_cases": [
      "Improving experimental designs before data collection",
      "Diagnosing statistical properties of research designs"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for experimental design",
      "how to diagnose research design in R",
      "pre-registration tools for experiments",
      "power analysis in R",
      "simulation for experimental design",
      "design diagnosis R package"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "contextual",
    "description": "Multi-armed bandit algorithms including Thompson Sampling, UCB, and LinUCB. Directly applicable to adaptive A/B testing and recommendation optimization with simulation and evaluation tools.",
    "category": "Experimental Design",
    "docs_url": "https://nth-iteration-labs.github.io/contextual/",
    "github_url": "https://github.com/Nth-iteration-labs/contextual",
    "url": "https://cran.r-project.org/package=contextual",
    "install": "install.packages(\"contextual\")",
    "tags": [
      "bandits",
      "Thompson-sampling",
      "UCB",
      "adaptive-experiments",
      "A/B-testing"
    ],
    "best_for": "Multi-armed bandits for adaptive A/B testing with Thompson Sampling, UCB, LinUCB",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "bandits",
      "adaptive-experiments",
      "A/B-testing"
    ],
    "summary": "The 'contextual' package implements multi-armed bandit algorithms such as Thompson Sampling, UCB, and LinUCB. It is designed for adaptive A/B testing and recommendation optimization, providing simulation and evaluation tools for users.",
    "use_cases": [
      "Adaptive A/B testing",
      "Recommendation system optimization"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for multi-armed bandits",
      "how to implement Thompson Sampling in R",
      "adaptive A/B testing in R",
      "recommendation optimization with R",
      "UCB algorithm in R",
      "evaluate bandit algorithms in R"
    ],
    "primary_use_cases": [
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "fabricatr",
    "description": "Simulates realistic social science data for power analysis and design testing. Creates hierarchical data structures with correlated variables matching real-world patterns.",
    "category": "Experimental Design",
    "docs_url": "https://declaredesign.org/r/fabricatr/",
    "github_url": "https://github.com/DeclareDesign/fabricatr",
    "url": "https://cran.r-project.org/package=fabricatr",
    "install": "install.packages(\"fabricatr\")",
    "tags": [
      "data-simulation",
      "power-analysis",
      "hierarchical-data",
      "synthetic-data",
      "design-testing"
    ],
    "best_for": "Simulating realistic hierarchical data for experimental power analysis and design testing",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "fabricatr simulates realistic social science data for power analysis and design testing. It is used by researchers and data scientists to create hierarchical data structures with correlated variables that reflect real-world patterns.",
    "use_cases": [
      "Simulating data for social science research",
      "Testing design methodologies in experimental studies"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R library for data simulation",
      "how to simulate social science data in R",
      "R package for power analysis",
      "creating hierarchical data in R",
      "synthetic data generation in R",
      "design testing with R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "randomizr",
    "description": "Proper randomization procedures for experiments with known assignment probabilities. Implements simple, complete, block, and cluster randomization with exact probability calculations for IPW estimation.",
    "category": "Experimental Design",
    "docs_url": "https://declaredesign.org/r/randomizr/",
    "github_url": "https://github.com/DeclareDesign/randomizr",
    "url": "https://cran.r-project.org/package=randomizr",
    "install": "install.packages(\"randomizr\")",
    "tags": [
      "randomization",
      "block-randomization",
      "cluster-randomization",
      "assignment-probability",
      "experiments"
    ],
    "best_for": "Proper experimental randomization with exact assignment probabilities for IPW",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "experimental-design"
    ],
    "summary": "The randomizr package provides proper randomization procedures for experiments with known assignment probabilities. It is used by researchers and practitioners conducting experiments that require randomization techniques.",
    "use_cases": [
      "Conducting randomized controlled trials",
      "Implementing block randomization in experiments"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "R package for randomization",
      "how to perform block randomization in R",
      "cluster randomization techniques in R",
      "randomization procedures for experiments in R"
    ],
    "primary_use_cases": [
      "block randomization",
      "cluster randomization"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "Nashpy",
    "description": "Computation of Nash equilibria for 2-player games. Support enumeration and Lemke-Howson algorithm.",
    "category": "Game Theory & Mechanism Design",
    "docs_url": "https://nashpy.readthedocs.io/",
    "github_url": "https://github.com/drvinceknight/Nashpy",
    "url": "https://github.com/drvinceknight/Nashpy",
    "install": "pip install nashpy",
    "tags": [
      "game theory",
      "Nash equilibrium"
    ],
    "best_for": "2-player Nash equilibrium computation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "game theory"
    ],
    "summary": "Nashpy is a Python library designed for the computation of Nash equilibria in 2-player games. It supports enumeration methods and the Lemke-Howson algorithm, making it useful for researchers and practitioners in game theory.",
    "use_cases": [
      "Computing Nash equilibria for strategic games",
      "Analyzing 2-player game scenarios",
      "Research in game theory applications"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for Nash equilibria",
      "how to compute Nash equilibria in python",
      "Nashpy documentation",
      "game theory library python",
      "Lemke-Howson algorithm python",
      "enumerate Nash equilibria python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "OpenSpiel",
    "description": "DeepMind's 70+ game environments with multi-agent RL algorithms including Alpha-Rank, Neural Fictitious Self-Play, and CFR variants.",
    "category": "Game Theory & Mechanism Design",
    "docs_url": "https://openspiel.readthedocs.io/",
    "github_url": "https://github.com/deepmind/open_spiel",
    "url": "https://github.com/deepmind/open_spiel",
    "install": "pip install open_spiel",
    "tags": [
      "game theory",
      "reinforcement learning",
      "multi-agent"
    ],
    "best_for": "Multi-agent RL and game-theoretic algorithms",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "game theory",
      "reinforcement learning",
      "multi-agent"
    ],
    "summary": "OpenSpiel is a library developed by DeepMind that provides over 70 game environments for multi-agent reinforcement learning. It includes various algorithms such as Alpha-Rank, Neural Fictitious Self-Play, and CFR variants, making it suitable for researchers and practitioners in the field of game theory and AI.",
    "use_cases": [
      "Testing multi-agent reinforcement learning algorithms",
      "Simulating game environments for research",
      "Developing AI strategies for competitive games"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for game theory",
      "how to implement multi-agent reinforcement learning in python",
      "DeepMind OpenSpiel tutorial",
      "game environments for reinforcement learning in python",
      "reinforcement learning algorithms in OpenSpiel",
      "multi-agent game theory library python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "fairpy",
    "description": "Fair division algorithms from academic papers. Implements cake-cutting and item allocation procedures.",
    "category": "Game Theory & Mechanism Design",
    "docs_url": "https://fairpy.readthedocs.io/",
    "github_url": "https://github.com/erelsgl/fairpy",
    "url": "https://github.com/erelsgl/fairpy",
    "install": "pip install fairpy",
    "tags": [
      "fair division",
      "allocation",
      "mechanism design"
    ],
    "best_for": "Fair division and cake-cutting algorithms",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "game theory",
      "mechanism design"
    ],
    "summary": "Fairpy provides implementations of fair division algorithms based on academic research, focusing on cake-cutting and item allocation procedures. It is useful for researchers and practitioners in game theory and mechanism design.",
    "use_cases": [
      "Dividing resources fairly among multiple parties",
      "Allocating items to participants in a fair manner"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for fair division",
      "how to implement cake-cutting in python",
      "item allocation algorithms in python",
      "fair allocation procedures python",
      "mechanism design library python",
      "fairpy documentation"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active"
  },
  {
    "name": "fairpyx",
    "description": "Course-seat allocation with capacity constraints. Practical fair division for university course assignment.",
    "category": "Game Theory & Mechanism Design",
    "docs_url": null,
    "github_url": "https://github.com/ariel-research/fairpyx",
    "url": "https://github.com/ariel-research/fairpyx",
    "install": "pip install fairpyx",
    "tags": [
      "fair division",
      "course allocation",
      "mechanism design"
    ],
    "best_for": "Course-seat allocation with constraints",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "fairpyx is a Python package designed for practical fair division in university course assignment, focusing on course-seat allocation with capacity constraints. It is useful for universities and educational institutions looking to optimize course assignments fairly.",
    "use_cases": [
      "Allocating seats in university courses fairly",
      "Optimizing course assignments based on student preferences"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for course allocation",
      "how to implement fair division in python",
      "course-seat allocation in python",
      "mechanism design for course assignment",
      "fairpyx documentation",
      "python fair division package",
      "university course assignment tools",
      "capacity constraints in course allocation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "pygambit",
    "description": "N-player extensive form games with Alan Turing Institute support. Computes Nash, perfect, and sequential equilibria.",
    "category": "Game Theory & Mechanism Design",
    "docs_url": "https://gambitproject.readthedocs.io/",
    "github_url": "https://github.com/gambitproject/gambit",
    "url": "https://github.com/gambitproject/gambit",
    "install": "pip install pygambit",
    "tags": [
      "game theory",
      "extensive form",
      "equilibrium"
    ],
    "best_for": "N-player extensive form game solving",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "pygambit is a Python package designed for analyzing N-player extensive form games, providing tools to compute Nash, perfect, and sequential equilibria. It is particularly useful for researchers and practitioners in game theory and mechanism design.",
    "use_cases": [
      "Analyzing strategic interactions in economics",
      "Studying competitive behaviors in multi-agent systems"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for game theory",
      "how to compute Nash equilibrium in python",
      "extensive form games analysis python",
      "tools for game theory in python",
      "sequential equilibria computation python",
      "pygambit documentation",
      "N-player game analysis python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "gamlss",
    "description": "Distributional regression where all parameters of a response distribution (location, scale, shape) can be modeled as functions of predictors, supporting 100+ distributions including highly skewed and kurtotic continuous and discrete distributions.",
    "category": "Generalized Additive Models",
    "docs_url": "https://www.gamlss.com/",
    "github_url": "https://github.com/gamlss-dev/gamlss",
    "url": "https://cran.r-project.org/package=gamlss",
    "install": "install.packages(\"gamlss\")",
    "tags": [
      "distributional-regression",
      "location-scale-shape",
      "flexible-distributions",
      "centile-estimation",
      "beyond-mean-modeling"
    ],
    "best_for": "Modeling non-normal responses where variance, skewness, or kurtosis depend on predictors, implementing Rigby & Stasinopoulos (2005)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The 'gamlss' package allows users to model all parameters of a response distribution as functions of predictors, supporting over 100 distributions. It is useful for statisticians and data scientists who need to analyze complex data distributions beyond traditional mean modeling.",
    "use_cases": [
      "Modeling highly skewed data distributions",
      "Estimating centiles for non-normal data"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for distributional regression",
      "how to model location scale shape in R",
      "R flexible distributions library",
      "gamlss usage examples",
      "R centile estimation package",
      "advanced regression modeling in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "mgcv",
    "description": "The definitive GAM implementation providing generalized additive (mixed) models with automatic smoothness estimation via REML/GCV/ML, supporting thin plate splines, tensor products, multiple distributions, and scalable fitting for large datasets.",
    "category": "Generalized Additive Models",
    "docs_url": "https://cran.r-project.org/web/packages/mgcv/mgcv.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=mgcv",
    "install": "install.packages(\"mgcv\")",
    "tags": [
      "GAM",
      "splines",
      "smoothing",
      "penalized-regression",
      "mixed-models"
    ],
    "best_for": "Flexible nonparametric regression with automatic smoothing parameter selection, implementing Wood (2017)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The mgcv package provides a comprehensive implementation of generalized additive models (GAMs) and mixed models in R, allowing users to estimate smooth functions and fit complex models to data. It is widely used by statisticians and data scientists for tasks involving non-linear relationships and large datasets.",
    "use_cases": [
      "Modeling non-linear relationships in data",
      "Analyzing large datasets with smooth functions"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for generalized additive models",
      "how to fit GAMs in R",
      "R package for smoothness estimation",
      "using mgcv for mixed models",
      "automated smoothness estimation in R",
      "GAM implementation in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "GeoLift",
    "description": "Meta's end-to-end synthetic control for geo experiments with multi-cell testing and power calculations.",
    "category": "Geo-Experiments & Lift Measurement",
    "docs_url": "https://facebookincubator.github.io/GeoLift/",
    "github_url": "https://github.com/facebookincubator/GeoLift",
    "url": "https://github.com/facebookincubator/GeoLift",
    "install": "pip install geolift",
    "tags": [
      "geo-experiments",
      "synthetic control",
      "incrementality"
    ],
    "best_for": "Meta's geo-level incrementality measurement",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "geo-experiments",
      "incrementality"
    ],
    "summary": "GeoLift is a Python package designed for conducting geo experiments using synthetic control methods. It is particularly useful for researchers and data scientists looking to measure lift and perform multi-cell testing.",
    "use_cases": [
      "Evaluating marketing campaign effectiveness",
      "Assessing policy impact in different regions"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for geo experiments",
      "how to measure lift in python",
      "synthetic control methods in python",
      "incrementality testing in python",
      "multi-cell testing in python",
      "python package for causal inference"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "matched_markets",
    "description": "Google's time-based regression with greedy search for optimal geo experiment groups.",
    "category": "Geo-Experiments & Lift Measurement",
    "docs_url": null,
    "github_url": "https://github.com/google/matched_markets",
    "url": "https://github.com/google/matched_markets",
    "install": "pip install matched-markets",
    "tags": [
      "geo-experiments",
      "market matching",
      "incrementality"
    ],
    "best_for": "Optimal geo experiment group selection",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "geo-experiments"
    ],
    "summary": "The matched_markets package provides a time-based regression method with a greedy search algorithm to create optimal groups for geo experiments. It is useful for data scientists and researchers looking to measure lift and incrementality in marketing experiments.",
    "use_cases": [
      "Optimizing geo experiment groups",
      "Measuring marketing lift",
      "Conducting A/B tests with geographic considerations"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for geo experiments",
      "how to measure incrementality in python",
      "optimal market matching in python",
      "time-based regression for experiments",
      "greedy search for geo experiments",
      "python package for lift measurement",
      "market matching algorithms in python",
      "geo experiment analysis in python"
    ],
    "primary_use_cases": [
      "geo experiment design",
      "incrementality measurement"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "trimmed_match",
    "description": "Google's robust analysis for paired geo experiments using trimmed statistics. Handles outliers in geo-level data.",
    "category": "Geo-Experiments & Lift Measurement",
    "docs_url": null,
    "github_url": "https://github.com/google/trimmed_match",
    "url": "https://github.com/google/trimmed_match",
    "install": "pip install trimmed-match",
    "tags": [
      "geo-experiments",
      "robust statistics",
      "incrementality"
    ],
    "best_for": "Robust paired geo experiment analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "geo-experiments",
      "robust statistics",
      "incrementality"
    ],
    "summary": "trimmed_match is a Python package designed for robust analysis of paired geo experiments using trimmed statistics. It is particularly useful for handling outliers in geo-level data, making it suitable for data scientists and researchers in the field of causal inference.",
    "use_cases": [
      "Analyzing the impact of marketing campaigns across different regions",
      "Evaluating the effectiveness of policy changes in urban areas"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for geo experiments",
      "how to analyze paired geo data in python",
      "robust statistics in geo-level analysis",
      "incrementality measurement in python",
      "handling outliers in geo experiments",
      "A/B testing with trimmed statistics in python"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "Awesome Economics",
    "description": "Curated list of economics resources including datasets, software, courses, and blogs.",
    "category": "Inference & Reporting Tools",
    "docs_url": null,
    "github_url": "https://github.com/antontarasenko/awesome-economics",
    "url": "https://github.com/antontarasenko/awesome-economics",
    "install": "",
    "tags": [
      "curated list",
      "resources"
    ],
    "best_for": "Comprehensive economics resource directory",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Awesome Economics is a curated list of resources for economics, including datasets, software, courses, and blogs. It is useful for anyone looking to explore various economics-related tools and information.",
    "use_cases": [],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for economics resources",
      "how to find economics datasets in python",
      "best economics courses in python",
      "python tools for economics blogs"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active"
  },
  {
    "name": "Computational Methods for practitioners",
    "description": "Open-source textbook by Richard Evans on computational methods for researchers using Python.",
    "category": "Inference & Reporting Tools",
    "docs_url": "https://opensourceecon.github.io/CompMethods/",
    "github_url": "https://github.com/OpenSourceEcon/CompMethods",
    "url": "https://opensourceecon.github.io/CompMethods/",
    "install": "",
    "tags": [
      "education",
      "computation",
      "textbook"
    ],
    "best_for": "Comprehensive computational economics course",
    "language": "Python",
    "difficulty": "beginner|intermediate",
    "prerequisites": [],
    "topic_tags": [
      "computation",
      "education"
    ],
    "summary": "This package is an open-source textbook designed for researchers to learn computational methods using Python. It is suitable for those interested in applying these methods in various research contexts.",
    "use_cases": [],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for computational methods",
      "how to use Python for research",
      "open-source textbook on Python",
      "learning computational methods in Python",
      "Python tools for researchers",
      "educational resources for Python"
    ],
    "api_complexity": "simple"
  },
  {
    "name": "Econ Project Templates",
    "description": "Cookiecutter templates for reproducible economics research projects. Standardized project structure.",
    "category": "Inference & Reporting Tools",
    "docs_url": "https://econ-project-templates.readthedocs.io/",
    "github_url": "https://github.com/OpenSourceEconomics/econ-project-templates",
    "url": "https://github.com/OpenSourceEconomics/econ-project-templates",
    "install": "",
    "tags": [
      "reproducibility",
      "templates",
      "workflow"
    ],
    "best_for": "Starting reproducible economics projects",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Econ Project Templates provides cookiecutter templates designed for reproducible economics research projects, offering a standardized project structure. It is useful for researchers in economics looking to streamline their project setup.",
    "use_cases": [],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for reproducible economics research",
      "how to create a standardized project structure in python",
      "cookiecutter templates for economics",
      "economics research project setup in python"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active"
  },
  {
    "name": "First Course in Causal Inference (Python)",
    "description": "Python implementation of Peng Ding's textbook 'A First Course in Causal Inference'. Educational resource with code examples.",
    "category": "Inference & Reporting Tools",
    "docs_url": "https://github.com/apoorvalal/ding_causalInference_python",
    "github_url": "https://github.com/apoorvalal/ding_causalInference_python",
    "url": "https://github.com/apoorvalal/ding_causalInference_python",
    "install": "",
    "tags": [
      "causal inference",
      "education",
      "textbook"
    ],
    "best_for": "Learning causal inference with Peng Ding's textbook",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "education"
    ],
    "summary": "This package provides a Python implementation of Peng Ding's textbook 'A First Course in Causal Inference', offering educational resources and code examples for learning causal inference. It is designed for students and practitioners interested in understanding causal analysis.",
    "use_cases": [],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to learn causal inference in python",
      "educational resources for causal analysis in python",
      "examples of causal inference in python"
    ],
    "api_complexity": "simple"
  },
  {
    "name": "Python Packages for Applied Economists",
    "description": "Curated collection of Python packages for applied researchers organized by functionality.",
    "category": "Inference & Reporting Tools",
    "docs_url": null,
    "github_url": "https://github.com/clibassi/python-packages-for-applied-economists",
    "url": "https://github.com/clibassi/python-packages-for-applied-economists",
    "install": "",
    "tags": [
      "curated list",
      "resources"
    ],
    "best_for": "Discovering econometrics packages by use case",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "This package provides a curated collection of Python packages specifically designed for applied researchers. It is useful for those looking to enhance their research with various tools organized by functionality.",
    "use_cases": [],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for applied economists",
      "how to analyze data in python",
      "python packages for statistical analysis",
      "best python tools for econometrics",
      "resources for econometric analysis in python",
      "python libraries for data reporting"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active"
  },
  {
    "name": "clusterbootstraps",
    "description": "Wild cluster bootstrap and pairs cluster bootstrap implementations for clustered standard errors.",
    "category": "Inference & Reporting Tools",
    "docs_url": null,
    "github_url": "https://github.com/BingkunLin/clusterbootstraps",
    "url": "https://pypi.org/project/clusterbootstraps/",
    "install": "pip install clusterbootstraps",
    "tags": [
      "bootstrap",
      "clustered errors",
      "inference"
    ],
    "best_for": "Alternative cluster bootstrap implementations",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The clusterbootstraps package provides implementations for wild cluster bootstrap and pairs cluster bootstrap methods, which are useful for calculating clustered standard errors. It is primarily used by data scientists and statisticians working with clustered data.",
    "use_cases": [],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for cluster bootstrap",
      "how to calculate clustered standard errors in python",
      "wild cluster bootstrap implementation python",
      "pairs cluster bootstrap python",
      "bootstrapping techniques in python",
      "clustered errors analysis python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "maketables",
    "description": "Publication-ready regression tables for pyfixest, statsmodels, linearmodels. Outputs HTML (great-tables), LaTeX, Word.",
    "category": "Inference & Reporting Tools",
    "docs_url": null,
    "github_url": "https://github.com/py-econometrics/maketables",
    "url": "https://github.com/py-econometrics/maketables",
    "install": "pip install maketables",
    "tags": [
      "reporting",
      "tables",
      "visualization"
    ],
    "best_for": "Multi-format regression tables from pyfixest/statsmodels",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "maketables is a Python package that generates publication-ready regression tables for various statistical modeling libraries such as pyfixest, statsmodels, and linearmodels. It is useful for researchers and data scientists who need to present their regression results in a clear and professional format.",
    "use_cases": [
      "Generating regression tables for academic publications",
      "Creating HTML reports for data analysis"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for regression tables",
      "how to create publication-ready tables in python",
      "maketables package documentation",
      "best python libraries for reporting",
      "generate LaTeX tables in python",
      "visualization tools for regression results"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active"
  },
  {
    "name": "ShiftShareSE",
    "description": "Implements correct standard errors for Bartik/shift-share instrumental variables designs following Ad\u00e3o, Koles\u00e1r, and Morales (2019 QJE). Standard clustered SEs are typically incorrect for shift-share\u2014this package provides econometrically valid inference.",
    "category": "Instrumental Variables",
    "docs_url": "https://cran.r-project.org/web/packages/ShiftShareSE/ShiftShareSE.pdf",
    "github_url": "https://github.com/kolesarm/ShiftShareSE",
    "url": "https://cran.r-project.org/package=ShiftShareSE",
    "install": "install.packages(\"ShiftShareSE\")",
    "tags": [
      "shift-share",
      "Bartik",
      "instrumental-variables",
      "standard-errors",
      "regional-economics"
    ],
    "best_for": "Correct standard errors for Bartik/shift-share IV designs, implementing Ad\u00e3o, Koles\u00e1r & Morales (2019 QJE)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "econometrics"
    ],
    "summary": "ShiftShareSE implements correct standard errors for Bartik/shift-share instrumental variables designs, providing econometrically valid inference. It is primarily used by researchers and practitioners in regional economics.",
    "use_cases": [
      "Estimating the impact of regional economic policies",
      "Analyzing labor market effects using shift-share designs"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for shift-share analysis",
      "how to implement Bartik IV in R",
      "standard errors for shift-share designs in R",
      "econometric inference with R package",
      "Bartik instrumental variables R",
      "shift-share standard errors R package"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Ad\u00e3o, Koles\u00e1r, and Morales (2019)",
    "maintenance_status": "active"
  },
  {
    "name": "gmm",
    "description": "Generalized Method of Moments estimation implementing two-step GMM, iterated GMM, and continuous updated estimator (CUE) with HAC covariance matrices. Supports linear and nonlinear moment conditions.",
    "category": "Instrumental Variables",
    "docs_url": "https://cran.r-project.org/web/packages/gmm/gmm.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=gmm",
    "install": "install.packages(\"gmm\")",
    "tags": [
      "GMM",
      "method-of-moments",
      "HAC",
      "instrumental-variables",
      "CUE"
    ],
    "best_for": "Generalized Method of Moments estimation with two-step, iterated, and CUE estimators",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "instrumental-variables"
    ],
    "summary": "The gmm package provides tools for Generalized Method of Moments estimation, including two-step GMM, iterated GMM, and continuous updated estimator (CUE) with HAC covariance matrices. It is used by statisticians and data scientists for estimating models with linear and nonlinear moment conditions.",
    "use_cases": [
      "Estimating parameters in econometric models",
      "Conducting hypothesis tests with GMM",
      "Analyzing time series data with HAC covariance"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for GMM estimation",
      "how to perform two-step GMM in R",
      "HAC covariance matrices in R",
      "GMM method-of-moments R",
      "instrumental variables estimation R",
      "CUE estimation in R"
    ],
    "primary_use_cases": [
      "parameter estimation using GMM",
      "hypothesis testing with moment conditions"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "ivmodel",
    "description": "Specialized package for weak instrument diagnostics implementing Anderson-Rubin tests, k-class estimators (LIML, Fuller), and sensitivity analysis following Jiang et al. (2015). Essential when instrument strength is questionable.",
    "category": "Instrumental Variables",
    "docs_url": "https://cran.r-project.org/web/packages/ivmodel/ivmodel.pdf",
    "github_url": "https://github.com/hyunseungkang/ivmodel",
    "url": "https://cran.r-project.org/package=ivmodel",
    "install": "install.packages(\"ivmodel\")",
    "tags": [
      "instrumental-variables",
      "weak-instruments",
      "Anderson-Rubin",
      "LIML",
      "sensitivity-analysis"
    ],
    "best_for": "Weak instrument diagnostics with Anderson-Rubin tests and k-class estimators (LIML, Fuller)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The ivmodel package provides tools for weak instrument diagnostics, including Anderson-Rubin tests and k-class estimators like LIML and Fuller. It is essential for researchers dealing with questionable instrument strength in their econometric models.",
    "use_cases": [
      "Evaluating instrument strength in econometric models",
      "Conducting sensitivity analysis for instrumental variable estimates"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for weak instrument diagnostics",
      "how to perform Anderson-Rubin tests in R",
      "R LIML estimator package",
      "sensitivity analysis in R",
      "diagnosing weak instruments R",
      "ivmodel package usage",
      "econometrics package for R"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Jiang et al. (2015)",
    "maintenance_status": "active"
  },
  {
    "name": "ivreg",
    "description": "Modern implementation of two-stage least squares (2SLS) instrumental variables regression with comprehensive diagnostics including hat values, studentized residuals, and component-plus-residual plots. Successor to AER's ivreg() function with superior diagnostic tools.",
    "category": "Instrumental Variables",
    "docs_url": "https://zeileis.github.io/ivreg/",
    "github_url": "https://github.com/zeileis/ivreg",
    "url": "https://cran.r-project.org/package=ivreg",
    "install": "install.packages(\"ivreg\")",
    "tags": [
      "instrumental-variables",
      "2SLS",
      "IV-regression",
      "endogeneity",
      "diagnostics"
    ],
    "best_for": "Modern 2SLS instrumental variables regression with comprehensive diagnostic tools",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "diagnostics"
    ],
    "summary": "The ivreg package provides a modern implementation of two-stage least squares (2SLS) instrumental variables regression, offering comprehensive diagnostics such as hat values and studentized residuals. It is designed for users needing advanced regression analysis tools, particularly in the context of endogeneity.",
    "use_cases": [
      "Analyzing the impact of policy changes using instrumental variables",
      "Estimating causal relationships in economics",
      "Conducting regression analysis with endogeneity concerns"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for instrumental variables regression",
      "how to perform 2SLS in R",
      "diagnostics for IV regression in R",
      "R library for endogeneity correction",
      "hat values in R ivreg",
      "studentized residuals in R",
      "component-plus-residual plots in R"
    ],
    "primary_use_cases": [
      "instrumental variables regression",
      "diagnostic analysis for regression models"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "AER"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "momentfit",
    "description": "Modern S4-based implementation of Generalized Method of Moments supporting systems of equations, nonlinear moment conditions, and hypothesis testing. Successor to gmm package with object-oriented design.",
    "category": "Instrumental Variables",
    "docs_url": "https://cran.r-project.org/web/packages/momentfit/momentfit.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=momentfit",
    "install": "install.packages(\"momentfit\")",
    "tags": [
      "GMM",
      "S4-class",
      "systems-estimation",
      "moment-conditions",
      "hypothesis-testing"
    ],
    "best_for": "Modern object-oriented GMM estimation for systems of equations",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "instrumental-variables",
      "hypothesis-testing"
    ],
    "summary": "Momentfit is a modern S4-based implementation of the Generalized Method of Moments (GMM) that supports systems of equations and nonlinear moment conditions. It is designed for users who need to perform hypothesis testing in econometric models.",
    "use_cases": [
      "Estimating parameters in econometric models",
      "Testing hypotheses in economic research"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for Generalized Method of Moments",
      "how to perform hypothesis testing in R",
      "R GMM implementation",
      "S4 class in R for econometrics",
      "nonlinear moment conditions in R",
      "systems of equations in R"
    ],
    "primary_use_cases": [
      "parameter estimation in econometrics",
      "hypothesis testing in econometric models"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "gmm"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "systemfit",
    "description": "Simultaneous systems estimation implementing Seemingly Unrelated Regression (SUR), two-stage least squares (2SLS), and three-stage least squares (3SLS). Critical for demand systems and structural macro models.",
    "category": "Instrumental Variables",
    "docs_url": "https://cran.r-project.org/web/packages/systemfit/systemfit.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=systemfit",
    "install": "install.packages(\"systemfit\")",
    "tags": [
      "SUR",
      "2SLS",
      "3SLS",
      "systems-estimation",
      "demand-systems"
    ],
    "best_for": "Simultaneous equation systems: SUR, 2SLS, and 3SLS estimation for demand systems and structural models",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "instrumental-variables",
      "econometrics"
    ],
    "summary": "The systemfit package allows for simultaneous estimation of systems of equations using methods such as Seemingly Unrelated Regression (SUR), two-stage least squares (2SLS), and three-stage least squares (3SLS). It is particularly useful for economists and researchers working on demand systems and structural macroeconomic models.",
    "use_cases": [
      "Estimating demand systems for consumer goods",
      "Analyzing structural macroeconomic models",
      "Conducting policy impact assessments"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for simultaneous systems estimation",
      "how to perform SUR in R",
      "R 2SLS implementation",
      "best practices for demand systems in R",
      "three-stage least squares R tutorial",
      "econometric modeling with R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "py-econometrics `gmm`",
    "description": "Lightweight package for setting up and estimating custom GMM models based on user-defined moment conditions.",
    "category": "Instrumental Variables (IV) & GMM",
    "docs_url": "https://github.com/py-econometrics/gmm",
    "github_url": null,
    "url": "https://github.com/py-econometrics/gmm",
    "install": "pip install gmm",
    "tags": [
      "IV",
      "GMM"
    ],
    "best_for": "Endogeneity correction, 2SLS, moment estimation",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "py-econometrics `gmm` is a lightweight package designed for setting up and estimating custom Generalized Method of Moments (GMM) models based on user-defined moment conditions. It is useful for researchers and practitioners in econometrics who need to implement GMM estimation tailored to their specific requirements.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for GMM estimation",
      "how to set up custom GMM models in python",
      "GMM models in econometrics python",
      "estimating moment conditions in python",
      "lightweight GMM package python",
      "using GMM for econometric analysis in python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "CausalMotifs",
    "description": "Meta's library for estimating heterogeneous spillover effects in A/B tests. Handles network interference.",
    "category": "Interference & Spillovers",
    "docs_url": "https://github.com/facebookresearch/CausalMotifs",
    "github_url": "https://github.com/facebookresearch/CausalMotifs",
    "url": "https://github.com/facebookresearch/CausalMotifs",
    "install": "pip install causal-motifs",
    "tags": [
      "network interference",
      "spillovers",
      "A/B testing"
    ],
    "best_for": "Spillover effects in social networks",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "network-analysis"
    ],
    "summary": "CausalMotifs is a library designed to estimate heterogeneous spillover effects in A/B tests, particularly in the presence of network interference. It is useful for researchers and data scientists conducting experiments where interactions between subjects may affect outcomes.",
    "use_cases": [
      "Estimating spillover effects in marketing campaigns",
      "Analyzing the impact of social networks on treatment effects"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for estimating spillover effects",
      "how to analyze A/B tests with network interference in python",
      "CausalMotifs documentation",
      "best practices for A/B testing in python",
      "network interference analysis tools",
      "spillover effects in experiments python"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "spilled_t",
    "description": "Treatment and spillover effect estimation under network interference. Separates direct and indirect effects.",
    "category": "Interference & Spillovers",
    "docs_url": "https://github.com/mpleung/spilled_t",
    "github_url": "https://github.com/mpleung/spilled_t",
    "url": "https://github.com/mpleung/spilled_t",
    "install": "pip install spilled_t",
    "tags": [
      "network interference",
      "spillovers",
      "treatment effects"
    ],
    "best_for": "Separating direct and spillover effects",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "network-analysis"
    ],
    "summary": "The 'spilled_t' package estimates treatment and spillover effects in the presence of network interference. It is useful for researchers and practitioners who need to separate direct and indirect effects in their analyses.",
    "use_cases": [
      "Estimating treatment effects in social networks",
      "Analyzing the impact of interventions in connected populations"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for treatment effect estimation",
      "how to analyze spillover effects in python",
      "network interference analysis in python",
      "estimating indirect effects with python",
      "spillover effect estimation library",
      "causal inference tools in python"
    ],
    "primary_use_cases": [
      "treatment effect estimation",
      "spillover effect analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "testinterference",
    "description": "Statistical tests for SUTVA violations and spillover hypotheses. Detects network interference in experiments.",
    "category": "Interference & Spillovers",
    "docs_url": "https://github.com/tkhdyanagi/testinterference",
    "github_url": "https://github.com/tkhdyanagi/testinterference",
    "url": "https://github.com/tkhdyanagi/testinterference",
    "install": "pip install testinterference",
    "tags": [
      "SUTVA",
      "spillovers",
      "hypothesis testing"
    ],
    "best_for": "Testing for spillover effects",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "hypothesis-testing"
    ],
    "summary": "The testinterference package provides statistical tests to identify violations of the Stable Unit Treatment Value Assumption (SUTVA) and to analyze spillover effects in experimental designs. It is useful for researchers and practitioners in the field of causal inference who are investigating network interference in experiments.",
    "use_cases": [
      "Analyzing network interference in randomized controlled trials",
      "Testing for SUTVA violations in experimental data"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for SUTVA violations",
      "how to test for spillover effects in python",
      "statistical tests for interference in experiments",
      "detecting network interference in python",
      "hypothesis testing for spillovers",
      "python package for causal inference"
    ],
    "primary_use_cases": [
      "SUTVA violation detection",
      "spillover hypothesis testing"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "glmnet",
    "description": "Efficient procedures for fitting regularized generalized linear models via penalized maximum likelihood. Implements LASSO, ridge regression, and elastic net with extremely fast coordinate descent algorithms. Foundation for high-dimensional regression and causal ML.",
    "category": "Machine Learning",
    "docs_url": "https://glmnet.stanford.edu/",
    "github_url": null,
    "url": "https://cran.r-project.org/package=glmnet",
    "install": "install.packages(\"glmnet\")",
    "tags": [
      "LASSO",
      "ridge",
      "elastic-net",
      "regularization",
      "high-dimensional"
    ],
    "best_for": "LASSO, ridge, and elastic net regularization\u2014foundation for high-dimensional regression and causal ML",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "glmnet provides efficient procedures for fitting regularized generalized linear models through penalized maximum likelihood. It is widely used by data scientists and statisticians for high-dimensional regression and causal machine learning applications.",
    "use_cases": [
      "Fitting LASSO models for feature selection",
      "Applying ridge regression for multicollinearity issues"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for LASSO",
      "how to perform ridge regression in R",
      "glmnet tutorial",
      "R elastic net example",
      "high-dimensional regression in R",
      "causal ML with glmnet"
    ],
    "primary_use_cases": [
      "high-dimensional regression",
      "causal ML"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "caret",
      "glm",
      "lme4"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "ranger",
    "description": "Fast implementation of random forests particularly suited for high-dimensional data. Provides survival forests, classification, and regression with efficient memory usage. Core backend for grf's causal forests.",
    "category": "Machine Learning",
    "docs_url": "https://cran.r-project.org/web/packages/ranger/ranger.pdf",
    "github_url": "https://github.com/imbs-hl/ranger",
    "url": "https://cran.r-project.org/package=ranger",
    "install": "install.packages(\"ranger\")",
    "tags": [
      "random-forests",
      "survival-forests",
      "high-dimensional",
      "fast",
      "causal-forests"
    ],
    "best_for": "Fast random forests for high-dimensional data\u2014backend for grf causal forests",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "machine-learning",
      "random-forests",
      "survival-analysis"
    ],
    "summary": "Ranger is a fast implementation of random forests that is particularly suited for high-dimensional data. It provides capabilities for survival forests, classification, and regression, making it useful for data scientists and statisticians working with complex datasets.",
    "use_cases": [
      "Predicting outcomes in high-dimensional datasets",
      "Conducting survival analysis with random forests"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for random forests",
      "how to implement survival forests in R",
      "fast random forests in R",
      "R package for high-dimensional data analysis"
    ],
    "primary_use_cases": [
      "causal forest estimation"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "randomForest",
      "grf"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "tidymodels",
    "description": "Modern framework for modeling and machine learning using tidyverse principles. Meta-package including parsnip (model specification), recipes (preprocessing), workflows, tune (hyperparameter tuning), and yardstick (metrics). Successor to caret.",
    "category": "Machine Learning",
    "docs_url": "https://www.tidymodels.org/",
    "github_url": "https://github.com/tidymodels/tidymodels",
    "url": "https://cran.r-project.org/package=tidymodels",
    "install": "install.packages(\"tidymodels\")",
    "tags": [
      "machine-learning",
      "tidyverse",
      "modeling-framework",
      "hyperparameter-tuning",
      "preprocessing"
    ],
    "best_for": "Modern tidyverse-native ML framework with reproducible workflows\u2014successor to caret",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "tidymodels is a modern framework for modeling and machine learning that adheres to tidyverse principles. It is designed for data scientists and analysts who want to streamline their modeling processes using a cohesive set of tools.",
    "use_cases": [
      "Building predictive models",
      "Performing hyperparameter tuning",
      "Preprocessing data for analysis"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for machine learning",
      "how to do hyperparameter tuning in R",
      "tidyverse modeling framework",
      "best practices for preprocessing in R",
      "R package for model specification",
      "how to use tidymodels for machine learning"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "caret"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "xgboost",
    "description": "Extreme Gradient Boosting implementing state-of-the-art gradient boosted decision trees. Highly efficient, scalable, and portable with interfaces to R, Python, and other languages. Essential for prediction in double ML workflows.",
    "category": "Machine Learning",
    "docs_url": "https://xgboost.readthedocs.io/",
    "github_url": "https://github.com/dmlc/xgboost",
    "url": "https://cran.r-project.org/package=xgboost",
    "install": "install.packages(\"xgboost\")",
    "tags": [
      "gradient-boosting",
      "XGBoost",
      "prediction",
      "machine-learning",
      "ensemble"
    ],
    "best_for": "State-of-the-art gradient boosting for prediction in causal ML and double ML workflows",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn-basics"
    ],
    "topic_tags": [
      "machine-learning",
      "ensemble"
    ],
    "summary": "XGBoost is an implementation of gradient boosted decision trees designed for efficiency and scalability. It is widely used in machine learning tasks, particularly for prediction in double ML workflows.",
    "use_cases": [
      "predicting outcomes in machine learning competitions",
      "building predictive models for business analytics"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for gradient boosting",
      "how to use XGBoost in R",
      "XGBoost for prediction",
      "best practices for XGBoost",
      "XGBoost tutorial",
      "gradient boosting with XGBoost"
    ],
    "primary_use_cases": [
      "prediction in double ML workflows"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "R",
      "Python"
    ],
    "related_packages": [
      "LightGBM",
      "CatBoost"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "emmeans",
    "description": "Estimated Marginal Means (least-squares means) for factorial designs. Computes adjusted means and contrasts for balanced and unbalanced designs, with support for mixed models and Bayesian models.",
    "category": "Marginal Effects",
    "docs_url": "https://rvlenth.github.io/emmeans/",
    "github_url": "https://github.com/rvlenth/emmeans",
    "url": "https://cran.r-project.org/package=emmeans",
    "install": "install.packages(\"emmeans\")",
    "tags": [
      "marginal-means",
      "least-squares-means",
      "factorial-designs",
      "contrasts",
      "mixed-models"
    ],
    "best_for": "Estimated marginal means for factorial designs with interaction interpretation",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "marginal-effects",
      "mixed-models"
    ],
    "summary": "The emmeans package computes estimated marginal means, also known as least-squares means, for factorial designs. It is used by statisticians and data scientists to analyze and interpret the effects of factors in both balanced and unbalanced designs, including mixed and Bayesian models.",
    "use_cases": [
      "Analyzing the effects of different treatments in an experiment",
      "Comparing group means in a factorial design"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for estimated marginal means",
      "how to compute least-squares means in R",
      "R contrasts for factorial designs",
      "mixed models analysis in R",
      "bayesian models with emmeans",
      "marginal effects in R"
    ],
    "primary_use_cases": [
      "A/B test analysis",
      "contrasts in factorial designs"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "marginaleffects",
    "description": "Modern standard for interpreting regression results\u2014up to 1000\u00d7 faster than margins. Computes marginal effects, predictions, contrasts, and slopes for 100+ model classes. Published in JSS 2024.",
    "category": "Marginal Effects",
    "docs_url": "https://marginaleffects.com/",
    "github_url": "https://github.com/vincentarelbundock/marginaleffects",
    "url": "https://cran.r-project.org/package=marginaleffects",
    "install": "install.packages(\"marginaleffects\")",
    "tags": [
      "marginal-effects",
      "predictions",
      "contrasts",
      "interpretation",
      "slopes"
    ],
    "best_for": "Modern marginal effects interpretation\u20141000\u00d7 faster than margins with 100+ model support, JSS 2024",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "interpretation"
    ],
    "summary": "The marginaleffects package provides a modern standard for interpreting regression results, allowing users to compute marginal effects, predictions, contrasts, and slopes for over 100 model classes. It is designed for statisticians and data scientists who need efficient and accurate interpretation of regression outputs.",
    "use_cases": [
      "Interpreting regression results quickly",
      "Comparing model predictions",
      "Analyzing contrasts between groups"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for marginal effects",
      "how to compute predictions in R",
      "R package for regression interpretation",
      "marginal effects in R",
      "how to use contrasts in R",
      "R slopes analysis package"
    ],
    "primary_use_cases": [
      "computing marginal effects",
      "making predictions from regression models"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "JSS (2024)",
    "maintenance_status": "active"
  },
  {
    "name": "Lifetimes",
    "description": "Analyze customer lifetime value (CLV) using probabilistic models (BG/NBD, Pareto/NBD) to predict purchases.",
    "category": "Marketing Mix Models (MMM) & Business Analytics",
    "docs_url": "https://lifetimes.readthedocs.io/en/latest/",
    "github_url": "https://github.com/CamDavidsonPilon/lifetimes",
    "url": "https://github.com/CamDavidsonPilon/lifetimes",
    "install": "pip install lifetimes",
    "tags": [
      "marketing",
      "analytics"
    ],
    "best_for": "Marketing ROI, media mix optimization, attribution",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "bayesian",
      "analytics"
    ],
    "summary": "Lifetimes is a Python package designed to analyze customer lifetime value (CLV) using probabilistic models such as BG/NBD and Pareto/NBD. It is used by data scientists and marketers to predict customer purchases and optimize marketing strategies.",
    "use_cases": [
      "Predicting customer purchases over time",
      "Estimating customer lifetime value for marketing campaigns"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for customer lifetime value analysis",
      "how to predict purchases in python",
      "BG/NBD model implementation in python",
      "Pareto/NBD model for marketing",
      "analyze customer behavior with python",
      "customer analytics library python"
    ],
    "primary_use_cases": [
      "customer lifetime value analysis",
      "purchase prediction"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "MaMiMo",
    "description": "Lightweight Python library focused specifically on Marketing Mix Modeling implementation.",
    "category": "Marketing Mix Models (MMM) & Business Analytics",
    "docs_url": null,
    "github_url": "https://github.com/Garve/mamimo",
    "url": "https://github.com/Garve/mamimo",
    "install": "pip install mamimo",
    "tags": [
      "marketing",
      "analytics"
    ],
    "best_for": "Marketing ROI, media mix optimization, attribution",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "marketing",
      "analytics"
    ],
    "summary": "MaMiMo is a lightweight Python library designed for implementing Marketing Mix Modeling. It is particularly useful for analysts and data scientists working in marketing analytics.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for marketing mix modeling",
      "how to do marketing analytics in python",
      "lightweight python library for MMM",
      "best practices for marketing mix modeling in python",
      "implementing marketing mix models in python",
      "python tools for business analytics"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active"
  },
  {
    "name": "mmm_stan",
    "description": "Python/STAN implementation of Bayesian Marketing Mix Models.",
    "category": "Marketing Mix Models (MMM) & Business Analytics",
    "docs_url": null,
    "github_url": "https://github.com/sibylhe/mmm_stan",
    "url": "https://github.com/sibylhe/mmm_stan",
    "install": "GitHub Repository",
    "tags": [
      "marketing",
      "analytics",
      "Bayesian"
    ],
    "best_for": "Marketing ROI, media mix optimization, attribution",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "bayesian",
      "marketing",
      "analytics"
    ],
    "summary": "mmm_stan is a Python implementation of Bayesian Marketing Mix Models, designed to help businesses analyze and optimize their marketing strategies. It is primarily used by data scientists and marketers looking to leverage Bayesian methods for marketing analytics.",
    "use_cases": [
      "optimizing marketing spend",
      "analyzing the effectiveness of advertising campaigns"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for Bayesian Marketing Mix Models",
      "how to analyze marketing data with Python",
      "Bayesian analytics for marketing",
      "marketing mix modeling in Python",
      "using STAN for marketing analysis",
      "Bayesian methods for business analytics"
    ],
    "primary_use_cases": [
      "marketing mix modeling",
      "Bayesian analysis of marketing data"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "ziln_cltv",
    "description": "Google's Zero-Inflated Lognormal loss for heavily-tailed LTV distributions. Outputs both predicted LTV and churn probability.",
    "category": "Marketing Mix Models (MMM) & Business Analytics",
    "docs_url": null,
    "github_url": "https://github.com/google/lifetime_value",
    "url": "https://github.com/google/lifetime_value",
    "install": "pip install lifetime-value",
    "tags": [
      "LTV",
      "customer analytics",
      "churn"
    ],
    "best_for": "Customer LTV with zero-inflated distributions",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "customer analytics",
      "churn prediction"
    ],
    "summary": "The ziln_cltv package provides Google's Zero-Inflated Lognormal loss model for predicting customer Lifetime Value (LTV) and churn probability, specifically designed for heavily-tailed distributions. It is useful for marketers and data scientists looking to analyze customer behavior and retention.",
    "use_cases": [
      "Predicting customer lifetime value for subscription services",
      "Analyzing churn rates in e-commerce",
      "Estimating LTV for marketing campaigns"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for LTV prediction",
      "how to predict churn probability in python",
      "zero-inflated lognormal model python",
      "customer analytics tools in python",
      "python package for marketing mix models",
      "predicting customer lifetime value using python",
      "churn analysis with python",
      "business analytics library in python"
    ],
    "primary_use_cases": [
      "customer lifetime value prediction",
      "churn probability estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "algmatch",
    "description": "Student-Project Allocation with lecturer preferences. Extends matching to three-sided markets.",
    "category": "Matching & Market Design",
    "docs_url": null,
    "github_url": null,
    "url": "https://pypi.org/project/algmatch/",
    "install": "pip install algmatch",
    "tags": [
      "matching",
      "market design",
      "allocation"
    ],
    "best_for": "Student-project-lecturer allocation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "algmatch is a Python package designed for student-project allocation that incorporates lecturer preferences, extending the matching process to three-sided markets. It is useful for educational institutions and researchers involved in allocation problems.",
    "use_cases": [
      "Allocating students to projects based on preferences",
      "Facilitating lecturer involvement in project assignments"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "python library for student-project allocation",
      "how to match students with projects in python",
      "python matching algorithms for three-sided markets",
      "allocation problems in python",
      "lecturer preferences in project allocation python",
      "student project matching library",
      "market design algorithms in python",
      "how to implement matching in python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "deep-opt-auctions",
    "description": "Neural network optimal auction design. Implements RegretNet, RochetNet for mechanism design.",
    "category": "Matching & Market Design",
    "docs_url": "https://github.com/saisrivatsan/deep-opt-auctions",
    "github_url": "https://github.com/saisrivatsan/deep-opt-auctions",
    "url": "https://github.com/saisrivatsan/deep-opt-auctions",
    "install": "Install from GitHub",
    "tags": [
      "auctions",
      "mechanism design",
      "deep learning"
    ],
    "best_for": "Neural network auction design",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The deep-opt-auctions package provides neural network-based solutions for optimal auction design, specifically implementing RegretNet and RochetNet for mechanism design. It is useful for researchers and practitioners in the fields of economics and machine learning.",
    "use_cases": [
      "Designing optimal auctions using neural networks",
      "Analyzing auction mechanisms with deep learning"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for optimal auction design",
      "how to implement neural networks for auctions in python",
      "deep learning for mechanism design",
      "auctions with deep learning in python",
      "RegretNet implementation in python",
      "RochetNet auction design python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "kep_solver",
    "description": "Kidney exchange optimization with hierarchical objectives. Production-ready for kidney paired donation.",
    "category": "Matching & Market Design",
    "docs_url": "https://kep-solver.readthedocs.io/en/latest/",
    "github_url": "https://gitlab.com/wpettersson/kep_solver",
    "url": "https://pypi.org/project/kep_solver/",
    "install": "pip install kep-solver",
    "tags": [
      "matching",
      "market design",
      "kidney exchange"
    ],
    "best_for": "Kidney exchange program optimization",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [],
    "summary": "The kep_solver package provides optimization solutions for kidney exchange programs, focusing on hierarchical objectives. It is designed for use in kidney paired donation scenarios, making it suitable for healthcare professionals and researchers in the field of organ transplantation.",
    "use_cases": [
      "Optimizing kidney paired donation matches",
      "Improving efficiency in organ transplantation programs"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for kidney exchange optimization",
      "how to optimize kidney paired donation in python",
      "matching algorithms for kidney exchange",
      "market design tools for healthcare",
      "hierarchical objectives in optimization",
      "python matching and market design library"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "matching",
    "description": "Implements Stable Marriage, Hospital-Resident, Student-Allocation, and Stable Roommates using Gale-Shapley (JOSS paper).",
    "category": "Matching & Market Design",
    "docs_url": "https://daffidwilde.github.io/matching/",
    "github_url": "https://github.com/daffidwilde/matching",
    "url": "https://github.com/daffidwilde/matching",
    "install": "pip install matching",
    "tags": [
      "matching",
      "market design",
      "Gale-Shapley"
    ],
    "best_for": "Classic two-sided matching algorithms",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The matching package implements algorithms for Stable Marriage, Hospital-Resident, Student-Allocation, and Stable Roommates problems using the Gale-Shapley method. It is useful for researchers and practitioners in matching and market design.",
    "use_cases": [
      "Allocating students to schools",
      "Matching residents to hospitals",
      "Organizing stable marriages",
      "Creating roommate assignments"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for stable marriage",
      "how to implement Gale-Shapley in python",
      "matching algorithms in python",
      "hospital-resident matching python",
      "student allocation algorithms python",
      "stable roommates implementation python"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Gale & Shapley (1962)",
    "maintenance_status": "active"
  },
  {
    "name": "scarfmatch",
    "description": "Matching with couples using Scarf's algorithm. Essential for NRMP-style medical residency matching.",
    "category": "Matching & Market Design",
    "docs_url": null,
    "github_url": "https://github.com/dwtang/scarf",
    "url": "https://pypi.org/project/scarfmatch/",
    "install": "pip install scarfmatch",
    "tags": [
      "matching",
      "market design",
      "couples"
    ],
    "best_for": "Residency matching with couples",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [],
    "summary": "Scarfmatch implements Scarf's algorithm for matching couples, which is essential for NRMP-style medical residency matching. It is designed for users involved in market design and matching processes.",
    "use_cases": [
      "Matching couples for medical residency",
      "Designing matching algorithms for market scenarios"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for matching couples",
      "how to use Scarf's algorithm in python",
      "NRMP-style matching in python",
      "market design algorithms in python"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active"
  },
  {
    "name": "glmmTMB",
    "description": "Fit generalized linear mixed models with extensions including zero-inflation, hurdle models, heteroscedasticity, and autocorrelation using Template Model Builder (TMB) with automatic differentiation and Laplace approximation.",
    "category": "Mixed Effects",
    "docs_url": "https://glmmtmb.github.io/glmmTMB/",
    "github_url": "https://github.com/glmmTMB/glmmTMB",
    "url": "https://cran.r-project.org/package=glmmTMB",
    "install": "install.packages(\"glmmTMB\")",
    "tags": [
      "GLMM",
      "zero-inflation",
      "negative-binomial",
      "TMB",
      "overdispersion"
    ],
    "best_for": "Zero-inflated, overdispersed, or complex GLMMs beyond lme4 capabilities, implementing Brooks et al. (2017)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "glmmTMB is an R package that fits generalized linear mixed models with various extensions such as zero-inflation and hurdle models. It is used by statisticians and data scientists for modeling complex data structures.",
    "use_cases": [
      "Modeling count data with excess zeros",
      "Analyzing longitudinal data with random effects"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for generalized linear mixed models",
      "how to fit mixed effects models in R",
      "zero-inflation models in R",
      "hurdle models R package",
      "TMB for mixed models",
      "R package for overdispersion handling",
      "fit autocorrelation in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "lme4",
    "description": "Fit linear and generalized linear mixed-effects models using S4 classes with Eigen C++ library for efficient computation, supporting arbitrarily nested and crossed random effects structures for hierarchical and longitudinal data.",
    "category": "Mixed Effects",
    "docs_url": "https://cran.r-project.org/web/packages/lme4/vignettes/",
    "github_url": "https://github.com/lme4/lme4",
    "url": "https://cran.r-project.org/package=lme4",
    "install": "install.packages(\"lme4\")",
    "tags": [
      "linear-mixed-models",
      "GLMM",
      "random-effects",
      "hierarchical-models",
      "repeated-measures"
    ],
    "best_for": "Standard linear and generalized linear mixed-effects modeling with crossed/nested random effects, implementing Bates et al. (2015)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The lme4 package is designed to fit linear and generalized linear mixed-effects models using S4 classes. It is utilized by statisticians and data scientists for analyzing hierarchical and longitudinal data with complex random effects structures.",
    "use_cases": [
      "Analyzing longitudinal data",
      "Modeling hierarchical data structures"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for linear mixed models",
      "how to fit generalized linear mixed models in R",
      "lme4 documentation",
      "lme4 examples",
      "R mixed effects modeling",
      "lme4 random effects structures"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "nlme"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "lmerTest",
    "description": "Provides p-values for lme4 model fits via Satterthwaite's or Kenward-Roger degrees of freedom methods, with Type I/II/III ANOVA tables, model selection tools (step, drop1), and least-squares means calculations.",
    "category": "Mixed Effects",
    "docs_url": "https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf",
    "github_url": "https://github.com/runehaubo/lmerTestR",
    "url": "https://cran.r-project.org/package=lmerTest",
    "install": "install.packages(\"lmerTest\")",
    "tags": [
      "p-values",
      "Satterthwaite",
      "Kenward-Roger",
      "ANOVA",
      "hypothesis-testing"
    ],
    "best_for": "Getting p-values and formal hypothesis tests for lme4 linear mixed models, implementing Kuznetsova et al. (2017)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "hypothesis-testing",
      "mixed-effects-models"
    ],
    "summary": "lmerTest provides p-values for lme4 model fits using Satterthwaite's or Kenward-Roger degrees of freedom methods. It is used by statisticians and data scientists for model selection and ANOVA analysis in mixed effects models.",
    "use_cases": [
      "Conducting ANOVA for mixed effects models",
      "Performing model selection for linear mixed models"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for mixed effects models",
      "how to get p-values for lme4 models in R",
      "ANOVA tables in R",
      "model selection tools in R",
      "Satterthwaite method in R",
      "Kenward-Roger method in R"
    ],
    "primary_use_cases": [
      "ANOVA analysis for mixed effects models",
      "p-value calculation for lme4 model fits"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "lme4"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "nlme",
    "description": "Fit Gaussian linear and nonlinear mixed-effects models with flexible correlation structures, variance functions for heteroscedasticity, and nested random effects. Ships with base R and offers more variance-covariance flexibility than lme4.",
    "category": "Mixed Effects",
    "docs_url": "https://cran.r-project.org/web/packages/nlme/nlme.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=nlme",
    "install": "install.packages(\"nlme\")",
    "tags": [
      "nonlinear-mixed-models",
      "autocorrelation",
      "heteroscedasticity",
      "repeated-measures",
      "longitudinal"
    ],
    "best_for": "Models requiring custom correlation structures, variance functions, or nonlinear mixed effects, implementing Pinheiro & Bates (2000)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "mixed-effects-models",
      "longitudinal-data"
    ],
    "summary": "The nlme package is designed to fit Gaussian linear and nonlinear mixed-effects models, accommodating flexible correlation structures and variance functions for heteroscedasticity. It is widely used by statisticians and data scientists working with complex hierarchical data.",
    "use_cases": [
      "Modeling repeated measures data",
      "Analyzing longitudinal studies"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for mixed-effects models",
      "how to fit nonlinear mixed models in R",
      "R nlme package documentation",
      "Gaussian mixed-effects models in R",
      "nlme vs lme4 in R",
      "how to handle heteroscedasticity in R"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "lme4"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "car",
    "description": "Functions accompanying 'An R Companion to Applied Regression.' Provides advanced regression diagnostics including variance inflation factors (VIF), Type II/III ANOVA, influence measures, linear hypothesis testing, power transformations (Box-Cox), and comprehensive diagnostic plots.",
    "category": "Model Diagnostics",
    "docs_url": "https://www.john-fox.ca/Companion/index.html",
    "github_url": null,
    "url": "https://cran.r-project.org/package=car",
    "install": "install.packages(\"car\")",
    "tags": [
      "regression-diagnostics",
      "VIF",
      "ANOVA",
      "hypothesis-testing",
      "influence-diagnostics"
    ],
    "best_for": "Classical regression diagnostics: VIF for multicollinearity, Type II/III ANOVA, linear hypothesis tests, from Fox & Weisberg (2019)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The 'car' package provides functions for advanced regression diagnostics, including variance inflation factors, ANOVA, and influence measures. It is primarily used by statisticians and data scientists for performing thorough regression analysis.",
    "use_cases": [
      "Assessing multicollinearity in regression models",
      "Evaluating the influence of data points on regression results"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for regression diagnostics",
      "how to calculate VIF in R",
      "ANOVA analysis in R",
      "influence measures in regression R",
      "Box-Cox transformation R",
      "diagnostic plots in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "performance",
    "description": "Utilities for computing indices of model quality and goodness of fit, including R\u00b2, RMSE, ICC, AIC/BIC. Provides functions to check models for overdispersion, zero-inflation, multicollinearity (VIF), convergence, and singularity. Supports mixed effects and Bayesian models.",
    "category": "Model Diagnostics",
    "docs_url": "https://easystats.github.io/performance/",
    "github_url": "https://github.com/easystats/performance",
    "url": "https://cran.r-project.org/package=performance",
    "install": "install.packages(\"performance\")",
    "tags": [
      "model-diagnostics",
      "R-squared",
      "assumption-checking",
      "VIF",
      "goodness-of-fit"
    ],
    "best_for": "Comprehensive model quality assessment, especially the check_model() visual diagnostic panel, implementing L\u00fcdecke et al. (2021, JOSS)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "model-diagnostics",
      "goodness-of-fit"
    ],
    "summary": "The 'performance' package provides utilities for assessing model quality and goodness of fit through various indices such as R\u00b2 and RMSE. It is useful for statisticians and data scientists working with mixed effects and Bayesian models.",
    "use_cases": [
      "Evaluating the fit of a mixed effects model",
      "Checking for zero-inflation in count data models"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for model diagnostics",
      "how to compute R-squared in R",
      "functions for checking overdispersion in R",
      "R utilities for goodness of fit",
      "how to assess multicollinearity in R",
      "R package for Bayesian model evaluation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "see",
    "description": "Visualization toolbox for the easystats ecosystem built on ggplot2. Provides publication-ready plotting methods for model parameters, predictions, and performance diagnostics from all easystats packages via simple plot() calls.",
    "category": "Model Diagnostics",
    "docs_url": "https://easystats.github.io/see/",
    "github_url": "https://github.com/easystats/see",
    "url": "https://cran.r-project.org/package=see",
    "install": "install.packages(\"see\")",
    "tags": [
      "visualization",
      "ggplot2",
      "diagnostic-plots",
      "publication-ready",
      "easystats"
    ],
    "best_for": "Publication-ready visualizations of model diagnostics with a simple plot() interface, implementing L\u00fcdecke et al. (2021, JOSS)",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The 'see' package is a visualization toolbox designed for the easystats ecosystem, leveraging ggplot2 to create publication-ready plots. It is useful for users looking to visualize model parameters, predictions, and performance diagnostics easily.",
    "use_cases": [
      "Creating publication-ready plots for model outputs",
      "Visualizing predictions from statistical models"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "visualization toolbox for easystats",
      "ggplot2 diagnostic plots",
      "how to visualize model parameters in R",
      "publication-ready plots in R",
      "easystats visualization methods",
      "R package for performance diagnostics"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active"
  },
  {
    "name": "CausalNLP",
    "description": "Causal inference for text data. Estimate treatment effects from unstructured text using NLP.",
    "category": "Natural Language Processing for Economics",
    "docs_url": null,
    "github_url": "https://github.com/amaiya/causalnlp",
    "url": "https://github.com/amaiya/causalnlp",
    "install": "pip install causalnlp",
    "tags": [
      "NLP",
      "causal inference",
      "text"
    ],
    "best_for": "Causal effects from text data",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "natural-language-processing"
    ],
    "summary": "CausalNLP is a package designed for causal inference in text data, enabling users to estimate treatment effects from unstructured text using natural language processing techniques. It is particularly useful for researchers and practitioners in economics who work with text data.",
    "use_cases": [
      "Analyzing the impact of a marketing campaign based on customer reviews",
      "Estimating the effect of policy changes on public sentiment from social media posts"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference in text",
      "how to estimate treatment effects from text in python",
      "NLP for causal analysis in python",
      "causal inference tools for text data",
      "using CausalNLP for text analysis",
      "text data treatment effects estimation python"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "causalml",
      "DoWhy"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "Gensim",
    "description": "Library focused on topic modeling (LDA, LSI) and document similarity analysis.",
    "category": "Natural Language Processing for Economics",
    "docs_url": "https://radimrehurek.com/gensim/",
    "github_url": "https://github.com/RaRe-Technologies/gensim",
    "url": "https://github.com/RaRe-Technologies/gensim",
    "install": "pip install gensim",
    "tags": [
      "NLP",
      "text analysis"
    ],
    "best_for": "Text analysis, sentiment analysis, document classification",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "Gensim is a library focused on topic modeling and document similarity analysis. It is widely used by researchers and practitioners in the field of natural language processing to analyze large text corpora.",
    "use_cases": [
      "Analyzing customer feedback for insights",
      "Building recommendation systems based on text data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for topic modeling",
      "how to do document similarity analysis in python",
      "Gensim tutorial",
      "Gensim LDA example",
      "text analysis with Gensim",
      "using Gensim for NLP"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "spaCy",
      "NLTK"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "Transformers",
    "description": "Access to thousands of pre-trained models for NLP tasks like text classification, summarization, embeddings, etc.",
    "category": "Natural Language Processing for Economics",
    "docs_url": "https://huggingface.co/transformers/",
    "github_url": "https://github.com/huggingface/transformers",
    "url": "https://github.com/huggingface/transformers",
    "install": "pip install transformers",
    "tags": [
      "NLP",
      "text analysis"
    ],
    "best_for": "Text analysis, sentiment analysis, document classification",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Transformers provides access to thousands of pre-trained models specifically designed for various natural language processing tasks such as text classification, summarization, and embeddings. It is widely used by data scientists and researchers in the field of economics to enhance their NLP capabilities.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for NLP",
      "how to use pre-trained models in python",
      "text classification with Transformers",
      "summarization using Transformers",
      "NLP tasks in python",
      "accessing pre-trained models for text analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "spaCy",
    "description": "Industrial-strength NLP library for efficient text processing pipelines (NER, POS tagging, etc.).",
    "category": "Natural Language Processing for Economics",
    "docs_url": "https://spacy.io/",
    "github_url": "https://github.com/explosion/spaCy",
    "url": "https://github.com/explosion/spaCy",
    "install": "pip install spacy",
    "tags": [
      "NLP",
      "text analysis"
    ],
    "best_for": "Text analysis, sentiment analysis, document classification",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "spaCy is an industrial-strength NLP library designed for efficient text processing pipelines, including tasks such as Named Entity Recognition (NER) and Part-of-Speech (POS) tagging. It is widely used by data scientists and researchers in various fields, including economics, for natural language processing tasks.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for NLP",
      "how to do text analysis in python",
      "spaCy tutorial",
      "using spaCy for NER",
      "spaCy POS tagging example",
      "best NLP libraries in python"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "NLTK",
      "Transformers"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "ggraph",
    "description": "Grammar of graphics for network data built on ggplot2. Provides layouts, geometries, and faceting specifically designed for network visualization with publication-quality output.",
    "category": "Network Analysis",
    "docs_url": "https://ggraph.data-imaginist.com/",
    "github_url": "https://github.com/thomasp85/ggraph",
    "url": "https://cran.r-project.org/package=ggraph",
    "install": "install.packages(\"ggraph\")",
    "tags": [
      "networks",
      "visualization",
      "ggplot2",
      "graph-layouts",
      "publication-ready"
    ],
    "best_for": "Publication-quality network visualization using ggplot2 grammar",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "network visualization",
      "graphics"
    ],
    "summary": "ggraph is a package that provides a grammar of graphics specifically designed for visualizing network data using ggplot2. It is used by data scientists and researchers who need to create publication-quality network visualizations.",
    "use_cases": [
      "Creating visualizations for social network analysis",
      "Visualizing complex relationships in data"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for network visualization",
      "how to create network graphs in R",
      "ggplot2 network layouts",
      "visualizing networks with ggraph",
      "network data graphics in R",
      "publication-quality network visualizations R"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "ggplot2"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "igraph",
    "description": "Comprehensive network analysis library with efficient algorithms for network creation, manipulation, and analysis. Provides centrality measures, community detection, graph visualization, and network statistics.",
    "category": "Network Analysis",
    "docs_url": "https://igraph.org/r/",
    "github_url": "https://github.com/igraph/rigraph",
    "url": "https://cran.r-project.org/package=igraph",
    "install": "install.packages(\"igraph\")",
    "tags": [
      "networks",
      "graph-algorithms",
      "centrality",
      "community-detection",
      "network-statistics"
    ],
    "best_for": "Comprehensive network analysis with efficient algorithms for centrality, community detection, and visualization",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "network-analysis",
      "graph-theory"
    ],
    "summary": "igraph is a comprehensive network analysis library that provides efficient algorithms for creating, manipulating, and analyzing networks. It is used by researchers and data scientists for tasks such as centrality measures, community detection, and graph visualization.",
    "use_cases": [
      "Analyzing social networks",
      "Visualizing transportation networks"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for network analysis",
      "how to visualize graphs in R",
      "R package for community detection",
      "network statistics in R",
      "efficient algorithms for graph manipulation in R",
      "centrality measures in R"
    ],
    "primary_use_cases": [
      "community detection",
      "graph visualization"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "network",
      "statnet"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "sna",
    "description": "Social network analysis tools including network visualization, centrality measures, and statistical models for network data. Part of the statnet suite for network regression and exponential random graph models.",
    "category": "Network Analysis",
    "docs_url": "https://cran.r-project.org/web/packages/sna/sna.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=sna",
    "install": "install.packages(\"sna\")",
    "tags": [
      "social-networks",
      "network-regression",
      "statnet",
      "ERGM",
      "centrality"
    ],
    "best_for": "Social network analysis and network regression as part of the statnet suite",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The 'sna' package provides tools for social network analysis, including visualization, centrality measures, and statistical models for analyzing network data. It is part of the statnet suite, which is commonly used by researchers and practitioners in the field of network analysis.",
    "use_cases": [
      "Analyzing social media interactions",
      "Studying collaboration networks in research"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for social network analysis",
      "how to visualize networks in R",
      "centrality measures in R",
      "statnet suite for network regression",
      "exponential random graph models in R",
      "R tools for network data analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "statnet"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "tidygraph",
    "description": "Tidy data interface for network/graph data. Extends dplyr verbs to work with nodes and edges, enabling pipe-friendly network manipulation that integrates seamlessly with ggraph for visualization.",
    "category": "Network Analysis",
    "docs_url": "https://tidygraph.data-imaginist.com/",
    "github_url": "https://github.com/thomasp85/tidygraph",
    "url": "https://cran.r-project.org/package=tidygraph",
    "install": "install.packages(\"tidygraph\")",
    "tags": [
      "networks",
      "tidyverse",
      "graph-manipulation",
      "dplyr",
      "pipes"
    ],
    "best_for": "Tidy manipulation of network data with dplyr-style verbs for nodes and edges",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "network analysis",
      "data manipulation",
      "visualization"
    ],
    "summary": "The tidygraph package provides a tidy data interface for network and graph data, allowing users to manipulate nodes and edges using dplyr verbs. It is designed for R users who want to perform network analysis and visualization seamlessly with ggraph.",
    "use_cases": [
      "Analyzing social networks",
      "Visualizing relationships in data",
      "Manipulating graph structures for analysis"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "R package for network analysis",
      "how to manipulate graph data in R",
      "tidygraph documentation",
      "visualizing networks in R",
      "dplyr for graph data",
      "network manipulation with tidyverse"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "igraph",
      "ggraph"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "Faer",
    "description": "State-of-the-art linear algebra for Rust with Cholesky, QR, SVD decompositions and multithreaded solvers for large systems.",
    "category": "Numerical Optimization & Computational Tools",
    "docs_url": "https://docs.rs/faer",
    "github_url": "https://github.com/sarah-quinones/faer-rs",
    "url": "https://crates.io/crates/faer",
    "install": "cargo add faer",
    "tags": [
      "rust",
      "linear algebra",
      "matrix",
      "performance"
    ],
    "best_for": "High-performance matrix decompositions for custom estimators",
    "language": "Rust",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Faer is a Rust library that provides state-of-the-art linear algebra capabilities, including Cholesky, QR, and SVD decompositions, along with multithreaded solvers for handling large systems. It is suitable for users needing high-performance matrix operations in Rust.",
    "use_cases": [
      "Solving large systems of equations",
      "Performing matrix decompositions for data analysis"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "rust library for linear algebra",
      "how to perform matrix decomposition in rust",
      "multithreaded solvers in rust",
      "best rust libraries for numerical optimization",
      "linear algebra performance in rust",
      "rust matrix operations library"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "JAX",
    "description": "High-performance numerical computing with autograd and XLA compilation on CPU/GPU/TPU.",
    "category": "Numerical Optimization & Computational Tools",
    "docs_url": "https://jax.readthedocs.io/",
    "github_url": "https://github.com/google/jax",
    "url": "https://github.com/google/jax",
    "install": "pip install jax",
    "tags": [
      "optimization",
      "computation"
    ],
    "best_for": "Solving optimization problems, numerical methods",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "JAX is a high-performance numerical computing library that supports automatic differentiation and XLA compilation for efficient execution on CPU, GPU, and TPU. It is used by researchers and developers in machine learning and scientific computing to optimize and accelerate their computations.",
    "use_cases": [],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for numerical computing",
      "how to perform autograd in python",
      "JAX optimization techniques",
      "XLA compilation in JAX",
      "high-performance computing with JAX",
      "JAX for machine learning"
    ],
    "api_complexity": "advanced",
    "related_packages": [
      "NumPy",
      "TensorFlow",
      "PyTorch"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "Nalgebra",
    "description": "General-purpose linear algebra library for Rust with dense and sparse matrices, widely used in graphics and physics.",
    "category": "Numerical Optimization & Computational Tools",
    "docs_url": "https://www.nalgebra.org/",
    "github_url": "https://github.com/dimforge/nalgebra",
    "url": "https://crates.io/crates/nalgebra",
    "install": "cargo add nalgebra",
    "tags": [
      "rust",
      "linear algebra",
      "matrix",
      "sparse"
    ],
    "best_for": "General-purpose linear algebra in Rust",
    "language": "Rust",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Nalgebra is a general-purpose linear algebra library for Rust that supports both dense and sparse matrices. It is widely used in fields such as graphics and physics for various computational tasks.",
    "use_cases": [
      "Performing matrix calculations in graphics applications",
      "Solving linear equations in physics simulations"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "rust library for linear algebra",
      "how to perform matrix operations in rust",
      "sparse matrix library in rust",
      "rust linear algebra for graphics",
      "physics simulations in rust",
      "rust numerical optimization library"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "Ndarray",
    "description": "N-dimensional array library for Rust\u2014the NumPy equivalent with slicing, broadcasting, and BLAS/LAPACK integration.",
    "category": "Numerical Optimization & Computational Tools",
    "docs_url": "https://docs.rs/ndarray",
    "github_url": "https://github.com/rust-ndarray/ndarray",
    "url": "https://crates.io/crates/ndarray",
    "install": "cargo add ndarray",
    "tags": [
      "rust",
      "arrays",
      "numpy",
      "scientific computing"
    ],
    "best_for": "NumPy-style N-dimensional arrays in Rust",
    "language": "Rust",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Ndarray is an N-dimensional array library for Rust, designed to provide functionality similar to NumPy, including features like slicing, broadcasting, and integration with BLAS/LAPACK. It is useful for developers and researchers in scientific computing who require efficient array manipulation in Rust.",
    "use_cases": [
      "Data analysis in scientific computing",
      "Machine learning model development"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "rust library for n-dimensional arrays",
      "how to use ndarray in rust",
      "scientific computing with rust",
      "rust equivalent of numpy",
      "array manipulation in rust",
      "broadcasting in rust arrays"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "PyTorch",
    "description": "Popular deep learning framework with flexible automatic differentiation.",
    "category": "Numerical Optimization & Computational Tools",
    "docs_url": "https://pytorch.org/",
    "github_url": "https://github.com/pytorch/pytorch",
    "url": "https://github.com/pytorch/pytorch",
    "install": "(See PyTorch website)",
    "tags": [
      "optimization",
      "computation",
      "machine learning"
    ],
    "best_for": "Solving optimization problems, numerical methods",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "machine learning"
    ],
    "summary": "PyTorch is a popular deep learning framework that provides flexible automatic differentiation for building and training neural networks. It is widely used by researchers and practitioners in the field of machine learning.",
    "use_cases": [
      "Building neural networks",
      "Training models for image classification"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for deep learning",
      "how to use PyTorch for machine learning",
      "PyTorch tutorial",
      "deep learning framework comparison",
      "PyTorch automatic differentiation",
      "PyTorch optimization techniques"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "TensorFlow",
      "Keras"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "jaxonometrics",
    "description": "JAX-ecosystem implementations of standard econometrics routines for GPU computation.",
    "category": "Numerical Optimization & Computational Tools",
    "docs_url": null,
    "github_url": "https://github.com/py-econometrics/jaxonometrics",
    "url": "https://github.com/py-econometrics/jaxonometrics",
    "install": "GitHub Repository",
    "tags": [
      "optimization",
      "JAX",
      "GPU"
    ],
    "best_for": "GPU-accelerated econometrics with JAX",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "jaxonometrics provides implementations of standard econometrics routines optimized for GPU computation using the JAX ecosystem. It is suitable for users looking to leverage GPU capabilities for econometric analysis.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for econometrics",
      "GPU computation in econometrics",
      "JAX econometrics routines",
      "how to perform econometrics with JAX",
      "optimizing econometrics with GPU",
      "jaxonometrics package usage",
      "econometrics for data science in Python"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "JAX"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "torchonometrics",
    "description": "Econometrics implementations in PyTorch. Leverages autodiff and GPU acceleration for econometric methods.",
    "category": "Numerical Optimization & Computational Tools",
    "docs_url": null,
    "github_url": "https://github.com/apoorvalal/torchonometrics",
    "url": "https://github.com/apoorvalal/torchonometrics",
    "install": "GitHub Repository",
    "tags": [
      "optimization",
      "computation",
      "PyTorch"
    ],
    "best_for": "GPU-accelerated econometrics with PyTorch autodiff",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "econometrics",
      "optimization",
      "deep-learning"
    ],
    "summary": "torchonometrics provides econometric methods implemented in PyTorch, utilizing automatic differentiation and GPU acceleration. It is designed for users interested in applying econometric techniques in a computationally efficient manner.",
    "use_cases": [
      "Estimating econometric models",
      "Running simulations for econometric analysis"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for econometrics",
      "how to do econometric analysis in python",
      "PyTorch econometrics package",
      "optimizing econometric models with PyTorch",
      "GPU acceleration for econometrics in python",
      "autodiff econometrics library"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "PyTorch"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "Argmin",
    "description": "Numerical optimization framework for Rust with Newton, BFGS, L-BFGS, trust region, and derivative-free methods for MLE/GMM.",
    "category": "Optimization",
    "docs_url": "https://docs.rs/argmin",
    "github_url": "https://github.com/argmin-rs/argmin",
    "url": "https://crates.io/crates/argmin",
    "install": "cargo add argmin",
    "tags": [
      "rust",
      "optimization",
      "BFGS",
      "MLE",
      "GMM"
    ],
    "best_for": "Maximum Likelihood and GMM estimation in Rust",
    "language": "Rust",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Argmin is a numerical optimization framework designed for Rust programming language. It provides various optimization methods such as Newton, BFGS, L-BFGS, trust region, and derivative-free methods for maximum likelihood estimation (MLE) and generalized method of moments (GMM).",
    "use_cases": [],
    "audience": [],
    "synthetic_questions": [
      "Rust library for numerical optimization",
      "how to perform MLE in Rust",
      "Rust optimization framework",
      "BFGS method implementation in Rust",
      "derivative-free optimization in Rust",
      "trust region methods in Rust"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "cvxpy",
    "description": "Domain-specific language for convex optimization problems. Write math as code \u2014 the standard for convex problems.",
    "category": "Optimization",
    "docs_url": "https://www.cvxpy.org/",
    "github_url": "https://github.com/cvxpy/cvxpy",
    "url": "https://www.cvxpy.org/",
    "install": "pip install cvxpy",
    "tags": [
      "convex optimization",
      "linear programming",
      "quadratic programming"
    ],
    "best_for": "Convex optimization with intuitive syntax",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Cvxpy is a domain-specific language designed for formulating and solving convex optimization problems. It allows users to express mathematical problems in a code format, making it suitable for researchers and practitioners in optimization.",
    "use_cases": [
      "Solving linear programming problems",
      "Formulating and solving quadratic programming problems"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for convex optimization",
      "how to solve linear programming in python",
      "quadratic programming in python",
      "cvxpy tutorial",
      "convex optimization problems in python",
      "cvxpy examples"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "cvxopt",
      "scipy.optimize"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "gurobipy",
    "description": "Python interface for Gurobi, the best-in-class commercial solver. LP, QP, MIP, and MIQP.",
    "category": "Optimization",
    "docs_url": "https://www.gurobi.com/documentation/",
    "github_url": null,
    "url": "https://www.gurobi.com/",
    "install": "pip install gurobipy",
    "tags": [
      "optimization",
      "solver",
      "MIP",
      "commercial"
    ],
    "best_for": "Best-in-class solver \u2014 free for academics",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Gurobipy is a Python interface for the Gurobi optimizer, which is a leading commercial solver for linear programming (LP), quadratic programming (QP), mixed-integer programming (MIP), and mixed-integer quadratic programming (MIQP). It is used by data scientists and operations researchers to solve complex optimization problems.",
    "use_cases": [
      "Optimizing supply chain logistics",
      "Resource allocation in project management"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for optimization",
      "how to solve MIP in python",
      "Gurobi python interface",
      "best commercial solver for LP",
      "using Gurobi for optimization",
      "Gurobi python examples"
    ],
    "api_complexity": "advanced",
    "related_packages": [
      "cvxpy",
      "Pyomo"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "ortools",
    "description": "Google's operations research toolkit. Constraint programming, routing, linear/integer programming, and scheduling.",
    "category": "Optimization",
    "docs_url": "https://developers.google.com/optimization",
    "github_url": "https://github.com/google/or-tools",
    "url": "https://developers.google.com/optimization",
    "install": "pip install ortools",
    "tags": [
      "OR",
      "routing",
      "scheduling",
      "constraint programming"
    ],
    "best_for": "Production-ready combinatorial optimization",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "OR-Tools is Google's operations research toolkit designed for solving optimization problems. It is used by data scientists and operations researchers for tasks such as routing, scheduling, and constraint programming.",
    "use_cases": [
      "Optimizing delivery routes",
      "Scheduling tasks in a manufacturing process"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for optimization",
      "how to do routing in python",
      "python scheduling library",
      "constraint programming in python",
      "google operations research toolkit",
      "linear programming in python",
      "integer programming python library"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "PuLP",
      "SciPy"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "scipy.optimize",
    "description": "Optimization algorithms built into SciPy. Minimization, root finding, curve fitting, and linear programming.",
    "category": "Optimization",
    "docs_url": "https://docs.scipy.org/doc/scipy/reference/optimize.html",
    "github_url": "https://github.com/scipy/scipy",
    "url": "https://docs.scipy.org/doc/scipy/reference/optimize.html",
    "install": "pip install scipy",
    "tags": [
      "optimization",
      "minimization",
      "root finding"
    ],
    "best_for": "General-purpose optimization \u2014 start here for basics",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "scipy.optimize provides a collection of optimization algorithms for tasks such as minimization, root finding, curve fitting, and linear programming. It is widely used by data scientists and researchers in various fields requiring optimization techniques.",
    "use_cases": [
      "Minimizing a cost function in machine learning",
      "Finding roots of equations",
      "Fitting a curve to data points"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for optimization",
      "how to minimize a function in python",
      "root finding algorithms in python",
      "curve fitting with scipy",
      "linear programming in python",
      "scipy.optimize examples",
      "using scipy for optimization tasks"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "numpy",
      "scikit-learn"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "FixedEffectModel",
    "description": "Panel data modeling with IV tests (weak IV, over-identification, endogeneity) and 2-step GMM estimation.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": null,
    "github_url": "https://github.com/ksecology/FixedEffectModel",
    "url": "https://github.com/ksecology/FixedEffectModel",
    "install": "pip install FixedEffectModel",
    "tags": [
      "panel data",
      "fixed effects",
      "IV"
    ],
    "best_for": "Panel regression with comprehensive IV diagnostics",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "causal-inference",
      "panel-data"
    ],
    "summary": "The FixedEffectModel package provides tools for panel data modeling, including instrumental variable tests and two-step GMM estimation. It is useful for researchers and data scientists working with panel datasets who need to address issues like endogeneity and weak instruments.",
    "use_cases": [
      "Analyzing economic data with fixed effects",
      "Testing for endogeneity in panel datasets"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for panel data modeling",
      "how to perform IV tests in python",
      "python package for GMM estimation",
      "panel data analysis in python",
      "how to handle endogeneity in panel data",
      "weak IV tests in python",
      "over-identification tests in python"
    ],
    "primary_use_cases": [
      "panel data modeling",
      "IV tests",
      "GMM estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "FixedEffectModelPyHDFE",
    "description": "Solves linear models with high-dimensional fixed effects, supporting robust variance calculation and IV.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://pypi.org/project/FixedEffectModelPyHDFE/",
    "github_url": null,
    "url": "https://pypi.org/project/FixedEffectModelPyHDFE/",
    "install": "pip install FixedEffectModelPyHDFE",
    "tags": [
      "panel data",
      "fixed effects"
    ],
    "best_for": "Longitudinal analysis, controlling for unobserved heterogeneity",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "panel data",
      "fixed effects"
    ],
    "summary": "FixedEffectModelPyHDFE solves linear models with high-dimensional fixed effects, allowing for robust variance calculation and instrumental variable support. It is primarily used by data scientists and researchers working with panel data.",
    "use_cases": [
      "Analyzing panel data with multiple fixed effects",
      "Estimating treatment effects in longitudinal studies"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for fixed effects models",
      "how to calculate robust variance in python",
      "linear models with high-dimensional fixed effects python",
      "IV regression in python",
      "panel data analysis in python",
      "fixed effects estimation python"
    ],
    "primary_use_cases": [
      "high-dimensional fixed effects estimation",
      "robust variance calculation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "Linearmodels",
    "description": "Estimation of fixed, random, pooled OLS models for panel data. Also Fama-MacBeth and between/first-difference estimators.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://bashtage.github.io/linearmodels/",
    "github_url": "https://github.com/bashtage/linearmodels",
    "url": "https://github.com/bashtage/linearmodels",
    "install": "pip install linearmodels",
    "tags": [
      "panel data",
      "fixed effects"
    ],
    "best_for": "Longitudinal analysis, controlling for unobserved heterogeneity",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "panel data",
      "fixed effects"
    ],
    "summary": "Linearmodels is a Python package designed for the estimation of fixed, random, and pooled OLS models for panel data. It is useful for researchers and data scientists working with econometric models and time-series data.",
    "use_cases": [
      "Estimating fixed effects models for economic data",
      "Conducting Fama-MacBeth regression analysis"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for panel data analysis",
      "how to estimate fixed effects in python",
      "panel data regression in python",
      "Fama-MacBeth estimator python",
      "random effects model python",
      "how to use linearmodels package"
    ],
    "primary_use_cases": [
      "fixed effects estimation",
      "random effects estimation"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "statsmodels"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "PyFixest",
    "description": "Fast estimation of linear models with multiple high-dimensional fixed effects (like R's `fixest`). Supports OLS, IV, Poisson, robust/cluster SEs.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://github.com/py-econometrics/pyfixest",
    "github_url": null,
    "url": "https://github.com/py-econometrics/pyfixest",
    "install": "pip install pyfixest",
    "tags": [
      "panel data",
      "fixed effects"
    ],
    "best_for": "Longitudinal analysis, controlling for unobserved heterogeneity",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [],
    "summary": "PyFixest is a Python package designed for fast estimation of linear models with multiple high-dimensional fixed effects, similar to R's fixest. It is particularly useful for econometricians and data scientists working with panel data.",
    "use_cases": [
      "Estimating the impact of policy changes using panel data",
      "Analyzing consumer behavior across different demographics"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for fixed effects estimation",
      "how to estimate linear models with fixed effects in python",
      "fast estimation of panel data models in python",
      "python package for OLS with fixed effects",
      "how to use PyFixest",
      "linear models with high-dimensional fixed effects in python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "alpaca",
    "description": "Fits generalized linear models (Poisson, negative binomial, logit, probit, Gamma) with high-dimensional k-way fixed effects. Partials out factors during log-likelihood optimization and provides robust/multi-way clustered standard errors, fixed effects recovery, and analytical bias corrections for binary choice models.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://cran.r-project.org/web/packages/alpaca/vignettes/howto.html",
    "github_url": "https://github.com/amrei-stammann/alpaca",
    "url": "https://cran.r-project.org/package=alpaca",
    "install": "install.packages(\"alpaca\")",
    "tags": [
      "glm",
      "fixed-effects",
      "poisson-regression",
      "negative-binomial",
      "gravity-models"
    ],
    "best_for": "Nonlinear panel models (Poisson, logit, probit, negative binomial) with multiple high-dimensional fixed effects, especially structural gravity models for international trade",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The alpaca package fits generalized linear models with high-dimensional k-way fixed effects, optimizing log-likelihood while providing robust standard errors and bias corrections for binary choice models. It is used by data scientists and researchers working with panel data and fixed effects models.",
    "use_cases": [
      "Analyzing panel data with fixed effects",
      "Estimating models with high-dimensional covariates"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for generalized linear models",
      "how to fit Poisson regression in R",
      "R fixed effects model library",
      "robust standard errors in R",
      "negative binomial regression R package",
      "R package for binary choice models"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "bife",
    "description": "Estimates fixed effects binary choice models (logit and probit) with potentially many individual fixed effects using a pseudo-demeaning algorithm. Addresses the incidental parameters problem through analytical bias correction based on Fern\u00e1ndez-Val (2009) and computes average partial effects.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://cran.r-project.org/web/packages/bife/vignettes/howto.html",
    "github_url": "https://github.com/amrei-stammann/bife",
    "url": "https://cran.r-project.org/package=bife",
    "install": "install.packages(\"bife\")",
    "tags": [
      "binary-choice",
      "fixed-effects",
      "logit-probit",
      "bias-correction",
      "panel-data"
    ],
    "best_for": "Fast estimation of fixed effects logit/probit models on large panel data with analytical bias correction for the incidental parameters problem",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The 'bife' package estimates fixed effects binary choice models, specifically logit and probit models, using a pseudo-demeaning algorithm. It is designed for researchers and analysts dealing with panel data who need to address the incidental parameters problem and compute average partial effects.",
    "use_cases": [
      "Estimating binary choice models with individual fixed effects",
      "Conducting panel data analysis with bias correction"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for fixed effects binary choice models",
      "how to estimate logit models with fixed effects in R",
      "R package for bias correction in binary choice models",
      "panel data analysis in R",
      "average partial effects in fixed effects models",
      "how to use bife package in R"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Fern\u00e1ndez-Val (2009)",
    "maintenance_status": "active"
  },
  {
    "name": "duckreg",
    "description": "Out-of-core regression (OLS/IV) for very large datasets using DuckDB aggregation. Handles data that doesn't fit in memory.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://github.com/py-econometrics/duckreg",
    "github_url": null,
    "url": "https://github.com/py-econometrics/duckreg",
    "install": "pip install duckreg",
    "tags": [
      "panel data",
      "fixed effects"
    ],
    "best_for": "Longitudinal analysis, controlling for unobserved heterogeneity",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "duckdb"
    ],
    "topic_tags": [
      "panel data",
      "fixed effects"
    ],
    "summary": "duckreg is an out-of-core regression package designed for very large datasets using DuckDB aggregation. It is particularly useful for users needing to handle data that exceeds memory capacity.",
    "use_cases": [
      "Analyzing large panel datasets",
      "Performing regression analysis on data that doesn't fit in memory"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for out-of-core regression",
      "how to perform OLS regression in python",
      "duckdb aggregation for large datasets",
      "panel data analysis in python",
      "fixed effects regression in python",
      "handling large datasets in python",
      "python regression for big data"
    ],
    "primary_use_cases": [
      "out-of-core regression",
      "large dataset analysis"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "DuckDB"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "fixest",
    "description": "Fast and comprehensive package for estimating econometric models with multiple high-dimensional fixed effects, including OLS, GLM, Poisson, and negative binomial models. Features native support for clustered standard errors (up to four-way), instrumental variables, and modern difference-in-differences estimators including Sun-Abraham for staggered treatments.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://lrberge.github.io/fixest/",
    "github_url": "https://github.com/lrberge/fixest",
    "url": "https://cran.r-project.org/package=fixest",
    "install": "install.packages(\"fixest\")",
    "tags": [
      "fixed-effects",
      "panel-data",
      "clustered-standard-errors",
      "difference-in-differences",
      "instrumental-variables"
    ],
    "best_for": "Fast, production-ready estimation of linear/GLM models with multiple high-dimensional fixed effects and publication-quality regression tables via etable()",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "panel-data"
    ],
    "summary": "The fixest package provides a fast and comprehensive solution for estimating econometric models with multiple high-dimensional fixed effects. It is particularly useful for researchers and data scientists working with panel data and fixed effects models.",
    "use_cases": [
      "Estimating models with multiple fixed effects",
      "Conducting difference-in-differences analysis"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for estimating econometric models",
      "how to use fixest for fixed effects",
      "difference-in-differences in R",
      "clustered standard errors in R",
      "instrumental variables in R",
      "Poisson regression with fixest"
    ],
    "primary_use_cases": [
      "estimating econometric models",
      "difference-in-differences analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "lfe",
    "description": "Efficiently estimates linear models with multiple high-dimensional fixed effects using the Method of Alternating Projections. Designed for datasets with factors having thousands of levels (hundreds of thousands of dummy variables), with full support for 2SLS instrumental variables and multi-way clustered standard errors.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://cran.r-project.org/web/packages/lfe/lfe.pdf",
    "github_url": "https://github.com/r-econometrics/lfe",
    "url": "https://cran.r-project.org/package=lfe",
    "install": "install.packages(\"lfe\")",
    "tags": [
      "high-dimensional-fe",
      "worker-firm",
      "memory-efficient",
      "instrumental-variables",
      "clustered-se"
    ],
    "best_for": "AKM-style wage decompositions and matched employer-employee data with hundreds of thousands of worker/firm fixed effects",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The lfe package efficiently estimates linear models with multiple high-dimensional fixed effects using the Method of Alternating Projections. It is designed for datasets with factors having thousands of levels and supports 2SLS instrumental variables and multi-way clustered standard errors.",
    "use_cases": [],
    "audience": [],
    "synthetic_questions": [
      "R package for high-dimensional fixed effects",
      "how to estimate linear models with fixed effects in R",
      "R library for 2SLS instrumental variables",
      "memory efficient R package for panel data",
      "clustered standard errors in R",
      "efficient estimation of linear models in R",
      "R package for multi-way clustered standard errors"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "panelhetero",
    "description": "Heterogeneity analysis across units in panel data. Detects and characterizes unit-level variation.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://github.com/tkhdyanagi/panelhetero",
    "github_url": "https://github.com/tkhdyanagi/panelhetero",
    "url": "https://github.com/tkhdyanagi/panelhetero",
    "install": "pip install panelhetero",
    "tags": [
      "panel data",
      "heterogeneity",
      "unit effects"
    ],
    "best_for": "Unit heterogeneity in panels",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "panel data",
      "heterogeneity"
    ],
    "summary": "The panelhetero package facilitates heterogeneity analysis across units in panel data, allowing users to detect and characterize unit-level variation. It is primarily used by data scientists and researchers working with panel data to understand variations among different units.",
    "use_cases": [
      "Analyzing economic data across different regions",
      "Studying the impact of policy changes over time on various units"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for panel data analysis",
      "how to analyze heterogeneity in panel data using python",
      "detecting unit-level variation in panel data python",
      "characterizing unit effects in panel data",
      "panel data heterogeneity analysis python",
      "tools for panel data analysis in python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "panelr",
    "description": "Automates within-between (hybrid) model specification for panel/longitudinal data, combining fixed effects robustness to time-invariant confounding with random effects ability to estimate time-invariant coefficients. Uses lme4 for multilevel estimation with optional Bayesian (brms) and GEE (geepack) backends.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://panelr.jacob-long.com/",
    "github_url": "https://github.com/jacob-long/panelr",
    "url": "https://cran.r-project.org/package=panelr",
    "install": "install.packages(\"panelr\")",
    "tags": [
      "hybrid-models",
      "within-between",
      "panel-data",
      "longitudinal-analysis",
      "bell-jones"
    ],
    "best_for": "Researchers needing fixed effects-equivalent estimates while retaining time-invariant predictors and random slopes",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "panel-data",
      "longitudinal-analysis",
      "fixed-effects",
      "random-effects"
    ],
    "summary": "The panelr package automates the specification of hybrid models for panel and longitudinal data, leveraging fixed effects for robustness against time-invariant confounding and random effects for estimating time-invariant coefficients. It is primarily used by data scientists and researchers working with complex longitudinal datasets.",
    "use_cases": [
      "Analyzing the impact of policy changes over time",
      "Evaluating the effectiveness of a treatment across different groups"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for hybrid models",
      "how to analyze panel data in R",
      "longitudinal data analysis in R",
      "fixed effects vs random effects in R",
      "automate panel model specification R",
      "R lme4 package usage",
      "Bayesian modeling in R"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "lme4",
      "brms",
      "geepack"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "plm",
    "description": "Comprehensive econometrics package for linear panel models providing fixed effects (within), random effects, between, first-difference, Hausman-Taylor, and nested random effects estimators. Includes GMM, FGLS, and extensive diagnostic tests for serial correlation, cross-sectional dependence, and panel unit roots.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://cran.r-project.org/web/packages/plm/vignettes/",
    "github_url": "https://github.com/ycroissant/plm",
    "url": "https://cran.r-project.org/package=plm",
    "install": "install.packages(\"plm\")",
    "tags": [
      "panel-data",
      "econometrics",
      "fixed-effects",
      "random-effects",
      "hausman-test"
    ],
    "best_for": "Comprehensive panel data analysis requiring within/between/random effects estimation, Hausman tests, and extensive diagnostic testing",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "panel-data",
      "econometrics"
    ],
    "summary": "The plm package is a comprehensive econometrics tool designed for linear panel models, offering various estimators such as fixed effects and random effects. It is primarily used by researchers and analysts in economics and social sciences to analyze panel data.",
    "use_cases": [
      "Estimating fixed effects models for economic data",
      "Conducting diagnostic tests for panel data"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for panel data analysis",
      "how to perform fixed effects in R",
      "GMM estimation in R",
      "Hausman-Taylor estimator in R",
      "diagnostic tests for panel data in R",
      "random effects model R package"
    ],
    "primary_use_cases": [
      "fixed effects estimation",
      "random effects estimation"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "plm"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "pydynpd",
    "description": "Estimation of dynamic panel data models using Arellano-Bond (Difference GMM) and Blundell-Bond (System GMM). Includes Windmeijer correction & tests.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://doi.org/10.21105/joss.04416",
    "github_url": "https://github.com/dazhwu/pydynpd",
    "url": "https://github.com/dazhwu/pydynpd",
    "install": "pip install pydynpd",
    "tags": [
      "panel data",
      "fixed effects"
    ],
    "best_for": "Longitudinal analysis, controlling for unobserved heterogeneity",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "panel data",
      "fixed effects"
    ],
    "summary": "pydynpd is a Python package designed for estimating dynamic panel data models using Arellano-Bond and Blundell-Bond methods. It is useful for researchers and practitioners working with panel data in econometrics.",
    "use_cases": [
      "Estimating dynamic panel data models",
      "Conducting econometric analysis on longitudinal data"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for dynamic panel data models",
      "how to estimate Arellano-Bond in python",
      "Blundell-Bond estimation in python",
      "panel data analysis with python",
      "difference GMM python package",
      "system GMM python library"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "WebPower",
    "description": "Comprehensive collection of tools for basic and advanced statistical power analysis including correlation, t-test, ANOVA, regression, mediation analysis, structural equation modeling (SEM), and multilevel models. Features both R package and web interface.",
    "category": "Power Analysis",
    "docs_url": "https://webpower.psychstat.org/",
    "github_url": "https://github.com/johnnyzhz/WebPower",
    "url": "https://cran.r-project.org/package=WebPower",
    "install": "install.packages(\"WebPower\")",
    "tags": [
      "power-analysis",
      "SEM",
      "mediation",
      "multilevel-models",
      "cluster-randomized-trials"
    ],
    "best_for": "Advanced power analysis for SEM, mediation, and cluster randomized trials, implementing Zhang & Yuan (2018)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "power-analysis"
    ],
    "summary": "WebPower is a comprehensive collection of tools for both basic and advanced statistical power analysis. It is used by researchers and statisticians to conduct various analyses including correlation, t-tests, ANOVA, regression, and more.",
    "use_cases": [
      "Conducting power analysis for a research study",
      "Performing mediation analysis in psychological research"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for power analysis",
      "how to conduct mediation analysis in R",
      "statistical power analysis tools",
      "WebPower R package",
      "ANOVA power analysis in R",
      "structural equation modeling tools in R"
    ],
    "primary_use_cases": [
      "statistical power analysis",
      "mediation analysis",
      "structural equation modeling"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "pwr",
    "description": "Provides basic power calculations using effect sizes and notation from Cohen (1988). Supports t-tests, chi-squared tests, one-way ANOVA, correlation tests, proportion tests, and general linear models with analytical (closed-form) solutions.",
    "category": "Power Analysis",
    "docs_url": "https://cran.r-project.org/web/packages/pwr/pwr.pdf",
    "github_url": "https://github.com/heliosdrm/pwr",
    "url": "https://cran.r-project.org/package=pwr",
    "install": "install.packages(\"pwr\")",
    "tags": [
      "power-analysis",
      "sample-size",
      "effect-size",
      "Cohen-d",
      "t-test"
    ],
    "best_for": "Basic power calculations for standard statistical tests following Cohen's conventions from Cohen (1988)",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The 'pwr' package provides basic power calculations for various statistical tests using effect sizes and notation from Cohen (1988). It is useful for researchers and statisticians who need to determine sample sizes or power for t-tests, ANOVA, and other statistical analyses.",
    "use_cases": [
      "Determining sample size for a t-test",
      "Calculating power for a one-way ANOVA"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "R package for power analysis",
      "how to calculate sample size in R",
      "R power calculations for t-tests",
      "effect size calculations in R",
      "Cohen's d in R",
      "ANOVA power analysis R",
      "R package for statistical tests"
    ],
    "primary_use_cases": [
      "sample size determination",
      "power analysis for statistical tests"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active"
  },
  {
    "name": "simr",
    "description": "Calculates power for generalized linear mixed models (GLMMs) using Monte Carlo simulation. Designed to work with lme4 models; supports LMMs and GLMMs with crossed random effects, non-normal responses, and complex variance structures where analytical solutions are unavailable.",
    "category": "Power Analysis",
    "docs_url": "https://cran.r-project.org/web/packages/simr/vignettes/fromscratch.html",
    "github_url": "https://github.com/pitakakariki/simr",
    "url": "https://cran.r-project.org/package=simr",
    "install": "install.packages(\"simr\")",
    "tags": [
      "power-analysis",
      "mixed-models",
      "simulation",
      "lme4",
      "GLMM"
    ],
    "best_for": "Power analysis for hierarchical/multilevel models via simulation when analytical solutions don't exist, implementing Green & MacLeod (2016, MEE)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The simr package calculates power for generalized linear mixed models (GLMMs) using Monte Carlo simulation. It is designed for researchers and statisticians who need to assess the power of their GLMM analyses, particularly in complex scenarios where analytical solutions are not available.",
    "use_cases": [
      "Assessing power for experimental designs with mixed models",
      "Evaluating sample size requirements for GLMMs"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for power analysis",
      "how to calculate power for GLMM in R",
      "Monte Carlo simulation for mixed models",
      "lme4 power analysis in R",
      "simulating power for generalized linear mixed models",
      "R simulation for mixed models"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "ADOpy",
    "description": "Bayesian Adaptive Design Optimization (ADO) for tuning experiments in real-time, with models for psychometric tasks.",
    "category": "Power Simulation & Design of Experiments",
    "docs_url": "https://adopy.readthedocs.io/en/latest/",
    "github_url": "https://github.com/adopy/adopy",
    "url": "https://github.com/adopy/adopy",
    "install": "pip install adopy",
    "tags": [
      "power analysis",
      "experiments",
      "Bayesian"
    ],
    "best_for": "Sample size calculation, experimental design, power analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "bayesian",
      "experiments"
    ],
    "summary": "ADOpy is a Python package designed for Bayesian Adaptive Design Optimization, enabling real-time tuning of experiments with models tailored for psychometric tasks. It is useful for researchers and practitioners involved in experimental design and analysis.",
    "use_cases": [
      "Optimizing experimental designs for psychometric tasks",
      "Real-time tuning of experiments based on Bayesian methods"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for Bayesian Adaptive Design Optimization",
      "how to tune experiments in real-time with Python",
      "Bayesian methods for experiments in Python",
      "psychometric task modeling in Python",
      "power analysis tools in Python",
      "experiments optimization library Python"
    ],
    "primary_use_cases": [
      "real-time experiment tuning",
      "Bayesian optimization for experiments"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "Adaptive",
    "description": "Parallel active learning library for adaptive function sampling/evaluation, with live plotting for monitoring.",
    "category": "Power Simulation & Design of Experiments",
    "docs_url": "https://adaptive.readthedocs.io/en/latest/",
    "github_url": "https://github.com/python-adaptive/adaptive",
    "url": "https://github.com/python-adaptive/adaptive",
    "install": "pip install adaptive",
    "tags": [
      "power analysis",
      "experiments"
    ],
    "best_for": "Sample size calculation, experimental design, power analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "Adaptive is a parallel active learning library designed for adaptive function sampling and evaluation, featuring live plotting for monitoring purposes. It is useful for researchers and practitioners involved in power analysis and experimental design.",
    "use_cases": [
      "Sampling functions adaptively based on previous evaluations",
      "Monitoring experiments in real-time with live plotting"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for adaptive function sampling",
      "how to do active learning in python",
      "parallel active learning library",
      "live plotting for monitoring experiments",
      "power analysis in python",
      "experiments with adaptive sampling in python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "Ambrosia",
    "description": "End-to-end A/B testing from MobileTeleSystems with PySpark support. Covers experiment design, multi-group splitting, matching, and inference.",
    "category": "Power Simulation & Design of Experiments",
    "docs_url": null,
    "github_url": "https://github.com/MobileTeleSystems/Ambrosia",
    "url": "https://github.com/MobileTeleSystems/Ambrosia",
    "install": "pip install ambrosia",
    "tags": [
      "A/B testing",
      "experimentation",
      "Spark"
    ],
    "best_for": "End-to-end A/B testing with PySpark",
    "language": "Spark",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "experimentation"
    ],
    "summary": "Ambrosia is an end-to-end A/B testing tool that supports experiment design, multi-group splitting, matching, and inference using PySpark. It is primarily used by data scientists and researchers conducting experiments in various domains.",
    "use_cases": [
      "Conducting A/B tests for mobile applications",
      "Designing experiments for marketing strategies"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for A/B testing",
      "how to conduct experiments with PySpark",
      "end-to-end A/B testing in Python",
      "A/B testing framework for Spark",
      "experiment design tools in Python",
      "multi-group splitting in A/B testing"
    ],
    "primary_use_cases": [
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "PySpark"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "DoEgen",
    "description": "Automates generation and optimization of designs, especially for mixed factor-level experiments; computes efficiency metrics.",
    "category": "Power Simulation & Design of Experiments",
    "docs_url": null,
    "github_url": "https://github.com/sebhaan/DoEgen",
    "url": "https://github.com/sebhaan/DoEgen",
    "install": "pip install DoEgen",
    "tags": [
      "power analysis",
      "experiments"
    ],
    "best_for": "Sample size calculation, experimental design, power analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "DoEgen automates the generation and optimization of designs for mixed factor-level experiments and computes efficiency metrics. It is useful for researchers and practitioners involved in experimental design and analysis.",
    "use_cases": [
      "Optimizing experimental designs for agricultural studies",
      "Generating designs for clinical trials"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for design of experiments",
      "how to optimize mixed factor-level experiments in python",
      "automate design generation in python",
      "efficiency metrics for experiments in python",
      "python power analysis tools",
      "experiments optimization python library"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "pyDOE2",
    "description": "Implements classical Design of Experiments: factorial (full/fractional), response surface (Box-Behnken, CCD), Latin Hypercube.",
    "category": "Power Simulation & Design of Experiments",
    "docs_url": "https://pythonhosted.org/pyDOE2/",
    "github_url": "https://github.com/clicumu/pyDOE2",
    "url": "https://github.com/clicumu/pyDOE2",
    "install": "pip install pyDOE2",
    "tags": [
      "power analysis",
      "experiments"
    ],
    "best_for": "Sample size calculation, experimental design, power analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "pyDOE2 implements classical Design of Experiments techniques, including factorial designs, response surface methods, and Latin Hypercube sampling. It is used by researchers and practitioners in fields requiring experimental design and analysis.",
    "use_cases": [
      "Designing experiments for product testing",
      "Optimizing processes in manufacturing"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for Design of Experiments",
      "how to perform factorial design in python",
      "response surface methodology in python",
      "Latin Hypercube sampling python",
      "pyDOE2 examples",
      "experimental design tools in python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "tea-tasting",
    "description": "Calculate A/B test statistics directly within data warehouses (BigQuery, ClickHouse, Snowflake, Spark) via Ibis interface. Supports CUPED/CUPAC.",
    "category": "Power Simulation & Design of Experiments",
    "docs_url": "https://tea-tasting.e10v.me/",
    "github_url": "https://github.com/e10v/tea-tasting",
    "url": "https://github.com/e10v/tea-tasting",
    "install": "pip install tea-tasting",
    "tags": [
      "A/B testing",
      "experimentation",
      "data warehouses"
    ],
    "best_for": "In-warehouse A/B test analysis with variance reduction",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "causal-inference",
      "experimentation"
    ],
    "summary": "The tea-tasting package allows users to calculate A/B test statistics directly within data warehouses using the Ibis interface. It is particularly useful for data scientists and analysts working with large datasets in environments like BigQuery, ClickHouse, Snowflake, and Spark.",
    "use_cases": [
      "Analyzing A/B test results in BigQuery",
      "Implementing CUPED for variance reduction",
      "Conducting experiments in Snowflake",
      "Performing statistical analysis in ClickHouse"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for A/B testing",
      "how to calculate A/B test statistics in python",
      "data warehouse experimentation tools",
      "Ibis interface for A/B testing",
      "CUPED in python",
      "CUPAC implementation in python",
      "python A/B testing library",
      "analyze A/B tests with data warehouses"
    ],
    "primary_use_cases": [
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "CausalImpact",
    "description": "Python port of Google's R package for estimating causal effects of interventions on time series using Bayesian structural time-series models.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": "https://google.github.io/CausalImpact/CausalImpact/CausalImpact.html",
    "github_url": "https://github.com/tcassou/causal_impact",
    "url": "https://github.com/tcassou/causal_impact",
    "install": "pip install causalimpact",
    "tags": [
      "DiD",
      "synthetic control",
      "RDD",
      "Bayesian"
    ],
    "best_for": "Policy evaluation, natural experiments, quasi-experiments",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "time-series",
      "bayesian"
    ],
    "summary": "CausalImpact is a Python port of Google's R package designed to estimate causal effects of interventions on time series data. It is primarily used by data scientists and researchers interested in evaluating the impact of specific interventions using Bayesian structural time-series models.",
    "use_cases": [
      "Evaluating the impact of a marketing campaign on sales",
      "Assessing the effect of a policy change on economic indicators"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for causal impact analysis",
      "how to estimate causal effects in python",
      "python time series intervention analysis",
      "bayesian structural time series python",
      "synthetic control method in python",
      "RDD analysis in python"
    ],
    "primary_use_cases": [
      "causal impact analysis",
      "intervention effect estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "Differences",
    "description": "Implements modern difference-in-differences methods for staggered adoption designs (e.g., Callaway & Sant'Anna).",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": "https://bernardodionisi.github.io/differences/",
    "github_url": "https://github.com/bernardodionisi/differences",
    "url": "https://github.com/bernardodionisi/differences",
    "install": "pip install differences",
    "tags": [
      "DiD",
      "synthetic control",
      "RDD"
    ],
    "best_for": "Policy evaluation, natural experiments, quasi-experiments",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "program-evaluation"
    ],
    "summary": "The Differences package implements modern difference-in-differences methods specifically designed for staggered adoption designs, such as those proposed by Callaway & Sant'Anna. It is useful for researchers and practitioners in program evaluation who need to analyze the effects of interventions over time.",
    "use_cases": [
      "Evaluating the impact of a policy change over time",
      "Analyzing the effects of staggered treatment adoption across groups"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for difference-in-differences",
      "how to implement staggered adoption in python",
      "difference-in-differences methods in python",
      "Callaway & Sant'Anna implementation in python",
      "program evaluation methods in python",
      "synthetic control methods python"
    ],
    "primary_use_cases": [
      "difference-in-differences analysis",
      "program evaluation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "SyntheticControlMethods",
    "description": "Implementation of synthetic control methods for comparative case studies when panel data is available.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": null,
    "github_url": "https://github.com/OscarEngelbrektson/SyntheticControlMethods",
    "url": "https://github.com/OscarEngelbrektson/SyntheticControlMethods",
    "install": "pip install SyntheticControlMethods",
    "tags": [
      "DiD",
      "synthetic control",
      "RDD"
    ],
    "best_for": "Policy evaluation, natural experiments, quasi-experiments",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "program-evaluation"
    ],
    "summary": "SyntheticControlMethods provides an implementation of synthetic control methods for comparative case studies using panel data. It is useful for researchers and practitioners in program evaluation and causal inference.",
    "use_cases": [
      "Evaluating the impact of a policy change in a specific region",
      "Comparing economic outcomes between treated and control groups"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for synthetic control methods",
      "how to implement synthetic control in python",
      "comparative case studies in python",
      "panel data analysis python",
      "program evaluation methods in python",
      "synthetic control methods tutorial"
    ],
    "primary_use_cases": [
      "comparative case studies",
      "program evaluation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "TFP CausalImpact",
    "description": "TensorFlow Probability port of Google's CausalImpact. Bayesian structural time-series for intervention effects.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": "https://github.com/google/tfp-causalimpact",
    "github_url": "https://github.com/google/tfp-causalimpact",
    "url": "https://github.com/google/tfp-causalimpact",
    "install": "pip install tfcausalimpact",
    "tags": [
      "causal impact",
      "time series",
      "Bayesian"
    ],
    "best_for": "TensorFlow-based causal impact analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "time-series",
      "bayesian"
    ],
    "summary": "TFP CausalImpact is a TensorFlow Probability port of Google's CausalImpact, designed for analyzing intervention effects using Bayesian structural time-series models. It is primarily used by data scientists and researchers interested in causal inference and time-series analysis.",
    "use_cases": [
      "Evaluating the impact of marketing campaigns",
      "Assessing policy changes on economic indicators"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for causal impact analysis",
      "how to analyze intervention effects in python",
      "time series analysis with Bayesian methods in python",
      "TFP CausalImpact tutorial",
      "using TensorFlow Probability for causal inference",
      "how to implement CausalImpact in python"
    ],
    "primary_use_cases": [
      "causal impact analysis",
      "intervention effect estimation"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "TensorFlow Probability"
    ],
    "related_packages": [
      "CausalImpact"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "csdid",
    "description": "Python adaptation of the R `did` package. Implements multi-period DiD with staggered treatment timing (Callaway & Sant\u2019Anna).",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": null,
    "github_url": "https://github.com/d2cml-ai/csdid",
    "url": "https://github.com/d2cml-ai/csdid",
    "install": "pip install csdid",
    "tags": [
      "DiD",
      "synthetic control",
      "RDD"
    ],
    "best_for": "Policy evaluation, natural experiments, quasi-experiments",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "program-evaluation"
    ],
    "summary": "The csdid package is a Python adaptation of the R `did` package, designed to implement multi-period Difference-in-Differences (DiD) with staggered treatment timing. It is primarily used by researchers and practitioners in program evaluation to analyze the effects of interventions over time.",
    "use_cases": [
      "Evaluating the impact of a policy change over multiple time periods",
      "Analyzing treatment effects with staggered implementation"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for difference-in-differences",
      "how to implement staggered treatment timing in python",
      "python causal inference tools",
      "multi-period DiD in python",
      "synthetic control methods in python",
      "RDD analysis in python"
    ],
    "primary_use_cases": [
      "multi-period DiD analysis",
      "program evaluation"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Callaway & Sant\u2019Anna (2021)",
    "related_packages": [
      "statsmodels",
      "causalml"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "didet",
    "description": "DiD with general treatment patterns. Handles effective treatment timing beyond simple staggered adoption.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": "https://github.com/tkhdyanagi/didet",
    "github_url": "https://github.com/tkhdyanagi/didet",
    "url": "https://github.com/tkhdyanagi/didet",
    "install": "pip install didet",
    "tags": [
      "DiD",
      "treatment timing",
      "causal inference"
    ],
    "best_for": "DiD with general treatment patterns",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "program-evaluation"
    ],
    "summary": "The didet package provides tools for Difference-in-Differences (DiD) analysis with a focus on general treatment patterns and effective treatment timing. It is useful for researchers and practitioners in economics and social sciences who are analyzing causal effects.",
    "use_cases": [
      "Evaluating the impact of policy changes over time",
      "Analyzing treatment effects in social experiments"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for DiD analysis",
      "how to analyze treatment timing in python",
      "causal inference tools in python",
      "difference-in-differences package python",
      "program evaluation methods in python",
      "python library for causal analysis"
    ],
    "primary_use_cases": [
      "causal inference analysis",
      "treatment timing evaluation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "didhetero",
    "description": "Doubly robust estimation for group-time conditional average treatment effects. UCB for heterogeneous DiD.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": "https://github.com/tkhdyanagi/didhetero",
    "github_url": "https://github.com/tkhdyanagi/didhetero",
    "url": "https://github.com/tkhdyanagi/didhetero",
    "install": "pip install didhetero",
    "tags": [
      "DiD",
      "heterogeneous effects",
      "doubly robust"
    ],
    "best_for": "Heterogeneous treatment effects in DiD",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "program-evaluation"
    ],
    "summary": "The didhetero package provides a framework for doubly robust estimation of group-time conditional average treatment effects, specifically focusing on heterogeneous effects in difference-in-differences analyses. It is primarily used by researchers and practitioners in program evaluation and causal inference.",
    "use_cases": [
      "Estimating treatment effects in policy evaluations",
      "Analyzing the impact of interventions over time"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for doubly robust estimation",
      "how to estimate heterogeneous treatment effects in python",
      "difference-in-differences analysis in python",
      "program evaluation methods in python",
      "python library for causal inference",
      "how to implement DiD in python"
    ],
    "primary_use_cases": [
      "doubly robust estimation",
      "group-time conditional average treatment effects"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "mlsynth",
    "description": "Implements advanced synthetic control methods: forward DiD, cluster SC, factor models, and proximal SC. Designed for single-treated-unit settings.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": "https://mlsynth.readthedocs.io/en/latest/",
    "github_url": "https://github.com/jgreathouse9/mlsynth",
    "url": "https://github.com/jgreathouse9/mlsynth",
    "install": "pip install mlsynth",
    "tags": [
      "DiD",
      "synthetic control",
      "RDD"
    ],
    "best_for": "Policy evaluation, natural experiments, quasi-experiments",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "program-evaluation",
      "synthetic-control"
    ],
    "summary": "mlsynth implements advanced synthetic control methods including forward DiD, cluster SC, factor models, and proximal SC, specifically designed for single-treated-unit settings. It is useful for researchers and practitioners in program evaluation.",
    "use_cases": [
      "Evaluating the impact of a policy intervention on a single unit",
      "Analyzing treatment effects in observational studies"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for synthetic control",
      "how to implement DiD in python",
      "python package for program evaluation",
      "synthetic control methods in python",
      "advanced causal inference in python",
      "factor models for program evaluation"
    ],
    "primary_use_cases": [
      "synthetic control analysis",
      "difference-in-differences estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "pycinc",
    "description": "Changes\u2011in\u2011Changes (CiC) estimator for distributional treatment effects (Athey\u00a0&\u00a0Imbens\u202f2006).",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": "https://pypi.org/project/pycinc/",
    "github_url": null,
    "url": "https://pypi.org/project/pycinc/",
    "install": "pip install pycinc",
    "tags": [
      "DiD",
      "synthetic control",
      "RDD",
      "causal inference"
    ],
    "best_for": "Policy evaluation, natural experiments, quasi-experiments",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "program-evaluation",
      "treatment-effects"
    ],
    "summary": "The pycinc package provides a Changes-in-Changes (CiC) estimator for analyzing distributional treatment effects, as proposed by Athey & Imbens in 2006. It is primarily used by researchers and practitioners in program evaluation and causal inference.",
    "use_cases": [
      "Estimating treatment effects in social programs",
      "Analyzing the impact of policy changes"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for distributional treatment effects",
      "how to estimate treatment effects in python",
      "python causal inference package",
      "Changes-in-Changes estimator python",
      "program evaluation methods in python",
      "synthetic control methods python"
    ],
    "primary_use_cases": [
      "distributional treatment effect estimation"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Athey & Imbens (2006)",
    "maintenance_status": "active"
  },
  {
    "name": "pyleebounds",
    "description": "Lee\u00a0(2009) sample\u2011selection bounds for treatment effects; trims treated distribution to match selection rates.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": "https://pypi.org/project/pyleebounds/",
    "github_url": null,
    "url": "https://pypi.org/project/pyleebounds/",
    "install": "pip install pyleebounds",
    "tags": [
      "DiD",
      "synthetic control",
      "RDD",
      "causal inference"
    ],
    "best_for": "Policy evaluation, natural experiments, quasi-experiments",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The pyleebounds package implements sample-selection bounds for treatment effects as described by Lee (2009). It is used by researchers and practitioners in program evaluation to adjust treated distributions to match selection rates.",
    "use_cases": [
      "Evaluating treatment effects in observational studies",
      "Adjusting for selection bias in experimental data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for treatment effects",
      "how to implement sample selection bounds in python",
      "python causal inference tools",
      "program evaluation methods in python",
      "python library for DiD analysis",
      "how to use synthetic control in python"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Lee (2009)",
    "maintenance_status": "active"
  },
  {
    "name": "pysyncon",
    "description": "Synthetic control method implementation compatible with R's Synth and augsynth packages.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": null,
    "github_url": "https://github.com/sdfordham/pysyncon",
    "url": "https://github.com/sdfordham/pysyncon",
    "install": "pip install pysyncon",
    "tags": [
      "synthetic control",
      "causal inference",
      "panel data"
    ],
    "best_for": "R Synth-compatible synthetic control in Python",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "panel-data"
    ],
    "summary": "pysyncon is a Python package that implements the synthetic control method, allowing users to analyze causal effects in observational data. It is particularly useful for researchers and practitioners in program evaluation who require robust methods for causal inference.",
    "use_cases": [
      "Evaluating the impact of a policy intervention",
      "Comparing treatment effects across different groups"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for synthetic control",
      "how to implement synthetic control in python",
      "causal inference methods in python",
      "panel data analysis in python",
      "synthetic control method python",
      "python package for program evaluation"
    ],
    "primary_use_cases": [
      "causal inference analysis",
      "program evaluation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "rdd",
    "description": "Toolkit for sharp RDD analysis, including bandwidth calculation and estimation, integrating with pandas.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": null,
    "github_url": "https://github.com/evan-magnusson/rdd",
    "url": "https://github.com/evan-magnusson/rdd",
    "install": "pip install rdd",
    "tags": [
      "DiD",
      "synthetic control",
      "RDD"
    ],
    "best_for": "Policy evaluation, natural experiments, quasi-experiments",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "causal-inference",
      "program-evaluation"
    ],
    "summary": "The rdd package provides a toolkit for sharp Regression Discontinuity Design (RDD) analysis, including bandwidth calculation and estimation. It is useful for researchers and practitioners in program evaluation who need to analyze causal effects.",
    "use_cases": [
      "Analyzing the impact of policy changes at a threshold",
      "Estimating treatment effects in educational interventions"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for RDD analysis",
      "how to calculate bandwidth in RDD",
      "RDD estimation in python",
      "sharp RDD toolkit python",
      "program evaluation methods in python",
      "python pandas RDD"
    ],
    "primary_use_cases": [
      "bandwidth calculation",
      "RDD estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "rdrobust",
    "description": "Comprehensive tools for Regression Discontinuity Designs (RDD), including optimal bandwidth selection, estimation, inference.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": "https://pypi.org/project/rdrobust/",
    "github_url": "https://github.com/rdpackages/rdrobust",
    "url": "https://github.com/rdpackages/rdrobust",
    "install": "pip install rdrobust",
    "tags": [
      "DiD",
      "synthetic control",
      "RDD"
    ],
    "best_for": "Policy evaluation, natural experiments, quasi-experiments",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "program-evaluation"
    ],
    "summary": "rdrobust provides comprehensive tools for conducting Regression Discontinuity Designs (RDD), including optimal bandwidth selection and estimation. It is used by researchers and practitioners in the field of program evaluation to analyze causal effects.",
    "use_cases": [
      "Evaluating the impact of a policy change using RDD",
      "Analyzing educational interventions with RDD"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for regression discontinuity",
      "how to perform RDD in python",
      "optimal bandwidth selection in python",
      "RDD estimation tools python",
      "program evaluation methods in python",
      "causal inference library python"
    ],
    "primary_use_cases": [
      "optimal bandwidth selection",
      "RDD estimation"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "rdd",
      "rdl"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "synthlearners",
    "description": "Fast synthetic control estimators for panel data problems. Optimized ATT estimation with multiple SC algorithms.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": null,
    "github_url": "https://github.com/apoorvalal/synthlearners",
    "url": "https://github.com/apoorvalal/synthlearners",
    "install": "pip install synthlearners",
    "tags": [
      "synthetic control",
      "causal inference",
      "panel data"
    ],
    "best_for": "Optimized synthetic control with multiple algorithm options",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "panel-data"
    ],
    "summary": "synthlearners provides fast synthetic control estimators specifically designed for panel data problems. It is used by researchers and practitioners in program evaluation to optimize average treatment effect estimation using multiple synthetic control algorithms.",
    "use_cases": [
      "Estimating treatment effects in policy evaluation",
      "Analyzing the impact of interventions in social sciences"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for synthetic control",
      "how to estimate ATT in python",
      "synthetic control methods in python",
      "panel data analysis with python",
      "causal inference tools in python",
      "fast synthetic control estimators python"
    ],
    "primary_use_cases": [
      "average treatment effect estimation",
      "program evaluation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "pyqreg",
    "description": "Fast quantile regression solver using interior point methods, supporting robust and clustered standard errors.",
    "category": "Quantile Regression & Distributional Methods",
    "docs_url": "https://github.com/mozjay0619/pyqreg",
    "github_url": null,
    "url": "https://github.com/mozjay0619/pyqreg",
    "install": "pip install pyqreg",
    "tags": [
      "quantile",
      "regression"
    ],
    "best_for": "Heterogeneous effects, distributional analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "pyqreg is a fast quantile regression solver that utilizes interior point methods to provide robust and clustered standard errors. It is designed for users who need efficient quantile regression analysis in their statistical modeling.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for quantile regression",
      "how to perform quantile regression in python",
      "fast quantile regression solver in python",
      "interior point methods for regression",
      "robust standard errors in python",
      "clustered standard errors in quantile regression"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "pyrifreg",
    "description": "Recentered Influence\u2011Function (RIF) regression for unconditional quantile & distributional effects (Firpo\u202fet\u202fal.,\u202f2008).",
    "category": "Quantile Regression & Distributional Methods",
    "docs_url": "https://github.com/vyasenov/pyrifreg",
    "github_url": null,
    "url": "https://github.com/vyasenov/pyrifreg",
    "install": "pip install pyrifreg",
    "tags": [
      "quantile",
      "regression"
    ],
    "best_for": "Heterogeneous effects, distributional analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "quantile-regression",
      "distributional-methods"
    ],
    "summary": "pyrifreg is a Python package that implements Recentered Influence-Function (RIF) regression for analyzing unconditional quantile and distributional effects. It is primarily used by researchers and practitioners interested in causal inference and econometrics.",
    "use_cases": [
      "Analyzing the impact of policy changes on income distribution",
      "Estimating the effects of treatment on different quantiles of the outcome variable"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for RIF regression",
      "how to perform quantile regression in python",
      "RIF regression for distributional effects",
      "quantile regression analysis python",
      "unconditional quantile effects python",
      "distributional methods in python"
    ],
    "primary_use_cases": [
      "unconditional quantile analysis",
      "distributional effect estimation"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Firpo et al. (2008)",
    "maintenance_status": "active"
  },
  {
    "name": "quantile-forest",
    "description": "Scikit-learn compatible implementation of Quantile Regression Forests for non-parametric estimation.",
    "category": "Quantile Regression & Distributional Methods",
    "docs_url": "https://zillow.github.io/quantile-forest/",
    "github_url": "https://github.com/zillow/quantile-forest",
    "url": "https://github.com/zillow/quantile-forest",
    "install": "pip install quantile-forest",
    "tags": [
      "quantile",
      "regression"
    ],
    "best_for": "Heterogeneous effects, distributional analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "Quantile Forest is a Scikit-learn compatible library that provides a non-parametric method for estimating quantiles using regression forests. It is useful for statisticians and data scientists who need to perform quantile regression analysis.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for quantile regression",
      "how to estimate quantiles in python",
      "scikit-learn compatible quantile forest",
      "quantile regression forests implementation",
      "non-parametric quantile estimation in python",
      "quantile forest tutorial"
    ],
    "primary_use_cases": [
      "quantile regression analysis"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "scikit-learn"
    ],
    "related_packages": [
      "quantregForest"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "broom",
    "description": "Converts messy output from 100+ statistical model types into consistent tidy tibbles using three verbs: tidy() for coefficient-level statistics, glance() for model-level summaries (R\u00b2, AIC), and augment() for fitted values and residuals.",
    "category": "Regression Output",
    "docs_url": "https://broom.tidymodels.org/",
    "github_url": "https://github.com/tidymodels/broom",
    "url": "https://cran.r-project.org/package=broom",
    "install": "install.packages(\"broom\")",
    "tags": [
      "tidy-data",
      "tidymodels",
      "statistical-models",
      "tidyverse",
      "modeling"
    ],
    "best_for": "Converting R statistical model output into consistent tidy data frames for analysis pipelines, based on Wickham (2014, JSS)",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The broom package converts messy output from over 100 statistical model types into consistent tidy tibbles. It is primarily used by data scientists and statisticians to streamline the process of extracting and summarizing model results.",
    "use_cases": [
      "Extracting coefficient-level statistics from regression models",
      "Generating model-level summaries like R\u00b2 and AIC"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for converting model output",
      "how to tidy statistical model results in R",
      "R tidyverse package for model summaries",
      "broom package documentation",
      "R tidy data for regression models",
      "how to use broom for model diagnostics"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active"
  },
  {
    "name": "gt",
    "description": "Build display tables from tabular data using a cohesive grammar of table parts (header, stub, body, footer). Enables progressive construction of publication-quality tables with extensive formatting, footnotes, and cell styling. Outputs to HTML, LaTeX, and RTF.",
    "category": "Regression Output",
    "docs_url": "https://gt.rstudio.com/",
    "github_url": "https://github.com/rstudio/gt",
    "url": "https://cran.r-project.org/package=gt",
    "install": "install.packages(\"gt\")",
    "tags": [
      "grammar-of-tables",
      "display-tables",
      "HTML-tables",
      "Posit",
      "formatting"
    ],
    "best_for": "Publication-ready display tables with precise formatting control and multiple output formats",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The 'gt' package allows users to build display tables from tabular data using a cohesive grammar of table parts. It is used by individuals looking to create publication-quality tables with extensive formatting options.",
    "use_cases": [
      "Creating formatted tables for academic publications",
      "Generating HTML reports with tables"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "R library for display tables",
      "how to format tables in R",
      "create HTML tables in R",
      "R package for publication-quality tables",
      "build tables from data frames in R",
      "R grammar of tables package"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "gtsummary",
    "description": "Creates publication-ready analytical and summary tables (Table 1 demographics, regression results, survival analyses) with one line of code. Auto-detects variable types, calculates appropriate statistics, and formats regression models with reference rows and appropriate headers.",
    "category": "Regression Output",
    "docs_url": "https://www.danieldsjoberg.com/gtsummary/",
    "github_url": "https://github.com/ddsjoberg/gtsummary",
    "url": "https://cran.r-project.org/package=gtsummary",
    "install": "install.packages(\"gtsummary\")",
    "tags": [
      "summary-tables",
      "Table1",
      "clinical-tables",
      "regression-tables",
      "reproducible-research"
    ],
    "best_for": "Table 1 demographics and regression summary tables for medical/scientific publications, implementing Sjoberg et al. (2021, R Journal)",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "gtsummary creates publication-ready analytical and summary tables with minimal code. It is commonly used by researchers and data scientists for generating demographic and regression result tables.",
    "use_cases": [
      "Generating Table 1 for clinical trials",
      "Creating regression result tables for research papers"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for summary tables",
      "how to create regression tables in R",
      "R library for publication-ready tables",
      "best R package for clinical tables",
      "gtsummary documentation",
      "R summary tables for demographics"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active"
  },
  {
    "name": "modelsummary",
    "description": "Creates publication-quality tables summarizing multiple statistical models side-by-side, plus coefficient plots, data summaries, and correlation matrices. Supports 100+ model types via broom/parameters with output to HTML, LaTeX, Word, PDF, PNG, and Excel.",
    "category": "Regression Output",
    "docs_url": "https://modelsummary.com/",
    "github_url": "https://github.com/vincentarelbundock/modelsummary",
    "url": "https://cran.r-project.org/package=modelsummary",
    "install": "install.packages(\"modelsummary\")",
    "tags": [
      "regression-tables",
      "model-summary",
      "coefficient-plots",
      "publication-tables",
      "tidyverse"
    ],
    "best_for": "Modern, flexible regression tables with extensive customization\u2014the successor to stargazer, implementing Arel-Bundock (2022, JSS)",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The modelsummary package creates publication-quality tables that summarize multiple statistical models side-by-side, along with coefficient plots, data summaries, and correlation matrices. It is primarily used by statisticians and data scientists who need to present model results clearly and effectively.",
    "use_cases": [
      "Generating tables for academic publications",
      "Creating visual summaries of model coefficients"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R library for regression tables",
      "how to create model summary tables in R",
      "R package for coefficient plots",
      "best R packages for statistical model summaries",
      "how to export tables to LaTeX in R",
      "R tools for data summaries"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active"
  },
  {
    "name": "stargazer",
    "description": "Produces well-formatted LaTeX, HTML/CSS, and ASCII regression tables with multiple models side-by-side, plus summary statistics tables. Widely used in economics with journal-specific formatting styles (AER, QJE, ASR).",
    "category": "Regression Output",
    "docs_url": "https://cran.r-project.org/web/packages/stargazer/vignettes/stargazer.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=stargazer",
    "install": "install.packages(\"stargazer\")",
    "tags": [
      "LaTeX-tables",
      "regression-output",
      "academic-publishing",
      "economics",
      "HTML-tables"
    ],
    "best_for": "Quick, publication-ready LaTeX tables for economics journals with classic formatting",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The stargazer package produces well-formatted regression tables in LaTeX, HTML/CSS, and ASCII formats, allowing users to display multiple models side-by-side along with summary statistics. It is widely used in the field of economics, particularly for academic publishing.",
    "use_cases": [
      "Generating regression tables for academic papers",
      "Creating formatted output for presentations"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for regression tables",
      "how to create LaTeX tables in R",
      "generate HTML regression output in R",
      "stargazer package documentation",
      "R academic publishing tools",
      "create summary statistics tables in R",
      "multiple regression models side by side in R"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active"
  },
  {
    "name": "texreg",
    "description": "Converts coefficients, standard errors, significance stars, and fit statistics from statistical models into LaTeX, HTML, Word, or console output. Highly extensible with support for custom model types and confidence intervals.",
    "category": "Regression Output",
    "docs_url": "https://cran.r-project.org/web/packages/texreg/vignettes/texreg.pdf",
    "github_url": "https://github.com/leifeld/texreg",
    "url": "https://cran.r-project.org/package=texreg",
    "install": "install.packages(\"texreg\")",
    "tags": [
      "LaTeX-tables",
      "HTML-tables",
      "model-comparison",
      "Word-export",
      "extensible"
    ],
    "best_for": "Highly extensible regression tables with easy custom model type extensions, implementing Leifeld (2013, JSS)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The texreg package converts coefficients, standard errors, significance stars, and fit statistics from statistical models into various output formats including LaTeX, HTML, and Word. It is highly extensible and supports custom model types and confidence intervals, making it useful for statisticians and data scientists.",
    "use_cases": [
      "Generating LaTeX tables for academic papers",
      "Exporting regression results to Word for reports"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for converting model output to LaTeX",
      "how to export regression results to Word in R",
      "R library for HTML tables from statistical models",
      "texreg package documentation",
      "R model comparison tables",
      "how to create LaTeX tables from R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "here",
    "description": "Simple path construction from project root. Uses heuristics to find project root (RStudio, .git, .here) enabling portable paths that work across different machines and working directories.",
    "category": "Reproducibility",
    "docs_url": "https://here.r-lib.org/",
    "github_url": "https://github.com/r-lib/here",
    "url": "https://cran.r-project.org/package=here",
    "install": "install.packages(\"here\")",
    "tags": [
      "paths",
      "project-management",
      "reproducibility",
      "portability",
      "working-directory"
    ],
    "best_for": "Portable file paths from project root for reproducible scripts",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The 'here' package simplifies path construction from the project root using heuristics to locate the root in various environments. It is useful for R users who need to create portable paths that function across different machines and working directories.",
    "use_cases": [
      "Creating paths for data files in R projects",
      "Ensuring reproducibility in R scripts"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "R package for path construction",
      "how to create portable paths in R",
      "R project root detection",
      "R package for reproducibility",
      "how to manage paths in R",
      "R paths across machines"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active"
  },
  {
    "name": "renv",
    "description": "Project-local R dependency management. Creates reproducible environments by recording package versions in a lockfile, isolating project libraries, and enabling version restore.",
    "category": "Reproducibility",
    "docs_url": "https://rstudio.github.io/renv/",
    "github_url": "https://github.com/rstudio/renv",
    "url": "https://cran.r-project.org/package=renv",
    "install": "install.packages(\"renv\")",
    "tags": [
      "reproducibility",
      "package-management",
      "dependency-isolation",
      "lockfile",
      "environments"
    ],
    "best_for": "Project-local package management for reproducible R environments",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The renv package provides project-local R dependency management, allowing users to create reproducible environments by recording package versions in a lockfile. It is useful for R developers who need to isolate project libraries and enable version restoration.",
    "use_cases": [
      "Managing dependencies for R projects",
      "Creating reproducible research environments"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "R package for dependency management",
      "how to create reproducible environments in R",
      "R lockfile management",
      "isolate R project libraries",
      "R package version control",
      "how to restore R package versions"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "rmarkdown",
    "description": "Dynamic documents combining R code with Markdown text. Generates reproducible reports in HTML, PDF, Word, and slides. Foundation for literate programming and reproducible research in R.",
    "category": "Reproducibility",
    "docs_url": "https://rmarkdown.rstudio.com/",
    "github_url": "https://github.com/rstudio/rmarkdown",
    "url": "https://cran.r-project.org/package=rmarkdown",
    "install": "install.packages(\"rmarkdown\")",
    "tags": [
      "literate-programming",
      "reproducible-research",
      "dynamic-documents",
      "reporting",
      "Markdown"
    ],
    "best_for": "Literate programming and reproducible reports combining R code with Markdown",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "rmarkdown is a package that allows users to create dynamic documents by combining R code with Markdown text. It is widely used by researchers and data analysts for generating reproducible reports in various formats such as HTML, PDF, and Word.",
    "use_cases": [
      "Generating reports for academic research",
      "Creating presentations with R code output"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "how to create dynamic documents in R",
      "R package for reproducible reports",
      "generate PDF reports with Rmarkdown",
      "using Markdown with R code",
      "Rmarkdown tutorial",
      "Rmarkdown for data analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "targets",
    "description": "Make-like pipeline toolkit for R. Declares dependencies between pipeline steps, skips up-to-date targets, and supports parallel execution. Standard for reproducible research workflows.",
    "category": "Reproducibility",
    "docs_url": "https://docs.ropensci.org/targets/",
    "github_url": "https://github.com/ropensci/targets",
    "url": "https://cran.r-project.org/package=targets",
    "install": "install.packages(\"targets\")",
    "tags": [
      "pipelines",
      "reproducibility",
      "make",
      "dependency-tracking",
      "parallel"
    ],
    "best_for": "Make-like reproducible pipelines with automatic dependency tracking and parallel execution",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The 'targets' package is a Make-like pipeline toolkit for R that helps users declare dependencies between pipeline steps, skip up-to-date targets, and supports parallel execution. It is designed for reproducible research workflows, making it useful for researchers and data scientists.",
    "use_cases": [
      "Building reproducible research workflows",
      "Managing complex data analysis pipelines"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R library for pipeline management",
      "how to create reproducible workflows in R",
      "R package for dependency tracking",
      "parallel execution in R",
      "make-like tools for R",
      "R package for data pipelines"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "clubSandwich",
    "description": "Provides cluster-robust variance estimators with small-sample corrections, including bias-reduced linearization (BRL/CR2). Includes functions for hypothesis testing with Satterthwaite degrees of freedom and Hotelling's T\u00b2 approximation\u2014essential when the number of clusters is small.",
    "category": "Robust Standard Errors",
    "docs_url": "https://jepusto.github.io/clubSandwich/",
    "github_url": "https://github.com/jepusto/clubSandwich",
    "url": "https://cran.r-project.org/package=clubSandwich",
    "install": "install.packages(\"clubSandwich\")",
    "tags": [
      "cluster-robust",
      "small-sample-corrections",
      "bias-reduced-linearization",
      "fixed-effects",
      "meta-analysis"
    ],
    "best_for": "Cluster-robust inference when the number of clusters is small, especially in panel data and meta-analysis, implementing Pustejovsky & Tipton (2018, JBES)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "robust-statistics",
      "hypothesis-testing"
    ],
    "summary": "The clubSandwich package provides cluster-robust variance estimators with small-sample corrections, including bias-reduced linearization. It is particularly useful for hypothesis testing in scenarios with a limited number of clusters.",
    "use_cases": [
      "Analyzing data with a small number of clusters",
      "Conducting hypothesis tests with cluster-robust methods"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for cluster-robust variance estimators",
      "how to perform hypothesis testing with small samples in R",
      "R library for bias-reduced linearization",
      "cluster-robust standard errors in R",
      "functions for Satterthwaite degrees of freedom in R",
      "Hotelling's T\u00b2 approximation in R"
    ],
    "primary_use_cases": [
      "hypothesis testing with Satterthwaite degrees of freedom",
      "calculating bias-reduced linearization"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "lmtest",
    "description": "Collection of tests for diagnostic checking in linear regression models. Provides the essential coeftest() function for testing coefficients with alternative variance-covariance matrices (pairs with sandwich), plus Breusch-Pagan, Durbin-Watson, and RESET tests.",
    "category": "Robust Standard Errors",
    "docs_url": "https://cran.r-project.org/web/packages/lmtest/vignettes/lmtest-intro.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=lmtest",
    "install": "install.packages(\"lmtest\")",
    "tags": [
      "regression-diagnostics",
      "heteroskedasticity-test",
      "Breusch-Pagan",
      "Durbin-Watson",
      "serial-correlation"
    ],
    "best_for": "Testing coefficient significance with robust SEs and diagnostic tests for regression assumptions, implementing Zeileis & Hothorn (2002)",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The lmtest package provides a collection of tests for diagnostic checking in linear regression models, including essential functions for testing coefficients and various statistical tests. It is primarily used by data scientists and statisticians working with regression analysis.",
    "use_cases": [
      "Testing for heteroskedasticity in regression models",
      "Performing serial correlation tests on residuals"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for regression diagnostics",
      "how to test coefficients in R",
      "Breusch-Pagan test in R",
      "Durbin-Watson test R package",
      "lmtest package documentation",
      "R linear regression model checks"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "sandwich",
    "description": "Object-oriented software for model-robust covariance matrix estimators including heteroscedasticity-consistent (HC0-HC5), heteroscedasticity- and autocorrelation-consistent (HAC/Newey-West), clustered, panel, and bootstrap covariances. Works with lm, glm, fixest, survival models, and many others.",
    "category": "Robust Standard Errors",
    "docs_url": "https://sandwich.R-Forge.R-project.org/",
    "github_url": null,
    "url": "https://cran.r-project.org/package=sandwich",
    "install": "install.packages(\"sandwich\")",
    "tags": [
      "robust-standard-errors",
      "heteroskedasticity-consistent",
      "HAC-covariance",
      "cluster-robust",
      "Newey-West"
    ],
    "best_for": "Computing robust standard errors for cross-sectional, time series, clustered, or panel data, implementing Zeileis (2004, 2006, 2020, JSS)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The 'sandwich' package provides object-oriented software for model-robust covariance matrix estimators, including various types of heteroscedasticity-consistent and clustered covariances. It is used by statisticians and data scientists working with linear and generalized linear models, among others.",
    "use_cases": [
      "Estimating robust standard errors for regression models",
      "Calculating clustered covariances in panel data analysis"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for robust standard errors",
      "how to calculate HAC covariance in R",
      "R library for heteroskedasticity-consistent estimators",
      "cluster-robust covariance in R",
      "bootstrap covariances in R",
      "R package for panel data covariance estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "(PySAL Core)",
    "description": "The broader PySAL ecosystem contains many tools for spatial data handling, weights, visualization, and analysis.",
    "category": "Spatial Econometrics",
    "docs_url": "https://pysal.org/",
    "github_url": "https://github.com/pysal/pysal",
    "url": "https://github.com/pysal/pysal",
    "install": "pip install pysal",
    "tags": [
      "spatial",
      "geography"
    ],
    "best_for": "Geographic data, spatial autocorrelation, regional analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "spatial-econometrics"
    ],
    "summary": "PySAL Core is part of the broader PySAL ecosystem, providing tools for spatial data handling, weights, visualization, and analysis. It is used by researchers and practitioners in the fields of spatial econometrics and geography.",
    "use_cases": [
      "Analyzing spatial data patterns",
      "Creating spatial weights for econometric models"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for spatial data analysis",
      "how to visualize spatial data in python",
      "tools for spatial econometrics in python",
      "spatial weights in python",
      "geography analysis with python",
      "spatial data handling in python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "Apache Sedona",
    "description": "Distributed spatial analytics engine (formerly GeoSpark) with spatial SQL, K-NN joins, and range queries for spatial econometrics.",
    "category": "Spatial Econometrics",
    "docs_url": "https://sedona.apache.org/",
    "github_url": "https://github.com/apache/sedona",
    "url": "https://github.com/apache/sedona",
    "install": "pip install apache-sedona",
    "tags": [
      "spark",
      "spatial",
      "GIS",
      "distributed"
    ],
    "best_for": "Constructing spatial weight matrices and distance-based instruments at scale",
    "language": "Spark",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Apache Sedona is a distributed spatial analytics engine that allows users to perform spatial SQL queries, K-NN joins, and range queries, making it suitable for spatial econometrics. It is used by data scientists and researchers working with spatial data.",
    "use_cases": [
      "Analyzing spatial data for urban planning",
      "Performing K-NN joins for location-based services"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for spatial analytics",
      "how to perform K-NN joins in Spark",
      "spatial SQL queries in Apache Sedona",
      "distributed GIS tools",
      "range queries for spatial econometrics",
      "Apache Sedona documentation",
      "spatial data analysis with Spark"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "Spark"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "PySAL (spreg)",
    "description": "The spatial regression `spreg` module of PySAL. Implements spatial lag, error, IV models, and diagnostics.",
    "category": "Spatial Econometrics",
    "docs_url": "https://pysal.org/spreg/",
    "github_url": "https://github.com/pysal/spreg",
    "url": "https://github.com/pysal/spreg",
    "install": "pip install spreg",
    "tags": [
      "spatial",
      "geography"
    ],
    "best_for": "Geographic data, spatial autocorrelation, regional analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "spatial-econometrics",
      "geography"
    ],
    "summary": "The `spreg` module of PySAL provides tools for spatial regression analysis, including spatial lag, error, and instrumental variable models. It is used by researchers and practitioners in spatial econometrics to analyze spatially correlated data.",
    "use_cases": [
      "Analyzing spatially correlated economic data",
      "Evaluating the impact of geographical factors on economic outcomes"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for spatial regression",
      "how to perform spatial econometrics in python",
      "spatial lag model in python",
      "spatial error model python",
      "IV models for spatial data",
      "diagnostics for spatial regression python"
    ],
    "primary_use_cases": [
      "spatial lag model estimation",
      "spatial error model analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "GeoPandas",
      "statsmodels"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "sf",
    "description": "The modern standard for spatial vector data in R, implementing Simple Features access (ISO 19125). Represents spatial data as data frames with geometry list-columns, enabling seamless tidyverse integration. Interfaces with GDAL (I/O), GEOS (geometry operations), PROJ (projections), and s2 (spherical geometry).",
    "category": "Spatial Econometrics",
    "docs_url": "https://r-spatial.github.io/sf/",
    "github_url": "https://github.com/r-spatial/sf",
    "url": "https://cran.r-project.org/package=sf",
    "install": "install.packages(\"sf\")",
    "tags": [
      "simple-features",
      "spatial-data",
      "vector-data",
      "tidyverse",
      "GDAL-GEOS-PROJ"
    ],
    "best_for": "Reading, writing, manipulating, and visualizing spatial vector data; foundation for all spatial workflows, implementing Pebesma (2018)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The 'sf' package provides a modern standard for handling spatial vector data in R, utilizing Simple Features access. It is designed for users who work with spatial data and integrates well with the tidyverse, making it suitable for data scientists and researchers in spatial econometrics.",
    "use_cases": [
      "Analyzing spatial data for econometric studies",
      "Visualizing geographic information using tidyverse"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for spatial vector data",
      "how to use simple features in R",
      "spatial data analysis in R",
      "R tidyverse spatial data",
      "interfaces with GDAL in R",
      "geometry operations in R"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "tidyverse"
    ],
    "related_packages": [
      "sp",
      "sfheaders"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "spatialreg",
    "description": "Comprehensive package for spatial regression model estimation, split from spdep in 2019. Provides maximum likelihood, two-stage least squares, and GMM estimation for spatial lag (SAR), spatial error (SEM), and combined (SARAR/SAC) models, plus Spatial Durbin and SLX variants with impact calculations.",
    "category": "Spatial Econometrics",
    "docs_url": "https://r-spatial.github.io/spatialreg/",
    "github_url": "https://github.com/r-spatial/spatialreg",
    "url": "https://cran.r-project.org/package=spatialreg",
    "install": "install.packages(\"spatialreg\")",
    "tags": [
      "spatial-regression",
      "maximum-likelihood",
      "spatial-lag",
      "spatial-error",
      "GMM"
    ],
    "best_for": "Estimating cross-sectional spatial regression models (SAR, SEM, SAC, SDM) with maximum likelihood or GMM, implementing Bivand & Piras (2015)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "spatial-econometrics"
    ],
    "summary": "The spatialreg package provides tools for estimating spatial regression models, including maximum likelihood and GMM estimation methods. It is used by researchers and practitioners in spatial econometrics to analyze spatial data and model spatial relationships.",
    "use_cases": [
      "Estimating spatial lag models for regional economic analysis",
      "Analyzing the impact of spatially correlated variables in urban studies"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for spatial regression",
      "how to estimate spatial lag model in R",
      "maximum likelihood estimation in spatial econometrics",
      "GMM for spatial error models",
      "spatial Durbin model in R",
      "spatial econometrics tools",
      "impact calculations in spatial models"
    ],
    "primary_use_cases": [
      "spatial lag model estimation",
      "spatial error model analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "spdep"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "spdep",
    "description": "The foundational R package for spatial weights matrix creation and spatial autocorrelation testing. Provides functions for creating spatial weights from polygon contiguities and point patterns, computing global statistics (Moran's I, Geary's C), local indicators (LISA), and Lagrange multiplier tests.",
    "category": "Spatial Econometrics",
    "docs_url": "https://r-spatial.github.io/spdep/",
    "github_url": "https://github.com/r-spatial/spdep",
    "url": "https://cran.r-project.org/package=spdep",
    "install": "install.packages(\"spdep\")",
    "tags": [
      "spatial-weights",
      "autocorrelation",
      "morans-i",
      "neighborhood-analysis",
      "spatial-statistics"
    ],
    "best_for": "Creating spatial weights matrices and testing for spatial autocorrelation in cross-sectional data, implementing Bivand & Wong (2018)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "spatial-econometrics",
      "spatial-statistics"
    ],
    "summary": "The spdep package is designed for creating spatial weights matrices and conducting spatial autocorrelation tests in R. It is commonly used by researchers and analysts in spatial econometrics to analyze spatial data patterns.",
    "use_cases": [
      "Analyzing spatial patterns in real estate data",
      "Testing for spatial autocorrelation in environmental studies"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "R package for spatial weights",
      "how to test spatial autocorrelation in R",
      "spatial analysis tools in R",
      "create spatial weights matrix R",
      "Moran's I calculation in R",
      "LISA analysis in R"
    ],
    "primary_use_cases": [
      "spatial weights matrix creation",
      "spatial autocorrelation testing"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "splm",
    "description": "Maximum likelihood and GMM estimation for spatial panel data models. Implements fixed and random effects specifications with spatial lag and/or spatial error components, including the Kapoor-Kelejian-Prucha (2007) GM estimator. Provides diagnostic tests for spatial autocorrelation in panel settings.",
    "category": "Spatial Econometrics",
    "docs_url": "https://cran.r-project.org/web/packages/splm/splm.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=splm",
    "install": "install.packages(\"splm\")",
    "tags": [
      "spatial-panel",
      "panel-data",
      "fixed-effects",
      "random-effects",
      "GMM"
    ],
    "best_for": "Estimating spatial econometric models with panel (longitudinal) data structures, implementing Millo & Piras (2012)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "spatial-econometrics",
      "panel-data"
    ],
    "summary": "The 'splm' package provides maximum likelihood and GMM estimation for spatial panel data models, allowing users to implement fixed and random effects specifications. It is primarily used by researchers and practitioners in spatial econometrics for analyzing spatially correlated data.",
    "use_cases": [
      "Estimating the impact of spatially correlated variables in economic models",
      "Conducting diagnostic tests for spatial autocorrelation in panel datasets"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for spatial panel data",
      "how to estimate spatial models in R",
      "GMM estimation for panel data in R",
      "spatial econometrics tools in R",
      "fixed effects models in R",
      "random effects models for spatial data"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Kapoor-Kelejian-Prucha (2007)",
    "maintenance_status": "active"
  },
  {
    "name": "Awesome Quant",
    "description": "Curated list of quantitative finance libraries and resources (many statistical/TS tools overlap with econometrics).",
    "category": "Standard Errors, Bootstrapping & Reporting",
    "docs_url": "https://wilsonfreitas.github.io/awesome-quant/",
    "github_url": null,
    "url": "https://wilsonfreitas.github.io/awesome-quant/",
    "install": "",
    "tags": [
      "bootstrap",
      "standard errors"
    ],
    "best_for": "Robust inference, clustered SEs, result presentation",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "quantitative-finance",
      "econometrics"
    ],
    "summary": "Awesome Quant is a curated list of quantitative finance libraries and resources that includes various statistical and time series tools, many of which overlap with econometrics. It is useful for individuals looking to explore quantitative finance methodologies.",
    "use_cases": [],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for quantitative finance",
      "how to find statistical tools in python",
      "resources for econometrics in python",
      "best python libraries for time series analysis",
      "bootstrap techniques in python",
      "standard errors calculation in python"
    ]
  },
  {
    "name": "Beyond Jupyter (TransferLab)",
    "description": "Teaches software design principles for ML\u2014modularity, abstraction, and reproducibility\u2014going beyond ad hoc Jupyter workflows. Focus on maintainable, production-quality ML code.",
    "category": "Standard Errors, Bootstrapping & Reporting",
    "docs_url": "https://transferlab.ai/trainings/beyond-jupyter/",
    "github_url": null,
    "url": "https://transferlab.ai/trainings/beyond-jupyter/",
    "install": "",
    "tags": [
      "bootstrap",
      "standard errors"
    ],
    "best_for": "Robust inference, clustered SEs, result presentation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python"
    ],
    "topic_tags": [],
    "summary": "Beyond Jupyter (TransferLab) teaches software design principles for machine learning, focusing on modularity, abstraction, and reproducibility. It is aimed at developers looking to create maintainable, production-quality ML code beyond ad hoc Jupyter workflows.",
    "use_cases": [
      "Developing production-quality ML applications",
      "Improving Jupyter workflows for better maintainability"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for software design in ML",
      "how to create maintainable ML code in python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "Causal Inference for the Brave and True",
    "description": "Modern introduction to causal inference methods (DiD, IV, RDD, Synth, ML-based) with Python code examples.",
    "category": "Standard Errors, Bootstrapping & Reporting",
    "docs_url": "https://matheusfacure.github.io/python-causality-handbook/",
    "github_url": null,
    "url": "https://matheusfacure.github.io/python-causality-handbook/",
    "install": "",
    "tags": [
      "bootstrap",
      "standard errors"
    ],
    "best_for": "Robust inference, clustered SEs, result presentation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "Causal Inference for the Brave and True is a modern introduction to various causal inference methods, including DiD, IV, RDD, and ML-based approaches, with practical Python code examples. It is aimed at data scientists and researchers looking to apply causal inference techniques in their work.",
    "use_cases": [
      "Analyzing the impact of a policy change",
      "Evaluating treatment effects in clinical trials"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to do causal analysis in python",
      "python examples for DiD",
      "using IV methods in python",
      "RDD implementation in python",
      "machine learning for causal inference python"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "Coding for Economists",
    "description": "Practical guide by A. Turrell on using Python for modern econometric research, data analysis, and workflows.",
    "category": "Standard Errors, Bootstrapping & Reporting",
    "docs_url": "https://aeturrell.github.io/coding-for-economists/",
    "github_url": null,
    "url": "https://aeturrell.github.io/coding-for-economists/",
    "install": "",
    "tags": [
      "bootstrap",
      "standard errors"
    ],
    "best_for": "Robust inference, clustered SEs, result presentation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "standard errors",
      "bootstrapping"
    ],
    "summary": "Coding for Economists is a practical guide that helps economists use Python for econometric research and data analysis. It is designed for those looking to enhance their workflows with modern programming techniques.",
    "use_cases": [
      "analyzing economic data",
      "performing bootstrapping techniques"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for econometric research",
      "how to do data analysis in python",
      "bootstrapping in python",
      "standard errors calculation in python"
    ],
    "primary_use_cases": [
      "bootstrapping",
      "standard error estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "Deep Learning Specialization (Coursera)",
    "description": "Intermediate 5-course series by Andrew Ng covering deep neural networks, CNNs, RNNs, transformers, and real-world DL applications using TensorFlow.",
    "category": "Standard Errors, Bootstrapping & Reporting",
    "docs_url": "https://www.coursera.org/specializations/deep-learning",
    "github_url": null,
    "url": "https://www.coursera.org/specializations/deep-learning",
    "install": "",
    "tags": [
      "bootstrap",
      "standard errors",
      "machine learning"
    ],
    "best_for": "Robust inference, clustered SEs, result presentation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "machine learning"
    ],
    "summary": "The Deep Learning Specialization is an intermediate series of five courses by Andrew Ng that covers deep neural networks, CNNs, RNNs, transformers, and real-world deep learning applications using TensorFlow. It is designed for individuals looking to enhance their understanding of deep learning techniques and applications.",
    "use_cases": [
      "Building deep learning models for image recognition",
      "Implementing RNNs for time series forecasting"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for deep learning",
      "how to implement CNN in python",
      "deep learning courses on Coursera",
      "real-world applications of deep learning",
      "transformers in deep learning",
      "using TensorFlow for RNNs",
      "standard errors in machine learning"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "TensorFlow"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "Machine Learning Specialization (Coursera)",
    "description": "Beginner-friendly 3-course series by Andrew Ng covering core ML methods (regression, classification, clustering, trees, NN) with hands-on projects.",
    "category": "Standard Errors, Bootstrapping & Reporting",
    "docs_url": "https://www.coursera.org/specializations/machine-learning-introduction/",
    "github_url": null,
    "url": "https://www.coursera.org/specializations/machine-learning-introduction/",
    "install": "",
    "tags": [
      "bootstrap",
      "standard errors"
    ],
    "best_for": "Robust inference, clustered SEs, result presentation",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Machine Learning Specialization is a beginner-friendly series of three courses by Andrew Ng that covers core machine learning methods such as regression, classification, clustering, decision trees, and neural networks. It is designed for individuals looking to gain practical experience through hands-on projects.",
    "use_cases": [],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for machine learning",
      "how to do regression in python",
      "machine learning projects for beginners",
      "introduction to classification in python",
      "what is clustering in machine learning",
      "neural networks tutorial python"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active"
  },
  {
    "name": "Python for Econometrics",
    "description": "Comprehensive intro notes by Kevin Sheppard covering Python basics, core libraries, and econometrics applications.",
    "category": "Standard Errors, Bootstrapping & Reporting",
    "docs_url": "https://www.kevinsheppard.com/files/teaching/python/notes/python_introduction_2023.pdf",
    "github_url": null,
    "url": "https://www.kevinsheppard.com/files/teaching/python/notes/python_introduction_2023.pdf",
    "install": "",
    "tags": [
      "bootstrap",
      "standard errors"
    ],
    "best_for": "Robust inference, clustered SEs, result presentation",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Python for Econometrics provides comprehensive introductory notes covering Python basics, core libraries, and applications in econometrics. It is useful for individuals looking to learn econometric methods using Python.",
    "use_cases": [],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for econometrics",
      "how to use python for standard errors",
      "bootstrapping in python",
      "python econometrics tutorial",
      "learn python for data analysis",
      "python econometrics applications"
    ],
    "api_complexity": "simple"
  },
  {
    "name": "QuantEcon Lectures",
    "description": "High-quality lecture series on quantitative economic modeling, computational tools, and economics using Python/Julia.",
    "category": "Standard Errors, Bootstrapping & Reporting",
    "docs_url": "https://quantecon.org/lectures/",
    "github_url": null,
    "url": "https://quantecon.org/lectures/",
    "install": "",
    "tags": [
      "bootstrap",
      "standard errors"
    ],
    "best_for": "Robust inference, clustered SEs, result presentation",
    "language": "Python",
    "difficulty": "beginner|intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "QuantEcon Lectures offers a high-quality lecture series focused on quantitative economic modeling and computational tools using Python and Julia. It is designed for those interested in economics and computational methods.",
    "use_cases": [],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for economic modeling",
      "how to use Python for bootstrapping",
      "quantitative economics lectures",
      "learning computational economics with Python",
      "standard errors in Python",
      "bootstrapping techniques in economics"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active"
  },
  {
    "name": "SciPy Bootstrap",
    "description": "Foundational module within SciPy for a wide range of statistical functions, distributions, and hypothesis tests (t-tests, ANOVA, chi\u00b2, KS, etc.).",
    "category": "Standard Errors, Bootstrapping & Reporting",
    "docs_url": "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html",
    "github_url": "https://github.com/scipy/scipy",
    "url": "https://github.com/scipy/scipy",
    "install": "pip install scipy",
    "tags": [
      "bootstrap",
      "standard errors",
      "inference",
      "hypothesis testing"
    ],
    "best_for": "Hypothesis tests, confidence intervals, multiple testing",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Scipy.stats is a foundational module within SciPy that provides a wide range of statistical functions, distributions, and hypothesis tests such as t-tests, ANOVA, and chi-squared tests. It is commonly used by data scientists and statisticians for statistical analysis and inference.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for bootstrap confidence intervals",
      "how to compute bootstrap statistics in python",
      "bootstrap methods in scipy",
      "scipy.stats.bootstrap example",
      "confidence intervals using bootstrap in python",
      "scipy bootstrap tutorial",
      "python library for statistical functions",
      "how to perform hypothesis testing in python",
      "python library for t-tests",
      "using ANOVA in python",
      "statistical distributions in python",
      "python scipy stats examples"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active",
    "primary_use_cases": [
      "hypothesis testing",
      "statistical analysis"
    ],
    "related_packages": [
      "scipy",
      "numpy"
    ]
  },
  {
    "name": "Stargazer",
    "description": "Python port of R's stargazer for creating publication-quality regression tables (HTML, LaTeX) from `statsmodels` & `linearmodels` results.",
    "category": "Standard Errors, Bootstrapping & Reporting",
    "docs_url": null,
    "github_url": "https://github.com/StatsReporting/stargazer",
    "url": "https://github.com/StatsReporting/stargazer",
    "install": "pip install stargazer",
    "tags": [
      "bootstrap",
      "standard errors"
    ],
    "best_for": "Robust inference, clustered SEs, result presentation",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Stargazer is a Python package that creates publication-quality regression tables in HTML and LaTeX formats from results obtained using statsmodels and linearmodels. It is primarily used by researchers and data scientists who need to present regression results in a professional format.",
    "use_cases": [
      "Generating regression tables for academic papers",
      "Creating reports with statistical results"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for regression tables",
      "how to create publication-quality tables in python",
      "stargazer python package",
      "generate LaTeX tables from statsmodels",
      "create HTML regression tables python",
      "reporting regression results in python"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active"
  },
  {
    "name": "The Missing Semester of Your CS Education (MIT)",
    "description": "Teaches essential developer tools often skipped in formal education\u2014command line, Git, Vim, scripting, debugging, etc.",
    "category": "Standard Errors, Bootstrapping & Reporting",
    "docs_url": "https://missing.csail.mit.edu/",
    "github_url": null,
    "url": "https://missing.csail.mit.edu/",
    "install": "",
    "tags": [
      "bootstrap",
      "standard errors"
    ],
    "best_for": "Robust inference, clustered SEs, result presentation",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Missing Semester of Your CS Education teaches essential developer tools that are often overlooked in formal education, including command line, Git, Vim, scripting, and debugging. It is aimed at individuals looking to enhance their software development skills.",
    "use_cases": [],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for command line tools",
      "how to use Git in Python",
      "Vim tutorials for developers",
      "scripting in Python",
      "debugging techniques in Python",
      "essential developer tools for CS education"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active"
  },
  {
    "name": "wildboottest",
    "description": "Fast implementation of various wild cluster bootstrap algorithms (WCR, WCU) for robust inference, especially with few clusters.",
    "category": "Standard Errors, Bootstrapping & Reporting",
    "docs_url": "https://py-econometrics.github.io/wildboottest/",
    "github_url": "https://github.com/py-econometrics/wildboottest",
    "url": "https://github.com/py-econometrics/wildboottest",
    "install": "pip install wildboottest",
    "tags": [
      "bootstrap",
      "standard errors"
    ],
    "best_for": "Robust inference, clustered SEs, result presentation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "bootstrapping",
      "standard errors"
    ],
    "summary": "The wildboottest package provides a fast implementation of various wild cluster bootstrap algorithms for robust inference, particularly useful in scenarios with few clusters. It is designed for statisticians and data scientists who need reliable statistical methods.",
    "use_cases": [
      "Performing robust statistical inference with limited clusters",
      "Conducting bootstrapping for standard error estimation"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for wild cluster bootstrap",
      "how to perform bootstrapping in python",
      "wildboottest documentation",
      "robust inference with wildboottest",
      "bootstrap algorithms in python",
      "standard errors with wildboottest"
    ],
    "primary_use_cases": [
      "wild cluster bootstrap inference"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "FilterPy",
    "description": "Focuses on Kalman filters (standard, EKF, UKF) and smoothers with a clear, pedagogical implementation style.",
    "category": "State Space & Volatility Models",
    "docs_url": "https://filterpy.readthedocs.io/en/latest/",
    "github_url": "https://github.com/rlabbe/filterpy",
    "url": "https://github.com/rlabbe/filterpy",
    "install": "pip install filterpy",
    "tags": [
      "volatility",
      "state space"
    ],
    "best_for": "GARCH, stochastic volatility, Kalman filtering",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "state space",
      "time-series"
    ],
    "summary": "FilterPy focuses on Kalman filters, including standard, EKF, and UKF implementations, along with smoothers. It is designed for those looking to understand and apply these concepts in a clear and pedagogical manner.",
    "use_cases": [
      "Tracking objects in motion",
      "Estimating the state of a system over time"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for Kalman filters",
      "how to implement EKF in python",
      "python library for state space models",
      "using FilterPy for smoothing",
      "FilterPy documentation",
      "FilterPy examples",
      "FilterPy installation guide"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "Metran",
    "description": "Specialized package for estimating Dynamic Factor Models (DFM) using state-space methods and Kalman filtering.",
    "category": "State Space & Volatility Models",
    "docs_url": null,
    "github_url": "https://github.com/pastas/metran",
    "url": "https://github.com/pastas/metran",
    "install": "pip install metran",
    "tags": [
      "volatility",
      "state space"
    ],
    "best_for": "GARCH, stochastic volatility, Kalman filtering",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "time-series",
      "state-space",
      "volatility"
    ],
    "summary": "Metran is a specialized package for estimating Dynamic Factor Models (DFM) using state-space methods and Kalman filtering. It is primarily used by data scientists and researchers working with time-series data to analyze volatility and state-space models.",
    "use_cases": [
      "Estimating dynamic factors in economic data",
      "Analyzing volatility in financial time-series"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for estimating Dynamic Factor Models",
      "how to use Kalman filtering in python",
      "state-space models in python",
      "volatility modeling with python",
      "Dynamic Factor Models python package",
      "python library for time-series analysis"
    ],
    "primary_use_cases": [
      "dynamic factor estimation",
      "volatility analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "PyKalman",
    "description": "Implements Kalman filter, smoother, and EM algorithm for parameter estimation, including support for missing values and UKF.",
    "category": "State Space & Volatility Models",
    "docs_url": "https://pypi.org/project/pykalman/",
    "github_url": "https://github.com/pykalman/pykalman",
    "url": "https://github.com/pykalman/pykalman",
    "install": "pip install pykalman",
    "tags": [
      "volatility",
      "state space"
    ],
    "best_for": "GARCH, stochastic volatility, Kalman filtering",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "time-series"
    ],
    "summary": "PyKalman implements Kalman filters, smoothers, and the EM algorithm for parameter estimation. It is useful for those working with state space models and is applicable in various fields such as finance and engineering.",
    "use_cases": [
      "Estimating parameters in financial models",
      "Smoothing time series data with missing values"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for Kalman filter",
      "how to estimate parameters with Kalman filter in python",
      "Kalman smoother python",
      "EM algorithm for state space models python",
      "handling missing values in time series python",
      "UKF implementation in python"
    ],
    "primary_use_cases": [
      "parameter estimation",
      "time series forecasting"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "pydlm",
      "filterpy"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "PyMC Statespace",
    "description": "(See Bayesian) Bayesian state-space modeling using PyMC, integrating Kalman filtering within MCMC for parameter estimation.",
    "category": "State Space & Volatility Models",
    "docs_url": "https://www.pymc.io/projects/examples/en/latest/blog/tag/time-series.html",
    "github_url": "https://github.com/jessegrabowski/pymc_statespace",
    "url": "https://github.com/jessegrabowski/pymc_statespace",
    "install": "pip install pymc-statespace",
    "tags": [
      "volatility",
      "state space",
      "Bayesian"
    ],
    "best_for": "GARCH, stochastic volatility, Kalman filtering",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "bayesian",
      "state space",
      "time-series"
    ],
    "summary": "PyMC Statespace is a Python package for Bayesian state-space modeling that integrates Kalman filtering within MCMC for parameter estimation. It is useful for researchers and practitioners working with time-series data and Bayesian methods.",
    "use_cases": [
      "Modeling financial time series data",
      "Estimating parameters in dynamic systems"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for Bayesian state-space modeling",
      "how to implement Kalman filtering in Python",
      "MCMC parameter estimation in Python",
      "Bayesian time-series analysis with PyMC",
      "state space modeling in Python",
      "volatility modeling using PyMC"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "stochvol",
    "description": "Efficient Bayesian estimation of stochastic volatility (SV) models using MCMC.",
    "category": "State Space & Volatility Models",
    "docs_url": "https://stochvol.readthedocs.io/en/latest/",
    "github_url": "https://github.com/gregorkastner/stochvol",
    "url": "https://github.com/gregorkastner/stochvol",
    "install": "pip install stochvol",
    "tags": [
      "volatility",
      "state space",
      "Bayesian"
    ],
    "best_for": "GARCH, stochastic volatility, Kalman filtering",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "bayesian",
      "volatility",
      "state space"
    ],
    "summary": "The stochvol package provides efficient Bayesian estimation of stochastic volatility models using Markov Chain Monte Carlo (MCMC) methods. It is primarily used by statisticians and data scientists working with time series data that exhibit volatility.",
    "use_cases": [
      "Estimating volatility in financial time series",
      "Modeling asset returns with stochastic volatility"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for stochastic volatility",
      "how to estimate volatility in python",
      "bayesian estimation of SV models",
      "MCMC for volatility models in python",
      "state space models in python",
      "efficient Bayesian methods for time series"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "HypoRS",
    "description": "Hypothesis testing library for Rust with T-tests, Z-tests, ANOVA, Chi-square, designed to work seamlessly with Polars DataFrames.",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": "https://lib.rs/crates/hypors",
    "github_url": "https://github.com/astronights/hypors",
    "url": "https://crates.io/crates/hypors",
    "install": "cargo add hypors",
    "tags": [
      "rust",
      "hypothesis testing",
      "t-test",
      "ANOVA",
      "polars"
    ],
    "best_for": "Statistical hypothesis testing with Polars integration",
    "language": "Rust",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "HypoRS is a hypothesis testing library for Rust that provides T-tests, Z-tests, ANOVA, and Chi-square functionalities. It is designed to work seamlessly with Polars DataFrames, making it suitable for data analysis tasks.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "rust library for hypothesis testing",
      "how to perform T-tests in Rust",
      "ANOVA in Rust with Polars",
      "Chi-square test in Rust",
      "Z-tests using HypoRS",
      "statistical analysis in Rust"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "Polars"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "Pingouin",
    "description": "User-friendly interface for common statistical tests (ANOVA, ANCOVA, t-tests, correlations, chi\u00b2, reliability) built on pandas & scipy.",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": "https://pingouin-stats.org/",
    "github_url": "https://github.com/raphaelvallat/pingouin",
    "url": "https://github.com/raphaelvallat/pingouin",
    "install": "pip install pingouin",
    "tags": [
      "inference",
      "hypothesis testing"
    ],
    "best_for": "Hypothesis tests, confidence intervals, multiple testing",
    "language": "Python",
    "difficulty": "beginner|intermediate",
    "prerequisites": [
      "python-pandas",
      "scipy"
    ],
    "topic_tags": [],
    "summary": "Pingouin provides a user-friendly interface for conducting common statistical tests such as ANOVA, t-tests, and correlations. It is designed for users who need to perform statistical analysis without deep expertise in the underlying libraries.",
    "use_cases": [
      "Conducting ANOVA tests",
      "Performing t-tests for comparing means"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for statistical tests",
      "how to perform ANOVA in python",
      "user-friendly statistical analysis python",
      "Pingouin library usage",
      "statistical inference in python",
      "python hypothesis testing library"
    ],
    "primary_use_cases": [
      "ANOVA analysis",
      "t-test analysis"
    ],
    "api_complexity": "simple",
    "related_packages": [
      "scipy",
      "statsmodels"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "PyWhy-Stats",
    "description": "Part of the PyWhy ecosystem providing statistical methods specifically for causal applications, including various independence tests and power-divergence methods.",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": "https://pywhy-stats.readthedocs.io/",
    "github_url": "https://github.com/py-why/pywhy-stats",
    "url": "https://github.com/py-why/pywhy-stats",
    "install": "pip install pywhy-stats",
    "tags": [
      "inference",
      "hypothesis testing"
    ],
    "best_for": "Hypothesis tests, confidence intervals, multiple testing",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "statistical-methods"
    ],
    "summary": "PyWhy-Stats is part of the PyWhy ecosystem that provides statistical methods tailored for causal applications. It is used by data scientists and researchers focusing on causal inference and hypothesis testing.",
    "use_cases": [
      "conducting independence tests",
      "performing power-divergence analysis"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to perform hypothesis testing in python",
      "statistical methods for causal applications",
      "independence tests in python",
      "power-divergence methods in python",
      "PyWhy-Stats documentation"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "Statrs",
    "description": "Comprehensive statistical distributions for Rust (Normal, T, Gamma, etc.) with PDF, CDF, quantile functions\u2014the scipy.stats equivalent.",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": "https://docs.rs/statrs",
    "github_url": "https://github.com/statrs-dev/statrs",
    "url": "https://crates.io/crates/statrs",
    "install": "cargo add statrs",
    "tags": [
      "rust",
      "statistics",
      "distributions",
      "probability"
    ],
    "best_for": "Probability distributions and basic statistics in Rust",
    "language": "Rust",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "statistical-inference",
      "hypothesis-testing"
    ],
    "summary": "Statrs is a Rust library that provides comprehensive statistical distributions, including Normal, T, and Gamma distributions. It is useful for statisticians and data scientists who require statistical functions similar to those in scipy.stats.",
    "use_cases": [
      "Performing statistical analysis in Rust applications",
      "Developing data-driven applications that require statistical functions"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "Rust library for statistical distributions",
      "how to calculate PDF in Rust",
      "Rust statistics library",
      "distributions in Rust",
      "Rust CDF functions",
      "statistical analysis in Rust"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "expectation",
    "description": "E-values and game-theoretic probability for sequential testing. Enables early signal detection with proper error control.",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": null,
    "github_url": "https://github.com/jakorostami/expectation",
    "url": "https://pypi.org/project/expectation/",
    "install": "pip install expectation",
    "tags": [
      "sequential testing",
      "e-values",
      "hypothesis testing"
    ],
    "best_for": "E-value based sequential hypothesis testing",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python"
    ],
    "topic_tags": [
      "statistical-inference",
      "hypothesis-testing"
    ],
    "summary": "The expectation package provides tools for E-values and game-theoretic probability, facilitating early signal detection while maintaining proper error control. It is primarily used by statisticians and data scientists involved in sequential testing.",
    "use_cases": [
      "Detecting signals in clinical trials",
      "Evaluating A/B tests with proper error control"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for e-values",
      "how to perform sequential testing in python",
      "game-theoretic probability in python",
      "hypothesis testing with python",
      "early signal detection python",
      "statistical inference library python"
    ],
    "primary_use_cases": [
      "sequential testing",
      "hypothesis testing"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "gcimpute",
    "description": "Gaussian copula imputation for mixed variable types with streaming capability (Journal of Statistical Software 2024).",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": null,
    "github_url": "https://github.com/udellgroup/gcimpute",
    "url": "https://github.com/udellgroup/gcimpute",
    "install": "pip install gcimpute",
    "tags": [
      "missing data",
      "imputation"
    ],
    "best_for": "Mixed-type missing data imputation with copulas",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "gcimpute is a Python package designed for Gaussian copula imputation, specifically tailored for mixed variable types. It is useful for statisticians and data scientists dealing with missing data in their datasets.",
    "use_cases": [
      "Imputing missing values in survey data",
      "Handling incomplete datasets in machine learning"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for Gaussian copula imputation",
      "how to handle missing data in Python",
      "imputation techniques in Python",
      "best practices for mixed variable imputation",
      "streaming data imputation in Python",
      "Gaussian copula methods for data analysis"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Journal of Statistical Software (2024)",
    "maintenance_status": "active"
  },
  {
    "name": "hypothetical",
    "description": "Library focused on hypothesis testing: ANOVA/MANOVA, t-tests, chi-square, Fisher's exact, nonparametric tests (Mann-Whitney, Kruskal-Wallis, etc.).",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": null,
    "github_url": "https://github.com/aschleg/hypothetical",
    "url": "https://github.com/aschleg/hypothetical",
    "install": "pip install hypothetical",
    "tags": [
      "inference",
      "hypothesis testing"
    ],
    "best_for": "Hypothesis tests, confidence intervals, multiple testing",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "statistical-inference",
      "hypothesis-testing"
    ],
    "summary": "The hypothetical package is a library designed for performing various hypothesis testing methods including ANOVA, t-tests, and chi-square tests. It is useful for statisticians and data scientists who need to conduct rigorous statistical analyses.",
    "use_cases": [
      "Conducting A/B tests",
      "Analyzing experimental data",
      "Comparing group means",
      "Testing independence in categorical data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for hypothesis testing",
      "how to perform ANOVA in python",
      "python library for statistical inference",
      "how to do t-tests in python",
      "python chi-square test example",
      "Fisher's exact test in python",
      "nonparametric tests in python"
    ],
    "primary_use_cases": [
      "A/B test analysis",
      "ANOVA testing"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "scipy",
      "statsmodels"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "lifelines",
    "description": "Comprehensive library for survival analysis: Kaplan-Meier, Nelson-Aalen, Cox regression, AFT models, handling censored data.",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": "https://lifelines.readthedocs.io/en/latest/",
    "github_url": "https://github.com/CamDavidsonPilon/lifelines",
    "url": "https://github.com/CamDavidsonPilon/lifelines",
    "install": "pip install lifelines",
    "tags": [
      "inference",
      "hypothesis testing"
    ],
    "best_for": "Hypothesis tests, confidence intervals, multiple testing",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "survival-analysis"
    ],
    "summary": "Lifelines is a comprehensive library for survival analysis that includes methods such as Kaplan-Meier, Nelson-Aalen, and Cox regression. It is used by data scientists and statisticians to analyze time-to-event data and handle censored data effectively.",
    "use_cases": [
      "Analyzing patient survival times in clinical trials",
      "Estimating time to event for customer churn"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for survival analysis",
      "how to perform Kaplan-Meier in python",
      "Cox regression in python",
      "analyze censored data in python",
      "survival analysis with lifelines",
      "python lifelines documentation"
    ],
    "primary_use_cases": [
      "Kaplan-Meier estimation",
      "Cox proportional hazards model"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "statsmodels",
      "scikit-survival"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "miceforest",
    "description": "LightGBM-accelerated multiple imputation by chained equations. Fast MICE for large datasets.",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": "https://miceforest.readthedocs.io/",
    "github_url": "https://github.com/AnotherSamWilson/miceforest",
    "url": "https://github.com/AnotherSamWilson/miceforest",
    "install": "pip install miceforest",
    "tags": [
      "missing data",
      "imputation",
      "machine learning"
    ],
    "best_for": "Fast MICE imputation with LightGBM",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "missing data",
      "machine learning"
    ],
    "summary": "miceforest is a Python package that provides a fast implementation of multiple imputation by chained equations, accelerated by LightGBM. It is designed for handling large datasets with missing values, making it useful for data scientists and statisticians.",
    "use_cases": [
      "Imputing missing values in large datasets",
      "Preparing data for machine learning models"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for multiple imputation",
      "how to handle missing data in python",
      "fast MICE implementation python",
      "LightGBM for imputation",
      "imputation techniques in machine learning",
      "best practices for missing data in datasets"
    ],
    "primary_use_cases": [
      "multiple imputation",
      "data preprocessing for machine learning"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "mice",
      "missForest"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "savvi",
    "description": "Safe Anytime Valid Inference using e-processes and confidence sequences (Ramdas et al. 2023). Valid inference at any stopping time.",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": null,
    "github_url": "https://github.com/assuncaolfi/savvi",
    "url": "https://pypi.org/project/savvi/",
    "install": "pip install savvi",
    "tags": [
      "sequential testing",
      "A/B testing",
      "anytime valid"
    ],
    "best_for": "Always-valid sequential inference for experiments",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "statistical-inference",
      "hypothesis-testing"
    ],
    "summary": "Savvi is a Python package designed for safe anytime valid inference using e-processes and confidence sequences. It is particularly useful for researchers and data scientists conducting sequential testing and A/B testing.",
    "use_cases": [
      "Conducting A/B tests with valid inference",
      "Implementing sequential testing frameworks"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for valid inference",
      "how to perform A/B testing in python",
      "sequential testing library python",
      "safe inference python package",
      "confidence sequences in python",
      "statistical hypothesis testing python"
    ],
    "primary_use_cases": [
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Ramdas et al. (2023)",
    "maintenance_status": "active"
  },
  {
    "name": "Dolo",
    "description": "Framework for describing and solving economic models (DSGE, OLG, etc.) using a declarative YAML-based format.",
    "category": "Structural Econometrics & Estimation",
    "docs_url": "https://dolo.readthedocs.io/en/latest/",
    "github_url": "https://github.com/EconForge/dolo",
    "url": "https://github.com/EconForge/dolo",
    "install": "pip install dolo",
    "tags": [
      "structural",
      "estimation"
    ],
    "best_for": "Structural models, GMM estimation, BLP-style demand",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "structural-econometrics",
      "estimation"
    ],
    "summary": "Dolo is a framework designed for describing and solving economic models such as DSGE and OLG using a declarative YAML-based format. It is useful for economists and data scientists working with structural econometrics.",
    "use_cases": [
      "Modeling dynamic stochastic general equilibrium (DSGE) models",
      "Estimating overlapping generations (OLG) models"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for economic modeling",
      "how to solve DSGE models in python",
      "declarative framework for econometrics",
      "YAML-based economic model solver",
      "structural estimation in python",
      "OLG model implementation in python"
    ],
    "primary_use_cases": [
      "solving economic models",
      "structural estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "Greeners",
    "description": "Comprehensive Rust econometrics library with OLS, IV, panel data estimators, fixed effects, DiD, and heteroskedasticity-robust standard errors (HC0-HC3).",
    "category": "Structural Econometrics & Estimation",
    "docs_url": "https://docs.rs/greeners",
    "github_url": "https://github.com/sheep-farm/Greeners",
    "url": "https://crates.io/crates/greeners",
    "install": "cargo add greeners",
    "tags": [
      "rust",
      "econometrics",
      "IV",
      "panel data",
      "robust SE"
    ],
    "best_for": "Academic econometrics in Rust: IV, DiD, robust SEs",
    "language": "Rust",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "econometrics",
      "panel-data",
      "robust-statistics"
    ],
    "summary": "Greeners is a comprehensive Rust library for econometrics that provides various estimators including OLS, IV, and panel data methods. It is designed for researchers and practitioners in the field of econometrics who require robust statistical tools.",
    "use_cases": [
      "Estimating treatment effects using IV",
      "Analyzing panel data with fixed effects"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "Rust library for econometrics",
      "how to perform OLS in Rust",
      "IV estimation in Rust",
      "panel data analysis with Rust",
      "robust standard errors in Rust",
      "econometrics tools in Rust"
    ],
    "primary_use_cases": [
      "OLS estimation",
      "IV estimation",
      "panel data analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "HARK",
    "description": "Toolkit for solving, simulating, and estimating models with heterogeneous agents (e.g., consumption-saving).",
    "category": "Structural Econometrics & Estimation",
    "docs_url": "https://hark.readthedocs.io/en/latest/",
    "github_url": "https://github.com/econ-ark/HARK",
    "url": "https://github.com/econ-ark/HARK",
    "install": "pip install econ-ark",
    "tags": [
      "structural",
      "estimation"
    ],
    "best_for": "Structural models, GMM estimation, BLP-style demand",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "structural",
      "estimation"
    ],
    "summary": "HARK is a toolkit designed for solving, simulating, and estimating models with heterogeneous agents, particularly in the context of consumption-saving behavior. It is used by researchers and practitioners in structural econometrics to analyze complex economic models.",
    "use_cases": [
      "Simulating consumption-saving models",
      "Estimating parameters in heterogeneous agent models"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for heterogeneous agents",
      "how to simulate consumption-saving models in python",
      "HARK toolkit for structural econometrics",
      "estimating models with HARK",
      "HARK package documentation",
      "using HARK for economic modeling"
    ],
    "primary_use_cases": [
      "modeling consumption-saving behavior",
      "estimating heterogeneous agent models"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "QuantEcon.py",
    "description": "Core library for quantitative economics: dynamic programming, Markov chains, game theory, numerical methods.",
    "category": "Structural Econometrics & Estimation",
    "docs_url": "https://quantecon.org/python-lectures/",
    "github_url": "https://github.com/QuantEcon/QuantEcon.py",
    "url": "https://github.com/QuantEcon/QuantEcon.py",
    "install": "pip install quantecon",
    "tags": [
      "structural",
      "estimation"
    ],
    "best_for": "Structural models, GMM estimation, BLP-style demand",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "QuantEcon.py is a core library designed for quantitative economics, providing tools for dynamic programming, Markov chains, game theory, and numerical methods. It is primarily used by economists and data scientists working in the field of structural econometrics.",
    "use_cases": [
      "Modeling economic scenarios using dynamic programming",
      "Analyzing Markov processes in economic models"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for quantitative economics",
      "how to implement dynamic programming in python",
      "Markov chains in python",
      "game theory library python",
      "numerical methods for economics python",
      "structural estimation tools in python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "dcegm",
    "description": "JAX-compatible DC-EGM algorithm for discrete-continuous dynamic programming (Iskhakov et al. 2017).",
    "category": "Structural Econometrics & Estimation",
    "docs_url": null,
    "github_url": "https://github.com/OpenSourceEconomics/dcegm",
    "url": "https://github.com/OpenSourceEconomics/dcegm",
    "install": "pip install dcegm",
    "tags": [
      "structural",
      "dynamic programming",
      "JAX"
    ],
    "best_for": "Discrete-continuous choice models with EGM",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The dcegm package implements a JAX-compatible DC-EGM algorithm for solving discrete-continuous dynamic programming problems. It is useful for researchers and practitioners in structural econometrics and estimation.",
    "use_cases": [],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for dynamic programming",
      "how to implement DC-EGM in python",
      "JAX compatible algorithms for dynamic programming",
      "structural econometrics tools in python"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "JAX"
    ],
    "implements_paper": "Iskhakov et al. (2017)",
    "maintenance_status": "active"
  },
  {
    "name": "econpizza",
    "description": "Solve nonlinear heterogeneous agent models (HANK) with perfect foresight. Efficient perturbation and projection methods.",
    "category": "Structural Econometrics & Estimation",
    "docs_url": "https://econpizza.readthedocs.io/",
    "github_url": "https://github.com/gboehl/econpizza",
    "url": "https://github.com/gboehl/econpizza",
    "install": "pip install econpizza",
    "tags": [
      "structural",
      "DSGE",
      "HANK"
    ],
    "best_for": "Nonlinear HANK models with aggregate shocks",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "structural",
      "DSGE",
      "HANK"
    ],
    "summary": "Econpizza is a Python package designed to solve nonlinear heterogeneous agent models (HANK) using efficient perturbation and projection methods. It is primarily used by researchers and practitioners in structural econometrics.",
    "use_cases": [
      "Solving complex economic models",
      "Conducting simulations for policy analysis"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for solving HANK models",
      "how to use perturbation methods in python",
      "efficient projection methods in python",
      "nonlinear heterogeneous agent models python",
      "structural econometrics tools in python",
      "DSGE modeling in python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "gEconpy",
    "description": "DSGE modeling tools inspired by R's gEcon. Automatic first-order condition derivation with Dynare export.",
    "category": "Structural Econometrics & Estimation",
    "docs_url": null,
    "github_url": "https://github.com/jessegrabowski/gEconpy",
    "url": "https://github.com/jessegrabowski/gEconpy",
    "install": "pip install gEconpy",
    "tags": [
      "structural",
      "DSGE",
      "estimation"
    ],
    "best_for": "Symbolic DSGE derivation with Dynare compatibility",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "numpy"
    ],
    "topic_tags": [
      "structural-econometrics",
      "DSGE",
      "estimation"
    ],
    "summary": "gEconpy provides tools for Dynamic Stochastic General Equilibrium (DSGE) modeling, allowing users to automatically derive first-order conditions and export them to Dynare. It is designed for economists and researchers working in structural econometrics.",
    "use_cases": [
      "Modeling economic scenarios using DSGE",
      "Exporting models to Dynare for analysis"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for DSGE modeling",
      "how to derive first-order conditions in python",
      "DSGE estimation tools in python",
      "gEconpy documentation",
      "python econometrics library",
      "how to use Dynare with python"
    ],
    "primary_use_cases": [
      "DSGE modeling",
      "first-order condition derivation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "gegravity",
    "description": "General equilibrium structural gravity modeling for trade policy analysis. Only Python package for Anderson-van Wincoop GE gravity.",
    "category": "Structural Econometrics & Estimation",
    "docs_url": null,
    "github_url": "https://github.com/peter-herman/gegravity",
    "url": "https://pypi.org/project/gegravity/",
    "install": "pip install gegravity",
    "tags": [
      "trade",
      "gravity models",
      "structural"
    ],
    "best_for": "GE structural gravity for trade policy",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "structural",
      "trade"
    ],
    "summary": "Gegravity is a Python package designed for general equilibrium structural gravity modeling, specifically for trade policy analysis. It is the only package that implements the Anderson-van Wincoop GE gravity model, making it a valuable tool for researchers and practitioners in trade economics.",
    "use_cases": [
      "Analyzing the impact of trade policies on economic equilibrium",
      "Estimating trade flows between countries using structural models"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for gravity modeling",
      "how to analyze trade policy in python",
      "structural gravity models in python",
      "gegravity package usage",
      "trade analysis with python",
      "Anderson-van Wincoop model in python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "pydsge",
    "description": "DSGE model simulation, filtering, and Bayesian estimation. Handles occasionally binding constraints.",
    "category": "Structural Econometrics & Estimation",
    "docs_url": null,
    "github_url": "https://github.com/gboehl/pydsge",
    "url": "https://github.com/gboehl/pydsge",
    "install": "pip install pydsge",
    "tags": [
      "structural",
      "DSGE",
      "Bayesian"
    ],
    "best_for": "DSGE estimation with occasionally binding constraints",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "structural-econometrics",
      "DSGE",
      "bayesian"
    ],
    "summary": "pydsge is a Python package designed for simulating, filtering, and estimating DSGE models using Bayesian methods. It is particularly useful for economists and researchers working with macroeconomic models that involve occasionally binding constraints.",
    "use_cases": [
      "Simulating economic scenarios using DSGE models",
      "Estimating parameters of macroeconomic models"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for DSGE modeling",
      "how to estimate DSGE models in python",
      "Bayesian estimation for DSGE",
      "filtering DSGE models in python",
      "structural econometrics python package",
      "pydsge documentation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "pynare",
    "description": "Python wrapper/interface to Dynare for DSGE model solving. Bridge between Python workflows and Dynare.",
    "category": "Structural Econometrics & Estimation",
    "docs_url": null,
    "github_url": "https://github.com/gboehl/pynare",
    "url": "https://github.com/gboehl/pynare",
    "install": "pip install pynare",
    "tags": [
      "structural",
      "DSGE",
      "Dynare"
    ],
    "best_for": "Running Dynare models from Python",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "structural",
      "DSGE"
    ],
    "summary": "Pynare is a Python wrapper that facilitates the use of Dynare for solving Dynamic Stochastic General Equilibrium (DSGE) models. It serves as a bridge between Python workflows and the Dynare software, enabling users to leverage the capabilities of both tools.",
    "use_cases": [
      "Solving DSGE models",
      "Integrating Dynare with Python workflows"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for DSGE modeling",
      "how to use Dynare with Python",
      "pynare installation guide",
      "DSGE model solving in Python",
      "pynare examples",
      "Python wrapper for Dynare"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "respy",
    "description": "Simulation and estimation of finite-horizon dynamic discrete choice (DDC) models (e.g., labor/education choice).",
    "category": "Structural Econometrics & Estimation",
    "docs_url": "https://respy.readthedocs.io/en/latest/",
    "github_url": "https://github.com/OpenSourceEconomics/respy",
    "url": "https://github.com/OpenSourceEconomics/respy",
    "install": "pip install respy",
    "tags": [
      "structural",
      "estimation"
    ],
    "best_for": "Structural models, GMM estimation, BLP-style demand",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "structural",
      "estimation"
    ],
    "summary": "Respy is a Python package designed for the simulation and estimation of finite-horizon dynamic discrete choice models, which are commonly used in labor and education choice contexts. It is useful for researchers and practitioners in structural econometrics who need to model decision-making processes over time.",
    "use_cases": [
      "Estimating labor market decisions",
      "Modeling educational choices over time"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for dynamic discrete choice models",
      "how to estimate finite-horizon models in python",
      "simulation of labor choice models in python",
      "education choice modeling with python",
      "structural econometrics tools in python",
      "respy package usage examples",
      "dynamic choice simulation library python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "upper-envelope",
    "description": "Fast upper envelope scan for discrete-continuous dynamic programming. JAX and numba implementations.",
    "category": "Structural Econometrics & Estimation",
    "docs_url": null,
    "github_url": "https://github.com/OpenSourceEconomics/upper-envelope",
    "url": "https://github.com/OpenSourceEconomics/upper-envelope",
    "install": "pip install upper-envelope",
    "tags": [
      "structural",
      "dynamic programming",
      "optimization"
    ],
    "best_for": "Fast upper envelope computation for DC-EGM",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "optimization"
    ],
    "summary": "The upper-envelope package provides fast upper envelope scans for discrete-continuous dynamic programming problems. It is useful for researchers and practitioners in structural econometrics and estimation.",
    "use_cases": [
      "Dynamic programming for economic models",
      "Optimization in structural estimation"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for upper envelope scan",
      "how to perform dynamic programming in python",
      "JAX implementation for optimization",
      "numba for dynamic programming",
      "structural econometrics tools in python",
      "fast upper envelope algorithms in python"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "JAX",
      "numba"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "OpenMx",
    "description": "Extended SEM software with programmatic model specification via paths (RAM) or matrix algebra, supporting mixture distributions, item factor analysis, state space models, and behavior genetics twin studies.",
    "category": "Structural Equation Modeling",
    "docs_url": "https://openmx.ssri.psu.edu/",
    "github_url": "https://github.com/OpenMx/OpenMx",
    "url": "https://cran.r-project.org/package=OpenMx",
    "install": "install.packages(\"OpenMx\")",
    "tags": [
      "SEM",
      "matrix-algebra",
      "twin-studies",
      "behavior-genetics",
      "IFA"
    ],
    "best_for": "Complex/advanced SEM, behavior genetics, and researchers needing maximum specification flexibility, implementing Neale et al. (2016)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "structural-equation-modeling",
      "behavior-genetics"
    ],
    "summary": "OpenMx is an extended software package for structural equation modeling (SEM) that allows for programmatic model specification through paths or matrix algebra. It is used by researchers and practitioners in fields such as psychology, genetics, and social sciences for complex statistical modeling.",
    "use_cases": [
      "Modeling complex relationships in psychological research",
      "Analyzing twin studies for behavior genetics"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for structural equation modeling",
      "how to perform item factor analysis in R",
      "OpenMx tutorial",
      "R SEM software",
      "behavior genetics modeling in R",
      "mixture distributions in OpenMx"
    ],
    "primary_use_cases": [
      "item factor analysis",
      "state space models"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "blavaan",
    "description": "Bayesian latent variable analysis extending lavaan with MCMC estimation via Stan or JAGS, supporting Bayesian CFA, SEM, growth models, and model comparison with WAIC, LOO, and Bayes factors.",
    "category": "Structural Equation Modeling",
    "docs_url": "https://ecmerkle.github.io/blavaan/",
    "github_url": "https://github.com/ecmerkle/blavaan",
    "url": "https://cran.r-project.org/package=blavaan",
    "install": "install.packages(\"blavaan\")",
    "tags": [
      "Bayesian-SEM",
      "Stan",
      "JAGS",
      "MCMC",
      "latent-variables"
    ],
    "best_for": "Bayesian inference for SEM models using familiar lavaan syntax, implementing Merkle & Rosseel (2018)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "bayesian"
    ],
    "summary": "blavaan is a package for Bayesian latent variable analysis that extends the capabilities of lavaan by incorporating MCMC estimation through Stan or JAGS. It is utilized by researchers and practitioners interested in Bayesian approaches to confirmatory factor analysis (CFA), structural equation modeling (SEM), and growth models.",
    "use_cases": [
      "Conducting Bayesian CFA",
      "Estimating growth models using MCMC"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "Bayesian latent variable analysis in R",
      "how to perform Bayesian SEM in R",
      "blavaan package usage",
      "MCMC estimation with Stan in R",
      "latent variable modeling with JAGS",
      "model comparison in Bayesian analysis"
    ],
    "primary_use_cases": [
      "Bayesian CFA",
      "Bayesian SEM"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "lavaan"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "lavaan",
    "description": "Free, open-source latent variable analysis providing commercial-quality functionality for path analysis, confirmatory factor analysis, structural equation modeling, and growth curve models with intuitive model syntax.",
    "category": "Structural Equation Modeling",
    "docs_url": "https://lavaan.ugent.be/",
    "github_url": "https://github.com/yrosseel/lavaan",
    "url": "https://cran.r-project.org/package=lavaan",
    "install": "install.packages(\"lavaan\")",
    "tags": [
      "SEM",
      "CFA",
      "path-analysis",
      "latent-variables",
      "psychometrics"
    ],
    "best_for": "General-purpose structural equation modeling with accessible syntax for researchers, implementing Rosseel (2012)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "latent-variables",
      "psychometrics"
    ],
    "summary": "The lavaan package provides free, open-source functionality for latent variable analysis, including path analysis, confirmatory factor analysis, and structural equation modeling. It is commonly used by researchers and practitioners in psychology and social sciences for modeling complex relationships between variables.",
    "use_cases": [
      "Analyzing survey data to understand underlying factors",
      "Modeling growth trajectories in longitudinal studies"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for latent variable analysis",
      "how to perform SEM in R",
      "R confirmatory factor analysis tutorial",
      "path analysis in R",
      "latent variable modeling with lavaan",
      "R package for psychometrics"
    ],
    "primary_use_cases": [
      "confirmatory factor analysis",
      "structural equation modeling"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "sem",
      "lavaan.survey"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "SDV (Synthetic Data Vault)",
    "description": "Comprehensive library for generating synthetic tabular, relational, and time series data using various models.",
    "category": "Synthetic Data Generation",
    "docs_url": "https://sdv.dev/",
    "github_url": "https://github.com/sdv-dev/SDV",
    "url": "https://github.com/sdv-dev/SDV",
    "install": "pip install sdv",
    "tags": [
      "synthetic data",
      "simulation"
    ],
    "best_for": "Privacy-preserving data, simulation, augmentation",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "SDV is a comprehensive library for generating synthetic tabular, relational, and time series data using various models. It is used by data scientists and researchers to create realistic datasets for testing and simulation purposes.",
    "use_cases": [
      "Generating synthetic datasets for machine learning",
      "Simulating data for testing algorithms"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for synthetic data generation",
      "how to generate synthetic data in python",
      "synthetic data generation techniques",
      "best practices for synthetic data",
      "using SDV for simulation",
      "creating synthetic datasets with python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "Synthpop",
    "description": "Port of the R package for generating synthetic populations based on sample survey data.",
    "category": "Synthetic Data Generation",
    "docs_url": null,
    "github_url": "https://github.com/alan-turing-institute/synthpop",
    "url": "https://github.com/alan-turing-institute/synthpop",
    "install": "pip install synthpop",
    "tags": [
      "synthetic data",
      "simulation"
    ],
    "best_for": "Privacy-preserving data, simulation, augmentation",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "Synthpop is a Python package that generates synthetic populations based on sample survey data. It is useful for researchers and data scientists who need to create realistic datasets for analysis and simulation.",
    "use_cases": [
      "Creating synthetic datasets for testing algorithms",
      "Simulating survey data for research purposes"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for synthetic data generation",
      "how to generate synthetic populations in python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "quanteda",
    "description": "Comprehensive framework for quantitative text analysis. Provides fast text preprocessing, document-feature matrices, dictionary analysis, and integration with topic models. Standard for political science text analysis.",
    "category": "Text Analysis",
    "docs_url": "https://quanteda.io/",
    "github_url": "https://github.com/quanteda/quanteda",
    "url": "https://cran.r-project.org/package=quanteda",
    "install": "install.packages(\"quanteda\")",
    "tags": [
      "text-analysis",
      "NLP",
      "document-term-matrix",
      "text-preprocessing",
      "political-science"
    ],
    "best_for": "Comprehensive quantitative text analysis with fast preprocessing and document-feature matrices",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Quanteda is a comprehensive framework for quantitative text analysis that provides fast text preprocessing, document-feature matrices, and dictionary analysis. It is widely used in political science for text analysis.",
    "use_cases": [
      "Analyzing political speeches",
      "Conducting sentiment analysis on social media data"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for text analysis",
      "how to preprocess text in R",
      "document-feature matrices in R",
      "NLP tools for political science",
      "dictionary analysis in R",
      "quantitative text analysis framework"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "stm",
    "description": "Structural Topic Models incorporating document-level metadata as covariates affecting topic prevalence and content. Enables studying how topics vary across groups or time with uncertainty quantification.",
    "category": "Text Analysis",
    "docs_url": "https://www.structuraltopicmodel.com/",
    "github_url": "https://github.com/bstewart/stm",
    "url": "https://cran.r-project.org/package=stm",
    "install": "install.packages(\"stm\")",
    "tags": [
      "topic-models",
      "text-analysis",
      "covariates",
      "LDA",
      "document-metadata"
    ],
    "best_for": "Structural topic models with document metadata affecting topic prevalence and content",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "topic-models",
      "text-analysis",
      "covariates"
    ],
    "summary": "The stm package allows users to incorporate document-level metadata as covariates in Structural Topic Models, enabling the analysis of how topics vary across different groups or over time. It is particularly useful for researchers and data scientists interested in text analysis and topic modeling.",
    "use_cases": [
      "Analyzing how topics change over time",
      "Studying the impact of document metadata on topic prevalence"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for topic modeling",
      "how to analyze text with metadata in R",
      "R package for structural topic models",
      "how to use covariates in topic models R",
      "text analysis with R",
      "R package for LDA with metadata"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "text2vec",
    "description": "Efficient text vectorization with word embeddings (GloVe), topic models (LDA), and document similarity. Memory-efficient streaming API for large corpora with C++ backend.",
    "category": "Text Analysis",
    "docs_url": "https://text2vec.org/",
    "github_url": "https://github.com/dselivanov/text2vec",
    "url": "https://cran.r-project.org/package=text2vec",
    "install": "install.packages(\"text2vec\")",
    "tags": [
      "word-embeddings",
      "GloVe",
      "text-vectorization",
      "LDA",
      "document-similarity"
    ],
    "best_for": "Efficient word embeddings (GloVe) and text vectorization for large corpora",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "text2vec is an R package designed for efficient text vectorization using word embeddings like GloVe, topic models such as LDA, and document similarity. It is suitable for users dealing with large corpora and is particularly useful for data scientists and researchers in text analysis.",
    "use_cases": [
      "Analyzing large text corpora",
      "Building topic models for research"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for text vectorization",
      "how to use GloVe in R",
      "text similarity analysis in R",
      "topic modeling with LDA in R",
      "document similarity using R",
      "efficient text processing in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "tidytext",
    "description": "Tidy data principles for text mining. Converts text to tidy format (one-token-per-row), enabling analysis with dplyr, ggplot2, and other tidyverse tools. Accompanies the book 'Text Mining with R'.",
    "category": "Text Analysis",
    "docs_url": "https://juliasilge.github.io/tidytext/",
    "github_url": "https://github.com/juliasilge/tidytext",
    "url": "https://cran.r-project.org/package=tidytext",
    "install": "install.packages(\"tidytext\")",
    "tags": [
      "text-mining",
      "tidyverse",
      "tokenization",
      "sentiment-analysis",
      "NLP"
    ],
    "best_for": "Tidy text mining with dplyr and ggplot2 integration\u2014accompanies 'Text Mining with R'",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "text-mining",
      "NLP"
    ],
    "summary": "The tidytext package applies tidy data principles to text mining by converting text into a tidy format, which allows for easier analysis using tools from the tidyverse such as dplyr and ggplot2. It is particularly useful for data scientists and researchers working with text data.",
    "use_cases": [
      "Analyzing sentiment in customer reviews",
      "Tokenizing text for further analysis"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "R package for text mining",
      "how to tokenize text in R",
      "tidy data principles for text analysis",
      "text analysis with tidyverse in R",
      "R library for sentiment analysis",
      "how to use tidytext for NLP"
    ],
    "api_complexity": "simple",
    "framework_compatibility": [
      "tidyverse"
    ],
    "related_packages": [
      "tm",
      "textclean"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "ARCH",
    "description": "Specialized library for modeling and forecasting conditional volatility using ARCH, GARCH, EGARCH, and related models.",
    "category": "Time Series Econometrics",
    "docs_url": "https://arch.readthedocs.io/",
    "github_url": "https://github.com/bashtage/arch",
    "url": "https://github.com/bashtage/arch",
    "install": "pip install arch",
    "tags": [
      "time series",
      "econometrics"
    ],
    "best_for": "ARIMA, cointegration, VAR models",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "time-series",
      "econometrics"
    ],
    "summary": "ARCH is a specialized library for modeling and forecasting conditional volatility using various models such as ARCH, GARCH, and EGARCH. It is primarily used by data scientists and researchers in the field of econometrics to analyze time series data.",
    "use_cases": [
      "Forecasting financial market volatility",
      "Analyzing time series data for economic indicators"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for modeling conditional volatility",
      "how to forecast volatility in python",
      "GARCH model implementation in python",
      "time series econometrics library python",
      "using ARCH for financial modeling",
      "EGARCH model in python"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "statsmodels"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "KFAS",
    "description": "State space modeling framework for exponential family time series with computationally efficient Kalman filtering, smoothing, forecasting, and simulation. Supports observations from Gaussian, Poisson, binomial, negative binomial, and gamma distributions.",
    "category": "Time Series Econometrics",
    "docs_url": "https://cran.r-project.org/web/packages/KFAS/KFAS.pdf",
    "github_url": "https://github.com/helske/KFAS",
    "url": "https://cran.r-project.org/package=KFAS",
    "install": "install.packages(\"KFAS\")",
    "tags": [
      "state-space",
      "kalman-filter",
      "time-series",
      "forecasting",
      "exponential-family"
    ],
    "best_for": "Multivariate time series modeling with non-Gaussian observations (e.g., count data with Poisson), implementing Helske (2017)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "time-series",
      "state-space",
      "forecasting"
    ],
    "summary": "KFAS is a state space modeling framework designed for exponential family time series analysis. It provides computationally efficient Kalman filtering, smoothing, forecasting, and simulation, making it suitable for users working with various distributions.",
    "use_cases": [
      "Modeling time series data with Gaussian observations",
      "Forecasting using Poisson distributed data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for state space modeling",
      "how to perform Kalman filtering in R",
      "time series forecasting with KFAS",
      "R library for exponential family time series",
      "state space modeling in R",
      "how to use KFAS for forecasting"
    ],
    "primary_use_cases": [
      "Kalman filtering",
      "time series forecasting"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "Kats",
    "description": "Broad toolkit for time series analysis, including multivariate analysis, detection (outliers, change points, trends), feature extraction.",
    "category": "Time Series Econometrics",
    "docs_url": "https://facebookresearch.github.io/Kats/",
    "github_url": "https://github.com/facebookresearch/Kats",
    "url": "https://github.com/facebookresearch/Kats",
    "install": "pip install kats",
    "tags": [
      "time series",
      "econometrics"
    ],
    "best_for": "ARIMA, cointegration, VAR models",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "time-series",
      "econometrics"
    ],
    "summary": "Kats is a broad toolkit designed for time series analysis, providing functionalities for multivariate analysis, detection of outliers, change points, and trends, as well as feature extraction. It is used by data scientists and researchers working in econometrics and time series analysis.",
    "use_cases": [
      "Analyzing economic indicators over time",
      "Detecting anomalies in financial transactions"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for time series analysis",
      "how to detect outliers in time series python",
      "feature extraction in time series with python",
      "change point detection in python",
      "multivariate time series analysis python",
      "trends in time series data python"
    ],
    "primary_use_cases": [
      "outlier detection",
      "trend analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "statsmodels",
      "pandas"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "LocalProjections",
    "description": "Community implementations of Jord\u00e0 (2005) Local Projections for estimating impulse responses without VAR assumptions.",
    "category": "Time Series Econometrics",
    "docs_url": null,
    "github_url": "https://github.com/elenev/localprojections",
    "url": "https://github.com/elenev/localprojections",
    "install": "Install from source",
    "tags": [
      "time series",
      "econometrics"
    ],
    "best_for": "ARIMA, cointegration, VAR models",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "time series",
      "econometrics"
    ],
    "summary": "LocalProjections is a Python package that implements Jord\u00e0's (2005) Local Projections method for estimating impulse responses without relying on VAR assumptions. It is useful for economists and data scientists working with time series data.",
    "use_cases": [
      "Estimating impulse responses from economic shocks",
      "Analyzing the effects of policy changes on economic indicators"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for local projections",
      "how to estimate impulse responses in python",
      "local projections time series python",
      "community implementations of local projections",
      "time series econometrics python",
      "Jord\u00e0 local projections python"
    ],
    "primary_use_cases": [
      "estimating impulse responses"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Jord\u00e0 (2005)",
    "maintenance_status": "active"
  },
  {
    "name": "TS-Flint",
    "description": "Two Sigma's time-series library for Spark with optimized temporal joins, as-of joins, and distributed OLS for high-frequency data.",
    "category": "Time Series Econometrics",
    "docs_url": "https://ts-flint.readthedocs.io/",
    "github_url": "https://github.com/twosigma/flint",
    "url": "https://github.com/twosigma/flint",
    "install": "pip install ts-flint",
    "tags": [
      "spark",
      "time series",
      "temporal joins",
      "fintech"
    ],
    "best_for": "High-frequency financial data with inexact timestamp matching",
    "language": "Spark",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "time-series"
    ],
    "summary": "TS-Flint is a time-series library designed for Spark that provides optimized temporal joins, as-of joins, and distributed OLS for handling high-frequency data. It is primarily used in fintech applications for efficient data analysis.",
    "use_cases": [
      "Analyzing high-frequency financial data",
      "Performing temporal joins on large datasets"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for time series analysis",
      "how to perform temporal joins in Spark",
      "best practices for high-frequency data analysis",
      "distributed OLS in Spark",
      "fintech time series library",
      "using Spark for temporal data",
      "optimizing joins in Spark",
      "Two Sigma time series library"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "Spark"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "dlm",
    "description": "Maximum likelihood and Bayesian analysis of Normal linear state space models (Dynamic Linear Models). Features numerically stable SVD-based algorithms for Kalman filtering and smoothing, plus tools for MCMC-based Bayesian inference including forward filtering backward sampling (FFBS).",
    "category": "Time Series Econometrics",
    "docs_url": "https://cran.r-project.org/web/packages/dlm/vignettes/dlm.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=dlm",
    "install": "install.packages(\"dlm\")",
    "tags": [
      "state-space",
      "kalman-filter",
      "Bayesian",
      "time-series",
      "dynamic-linear-models"
    ],
    "best_for": "Bayesian analysis of linear Gaussian state space models with MCMC methods (Gibbs sampling), implementing Petris (2010)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "time-series",
      "bayesian"
    ],
    "summary": "The dlm package provides tools for maximum likelihood and Bayesian analysis of Normal linear state space models, specifically Dynamic Linear Models. It is used by statisticians and data scientists for Kalman filtering, smoothing, and MCMC-based Bayesian inference.",
    "use_cases": [
      "Kalman filtering for time series data",
      "Bayesian inference in dynamic models"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for dynamic linear models",
      "how to perform Kalman filtering in R",
      "Bayesian analysis with R",
      "state space models in R",
      "MCMC Bayesian inference R",
      "time series analysis R package"
    ],
    "primary_use_cases": [
      "Kalman filtering",
      "Bayesian inference"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "dynlm",
    "description": "Provides an interface for fitting dynamic linear regression models with extended formula syntax. Supports convenient lag operators L(), differencing d(), trend(), season(), and harmonic components while preserving time series attributes.",
    "category": "Time Series Econometrics",
    "docs_url": "https://cran.r-project.org/web/packages/dynlm/dynlm.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=dynlm",
    "install": "install.packages(\"dynlm\")",
    "tags": [
      "dynamic-regression",
      "lag-operator",
      "time-series-regression",
      "distributed-lags",
      "formula-syntax"
    ],
    "best_for": "Time series regression with easy specification of lags, differences, and seasonal patterns using formula syntax",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "time-series"
    ],
    "summary": "The dynlm package provides an interface for fitting dynamic linear regression models using an extended formula syntax. It is useful for users who need to apply lag operators and other time series features in their regression analyses.",
    "use_cases": [
      "Fitting dynamic linear regression models",
      "Analyzing time series data with lagged variables"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for dynamic linear regression",
      "how to fit time series regression in R",
      "R dynlm package examples",
      "dynamic regression models in R",
      "using lag operators in R",
      "time series econometrics R package"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "mFilter",
    "description": "Implements time series filters for extracting trend and cyclical components. Includes Hodrick-Prescott, Baxter-King, Christiano-Fitzgerald, Butterworth, and trigonometric regression filters commonly used in macroeconomics and business cycle analysis.",
    "category": "Time Series Econometrics",
    "docs_url": "https://cran.r-project.org/web/packages/mFilter/mFilter.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=mFilter",
    "install": "install.packages(\"mFilter\")",
    "tags": [
      "HP-filter",
      "Baxter-King",
      "trend-extraction",
      "business-cycles",
      "detrending"
    ],
    "best_for": "Decomposing time series into trend and cyclical components for business cycle analysis",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "time-series"
    ],
    "summary": "mFilter implements various time series filters to extract trend and cyclical components, which are essential in macroeconomics and business cycle analysis. It is used by economists and data scientists working with time series data.",
    "use_cases": [
      "Analyzing economic trends",
      "Detrending time series data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for time series filtering",
      "how to extract trend components in R",
      "R package for Hodrick-Prescott filter",
      "Baxter-King filter implementation in R",
      "trigonometric regression filters in R",
      "business cycle analysis with R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "strucchange",
    "description": "Testing, monitoring, and dating structural changes in linear regression models. Implements the generalized fluctuation test framework (CUSUM, MOSUM, recursive estimates) and F-test framework (Chow test, supF, aveF, expF) with breakpoint estimation and confidence intervals.",
    "category": "Time Series Econometrics",
    "docs_url": "https://cran.r-project.org/web/packages/strucchange/vignettes/strucchange-intro.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=strucchange",
    "install": "install.packages(\"strucchange\")",
    "tags": [
      "structural-break",
      "CUSUM",
      "Chow-test",
      "breakpoints",
      "parameter-stability"
    ],
    "best_for": "Detecting and dating parameter instability and structural breaks in regression relationships, implementing Zeileis et al. (2002)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "time-series"
    ],
    "summary": "The strucchange package is designed for testing, monitoring, and dating structural changes in linear regression models. It is used by econometricians and data scientists who need to analyze structural breaks in time series data.",
    "use_cases": [
      "Analyzing economic data for structural breaks",
      "Monitoring changes in time series data over time"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for structural change analysis",
      "how to test for structural breaks in R",
      "monitoring structural changes in regression models R",
      "CUSUM test in R",
      "Chow test implementation in R",
      "breakpoint estimation in R"
    ],
    "primary_use_cases": [
      "testing for structural changes",
      "breakpoint estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "tsDyn",
    "description": "Implements nonlinear autoregressive time series models including threshold AR (TAR/SETAR), smooth transition AR (STAR, LSTAR), and multivariate extensions (TVAR, TVECM). Enables regime-switching dynamics analysis with parametric and non-parametric approaches.",
    "category": "Time Series Econometrics",
    "docs_url": "https://github.com/MatthieuStigler/tsDyn/wiki",
    "github_url": "https://github.com/MatthieuStigler/tsDyn",
    "url": "https://cran.r-project.org/package=tsDyn",
    "install": "install.packages(\"tsDyn\")",
    "tags": [
      "nonlinear",
      "SETAR",
      "LSTAR",
      "threshold-VAR",
      "regime-switching"
    ],
    "best_for": "Modeling regime-switching dynamics and threshold cointegration in univariate and multivariate series",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "time-series"
    ],
    "summary": "tsDyn implements nonlinear autoregressive time series models, enabling analysis of regime-switching dynamics through various approaches. It is used by researchers and practitioners in econometrics and time series analysis.",
    "use_cases": [
      "Analyzing economic time series data",
      "Forecasting with regime-switching models"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for nonlinear autoregressive models",
      "how to analyze regime-switching dynamics in R",
      "R package for threshold AR models",
      "time series econometrics in R",
      "R library for smooth transition AR",
      "how to implement TAR models in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "urca",
    "description": "Implements unit root and cointegration tests commonly used in applied econometric analysis. Includes Augmented Dickey-Fuller, Phillips-Perron, KPSS, Elliott-Rothenberg-Stock, and Zivot-Andrews tests, plus Johansen's cointegration procedure for multivariate series.",
    "category": "Time Series Econometrics",
    "docs_url": "https://cran.r-project.org/web/packages/urca/urca.pdf",
    "github_url": "https://github.com/bpfaff/urca",
    "url": "https://cran.r-project.org/package=urca",
    "install": "install.packages(\"urca\")",
    "tags": [
      "unit-root",
      "cointegration",
      "ADF-test",
      "KPSS",
      "Johansen"
    ],
    "best_for": "Testing stationarity and finding cointegrating relationships in non-stationary time series, implementing Pfaff (2008)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "time-series",
      "econometrics"
    ],
    "summary": "The 'urca' package implements unit root and cointegration tests that are commonly used in applied econometric analysis. It is designed for researchers and practitioners who need to perform statistical tests on time series data.",
    "use_cases": [
      "Testing for unit roots in economic time series",
      "Analyzing cointegration between multiple economic indicators"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for unit root tests",
      "how to perform cointegration tests in R",
      "R ADF test implementation",
      "time series analysis in R",
      "Johansen cointegration procedure R",
      "KPSS test R package"
    ],
    "primary_use_cases": [
      "unit root testing",
      "cointegration analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "tseries",
      "forecast"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "vars",
    "description": "Comprehensive package for Vector Autoregression (VAR), Structural VAR (SVAR), and Structural Vector Error Correction (SVEC) models. Provides estimation, lag selection, diagnostic testing, forecasting, Granger causality analysis, impulse response functions, and forecast error variance decomposition.",
    "category": "Time Series Econometrics",
    "docs_url": "https://cran.r-project.org/web/packages/vars/vars.pdf",
    "github_url": "https://github.com/bpfaff/vars",
    "url": "https://cran.r-project.org/package=vars",
    "install": "install.packages(\"vars\")",
    "tags": [
      "VAR",
      "SVAR",
      "impulse-response",
      "Granger-causality",
      "FEVD"
    ],
    "best_for": "Multivariate time series analysis with impulse response functions and variance decomposition, implementing Pfaff (2008)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "time-series"
    ],
    "summary": "The vars package provides tools for estimating Vector Autoregression (VAR) and related models, including diagnostic testing and forecasting. It is used by econometricians and data scientists interested in time series analysis.",
    "use_cases": [
      "Estimating VAR models for economic data",
      "Conducting Granger causality tests",
      "Forecasting future values based on historical data"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for Vector Autoregression",
      "how to perform Granger causality analysis in R",
      "forecasting with VAR models in R",
      "impulse response functions in R",
      "Structural VAR analysis in R",
      "time series econometrics R package"
    ],
    "primary_use_cases": [
      "Granger causality analysis",
      "forecasting"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "Augurs",
    "description": "Time series forecasting and analysis for Rust with ETS, MSTL decomposition, seasonality detection, outlier detection, and Prophet-style models.",
    "category": "Time Series Forecasting",
    "docs_url": "https://docs.augu.rs/",
    "github_url": "https://github.com/grafana/augurs",
    "url": "https://crates.io/crates/augurs",
    "install": "cargo add augurs",
    "tags": [
      "rust",
      "time series",
      "forecasting",
      "ETS",
      "MSTL"
    ],
    "best_for": "Time series forecasting and structural analysis in Rust",
    "language": "Rust",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "time-series"
    ],
    "summary": "Augurs is a Rust package designed for time series forecasting and analysis, featuring methods like ETS, MSTL decomposition, and Prophet-style models. It is suitable for users interested in advanced time series techniques and analysis.",
    "use_cases": [
      "Forecasting future values in time series data",
      "Detecting seasonality in datasets"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "Rust library for time series forecasting",
      "how to do time series analysis in Rust",
      "time series decomposition in Rust",
      "outlier detection in Rust",
      "seasonality detection in Rust",
      "Prophet-style models in Rust"
    ],
    "primary_use_cases": [
      "time series forecasting",
      "seasonality detection"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "MLForecast",
    "description": "Scalable time series forecasting using machine learning models (e.g., LightGBM, XGBoost) as regressors.",
    "category": "Time Series Forecasting",
    "docs_url": "https://nixtla.github.io/mlforecast/",
    "github_url": "https://github.com/Nixtla/mlforecast",
    "url": "https://github.com/Nixtla/mlforecast",
    "install": "pip install mlforecast",
    "tags": [
      "forecasting",
      "time series",
      "machine learning"
    ],
    "best_for": "Prediction, demand forecasting, trend analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "time-series",
      "machine-learning"
    ],
    "summary": "MLForecast is a package designed for scalable time series forecasting using machine learning models such as LightGBM and XGBoost as regressors. It is useful for data scientists and analysts working with time series data who want to leverage machine learning techniques for forecasting.",
    "use_cases": [
      "Forecasting sales data",
      "Predicting stock prices"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for time series forecasting",
      "how to forecast time series with machine learning in python",
      "MLForecast documentation",
      "time series forecasting with LightGBM",
      "XGBoost for time series prediction",
      "scalable time series forecasting python"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "Prophet",
      "statsmodels"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "NeuralForecast",
    "description": "Deep learning models (N-BEATS, N-HiTS, Transformers, RNNs) for time series forecasting, built on PyTorch Lightning.",
    "category": "Time Series Forecasting",
    "docs_url": "https://nixtla.github.io/neuralforecast/",
    "github_url": "https://github.com/Nixtla/neuralforecast",
    "url": "https://github.com/Nixtla/neuralforecast",
    "install": "pip install neuralforecast",
    "tags": [
      "forecasting",
      "time series",
      "machine learning"
    ],
    "best_for": "Prediction, demand forecasting, trend analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "time-series",
      "machine-learning"
    ],
    "summary": "NeuralForecast provides deep learning models for time series forecasting, utilizing architectures such as N-BEATS, N-HiTS, Transformers, and RNNs. It is designed for data scientists and researchers looking to implement advanced forecasting techniques using PyTorch Lightning.",
    "use_cases": [
      "Forecasting sales data",
      "Predicting stock prices",
      "Weather forecasting"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for time series forecasting",
      "how to use deep learning for forecasting in python",
      "N-BEATS implementation in python",
      "time series prediction with PyTorch",
      "machine learning models for forecasting",
      "RNNs for time series analysis",
      "Transformers in time series forecasting"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "PyTorch Lightning"
    ],
    "related_packages": [
      "Prophet",
      "statsmodels"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "Prophet",
    "description": "Forecasting procedure for time series with strong seasonality and trend components, developed by Facebook.",
    "category": "Time Series Forecasting",
    "docs_url": "https://facebook.github.io/prophet/",
    "github_url": "https://github.com/facebook/prophet",
    "url": "https://github.com/facebook/prophet",
    "install": "pip install prophet",
    "tags": [
      "forecasting",
      "time series"
    ],
    "best_for": "Prediction, demand forecasting, trend analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "time-series"
    ],
    "summary": "Prophet is a forecasting procedure designed for time series data that exhibits strong seasonal patterns and trends. It is widely used by data scientists and analysts for predicting future values based on historical data.",
    "use_cases": [
      "Forecasting sales data",
      "Predicting website traffic"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for time series forecasting",
      "how to forecast with Prophet in python",
      "time series prediction using Prophet",
      "Prophet forecasting tutorial",
      "forecasting seasonal data in python",
      "using Prophet for trend analysis"
    ],
    "primary_use_cases": [
      "time series forecasting"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "statsmodels",
      "ARIMA"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "StatsForecast",
    "description": "Fast, scalable implementations of popular statistical forecasting models (ETS, ARIMA, Theta, etc.) optimized for performance.",
    "category": "Time Series Forecasting",
    "docs_url": "https://nixtla.github.io/statsforecast/",
    "github_url": "https://github.com/Nixtla/statsforecast",
    "url": "https://github.com/Nixtla/statsforecast",
    "install": "pip install statsforecast",
    "tags": [
      "forecasting",
      "time series"
    ],
    "best_for": "Prediction, demand forecasting, trend analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "time-series"
    ],
    "summary": "StatsForecast provides fast and scalable implementations of popular statistical forecasting models such as ETS, ARIMA, and Theta. It is designed for data scientists and analysts who need efficient forecasting solutions.",
    "use_cases": [
      "Forecasting sales trends",
      "Predicting stock prices"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for statistical forecasting",
      "how to implement ARIMA in python",
      "best python package for time series forecasting",
      "fast forecasting models in python",
      "scalable time series analysis python",
      "using ETS model in python"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "statsmodels",
      "prophet"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "fable",
    "description": "A tidyverse-native forecasting framework providing ETS, ARIMA, and other models for tidy time series (tsibble objects). Enables fitting multiple models across many time series simultaneously with a consistent formula-based interface.",
    "category": "Time Series Forecasting",
    "docs_url": "https://fable.tidyverts.org/",
    "github_url": "https://github.com/tidyverts/fable",
    "url": "https://cran.r-project.org/package=fable",
    "install": "install.packages(\"fable\")",
    "tags": [
      "time-series",
      "tidyverse",
      "ARIMA",
      "ETS",
      "tsibble"
    ],
    "best_for": "Tidy forecasting workflows handling many related time series with tidyverse-consistent syntax",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "time-series"
    ],
    "summary": "Fable is a forecasting framework designed for tidy time series data, allowing users to fit various models like ETS and ARIMA across multiple time series simultaneously. It is primarily used by data scientists and statisticians working with time series analysis in R.",
    "use_cases": [
      "Forecasting sales data over time",
      "Analyzing seasonal trends in web traffic"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for time series forecasting",
      "how to use ARIMA in R",
      "time series analysis with tidyverse",
      "forecasting with ETS in R",
      "fitting multiple models in R",
      "tsibble objects in R"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "tidyverse"
    ],
    "related_packages": [
      "forecast",
      "tsibble"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "forecast",
    "description": "The foundational R package for univariate time series forecasting. Provides methods for exponential smoothing via state space models (ETS), automatic ARIMA modeling with auto.arima(), TBATS for complex seasonality, and comprehensive model evaluation tools.",
    "category": "Time Series Forecasting",
    "docs_url": "https://pkg.robjhyndman.com/forecast/",
    "github_url": "https://github.com/robjhyndman/forecast",
    "url": "https://cran.r-project.org/package=forecast",
    "install": "install.packages(\"forecast\")",
    "tags": [
      "time-series",
      "ARIMA",
      "exponential-smoothing",
      "ETS",
      "auto.arima"
    ],
    "best_for": "Classical statistical forecasting for univariate time series with automatic model selection, implementing Hyndman & Khandakar (2008)",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "time-series"
    ],
    "summary": "The forecast package is designed for univariate time series forecasting in R. It provides various methods for modeling and evaluating time series data, making it suitable for statisticians and data scientists working with time series analysis.",
    "use_cases": [
      "Forecasting sales data",
      "Predicting stock prices",
      "Analyzing seasonal trends"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for time series forecasting",
      "how to use auto.arima in R",
      "exponential smoothing methods in R",
      "forecasting with ETS in R",
      "TBATS model in R",
      "time series analysis R package"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "forecastHybrid",
      "fable"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "pmdarima",
    "description": "ARIMA modeling with automatic parameter selection (auto-ARIMA), similar to R's `forecast::auto.arima`.",
    "category": "Time Series Forecasting",
    "docs_url": "https://alkaline-ml.com/pmdarima/",
    "github_url": "https://github.com/alkaline-ml/pmdarima",
    "url": "https://github.com/alkaline-ml/pmdarima",
    "install": "pip install pmdarima",
    "tags": [
      "forecasting",
      "time series"
    ],
    "best_for": "Prediction, demand forecasting, trend analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "time-series"
    ],
    "summary": "pmdarima is a Python package for ARIMA modeling with automatic parameter selection, similar to R's `forecast::auto.arima`. It is used by data scientists and statisticians for time series forecasting tasks.",
    "use_cases": [
      "Forecasting sales data",
      "Predicting stock prices"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for ARIMA modeling",
      "how to do time series forecasting in python",
      "auto-ARIMA package for python",
      "forecasting with pmdarima",
      "time series analysis in python",
      "best practices for ARIMA in python"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "statsmodels"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "prophet",
    "description": "Automatic forecasting procedure based on an additive decomposable model with non-linear trends, yearly/weekly/daily seasonality, and holiday effects. Robust to missing data, trend shifts, and outliers; designed for business time series with strong seasonal patterns.",
    "category": "Time Series Forecasting",
    "docs_url": "https://facebook.github.io/prophet/",
    "github_url": "https://github.com/facebook/prophet",
    "url": "https://cran.r-project.org/package=prophet",
    "install": "install.packages(\"prophet\")",
    "tags": [
      "time-series",
      "Facebook",
      "decomposable-model",
      "seasonality",
      "holidays"
    ],
    "best_for": "Business time series forecasting with multiple seasonalities, holiday effects, and automated tunable forecasts, implementing Taylor & Letham (2018)",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "time-series",
      "seasonality"
    ],
    "summary": "Prophet is an automatic forecasting procedure that uses an additive decomposable model to handle non-linear trends and seasonal patterns. It is particularly useful for business time series data with strong seasonal effects and is robust to missing data and outliers.",
    "use_cases": [
      "Forecasting sales with seasonal patterns",
      "Predicting website traffic during holidays"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "R library for time series forecasting",
      "how to forecast with seasonality in R",
      "automatic forecasting in R",
      "business time series forecasting R package",
      "R package for handling missing data in time series",
      "forecasting with holidays in R"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "forecast",
      "tsibble"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "sktime",
    "description": "Unified framework for various time series tasks, including forecasting with classical, ML, and deep learning models.",
    "category": "Time Series Forecasting",
    "docs_url": "https://www.sktime.net/en/latest/",
    "github_url": "https://github.com/sktime/sktime",
    "url": "https://github.com/sktime/sktime",
    "install": "pip install sktime",
    "tags": [
      "forecasting",
      "time series",
      "machine learning"
    ],
    "best_for": "Prediction, demand forecasting, trend analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "time-series",
      "machine-learning"
    ],
    "summary": "sktime is a unified framework designed for various time series tasks, including forecasting using classical, machine learning, and deep learning models. It is used by data scientists and researchers working with time series data.",
    "use_cases": [
      "Forecasting stock prices",
      "Predicting sales trends"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for time series forecasting",
      "how to forecast time series in python",
      "time series analysis with machine learning python",
      "sktime package for time series",
      "deep learning for time series forecasting python",
      "classical forecasting methods in python"
    ],
    "primary_use_cases": [
      "time series forecasting"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "statsmodels",
      "prophet"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "CatBoost",
    "description": "Gradient boosting library excelling with categorical features (minimal preprocessing needed). Robust against overfitting.",
    "category": "Tree & Ensemble Methods for Prediction",
    "docs_url": "https://catboost.ai/docs/",
    "github_url": "https://github.com/catboost/catboost",
    "url": "https://github.com/catboost/catboost",
    "install": "pip install catboost",
    "tags": [
      "machine learning",
      "prediction"
    ],
    "best_for": "Random forests, gradient boosting, prediction tasks",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "CatBoost is a gradient boosting library that excels in handling categorical features with minimal preprocessing. It is robust against overfitting, making it suitable for various machine learning tasks.",
    "use_cases": [
      "Predicting customer churn",
      "Forecasting sales trends"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for gradient boosting",
      "how to use CatBoost in Python",
      "CatBoost for categorical features",
      "machine learning with CatBoost",
      "gradient boosting library Python",
      "CatBoost tutorial",
      "CatBoost vs XGBoost",
      "how to prevent overfitting in CatBoost"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "XGBoost",
      "LightGBM"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "LightGBM",
    "description": "Fast, distributed gradient boosting (also supports RF). Known for speed, low memory usage, and handling large datasets.",
    "category": "Tree & Ensemble Methods for Prediction",
    "docs_url": "https://lightgbm.readthedocs.io/",
    "github_url": "https://github.com/microsoft/LightGBM",
    "url": "https://github.com/microsoft/LightGBM",
    "install": "pip install lightgbm",
    "tags": [
      "machine learning",
      "prediction"
    ],
    "best_for": "Random forests, gradient boosting, prediction tasks",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "LightGBM is a fast and efficient gradient boosting framework that is particularly well-suited for large datasets. It is widely used by data scientists and machine learning practitioners for its speed and low memory usage.",
    "use_cases": [
      "Predicting outcomes in large datasets",
      "Optimizing model performance in machine learning competitions"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for gradient boosting",
      "how to use LightGBM in python",
      "LightGBM for machine learning",
      "fast gradient boosting in python",
      "LightGBM tutorial",
      "LightGBM examples"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "XGBoost",
      "CatBoost"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "Linfa",
    "description": "Rust ML toolkit inspired by scikit-learn with GLMs, clustering (K-Means), PCA, SVM, and regularization (Lasso/Ridge).",
    "category": "Tree & Ensemble Methods for Prediction",
    "docs_url": "https://rust-ml.github.io/linfa/",
    "github_url": "https://github.com/rust-ml/linfa",
    "url": "https://crates.io/crates/linfa",
    "install": "cargo add linfa",
    "tags": [
      "rust",
      "machine learning",
      "clustering",
      "PCA",
      "SVM"
    ],
    "best_for": "scikit-learn style ML in Rust",
    "language": "Rust",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Linfa is a Rust machine learning toolkit that provides a variety of algorithms including generalized linear models, clustering methods like K-Means, principal component analysis, support vector machines, and regularization techniques such as Lasso and Ridge. It is designed for users familiar with machine learning concepts who are looking to implement these techniques in Rust.",
    "use_cases": [
      "Building machine learning models in Rust",
      "Performing clustering analysis on datasets"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "rust library for machine learning",
      "how to do clustering in rust",
      "rust implementation of PCA",
      "using SVM in rust",
      "rust toolkit for GLMs",
      "K-Means algorithm in rust"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "NGBoost",
    "description": "Extends gradient boosting to probabilistic prediction, providing uncertainty estimates alongside point predictions. Built on scikit-learn.",
    "category": "Tree & Ensemble Methods for Prediction",
    "docs_url": "https://stanfordmlgroup.github.io/ngboost/",
    "github_url": "https://github.com/stanfordmlgroup/ngboost",
    "url": "https://github.com/stanfordmlgroup/ngboost",
    "install": "pip install ngboost",
    "tags": [
      "machine learning",
      "prediction"
    ],
    "best_for": "Random forests, gradient boosting, prediction tasks",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "NGBoost extends gradient boosting to probabilistic prediction, providing uncertainty estimates alongside point predictions. It is built on scikit-learn and is useful for practitioners looking to incorporate uncertainty into their predictive models.",
    "use_cases": [
      "Predicting outcomes with uncertainty estimates",
      "Enhancing models with probabilistic predictions"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for probabilistic prediction",
      "how to use NGBoost for uncertainty estimates",
      "gradient boosting with uncertainty in python",
      "scikit-learn extension for probabilistic models",
      "machine learning library for predictions with uncertainty",
      "NGBoost documentation",
      "install NGBoost in python"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "scikit-learn"
    ],
    "related_packages": [
      "XGBoost",
      "LightGBM"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "SmartCore",
    "description": "Rust ML library with regression, classification, clustering, matrix decomposition (SVD, PCA), and model selection tools.",
    "category": "Tree & Ensemble Methods for Prediction",
    "docs_url": "https://docs.rs/smartcore",
    "github_url": "https://github.com/smartcorelib/smartcore",
    "url": "https://crates.io/crates/smartcore",
    "install": "cargo add smartcore",
    "tags": [
      "rust",
      "machine learning",
      "regression",
      "classification"
    ],
    "best_for": "Comprehensive ML algorithms in pure Rust",
    "language": "Rust",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "SmartCore is a Rust machine learning library that provides tools for regression, classification, clustering, matrix decomposition, and model selection. It is designed for data scientists and developers looking to implement machine learning algorithms in Rust.",
    "use_cases": [
      "Building predictive models",
      "Data analysis using machine learning techniques"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "Rust library for machine learning",
      "how to do regression in Rust",
      "Rust classification library",
      "machine learning clustering in Rust",
      "matrix decomposition in Rust",
      "model selection tools in Rust"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "XGBoost",
    "description": "High-performance, optimized gradient boosting library (also supports RF). Known for speed, efficiency, and winning competitions.",
    "category": "Tree & Ensemble Methods for Prediction",
    "docs_url": "https://xgboost.readthedocs.io/",
    "github_url": "https://github.com/dmlc/xgboost",
    "url": "https://github.com/dmlc/xgboost",
    "install": "pip install xgboost",
    "tags": [
      "machine learning",
      "prediction"
    ],
    "best_for": "Random forests, gradient boosting, prediction tasks",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "XGBoost is a high-performance, optimized gradient boosting library that supports both gradient boosting and random forests. It is widely used in machine learning competitions for its speed and efficiency.",
    "use_cases": [
      "predictive modeling",
      "classification tasks"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for gradient boosting",
      "how to use XGBoost in python",
      "XGBoost tutorial",
      "XGBoost vs random forest",
      "best practices for XGBoost",
      "XGBoost parameter tuning"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "LightGBM",
      "CatBoost"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "cuML (RAPIDS)",
    "description": "GPU-accelerated implementation of Random Forests for significant speedups on large datasets. Scikit-learn compatible API.",
    "category": "Tree & Ensemble Methods for Prediction",
    "docs_url": "https://docs.rapids.ai/api/cuml/stable/",
    "github_url": "https://github.com/rapidsai/cuml",
    "url": "https://github.com/rapidsai/cuml",
    "install": "conda install ... (See RAPIDS docs)",
    "tags": [
      "machine learning",
      "prediction"
    ],
    "best_for": "Random forests, gradient boosting, prediction tasks",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "cuML is a GPU-accelerated implementation of Random Forests designed for significant speedups on large datasets. It offers a Scikit-learn compatible API, making it accessible for users familiar with Scikit-learn.",
    "use_cases": [
      "large dataset classification",
      "regression tasks on big data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for GPU-accelerated Random Forests",
      "how to use cuML for machine learning",
      "scikit-learn compatible GPU library",
      "fast Random Forests in Python",
      "machine learning with cuML",
      "GPU machine learning libraries"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "scikit-learn"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "CausalLift",
    "description": "Uplift modeling for observational (non-RCT) data using inverse probability weighting.",
    "category": "Uplift Modeling",
    "docs_url": "https://causallift.readthedocs.io/",
    "github_url": "https://github.com/Minyus/causallift",
    "url": "https://github.com/Minyus/causallift",
    "install": "pip install causallift",
    "tags": [
      "uplift modeling",
      "observational data",
      "IPW"
    ],
    "best_for": "Uplift from observational data with IPW",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "uplift-modeling"
    ],
    "summary": "CausalLift is a Python package designed for uplift modeling using observational data through inverse probability weighting. It is primarily used by data scientists and researchers interested in causal inference and treatment effect estimation.",
    "use_cases": [
      "Estimating treatment effects in marketing campaigns",
      "Analyzing customer behavior in observational studies"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for uplift modeling",
      "how to perform causal inference in python",
      "observational data analysis in python",
      "inverse probability weighting python",
      "uplift modeling techniques",
      "python package for treatment effect estimation"
    ],
    "primary_use_cases": [
      "uplift modeling",
      "treatment effect estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "UpliftML",
    "description": "Booking.com's enterprise uplift modeling via PySpark and H2O. Six meta-learners plus Uplift Random Forest with ROI-constrained optimization.",
    "category": "Uplift Modeling",
    "docs_url": null,
    "github_url": "https://github.com/bookingcom/upliftml",
    "url": "https://github.com/bookingcom/upliftml",
    "install": "pip install upliftml",
    "tags": [
      "uplift modeling",
      "treatment effects",
      "marketing"
    ],
    "best_for": "Enterprise-scale uplift with ROI optimization",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "uplift modeling",
      "treatment effects",
      "marketing"
    ],
    "summary": "UpliftML is a package designed for enterprise uplift modeling using PySpark and H2O. It is particularly useful for marketing professionals looking to optimize treatment effects and ROI.",
    "use_cases": [
      "Optimizing marketing campaigns",
      "Estimating treatment effects for customer segmentation"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for uplift modeling",
      "how to optimize marketing treatment effects in python",
      "UpliftML documentation",
      "examples of uplift modeling with PySpark",
      "H2O uplift modeling in python",
      "best practices for uplift random forest",
      "ROI-constrained optimization in marketing",
      "treatment effects analysis with UpliftML"
    ],
    "primary_use_cases": [
      "uplift modeling",
      "ROI-constrained optimization"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "PySpark",
      "H2O"
    ],
    "related_packages": [
      "CausalML",
      "EconML"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "pylift",
    "description": "Wayfair's uplift modeling wrapping sklearn for speed with rigorous Qini curve evaluation.",
    "category": "Uplift Modeling",
    "docs_url": "https://pylift.readthedocs.io/",
    "github_url": "https://github.com/wayfair/pylift",
    "url": "https://github.com/wayfair/pylift",
    "install": "pip install pylift",
    "tags": [
      "uplift modeling",
      "treatment effects",
      "marketing"
    ],
    "best_for": "Fast uplift with Qini curve evaluation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "uplift modeling",
      "treatment effects",
      "marketing"
    ],
    "summary": "pylift is a Python package developed by Wayfair for uplift modeling, designed to enhance the speed of sklearn while providing rigorous Qini curve evaluation. It is primarily used by data scientists and researchers in marketing to analyze treatment effects.",
    "use_cases": [
      "Evaluating marketing campaign effectiveness",
      "Optimizing treatment allocation in experiments"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for uplift modeling",
      "how to evaluate Qini curve in python",
      "treatment effects analysis with python",
      "marketing uplift modeling python package"
    ],
    "primary_use_cases": [
      "uplift modeling",
      "treatment effect estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "cowplot",
    "description": "Publication-ready ggplot2 themes and plot arrangement utilities. Provides clean themes, plot annotations, and functions for combining plots with shared axes.",
    "category": "Visualization",
    "docs_url": "https://wilkelab.org/cowplot/",
    "github_url": "https://github.com/wilkelab/cowplot",
    "url": "https://cran.r-project.org/package=cowplot",
    "install": "install.packages(\"cowplot\")",
    "tags": [
      "ggplot2",
      "themes",
      "publication-ready",
      "plot-arrangement",
      "annotations"
    ],
    "best_for": "Publication-ready ggplot2 themes and multi-plot arrangements with annotations",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The cowplot package provides publication-ready themes and utilities for ggplot2 in R. It is designed for users who need clean themes and functions for arranging plots with shared axes, making it suitable for researchers and data scientists creating visualizations.",
    "use_cases": [
      "Creating publication-ready visualizations",
      "Arranging multiple ggplot2 plots in a single figure"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for ggplot2 themes",
      "how to arrange plots in R",
      "publication-ready plots in R",
      "ggplot2 plot annotations",
      "clean themes for ggplot2",
      "combine ggplots with shared axes"
    ],
    "api_complexity": "simple",
    "related_packages": [
      "ggplot2"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "patchwork",
    "description": "Compose multiple ggplot2 plots into publication-ready multi-panel figures. Uses intuitive operators (+, |, /) for arrangement with automatic alignment and shared legends.",
    "category": "Visualization",
    "docs_url": "https://patchwork.data-imaginist.com/",
    "github_url": "https://github.com/thomasp85/patchwork",
    "url": "https://cran.r-project.org/package=patchwork",
    "install": "install.packages(\"patchwork\")",
    "tags": [
      "ggplot2",
      "multi-panel",
      "figure-composition",
      "visualization",
      "publication-ready"
    ],
    "best_for": "Composing multi-panel ggplot2 figures with intuitive + and | operators",
    "language": "R",
    "difficulty": "beginner|intermediate",
    "prerequisites": [],
    "topic_tags": [
      "visualization"
    ],
    "summary": "The patchwork package allows users to compose multiple ggplot2 plots into publication-ready multi-panel figures. It is particularly useful for R users who need to create visually appealing and aligned figures for academic or professional presentations.",
    "use_cases": [
      "Creating multi-panel figures for academic papers",
      "Arranging plots for presentations"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for multi-panel figures",
      "how to compose ggplot2 plots in R",
      "visualization tools for R",
      "create publication-ready figures in R",
      "patchwork ggplot2 tutorial",
      "R multi-panel figure examples"
    ],
    "api_complexity": "simple",
    "related_packages": [
      "ggplot2"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "Ax (Meta Adaptive Experimentation)",
    "description": "Meta's open-source platform for adaptive experimentation. Bayesian optimization, multi-objective optimization, and automated experiment design. Built on BoTorch for AI-assisted experimentation.",
    "category": "Adaptive Experimentation & Bandits",
    "docs_url": "https://ax.dev/docs/why-ax",
    "github_url": "https://github.com/facebook/Ax",
    "url": "https://ax.dev",
    "install": "pip install ax-platform",
    "tags": [
      "adaptive experimentation",
      "Bayesian optimization",
      "multi-objective"
    ],
    "best_for": "Adaptive experimentation, Bayesian optimization, automated experiment design",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "null"
    ],
    "topic_tags": [
      "bayesian",
      "optimization",
      "experimentation"
    ],
    "summary": "Ax is Meta's open-source platform designed for adaptive experimentation, utilizing Bayesian optimization and automated experiment design. It is primarily used by data scientists and researchers looking to enhance their experimentation processes with AI-assisted methods.",
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for adaptive experimentation",
      "how to perform Bayesian optimization in python",
      "automated experiment design in python",
      "multi-objective optimization with Ax",
      "Meta Ax package documentation",
      "Ax platform for experimentation examples"
    ],
    "use_cases": [
      "Optimizing marketing strategies through A/B testing",
      "Designing experiments for product feature testing"
    ],
    "primary_use_cases": [
      "A/B test analysis",
      "multi-objective optimization"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "BoTorch"
    ],
    "related_packages": [
      "Optuna",
      "Hyperopt"
    ],
    "maintenance_status": "active"
  }
]