[
  {
    "name": "PyMC Labs Blog",
    "description": "Bayesian causal inference done right. MCMC, probabilistic programming, and causal models from the PyMC team.",
    "category": "Bayesian Methods",
    "url": "https://www.pymc-labs.com/blog-posts/",
    "type": "Blog",
    "level": "Medium",
    "tags": [
      "Causal Inference",
      "Blog"
    ],
    "domain": "Statistics",
    "image_url": "https://www.pymc-labs.com/images/pymc-labs-logo.png",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-programming",
      "bayesian-statistics",
      "causal-inference-fundamentals"
    ],
    "topic_tags": [
      "bayesian-causal-inference",
      "MCMC",
      "probabilistic-programming",
      "PyMC",
      "blog-posts"
    ],
    "summary": "PyMC Labs Blog provides practical guidance on implementing Bayesian causal inference methods using probabilistic programming. The blog covers MCMC techniques, causal model specification, and real-world applications from the team behind the PyMC library. It bridges theory and practice with code examples and case studies for data scientists working on causal problems.",
    "use_cases": [
      "Implementing Bayesian A/B testing with proper uncertainty quantification",
      "Building causal models to estimate treatment effects from observational data"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "PyMC causal inference tutorials",
      "Bayesian methods for A/B testing",
      "probabilistic programming causal models",
      "MCMC for causal inference examples"
    ]
  },
  {
    "name": "Coding for Economists",
    "description": "Arthur Turrell's practical guide. Python basics through advanced workflows \u2014 built specifically for econ researchers.",
    "category": "Python",
    "url": "https://aeturrell.github.io/coding-for-economists/",
    "type": "Online Book",
    "level": "Easy",
    "tags": [
      "Coding",
      "Online Book"
    ],
    "domain": "Programming",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-programming-concepts",
      "economics-fundamentals"
    ],
    "topic_tags": [
      "python-programming",
      "economics-research",
      "data-analysis",
      "beginner-guide",
      "online-book"
    ],
    "summary": "Arthur Turrell's comprehensive introduction to Python programming tailored specifically for economics researchers. Covers everything from basic syntax to advanced data workflows, with examples and applications directly relevant to economic analysis. Designed to take economists from zero programming experience to confidently implementing research projects in Python.",
    "use_cases": [
      "PhD student needs to learn Python for dissertation data analysis",
      "Economics researcher transitioning from Stata/R wants to pick up Python skills"
    ],
    "audience": [
      "Early-PhD",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "learn python for economics research",
      "beginner python guide for economists",
      "python programming economics phd",
      "coding tutorial economics students"
    ]
  },
  {
    "name": "The Missing Semester (MIT)",
    "description": "Command line, Git, debugging, shell scripting. The CS skills they don't teach in econ PhD programs but you absolutely need.",
    "category": "Software Engineering",
    "url": "https://missing.csail.mit.edu/",
    "type": "Course",
    "level": "Easy",
    "tags": [
      "Coding",
      "Course"
    ],
    "domain": "Programming",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-computer-literacy",
      "terminal-access"
    ],
    "topic_tags": [
      "command-line",
      "git-version-control",
      "shell-scripting",
      "debugging",
      "developer-tools"
    ],
    "summary": "MIT's comprehensive course covering essential software engineering skills that economics PhD programs typically skip. Teaches command line navigation, Git version control, debugging techniques, and shell scripting - foundational tools for any data scientist or researcher working with code.",
    "use_cases": [
      "Setting up reproducible research workflows with Git for thesis or paper code",
      "Automating data processing tasks and file management for large datasets"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "basic programming skills for economists",
      "git and command line tutorial for researchers",
      "software engineering fundamentals for data science",
      "missing computer science skills for PhD students"
    ]
  },
  {
    "name": "Beyond Jupyter",
    "description": "Software design principles for ML applications. Go from messy notebooks to maintainable, modular code with OOP essentials and refactoring guides.",
    "category": "Software Engineering",
    "url": "https://github.com/aai-institute/beyond-jupyter",
    "type": "Tutorial",
    "level": "Medium",
    "tags": [
      "Coding",
      "Tutorial"
    ],
    "domain": "Programming",
    "difficulty": "intermediate",
    "prerequisites": [
      "jupyter-notebooks",
      "python-classes",
      "git-version-control"
    ],
    "topic_tags": [
      "software-engineering",
      "code-refactoring",
      "object-oriented-programming",
      "MLOps",
      "tutorial"
    ],
    "summary": "This tutorial teaches data scientists how to transform experimental Jupyter notebooks into production-ready, maintainable code using object-oriented programming principles. It covers software design patterns, code organization, and refactoring techniques specifically tailored for machine learning applications. Essential for transitioning from prototype to scalable ML systems.",
    "use_cases": [
      "Converting a messy exploratory data analysis notebook into a reusable data pipeline for production",
      "Refactoring a complex model training script into modular components that can be easily tested and maintained"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "how to refactor jupyter notebook into production code",
      "object oriented programming for data science",
      "clean code practices for machine learning",
      "jupyter to production pipeline tutorial"
    ]
  },
  {
    "name": "The Theory and Practice of Revenue Management",
    "description": "Talluri & van Ryzin's comprehensive textbook. Dynamic pricing, capacity allocation, overbooking \u2014 the bible of RM.",
    "category": "Pricing & Revenue",
    "url": "http://ndl.ethernet.edu.et/bitstream/123456789/21707/1/306.pdf",
    "type": "Online Book",
    "level": "Hard",
    "tags": [
      "Pricing & Demand",
      "Online Book"
    ],
    "domain": "Domain Applications",
    "difficulty": "advanced",
    "prerequisites": [
      "linear-programming",
      "probability-theory",
      "dynamic-programming"
    ],
    "topic_tags": [
      "revenue-management",
      "dynamic-pricing",
      "capacity-allocation",
      "overbooking",
      "textbook"
    ],
    "summary": "Talluri & van Ryzin's foundational textbook covering the mathematical theory and practical implementation of revenue management systems. The comprehensive resource covers dynamic pricing models, capacity allocation algorithms, and overbooking strategies used across airlines, hotels, and other capacity-constrained industries. Essential reference for understanding both the optimization theory and real-world applications of revenue management.",
    "use_cases": [
      "Designing dynamic pricing algorithms for airline seat inventory management",
      "Building hotel room allocation systems that optimize revenue across different booking channels"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "revenue management textbook dynamic pricing",
      "capacity allocation optimization algorithms",
      "overbooking theory airline pricing",
      "Talluri van Ryzin revenue management book"
    ]
  },
  {
    "name": "Stitch Fix: Algorithms Tour",
    "description": "The single best piece of data journalism in tech. Interactive, animated tour of how they combine styles, logistics, and feedback loops.",
    "category": "Routing & Logistics",
    "url": "https://algorithms-tour.stitchfix.com/",
    "type": "Interactive",
    "level": "Easy",
    "tags": [
      "Pricing & Demand",
      "Interactive"
    ],
    "domain": "Optimization & Operations Research",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-statistics",
      "recommendation-systems"
    ],
    "topic_tags": [
      "recommendation-systems",
      "supply-chain",
      "interactive-visualization",
      "business-strategy",
      "data-journalism"
    ],
    "summary": "An award-winning interactive visualization that walks through Stitch Fix's end-to-end algorithmic approach to personalized fashion retail. The piece elegantly explains how they combine customer preference modeling, inventory optimization, and feedback loops in their styling algorithm. Perfect for understanding how data science creates business value in consumer-facing applications.",
    "use_cases": [
      "Understanding how recommendation systems work in practice at scale",
      "Learning to communicate complex algorithmic systems to non-technical stakeholders"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "how does stitch fix algorithm work",
      "interactive explanation of recommendation systems",
      "data science in fashion retail",
      "best examples of data journalism"
    ]
  },
  {
    "name": "Auctions in Ad Tech (Sanjiv Das)",
    "description": "GSP auctions, quality scores, AdRank \u2014 how Google/Meta ad auctions actually work. Chapter 21.",
    "category": "Ads & Attribution",
    "url": "https://srdas.github.io/MLBook/Auctions.html",
    "type": "Online Book",
    "level": "Medium",
    "tags": [
      "Auctions & Market Design",
      "Online Book"
    ],
    "domain": "Domain Applications",
    "difficulty": "intermediate",
    "prerequisites": [
      "microeconomics",
      "probability-theory",
      "game-theory"
    ],
    "topic_tags": [
      "generalized-second-price",
      "ad-auctions",
      "quality-score",
      "adrank",
      "mechanism-design"
    ],
    "summary": "Explains how Google and Meta's ad auction systems work, focusing on Generalized Second Price (GSP) auctions, quality scores, and AdRank calculations. Covers the economic theory behind how ads are selected and priced in real-time bidding environments. Essential reading for understanding the market mechanisms that power digital advertising platforms.",
    "use_cases": [
      "Understanding how to optimize ad bidding strategies and quality scores for digital marketing campaigns",
      "Designing auction mechanisms for marketplace platforms or advertising products"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "how do google ad auctions work",
      "generalized second price auction explained",
      "what is quality score in ad auctions",
      "adrank calculation mechanism design"
    ]
  },
  {
    "name": "Uber Engineering",
    "description": "Surge pricing, marketplace design, causal inference at scale. See how researchers tackle real problems at Uber.",
    "category": "Marketplace Economics",
    "url": "https://www.uber.com/blog/engineering/",
    "type": "Blog",
    "level": "Medium",
    "tags": [
      "Industry Blogs",
      "Blog"
    ],
    "domain": "Domain Applications",
    "image_url": "https://blog.uber-cdn.com/cdn-cgi/image/width=400,quality=80,onerror=redirect,format=auto/wp-content/uploads/2018/09/uber_blog_seo.png",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "causal-inference",
      "A-B-testing"
    ],
    "topic_tags": [
      "surge-pricing",
      "marketplace-design",
      "causal-inference",
      "tech-industry",
      "applied-economics"
    ],
    "summary": "Uber Engineering's blog showcases real-world applications of economics and data science at massive scale. Posts cover surge pricing algorithms, marketplace matching, and causal inference methods used to optimize rider-driver markets. Essential reading for understanding how economic theory translates to production systems serving millions of users.",
    "use_cases": [
      "Learning how surge pricing algorithms balance supply and demand in real-time marketplaces",
      "Understanding causal inference techniques for measuring the impact of marketplace interventions"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "How does Uber implement surge pricing",
      "Causal inference at tech companies",
      "Marketplace economics in practice",
      "Real world applications of pricing algorithms"
    ]
  },
  {
    "name": "Spotify R&D",
    "description": "How do you recommend songs to 500M users? Personalization, search, and ML at audio scale.",
    "category": "Frameworks & Strategy",
    "url": "https://research.atspotify.com/",
    "type": "Blog",
    "level": "Medium",
    "tags": [
      "Industry Blogs",
      "Blog"
    ],
    "domain": "Product Sense",
    "image_url": "https://images.ctfassets.net/p762jor363g1/49er1DCdgzSkzN1Xzn18Mr/8f3a13386c92fb3944d24c5ed975faaa/RS090_Transforming_AI_Research_into_Personalized_Listening__Spotify_at_NeurIPS_2025.png",
    "difficulty": "intermediate",
    "prerequisites": [
      "collaborative-filtering",
      "python-scikit-learn",
      "A-B-testing"
    ],
    "topic_tags": [
      "recommender-systems",
      "personalization",
      "audio-streaming",
      "industry-scale",
      "spotify"
    ],
    "summary": "Spotify's R&D blog documenting their approach to building recommendation systems and personalization features for hundreds of millions of users. The content covers machine learning techniques for music discovery, audio processing, and large-scale experimentation in the streaming industry. A valuable resource for understanding how theoretical ML concepts are applied to real-world recommendation challenges at massive scale.",
    "use_cases": [
      "Building recommendation systems for content platforms with millions of users",
      "Learning how to scale personalization algorithms from research prototypes to production systems"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "How does Spotify build recommendation systems",
      "Music recommendation algorithms at scale",
      "Spotify machine learning blog personalization",
      "Large scale recommender systems industry examples"
    ]
  },
  {
    "name": "DoorDash Engineering",
    "description": "marketplace analytics, delivery optimization, and experimentation. Great posts on real-time pricing and logistics.",
    "category": "Routing & Logistics",
    "url": "https://doordash.engineering/",
    "type": "Blog",
    "level": "Medium",
    "tags": [
      "Industry Blogs",
      "Blog"
    ],
    "domain": "Optimization & Operations Research",
    "image_url": "https://careersatdoordash.com/wp-content/uploads/2022/10/doordash-logo.png",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "A-B-testing",
      "SQL-queries"
    ],
    "topic_tags": [
      "marketplace-analytics",
      "delivery-optimization",
      "real-time-pricing",
      "logistics",
      "experimentation"
    ],
    "summary": "DoorDash Engineering's blog provides in-depth technical posts on marketplace analytics, delivery optimization, and experimentation methods used at scale. The content covers real-world applications of pricing algorithms, logistics optimization, and A/B testing in the food delivery industry. It's particularly valuable for understanding how theoretical concepts translate to production systems serving millions of users.",
    "use_cases": [
      "Learning how to implement dynamic pricing algorithms for marketplace platforms",
      "Understanding logistics optimization strategies for delivery route planning"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "DoorDash engineering blog marketplace analytics",
      "real-time pricing algorithms delivery platforms",
      "food delivery logistics optimization techniques",
      "A/B testing experimentation at DoorDash"
    ]
  },
  {
    "name": "Lyft: Quantifying Efficiency in Ridesharing",
    "description": "Efficiency isn't speed\u2014it's an economic equilibrium. A masterclass in defining the objective function for marketplace optimization.",
    "category": "Marketplace Economics",
    "url": "https://eng.lyft.com/quantifying-efficiency-in-ridesharing-marketplaces-affd53043db2",
    "type": "Article",
    "level": "Medium",
    "tags": [
      "Industry Blogs",
      "Blog"
    ],
    "domain": "Domain Applications",
    "difficulty": "intermediate",
    "prerequisites": [
      "microeconomics-equilibrium",
      "optimization-theory",
      "marketplace-design"
    ],
    "topic_tags": [
      "marketplace-optimization",
      "ridesharing-economics",
      "efficiency-metrics",
      "two-sided-markets",
      "objective-functions"
    ],
    "summary": "Lyft's approach to defining and measuring efficiency in their ridesharing marketplace, focusing on economic equilibrium rather than simple speed metrics. The post demonstrates how to construct proper objective functions for marketplace optimization that balance rider and driver interests. Essential reading for anyone working on two-sided marketplace problems or platform economics.",
    "use_cases": [
      "Designing KPIs for a two-sided marketplace platform",
      "Optimizing matching algorithms for supply-demand platforms"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "how to measure efficiency in ridesharing platforms",
      "marketplace optimization objective functions",
      "two-sided market equilibrium metrics",
      "Lyft marketplace economics case study"
    ]
  },
  {
    "name": "Noahpinion (Noah Smith)",
    "description": "applied analytics, AI, innovation, growth. Deep dives with data, accessible to non-specialists. The researcher's tech newsletter.",
    "category": "Frameworks & Strategy",
    "url": "https://noahpinion.substack.com/",
    "type": "Substack",
    "level": "Medium",
    "tags": [
      "Strategy & Analytics",
      "Substack"
    ],
    "domain": "Product Sense",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-economics",
      "data-interpretation"
    ],
    "topic_tags": [
      "tech-economics",
      "AI-strategy",
      "innovation-policy",
      "newsletter",
      "applied-research"
    ],
    "summary": "Noah Smith's accessible newsletter covering tech economics, AI trends, and innovation policy through data-driven analysis. Written for both practitioners and general audiences interested in understanding how technology shapes economic outcomes. Combines academic rigor with clear explanations of complex economic concepts.",
    "use_cases": [
      "Staying current on AI economic impacts and policy developments",
      "Finding accessible explanations of complex tech-economy relationships for presentations or research"
    ],
    "audience": [
      "Curious-browser",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "accessible tech economics newsletter",
      "AI economic impact analysis",
      "Noah Smith tech policy writing",
      "beginner-friendly innovation economics content"
    ]
  },
  {
    "name": "Ben Evans Newsletter",
    "description": "Tech market trends and strategic analysis. What's happening in tech and why it matters.",
    "category": "Frameworks & Strategy",
    "url": "https://www.ben-evans.com/newsletter",
    "type": "Newsletter",
    "level": "Medium",
    "tags": [
      "Strategy & Analytics",
      "Newsletter"
    ],
    "domain": "Product Sense",
    "difficulty": "beginner",
    "prerequisites": [
      "business-strategy-basics",
      "tech-industry-knowledge"
    ],
    "topic_tags": [
      "tech-trends",
      "market-analysis",
      "strategic-insights",
      "business-intelligence",
      "newsletter"
    ],
    "summary": "Ben Evans Newsletter provides strategic analysis of technology market trends and business developments. It translates complex tech industry movements into accessible insights about what's happening and why it matters. Popular among data scientists and researchers who need to understand the broader business context of their technical work.",
    "use_cases": [
      "Understanding market context when designing product experiments or choosing which metrics to prioritize",
      "Staying informed about industry trends that might impact data strategy or create new opportunities for analysis"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "tech industry newsletter for data scientists",
      "strategic analysis of technology trends",
      "understanding tech market developments",
      "ben evans tech newsletter insights"
    ]
  },
  {
    "name": "Stitch Fix Algorithms Blog",
    "description": "Demand forecasting, inventory optimization, and personalization. Unique blend of fashion retail + serious data science.",
    "category": "Routing & Logistics",
    "url": "https://multithreaded.stitchfix.com/algorithms/blog/",
    "type": "Blog",
    "level": "Medium",
    "tags": [
      "Industry Blogs",
      "Blog"
    ],
    "domain": "Optimization & Operations Research",
    "image_url": "https://multithreaded.stitchfix.com/assets/images/logo.svg",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "machine-learning-basics",
      "A-B-testing"
    ],
    "topic_tags": [
      "demand-forecasting",
      "inventory-optimization",
      "fashion-retail",
      "personalization",
      "industry-blog"
    ],
    "summary": "Stitch Fix's engineering blog showcasing real-world data science applications in fashion retail. Features detailed case studies on demand forecasting, inventory management, and personalization algorithms used at scale. Provides practical insights into how a major fashion company applies data science to solve complex business problems.",
    "use_cases": [
      "Learning how to implement demand forecasting for retail inventory",
      "Understanding personalization algorithms for recommendation systems"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "demand forecasting retail fashion",
      "Stitch Fix data science blog",
      "inventory optimization algorithms",
      "personalization retail case studies"
    ]
  },
  {
    "name": "Meta Engineering - Data Science",
    "description": "Large-scale experimentation, ML infrastructure, and data discovery at Facebook scale. Posts on causal inference and data tools.",
    "category": "Case Studies",
    "url": "https://engineering.fb.com/",
    "type": "Blog",
    "level": "Medium",
    "tags": [
      "Industry Blogs",
      "Blog"
    ],
    "domain": "Product Sense",
    "image_url": "https://engineering.fb.com/wp-content/uploads/2023/08/Meta_lockup_positive-primary_RGB.jpg",
    "difficulty": "intermediate",
    "prerequisites": [
      "a-b-testing",
      "python-pandas",
      "sql-queries"
    ],
    "topic_tags": [
      "experimentation",
      "causal-inference",
      "ml-infrastructure",
      "facebook",
      "industry-case-studies"
    ],
    "summary": "Meta's engineering blog covering large-scale experimentation and ML infrastructure used at Facebook. Provides insights into causal inference methods, data discovery tools, and operational challenges at billion-user scale. Valuable for understanding how tech giants implement data science in production.",
    "use_cases": [
      "Learning how to scale A/B testing infrastructure for millions of users",
      "Understanding ML deployment patterns and data tooling at major tech companies"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "How does Facebook run experiments at scale",
      "Meta data science infrastructure blog",
      "Large scale A/B testing case studies",
      "Facebook causal inference methods"
    ]
  },
  {
    "name": "Instacart Tech Blog",
    "description": "Marketplace balancing, delivery optimization, demand forecasting. Making on-demand grocery profitable.",
    "category": "Marketplace Economics",
    "url": "https://tech.instacart.com/",
    "type": "Blog",
    "level": "Medium",
    "tags": [
      "Industry Blogs",
      "Blog"
    ],
    "domain": "Domain Applications",
    "image_url": "https://miro.medium.com/v2/resize:fit:1200/1*F_5qR3i1O6M5n9n7RU2IPA.png",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "A-B-testing",
      "basic-econometrics"
    ],
    "topic_tags": [
      "marketplace-design",
      "demand-forecasting",
      "delivery-optimization",
      "on-demand-economics",
      "industry-case-studies"
    ],
    "summary": "Instacart's engineering blog covering technical approaches to marketplace economics challenges like balancing supply and demand, optimizing delivery routes, and forecasting grocery demand patterns. Posts blend engineering implementation details with economic insights from running a large-scale on-demand grocery platform. Valuable for understanding how marketplace theory translates to production systems at scale.",
    "use_cases": [
      "Learning how to implement demand forecasting models for perishable goods in a two-sided marketplace",
      "Understanding technical approaches to delivery route optimization and driver matching algorithms"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "instacart marketplace optimization techniques",
      "demand forecasting grocery delivery",
      "two sided marketplace balancing supply demand",
      "on-demand delivery economics case studies"
    ]
  },
  {
    "name": "Stripe Engineering",
    "description": "Payment economics, fraud detection ML, financial data infrastructure. Building economic infrastructure for the internet.",
    "category": "Trust & Safety",
    "url": "https://stripe.com/blog/engineering",
    "type": "Blog",
    "level": "Medium",
    "tags": [
      "Industry Blogs",
      "Blog"
    ],
    "domain": "Domain Applications",
    "image_url": "https://images.stripeassets.com/fzn2n1nzq965/2tPGiM6bmk10U1TUUjQ5OP/230aea369d4cb8a7b0015e8cd5cff6d6/Billing_analytics_blog_hero.jpg",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-scikit-learn",
      "SQL-databases",
      "statistical-testing"
    ],
    "topic_tags": [
      "payment-systems",
      "fraud-detection",
      "financial-infrastructure",
      "industry-engineering",
      "fintech"
    ],
    "summary": "Stripe Engineering's blog covers payment economics, machine learning for fraud detection, and financial data infrastructure. The content focuses on real-world engineering challenges in building economic infrastructure for internet payments. Posts cover both technical implementations and business economics of payment systems.",
    "use_cases": [
      "Building fraud detection models for financial transactions",
      "Designing scalable payment processing infrastructure"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "fraud detection machine learning payment systems",
      "stripe engineering blog financial infrastructure",
      "payment economics technical implementation",
      "fintech data science best practices"
    ]
  },
  {
    "name": "Amazon Science",
    "description": "Research from Amazon's scientists. Causal inference, supply chain optimization, pricing, and forecasting.",
    "category": "Routing & Logistics",
    "url": "https://www.amazon.science/",
    "type": "Blog",
    "level": "Medium",
    "tags": [
      "Industry Blogs",
      "Blog"
    ],
    "domain": "Optimization & Operations Research",
    "image_url": "https://assets.amazon.science/dims4/default/f1b4822/2147483647/strip/true/crop/1920x1080+0+0/resize/1200x675!/quality/90/?url=http%3A%2F%2Famazon-topics-brightspot.s3.amazonaws.com%2Fscience%2F2c%2Fe5%2Fe24e3df44b62a6d58e5807dae1b9%2Ftopblogs-promo-homepage.png",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-scikit-learn",
      "A/B-testing",
      "linear-regression"
    ],
    "topic_tags": [
      "causal-inference",
      "supply-chain",
      "pricing-optimization",
      "forecasting",
      "industry-research"
    ],
    "summary": "Amazon Science is the research publication platform showcasing cutting-edge work from Amazon's scientific teams. It covers applied research in causal inference for marketplace interventions, supply chain optimization algorithms, dynamic pricing strategies, and demand forecasting at scale. The content bridges academic rigor with practical implementation challenges faced in large-scale tech operations.",
    "use_cases": [
      "Learning how Amazon applies causal inference methods to measure the impact of product recommendations on sales",
      "Understanding scalable forecasting techniques used for inventory management across millions of products"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "How does Amazon do causal inference for marketplace experiments",
      "Supply chain optimization methods used at tech companies",
      "Amazon research on pricing algorithms",
      "Real-world applications of forecasting at scale"
    ]
  },
  {
    "name": "Mode SQL Tutorial",
    "description": "Interactive SQL lessons from basic to advanced. Great for learning JOINs, window functions, and subqueries with a real database.",
    "category": "SQL",
    "url": "https://mode.com/sql-tutorial/",
    "type": "Tutorial",
    "level": "Medium",
    "tags": [
      "SQL",
      "Tutorial"
    ],
    "domain": "Programming",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-database-concepts",
      "data-filtering-logic"
    ],
    "topic_tags": [
      "SQL",
      "database-queries",
      "data-analysis",
      "interactive-tutorial",
      "window-functions"
    ],
    "summary": "An interactive SQL tutorial covering fundamental to advanced database querying techniques using real datasets. Teaches essential skills like JOINs, window functions, and subqueries through hands-on exercises. Perfect for data scientists transitioning from other tools or learning SQL for the first time.",
    "use_cases": [
      "Junior data scientist needs to query company databases for analysis instead of relying on data exports",
      "PhD student wants to learn SQL to access research datasets stored in university database systems"
    ],
    "audience": [
      "Junior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "interactive SQL tutorial for beginners",
      "learn SQL JOINs and window functions",
      "best SQL tutorial with real database practice",
      "Mode SQL lessons for data analysis"
    ]
  },
  {
    "name": "SQLBolt",
    "description": "Learn SQL with interactive exercises. No setup required \u2014 run queries right in the browser. Perfect for beginners.",
    "category": "SQL",
    "url": "https://sqlbolt.com/",
    "type": "Interactive Course",
    "level": "Easy",
    "tags": [
      "SQL",
      "Interactive Course"
    ],
    "domain": "Programming",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-programming-concepts",
      "database-fundamentals"
    ],
    "topic_tags": [
      "SQL",
      "interactive-learning",
      "query-fundamentals",
      "database-queries",
      "beginner-tutorial"
    ],
    "summary": "SQLBolt is an interactive SQL tutorial platform that teaches database querying through hands-on exercises directly in the browser. It's designed for complete beginners who need to learn SQL without any prior database experience or local setup. The platform covers fundamental SQL concepts from basic SELECT statements to more complex joins and data manipulation.",
    "use_cases": [
      "New data scientist needs to quickly learn SQL for querying company databases",
      "Junior analyst preparing for technical interviews that include SQL questions"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "interactive SQL tutorial for beginners",
      "learn SQL without installing anything",
      "SQL exercises in browser",
      "beginner friendly SQL course"
    ]
  },
  {
    "name": "LeetCode SQL 50",
    "description": "50 essential SQL problems to master for interviews. CTEs, window functions, and common patterns used at FAANG.",
    "category": "SQL",
    "url": "https://leetcode.com/studyplan/top-sql-50/",
    "type": "Practice Problems",
    "level": "Medium",
    "tags": [
      "SQL",
      "Practice Problems"
    ],
    "domain": "Programming",
    "difficulty": "intermediate",
    "prerequisites": [
      "SQL-basic-queries",
      "SQL-joins",
      "SQL-aggregations"
    ],
    "topic_tags": [
      "SQL-practice",
      "interview-preparation",
      "window-functions",
      "CTEs",
      "FAANG-interviews"
    ],
    "summary": "A curated collection of 50 SQL problems covering essential interview topics including CTEs, window functions, and common data manipulation patterns. Specifically designed to prepare candidates for technical interviews at major tech companies. Focuses on practical problem-solving skills that translate directly to on-the-job SQL work.",
    "use_cases": [
      "Preparing for data scientist or analyst interviews at tech companies",
      "Practicing advanced SQL techniques before implementing complex analytics queries at work"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "SQL interview practice problems for tech companies",
      "LeetCode style SQL questions for data science interviews",
      "Advanced SQL practice with window functions and CTEs",
      "FAANG SQL interview preparation problems"
    ]
  },
  {
    "name": "DuckDB Documentation",
    "description": "Modern in-process SQL database. Runs on your laptop, reads Parquet directly, and is perfect for analytics. The new pandas killer.",
    "category": "SQL",
    "url": "https://duckdb.org/docs/",
    "type": "Docs",
    "level": "Medium",
    "tags": [
      "SQL",
      "Docs"
    ],
    "domain": "Programming",
    "difficulty": "beginner",
    "prerequisites": [
      "python-pandas",
      "SQL-basics",
      "command-line-interface"
    ],
    "topic_tags": [
      "duckdb",
      "in-process-database",
      "parquet",
      "analytics",
      "documentation"
    ],
    "summary": "DuckDB is a lightweight, in-process SQL database optimized for analytical workloads that runs directly on your machine without requiring a server. It excels at reading columnar formats like Parquet and provides pandas-like performance with familiar SQL syntax. The documentation covers installation, querying, and integration with Python data science workflows.",
    "use_cases": [
      "Analyzing large CSV or Parquet files that don't fit well in pandas",
      "Running SQL queries on local datasets without setting up PostgreSQL or other database servers"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "lightweight SQL database for data analysis",
      "how to query parquet files with SQL",
      "pandas alternative for large datasets",
      "DuckDB getting started tutorial"
    ]
  },
  {
    "name": "NeetCode",
    "description": "Curated LeetCode roadmap organized by pattern. Video explanations that actually make sense. The modern way to prep for coding interviews.",
    "category": "SQL",
    "url": "https://neetcode.io/",
    "type": "Course + Practice",
    "level": "Medium",
    "tags": [
      "LeetCode",
      "Course + Practice"
    ],
    "domain": "Programming",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-SQL-syntax",
      "database-fundamentals"
    ],
    "topic_tags": [
      "coding-interviews",
      "SQL-practice",
      "pattern-recognition",
      "video-tutorials"
    ],
    "summary": "NeetCode provides a structured approach to mastering SQL coding interview questions through pattern-based learning and clear video explanations. It's specifically designed for data scientists and software engineers preparing for technical interviews at tech companies. The platform focuses on building intuition around common SQL problem patterns rather than just memorizing solutions.",
    "use_cases": [
      "Preparing for data scientist interviews at FAANG companies",
      "Strengthening SQL skills for analytics engineer roles"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "best SQL interview prep course",
      "LeetCode SQL problems explained",
      "data science interview SQL practice",
      "structured SQL coding interview preparation"
    ]
  },
  {
    "name": "Blind 75",
    "description": "The 75 most important LeetCode problems. Arrays, strings, trees, graphs, DP \u2014 if you can solve these, you can handle any interview.",
    "category": "SQL",
    "url": "https://www.techinterviewhandbook.org/grind75",
    "type": "Problem Set",
    "level": "Medium",
    "tags": [
      "LeetCode",
      "Problem Set"
    ],
    "domain": "Programming",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-basics",
      "data-structures",
      "algorithm-complexity"
    ],
    "topic_tags": [
      "leetcode",
      "coding-interviews",
      "algorithm-practice",
      "data-structures",
      "programming-problems"
    ],
    "summary": "A curated collection of 75 essential LeetCode problems covering fundamental data structures and algorithms. This problem set is widely recognized as the gold standard for technical interview preparation at major tech companies. The problems span arrays, strings, trees, graphs, and dynamic programming with optimal difficulty progression.",
    "use_cases": [
      "Preparing for technical interviews at FAANG companies",
      "Building foundational algorithm skills before starting data science role"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "best leetcode problems for tech interviews",
      "essential coding problems for data scientists",
      "FAANG interview preparation problem set",
      "fundamental algorithms practice for DS roles"
    ]
  },
  {
    "name": "LeetCode Patterns",
    "description": "14 patterns to solve any coding interview question. Two pointers, sliding window, BFS/DFS, and more \u2014 with Python templates.",
    "category": "SQL",
    "url": "https://seanprashad.com/leetcode-patterns/",
    "type": "Study Guide",
    "level": "Medium",
    "tags": [
      "LeetCode",
      "Study Guide"
    ],
    "domain": "Programming",
    "difficulty": "beginner",
    "prerequisites": [
      "python-basics",
      "data-structures",
      "algorithm-fundamentals"
    ],
    "topic_tags": [
      "coding-interviews",
      "algorithm-patterns",
      "python-templates",
      "leetcode-prep"
    ],
    "summary": "A comprehensive study guide covering 14 essential coding patterns like two pointers, sliding window, and BFS/DFS with Python implementations. Designed to help data scientists and engineers systematically approach coding interview problems by recognizing common patterns. Provides reusable templates that can be adapted to solve hundreds of LeetCode-style questions.",
    "use_cases": [
      "Preparing for technical interviews at tech companies",
      "Building algorithmic problem-solving skills for data engineering roles"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "coding interview patterns python",
      "leetcode study guide templates",
      "how to solve coding interviews systematically",
      "data structures algorithms interview prep"
    ]
  },
  {
    "name": "Automate the Boring Stuff with Python",
    "description": "The best free Python book for non-programmers. Web scraping, Excel automation, file management \u2014 practical skills for data work.",
    "category": "Python",
    "url": "https://automatetheboringstuff.com/",
    "type": "Online Book",
    "level": "Easy",
    "tags": [
      "Automation",
      "Online Book"
    ],
    "domain": "Programming",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-python-syntax",
      "command-line-basics"
    ],
    "topic_tags": [
      "python-automation",
      "web-scraping",
      "excel-automation",
      "file-management",
      "beginner-programming"
    ],
    "summary": "A comprehensive free online book that teaches Python programming through practical automation projects. Perfect for economists and data workers who want to automate repetitive tasks like web scraping, Excel manipulation, and file organization without deep programming background. Focuses on immediately useful skills rather than theoretical computer science concepts.",
    "use_cases": [
      "Automatically downloading and organizing research papers from multiple websites",
      "Batch processing Excel files to extract specific data columns for analysis"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "best free Python book for beginners",
      "learn Python automation for data tasks",
      "Python web scraping tutorial book",
      "automate Excel tasks with Python"
    ]
  },
  {
    "name": "Scrapy Documentation",
    "description": "The industrial-strength web scraping framework for Python. Build spiders, handle anti-bot measures, and scale to millions of pages.",
    "category": "Python",
    "url": "https://docs.scrapy.org/",
    "type": "Docs",
    "level": "Medium",
    "tags": [
      "Automation",
      "Docs"
    ],
    "domain": "Programming",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-requests",
      "html-css-selectors",
      "xpath-expressions"
    ],
    "topic_tags": [
      "web-scraping",
      "data-collection",
      "python-framework",
      "automation",
      "spider-crawling"
    ],
    "summary": "Scrapy is a comprehensive Python framework for building web scrapers that can handle complex crawling tasks at scale. It provides built-in support for handling JavaScript, cookies, user agents, and rate limiting to avoid detection. Data scientists and researchers use it to systematically collect large datasets from websites for analysis and model training.",
    "use_cases": [
      "Collecting product pricing data from e-commerce sites for competitive analysis",
      "Scraping job postings from multiple career sites to analyze labor market trends"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python web scraping framework documentation",
      "how to build scalable web scrapers",
      "scrapy tutorial for data collection",
      "web crawling framework anti-bot detection"
    ]
  },
  {
    "name": "Real Python: Web Scraping",
    "description": "Practical guide to scraping with BeautifulSoup and requests. Parse HTML, handle pagination, and extract structured data.",
    "category": "Python",
    "url": "https://realpython.com/beautiful-soup-web-scraper-python/",
    "type": "Tutorial",
    "level": "Easy",
    "tags": [
      "Automation",
      "Tutorial"
    ],
    "domain": "Programming",
    "difficulty": "beginner",
    "prerequisites": [
      "python-basics",
      "HTML-fundamentals",
      "HTTP-requests"
    ],
    "topic_tags": [
      "web-scraping",
      "beautifulsoup",
      "data-extraction",
      "python-tutorial",
      "automation"
    ],
    "summary": "A comprehensive tutorial on web scraping using Python's BeautifulSoup and requests libraries. Covers HTML parsing, handling different webpage structures, and extracting structured data from websites. Perfect for data scientists who need to collect data from web sources for analysis projects.",
    "use_cases": [
      "Collecting product prices from e-commerce sites for market analysis",
      "Extracting job postings from career websites for labor market research"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "how to scrape websites with python",
      "beautifulsoup tutorial for beginners",
      "extract data from web pages python",
      "web scraping guide requests library"
    ]
  },
  {
    "name": "Python for Econometrics",
    "description": "Kevin Sheppard's comprehensive intro for researchers. NumPy, pandas, statsmodels, and econometric applications.",
    "category": "Python",
    "url": "https://bashtage.github.io/kevinsheppard.com/teaching/python/notes/",
    "type": "Online Book",
    "level": "Easy",
    "tags": [
      "Coding",
      "Online Book"
    ],
    "domain": "Programming",
    "difficulty": "beginner",
    "prerequisites": [
      "python-basics",
      "basic-statistics"
    ],
    "topic_tags": [
      "econometrics",
      "python-tutorial",
      "data-analysis",
      "statsmodels",
      "research-methods"
    ],
    "summary": "Kevin Sheppard's comprehensive textbook introducing Python programming for econometric analysis and empirical research. Covers essential data science libraries like NumPy, pandas, and statsmodels with focus on economic applications. Ideal for researchers transitioning from Stata/R to Python or learning quantitative methods.",
    "use_cases": [
      "PhD student learning to implement regression analysis and time series models in Python",
      "Economics researcher switching from Stata to Python for empirical work"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "python econometrics tutorial",
      "learn python for economics research",
      "Kevin Sheppard python book",
      "statsmodels pandas econometrics guide"
    ]
  },
  {
    "name": "Statistical Rethinking",
    "description": "Richard McElreath's Bayesian approach to statistics. PyMC3 translations available. The book that changed how many think about inference.",
    "category": "Bayesian Methods",
    "url": "https://xcelab.net/rm/statistical-rethinking/",
    "type": "Book + Lectures",
    "level": "Medium",
    "tags": [
      "Statistics",
      "Book + Lectures"
    ],
    "domain": "Statistics",
    "difficulty": "intermediate",
    "prerequisites": [
      "basic-probability",
      "linear-regression",
      "python-programming"
    ],
    "topic_tags": [
      "bayesian-inference",
      "statistical-modeling",
      "causal-inference",
      "textbook",
      "pymc"
    ],
    "summary": "A comprehensive introduction to Bayesian statistics that emphasizes building models from first principles rather than memorizing formulas. McElreath uses practical examples and clear explanations to teach concepts like prior specification, posterior sampling, and model comparison. The book includes companion code in R and community-created PyMC implementations.",
    "use_cases": [
      "Building hierarchical models for user behavior analysis with uncertainty quantification",
      "Running A/B tests with proper prior information and credible intervals instead of p-values"
    ],
    "audience": [
      "Early-PhD",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "best bayesian statistics textbook for data scientists",
      "how to learn bayesian methods with python",
      "statistical rethinking pymc3 tutorial",
      "bayesian approach to A/B testing book"
    ]
  },
  {
    "name": "Adam Fishman Newsletter",
    "description": "Ex-Patreon/Reforge. Growth loops, product strategy, and how to structure product analytics.",
    "category": "Frameworks & Strategy",
    "url": "https://www.fishmanafnewsletter.com/",
    "type": "Newsletter",
    "level": "Medium",
    "tags": [
      "Strategy & Analytics",
      "Newsletter"
    ],
    "domain": "Product Sense",
    "difficulty": "intermediate",
    "prerequisites": [
      "product-metrics",
      "A-B-testing",
      "cohort-analysis"
    ],
    "topic_tags": [
      "growth-strategy",
      "product-analytics",
      "business-metrics",
      "newsletter",
      "case-studies"
    ],
    "summary": "Newsletter by ex-Patreon/Reforge Adam Fishman covering growth loops, product strategy, and analytics structuring. Provides practical frameworks and real-world examples for product-driven growth. Ideal for data scientists and product managers working on user acquisition, retention, and monetization strategies.",
    "use_cases": [
      "Designing analytics infrastructure to track product-led growth metrics and user lifecycle stages",
      "Implementing growth loop frameworks to optimize user acquisition and retention funnels"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "growth loops framework product analytics",
      "how to structure product analytics team",
      "product strategy newsletter data science",
      "growth metrics and experimentation best practices"
    ]
  },
  {
    "name": "Sequoia: Data-Informed Product Building",
    "description": "Metric hierarchies, North Star metrics, and building data-informed products. The definitive framework for product metrics.",
    "category": "Metrics & Measurement",
    "url": "https://medium.com/sequoia-capital/data-informed-product-building-1e509a5c4112",
    "type": "Guide",
    "level": "Medium",
    "tags": [
      "Experimentation",
      "Guide"
    ],
    "domain": "Product Sense",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-statistics",
      "product-analytics"
    ],
    "topic_tags": [
      "north-star-metrics",
      "product-metrics",
      "data-driven-decisions",
      "metric-hierarchies",
      "framework"
    ],
    "summary": "Sequoia's comprehensive framework for building data-informed products through structured metric hierarchies and North Star metrics. This guide provides practical approaches for product teams to establish meaningful measurement systems that drive decision-making. It covers how to identify, prioritize, and implement metrics that align with business objectives.",
    "use_cases": [
      "Setting up a metric framework for a new product launch",
      "Restructuring existing product analytics to focus on key business drivers"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "how to choose north star metrics for product",
      "product metrics framework guide",
      "data informed product building best practices",
      "metric hierarchies for product teams"
    ]
  },
  {
    "name": "Convex Optimization (Boyd & Vandenberghe)",
    "description": "The bible of convex optimization \u2014 free online, universally cited. Covers LP, QP, SDP, and more.",
    "category": "Convex Optimization",
    "url": "https://web.stanford.edu/~boyd/cvxbook/",
    "type": "Online Book",
    "level": "Hard",
    "tags": [
      "Optimization",
      "Online Book"
    ],
    "domain": "Optimization & Operations Research",
    "difficulty": "intermediate",
    "prerequisites": [
      "linear-algebra",
      "multivariable-calculus",
      "python-numpy"
    ],
    "topic_tags": [
      "convex-optimization",
      "mathematical-optimization",
      "linear-programming",
      "quadratic-programming",
      "textbook"
    ],
    "summary": "The definitive textbook on convex optimization theory and algorithms by Boyd and Vandenberghe, available free online. Covers fundamental concepts like linear programming, quadratic programming, and semidefinite programming with rigorous mathematical treatment. Essential reading for anyone working on optimization problems in machine learning, operations research, or quantitative analysis.",
    "use_cases": [
      "Implementing portfolio optimization algorithms with quadratic constraints",
      "Designing machine learning models with convex loss functions and regularization"
    ],
    "audience": [
      "Early-PhD",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "convex optimization textbook Boyd Vandenberghe",
      "best book to learn convex optimization theory",
      "free online convex optimization resources",
      "mathematical optimization textbook recommendations"
    ]
  },
  {
    "name": "Stanford EE364A (YouTube)",
    "description": "Boyd's legendary lectures on convex optimization. The gold standard for learning optimization theory.",
    "category": "Convex Optimization",
    "url": "https://www.youtube.com/playlist?list=PL3940DD956CDF0622",
    "type": "Lectures",
    "level": "Hard",
    "tags": [
      "Optimization",
      "Lectures"
    ],
    "domain": "Optimization & Operations Research",
    "difficulty": "intermediate",
    "prerequisites": [
      "linear-algebra",
      "multivariable-calculus",
      "python-numpy"
    ],
    "topic_tags": [
      "convex-optimization",
      "mathematical-optimization",
      "video-lectures",
      "stanford",
      "theory"
    ],
    "summary": "Stephen Boyd's comprehensive video lecture series covering the fundamentals of convex optimization theory and applications. This Stanford course is widely regarded as the definitive introduction to optimization methods used throughout machine learning and operations research. The lectures build from basic convex analysis to advanced algorithms like interior-point methods.",
    "use_cases": [
      "Learning optimization theory before implementing ML algorithms like SVM or logistic regression",
      "Understanding the mathematical foundations behind hyperparameter tuning and model training"
    ],
    "audience": [
      "Early-PhD",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "convex optimization course videos",
      "Boyd Stanford optimization lectures",
      "learn convex optimization theory",
      "best optimization video lectures"
    ]
  },
  {
    "name": "Modeling Discrete Optimization (Coursera)",
    "description": "University of Melbourne's course on constraint programming, local search, and MIP. Covers MiniZinc modeling language.",
    "category": "Convex Optimization",
    "url": "https://www.coursera.org/learn/basic-modeling",
    "type": "Course",
    "level": "Medium",
    "tags": [
      "Optimization",
      "Course"
    ],
    "domain": "Optimization & Operations Research",
    "difficulty": "intermediate",
    "prerequisites": [
      "linear-algebra",
      "python-programming",
      "basic-optimization"
    ],
    "topic_tags": [
      "constraint-programming",
      "mixed-integer-programming",
      "local-search",
      "minizinc",
      "discrete-optimization"
    ],
    "summary": "University of Melbourne's comprehensive course teaching discrete optimization through constraint programming, local search algorithms, and mixed-integer programming. Students learn the MiniZinc modeling language to solve complex combinatorial problems. Ideal for data scientists who need to tackle scheduling, resource allocation, and other discrete decision problems.",
    "use_cases": [
      "Optimizing delivery routes and scheduling for logistics operations",
      "Resource allocation and task assignment in project management systems"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "constraint programming course for data scientists",
      "learn MiniZinc modeling language",
      "discrete optimization methods tutorial",
      "mixed integer programming online course"
    ]
  },
  {
    "name": "CVXPY Short Course",
    "description": "Hands-on convex optimization in Python. Learn to model and solve real problems with CVXPY.",
    "category": "Convex Optimization",
    "url": "https://www.cvxgrp.org/cvx_short_course/docs/index.html",
    "type": "Tutorial",
    "level": "Medium",
    "tags": [
      "Optimization",
      "Tutorial"
    ],
    "domain": "Optimization & Operations Research",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-numpy",
      "linear-algebra",
      "basic-optimization-theory"
    ],
    "topic_tags": [
      "convex-optimization",
      "python-programming",
      "mathematical-modeling",
      "operations-research",
      "hands-on-tutorial"
    ],
    "summary": "A practical tutorial for learning convex optimization using the CVXPY Python library. Covers how to formulate and solve real-world optimization problems with hands-on examples. Perfect for data scientists who need to move beyond basic modeling to solve resource allocation, portfolio optimization, and constraint satisfaction problems.",
    "use_cases": [
      "optimizing ad budget allocation across channels with budget constraints",
      "portfolio optimization with risk constraints and transaction costs"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "cvxpy tutorial for beginners",
      "hands on convex optimization python",
      "learn optimization modeling cvxpy",
      "convex optimization course data science"
    ]
  },
  {
    "name": "Walmart Global Tech",
    "description": "AI-driven retail tech, supply chain optimization, agentic AI, and developer experience. Posts on LLMs for product catalogs, delivery optimization, and cross-lingual search.",
    "category": "Routing & Logistics",
    "url": "https://tech.walmart.com/",
    "type": "Blog",
    "level": "Intermediate",
    "tags": [
      "Industry Blogs",
      "Blog"
    ],
    "domain": "Optimization & Operations Research",
    "image_url": "https://tech.walmart.com/content/dam/walmart-global-tech/images/global-tech/home-hero.jpg",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-scikit-learn",
      "SQL-joins",
      "A-B-testing"
    ],
    "topic_tags": [
      "retail-analytics",
      "supply-chain",
      "LLM-applications",
      "industry-blog",
      "logistics-optimization"
    ],
    "summary": "Walmart Global Tech's blog showcasing AI applications in retail operations, from LLM-powered product catalogs to delivery route optimization. Features real-world case studies and technical insights from one of the world's largest retailers implementing ML at scale. Covers both customer-facing AI features and backend supply chain automation.",
    "use_cases": [
      "Learning how major retailers apply LLMs to product search and recommendation systems",
      "Understanding supply chain optimization techniques for last-mile delivery and inventory management"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "How does Walmart use AI for supply chain optimization",
      "LLM applications in retail product catalogs",
      "Real world examples of ML in logistics",
      "Walmart tech blog machine learning case studies"
    ]
  },
  {
    "name": "Coursera Pricing Strategy Optimization (UVA/BCG)",
    "description": "Price elasticity, WTP estimation, segmentation \u2014 free to audit",
    "category": "Pricing & Revenue",
    "url": "https://www.coursera.org/specializations/uva-darden-bcg-pricing-strategy",
    "type": "Course",
    "level": "Medium",
    "tags": [
      "Pricing & Demand",
      "Course"
    ],
    "domain": "Domain Applications",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-statistics",
      "excel-or-spreadsheets"
    ],
    "topic_tags": [
      "price-elasticity",
      "willingness-to-pay",
      "customer-segmentation",
      "revenue-optimization",
      "online-course"
    ],
    "summary": "A foundational course covering price elasticity estimation, willingness-to-pay analysis, and customer segmentation for pricing decisions. Developed by UVA and BCG, it provides practical frameworks for revenue optimization without requiring advanced technical skills. Ideal for building core pricing strategy knowledge applicable across tech companies.",
    "use_cases": [
      "Setting optimal subscription prices for a SaaS product launch",
      "Designing tiered pricing structure for an e-commerce marketplace"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "pricing strategy course for beginners",
      "how to estimate price elasticity",
      "customer segmentation for pricing",
      "willingness to pay analysis tutorial"
    ]
  },
  {
    "name": "Chargebee: SaaS Pricing Models Guide",
    "description": "Usage-based pricing, value metrics, packaging strategies \u2014 free",
    "category": "Pricing & Revenue",
    "url": "https://www.chargebee.com/resources/guides/saas-pricing-models-guide/",
    "type": "Article",
    "level": "Medium",
    "tags": [
      "Pricing & Demand",
      "Article"
    ],
    "domain": "Domain Applications",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-economics",
      "business-metrics"
    ],
    "topic_tags": [
      "saas-pricing",
      "revenue-models",
      "pricing-strategy",
      "value-metrics",
      "subscription-business"
    ],
    "summary": "A comprehensive guide covering different SaaS pricing models including usage-based pricing, value metrics, and packaging strategies. Written by Chargebee for product managers and business teams looking to optimize their pricing approach. Provides practical frameworks for choosing and implementing pricing models in subscription businesses.",
    "use_cases": [
      "Setting up pricing for a new SaaS product launch",
      "Evaluating whether to switch from flat-rate to usage-based pricing"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "SaaS pricing model examples",
      "usage based pricing vs subscription",
      "how to choose SaaS pricing strategy",
      "value metrics for SaaS pricing"
    ]
  },
  {
    "name": "Monetizing Innovation (Ramanujam)",
    "description": "The industry bible \u2014 design products around price, not vice versa",
    "category": "Pricing & Revenue",
    "url": "https://www.simon-kucher.com/en/insights/monetizing-innovation",
    "type": "Article",
    "level": "Medium",
    "tags": [
      "Pricing & Demand",
      "Article"
    ],
    "domain": "Domain Applications",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-economics",
      "product-management-fundamentals"
    ],
    "topic_tags": [
      "pricing-strategy",
      "product-monetization",
      "value-based-pricing",
      "revenue-optimization",
      "business-strategy"
    ],
    "summary": "A comprehensive guide to pricing strategy that advocates for designing products around willingness-to-pay rather than cost-plus pricing. Written for product managers and business leaders, it provides frameworks for understanding customer value perception and implementing pricing strategies that maximize revenue. Essential reading for anyone involved in product pricing decisions at tech companies.",
    "use_cases": [
      "Product manager launching a new SaaS feature needs to determine optimal pricing tiers",
      "Business analyst evaluating whether to bundle services or price them separately"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "how to price new tech products",
      "value based pricing strategy guide",
      "product pricing frameworks for startups",
      "monetizing innovation pricing book"
    ]
  },
  {
    "name": "OpenView SaaS Pricing Guide",
    "description": "Free playbooks on usage-based pricing",
    "category": "Pricing & Revenue",
    "url": "https://openviewpartners.com/blog/saas-pricing-resource-guide/",
    "type": "Article",
    "level": "Medium",
    "tags": [
      "Pricing & Demand",
      "Blog"
    ],
    "domain": "Domain Applications",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-economics",
      "revenue-recognition"
    ],
    "topic_tags": [
      "usage-based-pricing",
      "saas-monetization",
      "pricing-strategy",
      "revenue-optimization",
      "playbook"
    ],
    "summary": "OpenView's comprehensive guide provides practical playbooks and frameworks for implementing usage-based pricing models in SaaS businesses. The resource covers pricing strategy fundamentals, implementation tactics, and real-world case studies from successful companies. It's designed for practitioners looking to transition from traditional subscription models or optimize existing usage-based approaches.",
    "use_cases": [
      "Product manager designing pricing tiers for a new API service",
      "Revenue team evaluating switch from flat subscription to consumption-based model"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "usage based pricing best practices",
      "how to implement consumption pricing SaaS",
      "saas pricing strategy guide",
      "usage based vs subscription pricing models"
    ]
  },
  {
    "name": "Lenny's Podcast: Madhavan Ramanujam",
    "description": "90 minutes on WTP conversations and behavioral pricing",
    "category": "Pricing & Revenue",
    "url": "https://www.lennyspodcast.com/the-art-and-science-of-pricing-madhavan-ramanujam-simon-kucher/",
    "type": "Podcast",
    "level": "Medium",
    "tags": [
      "Pricing & Demand",
      "Podcast"
    ],
    "domain": "Domain Applications",
    "difficulty": "intermediate",
    "prerequisites": [
      "price-elasticity",
      "conjoint-analysis",
      "consumer-surveys"
    ],
    "topic_tags": [
      "willingness-to-pay",
      "behavioral-pricing",
      "monetization-strategy",
      "podcast-interview"
    ],
    "summary": "A 90-minute podcast interview with Madhavan Ramanujam discussing willingness-to-pay research methods and behavioral pricing strategies. Covers practical approaches to understanding customer price sensitivity and implementing pricing decisions based on behavioral insights. Valuable for practitioners working on product monetization and pricing optimization.",
    "use_cases": [
      "Designing pricing experiments to test customer willingness-to-pay for new product features",
      "Developing behavioral pricing strategies for subscription products or marketplace platforms"
    ],
    "audience": [
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "willingness to pay research methods",
      "behavioral pricing strategies podcast",
      "how to measure customer price sensitivity",
      "pricing optimization interview Madhavan Ramanujam"
    ]
  },
  {
    "name": "Display Advertising with Real-Time Bidding",
    "description": "Free comprehensive RTB coverage on arXiv",
    "category": "Ads & Attribution",
    "url": "https://arxiv.org/abs/1610.03013",
    "type": "Paper",
    "level": "Medium",
    "tags": [
      "Auctions & Market Design",
      "Paper"
    ],
    "domain": "Domain Applications",
    "difficulty": "intermediate",
    "prerequisites": [
      "auction-theory",
      "game-theory",
      "python-optimization"
    ],
    "topic_tags": [
      "real-time-bidding",
      "programmatic-advertising",
      "auction-mechanisms",
      "computational-advertising"
    ],
    "summary": "Comprehensive academic paper covering real-time bidding systems in display advertising, including auction mechanisms, bidding strategies, and market dynamics. Provides theoretical foundations and practical insights for understanding how programmatic ad buying works at scale. Essential reading for anyone working on advertising platforms or auction-based marketplaces.",
    "use_cases": [
      "Building bidding algorithms for demand-side advertising platforms",
      "Designing auction mechanisms for ad exchanges and supply-side platforms"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "real time bidding auction theory",
      "programmatic advertising paper",
      "RTB mechanisms and strategies",
      "display advertising auction design"
    ]
  },
  {
    "name": "GSP Auction Paper (Edelman et al., AER 2007)",
    "description": "Foundational paper on search advertising auctions",
    "category": "Ads & Attribution",
    "url": "https://www.benedelman.org/publications/gsp-060801.pdf",
    "type": "Paper",
    "level": "Medium",
    "tags": [
      "Auctions & Market Design",
      "Paper"
    ],
    "domain": "Domain Applications",
    "difficulty": "intermediate",
    "prerequisites": [
      "game-theory-basics",
      "microeconomics-fundamentals",
      "Nash-equilibrium"
    ],
    "topic_tags": [
      "generalized-second-price",
      "search-advertising",
      "auction-theory",
      "mechanism-design",
      "sponsored-search"
    ],
    "summary": "Seminal paper analyzing the Generalized Second Price (GSP) auction mechanism used by Google AdWords and other search engines. Demonstrates that GSP auctions have multiple equilibria and are not truthful, unlike Vickrey-Clarke-Groves auctions. Essential reading for understanding how billions of dollars in search advertising are allocated and priced.",
    "use_cases": [
      "Designing auction mechanisms for online advertising platforms",
      "Understanding bidding strategies and equilibrium outcomes in search advertising"
    ],
    "audience": [
      "Early-PhD",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "How do Google AdWords auctions work",
      "GSP auction mechanism design paper",
      "Search advertising auction theory Edelman",
      "Generalized second price auction equilibrium"
    ]
  },
  {
    "name": "AdKDD Workshop Papers",
    "description": "Applied research from Google, Meta, Amazon",
    "category": "Ads & Attribution",
    "url": "https://www.adkdd.org/",
    "type": "Paper",
    "level": "Medium",
    "tags": [
      "Auctions & Market Design",
      "Paper"
    ],
    "domain": "Domain Applications",
    "difficulty": "advanced",
    "prerequisites": [
      "auction-theory",
      "causal-inference",
      "machine-learning-systems"
    ],
    "topic_tags": [
      "ad-auctions",
      "attribution-modeling",
      "computational-advertising",
      "marketplace-design",
      "applied-research"
    ],
    "summary": "Collection of cutting-edge research papers from industry practitioners at major tech companies focusing on advertising technology and attribution challenges. These papers bridge academic theory with real-world implementation constraints at scale. Covers topics like auction mechanisms, attribution modeling, and marketplace optimization with billions of users.",
    "use_cases": [
      "Understanding how Google/Meta solve attribution problems at scale",
      "Learning state-of-the-art auction design techniques used in production"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "latest research on ad auction mechanisms",
      "how does Google do attribution modeling",
      "industry papers on computational advertising",
      "AdKDD workshop best papers"
    ]
  },
  {
    "name": "PyMC-Marketing CLV Quickstart",
    "description": "CLV basics, RFM analysis, BG/NBD models \u2014 free official docs",
    "category": "Bayesian Methods",
    "url": "https://www.pymc-marketing.io/en/latest/notebooks/clv/clv_quickstart.html",
    "type": "Article",
    "level": "Medium",
    "tags": [
      "Strategy & Analytics",
      "Article"
    ],
    "domain": "Statistics",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "bayesian-statistics",
      "customer-analytics"
    ],
    "topic_tags": [
      "customer-lifetime-value",
      "bayesian-modeling",
      "rfm-analysis",
      "bg-nbd-model",
      "pymc"
    ],
    "summary": "Official PyMC-Marketing documentation covering customer lifetime value (CLV) fundamentals, RFM segmentation, and implementation of Beta-Geometric/Negative Binomial Distribution models. Provides hands-on tutorial for applying Bayesian methods to customer behavior modeling and revenue forecasting. Ideal for data scientists wanting to implement probabilistic CLV models in production.",
    "use_cases": [
      "Predicting future revenue from existing customers for subscription businesses",
      "Segmenting customers by purchase behavior to optimize marketing spend allocation"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "how to calculate customer lifetime value with bayesian methods",
      "PyMC marketing CLV tutorial",
      "BG/NBD model implementation python",
      "RFM analysis with probabilistic modeling"
    ]
  },
  {
    "name": "PyMC-Marketing Documentation",
    "description": "BG/NBD and Gamma-Gamma CLV tutorials",
    "category": "Bayesian Methods",
    "url": "https://www.pymc-marketing.io/en/stable/",
    "type": "Article",
    "level": "Medium",
    "tags": [
      "Strategy & Analytics",
      "Article"
    ],
    "domain": "Statistics",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-programming",
      "bayesian-inference",
      "customer-segmentation"
    ],
    "topic_tags": [
      "customer-lifetime-value",
      "bayesian-modeling",
      "marketing-analytics",
      "pymc",
      "clv-modeling"
    ],
    "summary": "PyMC-Marketing documentation provides comprehensive tutorials for implementing BG/NBD (Beta-Geometric/Negative Binomial Distribution) and Gamma-Gamma models for customer lifetime value analysis. These Bayesian approaches help businesses predict customer purchase behavior and monetary value over time. The documentation includes practical examples and code implementations for marketers and data scientists working on customer analytics.",
    "use_cases": [
      "E-commerce company wants to predict which customers will make repeat purchases and their expected value",
      "SaaS business needs to model customer churn probability and optimize retention spending"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "How to implement BG/NBD model for customer lifetime value",
      "PyMC customer lifetime value tutorial",
      "Bayesian CLV modeling with Python",
      "Gamma-Gamma model for customer monetary value prediction"
    ]
  },
  {
    "name": "Google Analytics for Marketing",
    "description": "Free analytics for marketing \u2014 official Google tutorials",
    "category": "Frameworks & Strategy",
    "url": "https://skillshop.exceedlms.com/student/path/508845-google-analytics-certification",
    "type": "Course",
    "level": "Medium",
    "tags": [
      "Strategy & Analytics",
      "Course"
    ],
    "domain": "Product Sense",
    "difficulty": "beginner",
    "prerequisites": [
      "web-analytics-basics",
      "digital-marketing-fundamentals"
    ],
    "topic_tags": [
      "google-analytics",
      "web-analytics",
      "marketing-attribution",
      "conversion-tracking",
      "tutorial"
    ],
    "summary": "Official Google tutorials covering Google Analytics fundamentals for marketing applications. Teaches how to set up tracking, measure website performance, and analyze user behavior to optimize marketing campaigns. Ideal for practitioners new to web analytics who need to understand customer acquisition and conversion metrics.",
    "use_cases": [
      "Setting up conversion tracking for e-commerce campaigns to measure ROI",
      "Analyzing user journey data to optimize marketing funnel performance"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "Google Analytics tutorial for beginners",
      "how to track marketing campaign performance",
      "web analytics course for data scientists",
      "Google Analytics setup guide"
    ]
  },
  {
    "name": "Bruce Hardie's CLV Papers",
    "description": "Mathematical foundations of CLV models",
    "category": "Growth & Retention",
    "url": "https://www.brucehardie.com/",
    "type": "Paper",
    "level": "Medium",
    "tags": [
      "Strategy & Analytics",
      "Paper"
    ],
    "domain": "Domain Applications",
    "difficulty": "advanced",
    "prerequisites": [
      "probability-theory",
      "maximum-likelihood-estimation",
      "beta-geometric-models"
    ],
    "topic_tags": [
      "customer-lifetime-value",
      "probabilistic-models",
      "retention-modeling",
      "clv-mathematics",
      "academic-papers"
    ],
    "summary": "Bruce Hardie's seminal academic papers establishing the mathematical foundations for customer lifetime value modeling, particularly the BG/NBD and Pareto/NBD models. These papers provide rigorous probabilistic frameworks for predicting customer behavior and calculating CLV in contractual and non-contractual settings. Essential reading for understanding the theoretical underpinnings of modern CLV approaches.",
    "use_cases": [
      "Building custom CLV models from first principles for unique business contexts",
      "Academic research extending or validating probabilistic customer behavior models"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "Bruce Hardie CLV mathematical foundations",
      "BG/NBD Pareto/NBD original papers",
      "probabilistic customer lifetime value models theory",
      "mathematical derivation of CLV models"
    ]
  },
  {
    "name": "MIT 15.S08 FinTech (Gary Gensler)",
    "description": "12 lectures from former SEC Chair on AI in finance, risk, and compliance \u2014 free",
    "category": "Case Studies",
    "url": "https://ocw.mit.edu/courses/15-s08-fintech-shaping-the-financial-world-spring-2020/",
    "type": "Course",
    "level": "Medium",
    "tags": [
      "ML & Data Science",
      "Course"
    ],
    "domain": "Product Sense",
    "difficulty": "intermediate",
    "prerequisites": [
      "basic-finance",
      "regulatory-frameworks",
      "python-basics"
    ],
    "topic_tags": [
      "fintech",
      "financial-regulation",
      "AI-governance",
      "compliance",
      "risk-management"
    ],
    "summary": "12-lecture course from former SEC Chair Gary Gensler covering AI applications in finance, regulatory compliance, and risk management. Provides regulatory perspective on fintech innovations including machine learning, blockchain, and algorithmic trading. Ideal for understanding how financial technology intersects with policy and compliance requirements.",
    "use_cases": [
      "Understanding regulatory constraints when building ML models for financial products",
      "Learning compliance requirements for AI-driven trading or lending algorithms"
    ],
    "audience": [
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "fintech regulation course",
      "AI in finance compliance",
      "Gary Gensler fintech lectures",
      "financial technology risk management course"
    ]
  },
  {
    "name": "Stripe: ML for Fraud Protection",
    "description": "The definitive intro: features, precision-recall tradeoffs, break-even calculations",
    "category": "Trust & Safety",
    "url": "https://stripe.com/guides/primer-on-machine-learning-for-fraud-protection",
    "type": "Article",
    "level": "Medium",
    "tags": [
      "ML & Data Science",
      "Article"
    ],
    "domain": "Domain Applications",
    "difficulty": "intermediate",
    "prerequisites": [
      "supervised-learning",
      "classification-metrics",
      "feature-engineering"
    ],
    "topic_tags": [
      "fraud-detection",
      "precision-recall",
      "feature-selection",
      "break-even-analysis",
      "payments"
    ],
    "summary": "Stripe's comprehensive guide to building machine learning systems for fraud detection in payment processing. Covers practical feature engineering, model evaluation using precision-recall curves, and economic break-even analysis to optimize fraud prevention systems. Essential reading for understanding real-world ML applications in fintech trust and safety.",
    "use_cases": [
      "Building fraud detection models for payment processors or e-commerce platforms",
      "Optimizing precision-recall tradeoffs in high-stakes classification problems with financial impact"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "fraud detection machine learning stripe",
      "precision recall tradeoffs payment fraud",
      "break even analysis fraud prevention",
      "feature engineering financial fraud detection"
    ]
  },
  {
    "name": "Fraud Detection Handbook (ULB)",
    "description": "From the team that created the Kaggle dataset \u2014 rigorous methodology",
    "category": "Trust & Safety",
    "url": "https://fraud-detection-handbook.github.io/fraud-detection-handbook/",
    "type": "Tool",
    "level": "Medium",
    "tags": [
      "ML & Data Science",
      "Tool"
    ],
    "domain": "Domain Applications",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-scikit-learn",
      "classification-algorithms",
      "imbalanced-datasets"
    ],
    "topic_tags": [
      "fraud-detection",
      "anomaly-detection",
      "imbalanced-learning",
      "financial-risk",
      "methodology"
    ],
    "summary": "A comprehensive handbook on fraud detection methodologies from the University of Brussels team behind the popular Kaggle credit card fraud dataset. Covers practical implementation of machine learning techniques for fraud detection, with emphasis on handling class imbalance and evaluation metrics specific to fraud use cases. Provides rigorous statistical approaches and real-world case studies for building production fraud detection systems.",
    "use_cases": [
      "Building credit card transaction fraud detection system for fintech company",
      "Implementing real-time anomaly detection for e-commerce payment processing"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "fraud detection machine learning handbook",
      "how to handle imbalanced fraud datasets",
      "credit card fraud detection methodology",
      "anomaly detection for financial transactions"
    ]
  },
  {
    "name": "Stripe: How We Built Radar",
    "description": "XGBoost\u2192DNN migration, 85% training time reduction",
    "category": "Case Studies",
    "url": "https://stripe.com/blog/how-we-built-it-stripe-radar",
    "type": "Article",
    "level": "Medium",
    "tags": [
      "ML & Data Science",
      "Blog"
    ],
    "domain": "Product Sense",
    "difficulty": "intermediate",
    "prerequisites": [
      "xgboost",
      "neural-networks",
      "model-deployment"
    ],
    "topic_tags": [
      "fraud-detection",
      "model-migration",
      "xgboost",
      "deep-learning",
      "fintech"
    ],
    "summary": "Stripe's case study on migrating their fraud detection system Radar from XGBoost to deep neural networks, achieving 85% reduction in training time while maintaining performance. The post details the technical challenges, architectural decisions, and lessons learned during this large-scale ML system transition at a major fintech company.",
    "use_cases": [
      "Migrating legacy ML models to more efficient architectures in production systems",
      "Optimizing training performance for large-scale fraud detection or risk scoring models"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "stripe radar xgboost neural network migration",
      "how to migrate xgboost to deep learning production",
      "fraud detection model architecture stripe",
      "reducing ML training time case study"
    ]
  },
  {
    "name": "PayPal: Graph Database for Fraud",
    "description": "Real-time fraud ring detection",
    "category": "Trust & Safety",
    "url": "https://medium.com/paypal-tech/how-paypal-uses-real-time-graph-database-and-graph-analysis-to-fight-fraud-96a2b918619a",
    "type": "Article",
    "level": "Medium",
    "tags": [
      "ML & Data Science",
      "Blog"
    ],
    "domain": "Domain Applications",
    "difficulty": "intermediate",
    "prerequisites": [
      "graph-databases",
      "neo4j",
      "fraud-detection-basics"
    ],
    "topic_tags": [
      "graph-databases",
      "fraud-detection",
      "real-time-systems",
      "paypal",
      "trust-safety"
    ],
    "summary": "PayPal's implementation of graph databases for detecting fraud rings in real-time by analyzing relationships between users, devices, and transactions. The system identifies suspicious patterns and connections that traditional rule-based systems might miss. Demonstrates practical application of graph technology at scale for financial crime prevention.",
    "use_cases": [
      "Building fraud detection systems that identify coordinated attacks across multiple accounts",
      "Implementing real-time risk scoring based on network effects and user relationships"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "how does PayPal detect fraud using graph databases",
      "graph database fraud detection implementation",
      "real-time fraud ring detection methods",
      "PayPal fraud prevention system architecture"
    ]
  },
  {
    "name": "LinkedIn: Defending Against Abuse at Scale",
    "description": "4M+ TPS, multi-layer defense architecture",
    "category": "Trust & Safety",
    "url": "https://engineering.linkedin.com/blog/2018/12/defending-against-abuse-at-linkedins-scale",
    "type": "Article",
    "level": "Medium",
    "tags": [
      "ML & Data Science",
      "Blog"
    ],
    "domain": "Domain Applications",
    "difficulty": "intermediate",
    "prerequisites": [
      "machine-learning-classification",
      "system-design-fundamentals",
      "python-scikit-learn"
    ],
    "topic_tags": [
      "abuse-detection",
      "fraud-prevention",
      "system-architecture",
      "real-time-ml",
      "content-moderation"
    ],
    "summary": "LinkedIn's technical deep-dive into their multi-layered abuse detection system that processes over 4 million transactions per second. The post covers their defense architecture combining rule-based systems, machine learning models, and real-time processing to combat spam, fake accounts, and malicious behavior. Essential reading for data scientists working on trust and safety problems at scale.",
    "use_cases": [
      "Building fraud detection systems for social platforms or marketplaces",
      "Designing real-time content moderation pipelines for user-generated content"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "How does LinkedIn detect fake accounts and spam",
      "Multi-layer abuse detection system architecture",
      "Real-time fraud detection at scale LinkedIn",
      "Trust and safety machine learning systems"
    ]
  },
  {
    "name": "Meta: Few-Shot Learner for Harmful Content",
    "description": "Adapts to new threats in weeks, 100+ languages",
    "category": "Case Studies",
    "url": "https://about.fb.com/news/2021/12/metas-new-ai-system-tackles-harmful-content/",
    "type": "Article",
    "level": "Medium",
    "tags": [
      "ML & Data Science",
      "Article"
    ],
    "domain": "Product Sense",
    "difficulty": "intermediate",
    "prerequisites": [
      "few-shot-learning",
      "content-moderation-systems",
      "transformer-models"
    ],
    "topic_tags": [
      "few-shot-learning",
      "content-moderation",
      "multilingual-ml",
      "meta-research"
    ],
    "summary": "Meta's approach to building content moderation systems that can quickly adapt to new types of harmful content using few-shot learning techniques. The system works across 100+ languages and can be deployed within weeks rather than months when new threats emerge. This represents a practical application of few-shot learning at massive scale in production environments.",
    "use_cases": [
      "Rapidly deploying models to detect emerging harmful content trends like new harassment patterns or misinformation tactics",
      "Scaling content moderation to new languages or regions without extensive labeled training data"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "How does Meta handle new types of harmful content detection",
      "Few shot learning for content moderation at scale",
      "Multilingual content moderation systems",
      "Adapting ML models quickly to new online threats"
    ]
  },
  {
    "name": "Netflix: RAD Outlier Detection",
    "description": "Robust PCA at terabyte scale",
    "category": "Trust & Safety",
    "url": "https://netflixtechblog.com/rad-outlier-detection-on-big-data-d6b0ff32fb44",
    "type": "Article",
    "level": "Medium",
    "tags": [
      "ML & Data Science",
      "Blog"
    ],
    "domain": "Domain Applications",
    "difficulty": "intermediate",
    "prerequisites": [
      "principal-component-analysis",
      "python-sklearn",
      "distributed-computing"
    ],
    "topic_tags": [
      "robust-pca",
      "outlier-detection",
      "scalable-ml",
      "anomaly-detection",
      "netflix-engineering"
    ],
    "summary": "Netflix's Robust Anomaly Detection (RAD) system uses robust Principal Component Analysis to identify outliers in massive datasets at terabyte scale. The system handles high-dimensional data with noise and missing values to detect anomalous patterns in streaming behavior and content metrics. It's designed for production environments requiring both accuracy and computational efficiency.",
    "use_cases": [
      "detecting fraudulent account activity in streaming platform user behavior",
      "identifying anomalous content performance metrics across millions of titles"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "robust PCA outlier detection at scale",
      "Netflix anomaly detection system",
      "terabyte scale outlier detection methods",
      "robust principal component analysis production implementation"
    ]
  },
  {
    "name": "Google Research: Self-Supervised Anomaly Detection",
    "description": "Contrastive learning, CutPaste algorithm",
    "category": "Trust & Safety",
    "url": "https://ai.googleblog.com/2021/09/discovering-anomalous-data-with-self.html",
    "type": "Article",
    "level": "Medium",
    "tags": [
      "ML & Data Science",
      "Blog"
    ],
    "domain": "Domain Applications",
    "difficulty": "intermediate",
    "prerequisites": [
      "pytorch-training",
      "computer-vision",
      "contrastive-learning"
    ],
    "topic_tags": [
      "self-supervised-learning",
      "anomaly-detection",
      "contrastive-learning",
      "cutpaste-algorithm",
      "trust-safety"
    ],
    "summary": "Google Research's approach to anomaly detection using self-supervised learning, specifically the CutPaste algorithm that learns normal patterns through contrastive learning. The method trains models to distinguish between normal samples and artificially corrupted versions, enabling detection of anomalies without labeled defect data. Particularly useful for trust and safety applications where anomalous content or behavior needs to be identified.",
    "use_cases": [
      "detecting fraudulent user accounts or suspicious activity patterns on platforms",
      "identifying defective products or quality issues in manufacturing without labeled defect examples"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "self supervised anomaly detection methods",
      "CutPaste algorithm implementation",
      "contrastive learning for fraud detection",
      "Google Research anomaly detection trust safety"
    ]
  },
  {
    "name": "Stanford FinTech Lab: Rob Wang (Block)",
    "description": "Industry talk on fraud ML tradeoffs",
    "category": "Trust & Safety",
    "url": "https://fintech.stanford.edu/events/aftlab-seminars/rob-wang-square-machine-learning-financial-fraud-detection",
    "type": "Article",
    "level": "Medium",
    "tags": [
      "ML & Data Science",
      "Article"
    ],
    "domain": "Domain Applications",
    "difficulty": "intermediate",
    "prerequisites": [
      "scikit-learn",
      "precision-recall-curves",
      "feature-engineering"
    ],
    "topic_tags": [
      "fraud-detection",
      "ml-tradeoffs",
      "fintech",
      "industry-talk",
      "trust-safety"
    ],
    "summary": "Industry talk by Rob Wang from Block discussing machine learning tradeoffs in fraud detection systems. Covers practical challenges of balancing false positives vs false negatives in production fraud models. Provides real-world insights from Block's experience building scalable fraud prevention systems.",
    "use_cases": [
      "Building fraud detection models for payment platforms",
      "Optimizing precision-recall tradeoffs in financial risk systems"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "fraud detection machine learning tradeoffs",
      "Block fraud ML Rob Wang",
      "fintech fraud detection best practices",
      "precision recall fraud models production"
    ]
  },
  {
    "name": "IEEE-CIS Fraud: 1st Place Solution (Chris Deotte)",
    "description": "Kaggle Grandmaster, 262 features, RAPIDS GPU",
    "category": "Trust & Safety",
    "url": "https://developer.nvidia.com/blog/leveraging-machine-learning-to-detect-fraud-tips-to-developing-a-winning-kaggle-solution/",
    "type": "Article",
    "level": "Medium",
    "tags": [
      "ML & Data Science",
      "Blog"
    ],
    "domain": "Domain Applications",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "xgboost",
      "feature-engineering"
    ],
    "topic_tags": [
      "fraud-detection",
      "kaggle-competition",
      "feature-engineering",
      "gpu-acceleration",
      "RAPIDS"
    ],
    "summary": "A detailed walkthrough of the winning solution for the IEEE-CIS Fraud Detection Kaggle competition by Chris Deotte, featuring 262 engineered features and RAPIDS GPU acceleration. The solution demonstrates advanced feature engineering techniques and optimization strategies for large-scale fraud detection problems. Provides practical insights into competition-winning approaches that can be applied to real-world fraud detection systems.",
    "use_cases": [
      "Building fraud detection models for payment processors or financial institutions",
      "Learning advanced feature engineering techniques for tabular data competitions"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "kaggle fraud detection winning solution",
      "how to engineer features for fraud detection",
      "RAPIDS GPU fraud detection implementation",
      "Chris Deotte kaggle competition approach"
    ]
  },
  {
    "name": "scikit-learn: Outlier Detection",
    "description": "Isolation Forest, LOF, One-Class SVM comparison",
    "category": "Trust & Safety",
    "url": "https://scikit-learn.org/stable/modules/outlier_detection.html",
    "type": "Article",
    "level": "Medium",
    "tags": [
      "ML & Data Science",
      "Article"
    ],
    "domain": "Domain Applications",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-scikit-learn",
      "unsupervised-learning",
      "pandas-dataframes"
    ],
    "topic_tags": [
      "outlier-detection",
      "anomaly-detection",
      "isolation-forest",
      "one-class-svm",
      "trust-safety"
    ],
    "summary": "This resource compares three popular scikit-learn outlier detection methods: Isolation Forest, Local Outlier Factor (LOF), and One-Class SVM. It's designed for data scientists working on trust and safety problems who need to identify anomalous behavior in their datasets. The comparison helps practitioners choose the right algorithm based on their specific data characteristics and performance requirements.",
    "use_cases": [
      "detecting fraudulent user accounts or transactions in marketplace platforms",
      "identifying spam content or abusive behavior in social media feeds"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "how to detect outliers in scikit-learn",
      "isolation forest vs one class svm comparison",
      "best outlier detection algorithm for fraud detection",
      "scikit-learn anomaly detection methods tutorial"
    ]
  },
  {
    "name": "Music Tomorrow: Spotify Deep Dive",
    "description": "Audio features, accessible depth",
    "category": "Case Studies",
    "url": "https://www.music-tomorrow.com/blog/how-spotify-recommendation-system-works-complete-guide",
    "type": "Article",
    "level": "Medium",
    "tags": [
      "ML & Data Science",
      "Blog"
    ],
    "domain": "Product Sense",
    "difficulty": "beginner",
    "prerequisites": [
      "python-pandas",
      "basic-ML-concepts",
      "data-visualization"
    ],
    "topic_tags": [
      "spotify",
      "audio-features",
      "music-analytics",
      "recommendation-systems",
      "case-study"
    ],
    "summary": "A deep dive case study exploring Spotify's use of audio features and data science techniques in music recommendation and analysis. Provides accessible explanations of how music streaming platforms leverage machine learning and audio data. Ideal for understanding real-world applications of data science in the entertainment industry.",
    "use_cases": [
      "Learning how recommendation systems work in practice at major tech companies",
      "Understanding audio feature engineering for music analysis projects"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "How does Spotify recommendation algorithm work",
      "Spotify audio features machine learning",
      "Music streaming data science case study",
      "Audio data analysis techniques Spotify"
    ]
  },
  {
    "name": "Georgia Tech (Ratliff): 10 Rules for Supply Chain Optimization",
    "description": "Practitioner checklist for scoping, data readiness, constraints, deployment \u2014 free PDF",
    "category": "Routing & Logistics",
    "url": "https://www.scl.gatech.edu/sites/default/files/downloads/gtscl-10_rules_supply_chain_logistics_optimization_2.pdf",
    "type": "Article",
    "level": "Medium",
    "tags": [
      "Optimization",
      "Article"
    ],
    "domain": "Optimization & Operations Research",
    "difficulty": "beginner",
    "prerequisites": [
      "linear-programming",
      "python-pandas",
      "SQL-joins"
    ],
    "topic_tags": [
      "supply-chain",
      "optimization-checklist",
      "practitioner-guide",
      "deployment",
      "scoping"
    ],
    "summary": "A practical checklist from Georgia Tech providing 10 essential rules for successfully implementing supply chain optimization projects. Covers project scoping, data preparation, constraint identification, and deployment considerations for practitioners new to optimization work. Serves as a field guide to avoid common pitfalls when moving from theoretical optimization to real-world supply chain problems.",
    "use_cases": [
      "Setting up a warehouse routing optimization project and need to validate data quality and business constraints",
      "Leading a supply chain cost reduction initiative and want to ensure proper project scoping before building models"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "supply chain optimization best practices checklist",
      "how to scope optimization projects in logistics",
      "data requirements for supply chain optimization",
      "deployment guide for routing optimization models"
    ]
  },
  {
    "name": "Google OR-Tools: VRP + VRPTW Tutorial",
    "description": "Core logistics vocabulary (depot, fleet, constraints) with working Python baseline",
    "category": "Linear Programming",
    "url": "https://developers.google.com/optimization/routing/vrp",
    "type": "Article",
    "level": "Medium",
    "tags": [
      "Optimization",
      "Article"
    ],
    "domain": "Optimization & Operations Research",
    "difficulty": "beginner",
    "prerequisites": [
      "python-basics",
      "graph-theory-fundamentals"
    ],
    "topic_tags": [
      "vehicle-routing",
      "constraint-programming",
      "logistics-optimization",
      "OR-tools",
      "tutorial"
    ],
    "summary": "A hands-on tutorial covering Google OR-Tools for solving Vehicle Routing Problems (VRP) and Vehicle Routing Problems with Time Windows (VRPTW). Introduces core logistics concepts like depots, fleets, and routing constraints with practical Python implementations. Perfect for data scientists entering logistics optimization or operations research applications.",
    "use_cases": [
      "Optimizing delivery routes for e-commerce last-mile logistics",
      "Scheduling service technician visits with time window constraints"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "how to solve vehicle routing problem python",
      "OR-Tools VRP tutorial beginner",
      "delivery route optimization with time windows",
      "Google OR-Tools logistics optimization guide"
    ]
  },
  {
    "name": "DoorDash: ML + Optimization for Dispatch",
    "description": "Clearest 'real system' explanation: predictions feed optimizer, then simulation closes the loop",
    "category": "Routing & Logistics",
    "url": "https://careersatdoordash.com/blog/using-ml-and-optimization-to-solve-doordashs-dispatch-problem/",
    "type": "Article",
    "level": "Medium",
    "tags": [
      "Optimization",
      "Blog"
    ],
    "domain": "Optimization & Operations Research",
    "difficulty": "intermediate",
    "prerequisites": [
      "linear-programming",
      "python-optimization",
      "discrete-choice-models"
    ],
    "topic_tags": [
      "dispatch-optimization",
      "real-time-matching",
      "simulation",
      "delivery-logistics",
      "blog-post"
    ],
    "summary": "DoorDash's technical blog post explaining how machine learning predictions feed into optimization algorithms for driver dispatch, with simulation validating the complete system. Provides rare insight into how prediction and optimization components work together in a production logistics system. Ideal for understanding the ML-to-optimization pipeline in real-time matching problems.",
    "use_cases": [
      "Building driver-rider matching systems for ride sharing platforms",
      "Designing delivery optimization systems that balance predicted demand with routing constraints"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "How does machine learning connect to optimization in dispatch systems",
      "Real world examples of ML feeding optimization algorithms",
      "DoorDash technical architecture for driver matching",
      "Production systems combining prediction and optimization"
    ]
  },
  {
    "name": "Instacart: Predicting Availability of 200M Grocery Items",
    "description": "XGBoost with 130 features scoring 200M+ items every 60 minutes. 15x items with 1/5 resources.",
    "category": "Case Studies",
    "url": "https://tech.instacart.com/predicting-real-time-availability-of-200-million-grocery-items-in-us-canada-stores-61f43a16eafe",
    "type": "Article",
    "level": "Medium",
    "tags": [
      "Forecasting",
      "Blog"
    ],
    "domain": "Product Sense",
    "difficulty": "intermediate",
    "prerequisites": [
      "xgboost",
      "python-pandas",
      "feature-engineering"
    ],
    "topic_tags": [
      "demand-forecasting",
      "inventory-management",
      "xgboost",
      "real-time-ml",
      "grocery-retail"
    ],
    "summary": "Instacart's case study on building a real-time availability prediction system using XGBoost with 130 features to score 200M+ grocery items every hour. The system achieved 15x scale improvement while using only 1/5 of the original computational resources. Details implementation challenges and solutions for high-frequency demand forecasting in grocery retail.",
    "use_cases": [
      "Building real-time inventory availability systems for e-commerce platforms",
      "Implementing large-scale demand forecasting for retail operations"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "instacart inventory prediction case study",
      "real-time demand forecasting at scale",
      "xgboost for grocery availability prediction",
      "how to predict product availability millions of items"
    ]
  },
  {
    "name": "MIT 15.053: Optimization Methods in Management Science",
    "description": "Undergraduate course on LP with geometry and visualization before algebra. Interactive spreadsheet exercises.",
    "category": "Convex Optimization",
    "url": "https://ocw.mit.edu/courses/15-053-optimization-methods-in-management-science-spring-2013/",
    "type": "Course",
    "level": "Easy",
    "tags": [
      "Optimization",
      "Course"
    ],
    "domain": "Optimization & Operations Research",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-algebra",
      "excel-spreadsheets"
    ],
    "topic_tags": [
      "linear-programming",
      "optimization-fundamentals",
      "interactive-learning",
      "management-science",
      "geometric-interpretation"
    ],
    "summary": "MIT's undergraduate optimization course that teaches linear programming through geometric visualization and interactive spreadsheet exercises before diving into algebraic methods. Designed to build intuition for optimization problems in business and management contexts. Perfect for learning the foundations of constrained optimization with hands-on practice.",
    "use_cases": [
      "Learning linear programming fundamentals with visual intuition before tackling complex optimization algorithms",
      "Understanding resource allocation and production planning problems in business contexts"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "beginner friendly linear programming course",
      "optimization course with spreadsheet exercises",
      "learn linear programming with geometric visualization",
      "MIT optimization methods management science course"
    ]
  },
  {
    "name": "QuantEcon: Linear Programming Introduction",
    "description": "Python notebooks from Nobel Laureate Sargent. LP with economics applications using SciPy and OR-Tools.",
    "category": "Linear Programming",
    "url": "https://intro.quantecon.org/lp_intro.html",
    "type": "Tutorial",
    "level": "Easy",
    "tags": [
      "Optimization",
      "Tutorial"
    ],
    "domain": "Optimization & Operations Research",
    "difficulty": "beginner",
    "prerequisites": [
      "python-basics",
      "scipy-fundamentals",
      "basic-microeconomics"
    ],
    "topic_tags": [
      "linear-programming",
      "optimization-tutorial",
      "economics-applications",
      "python-notebooks",
      "quantecon"
    ],
    "summary": "Interactive Python notebooks teaching linear programming fundamentals through economics applications, created by Nobel Laureate Thomas Sargent. Covers LP theory and implementation using SciPy and OR-Tools with real economic examples. Perfect introduction to optimization methods for economists and data scientists.",
    "use_cases": [
      "Resource allocation problems in economics research",
      "Learning optimization fundamentals for data science interviews"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "linear programming tutorial for economists",
      "python optimization with scipy examples",
      "thomas sargent quantecon linear programming",
      "beginner guide to LP in economics"
    ]
  },
  {
    "name": "Real Python: Linear Programming with Python",
    "description": "Comprehensive tutorial covering visualization, feasible regions, SciPy, PuLP, and mixed-integer programming.",
    "category": "Linear Programming",
    "url": "https://realpython.com/linear-programming-python/",
    "type": "Tutorial",
    "level": "Easy",
    "tags": [
      "Optimization",
      "Tutorial"
    ],
    "domain": "Optimization & Operations Research",
    "difficulty": "beginner",
    "prerequisites": [
      "python-basics",
      "numpy-arrays",
      "basic-algebra"
    ],
    "topic_tags": [
      "linear-programming",
      "optimization",
      "scipy",
      "pulp",
      "mixed-integer-programming"
    ],
    "summary": "A comprehensive Python tutorial that teaches linear programming fundamentals through practical implementation using SciPy and PuLP libraries. Covers problem visualization, feasible region analysis, and extends to mixed-integer programming scenarios. Perfect for data scientists and economists who need to solve constrained optimization problems in their work.",
    "use_cases": [
      "Resource allocation optimization in tech companies (budget, staffing, server capacity)",
      "Supply chain optimization for marketplace platforms (inventory, shipping, cost minimization)"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python linear programming tutorial",
      "how to solve optimization problems with scipy",
      "linear programming with pulp python",
      "mixed integer programming python guide"
    ]
  },
  {
    "name": "SciPy Lecture Notes: Mathematical Optimization",
    "description": "Academic tutorial with visual explanations. Gradient descent, BFGS, Nelder-Mead with convergence visualizations.",
    "category": "Convex Optimization",
    "url": "https://scipy-lectures.org/advanced/mathematical_optimization/",
    "type": "Tutorial",
    "level": "Medium",
    "tags": [
      "Optimization",
      "Tutorial"
    ],
    "domain": "Optimization & Operations Research",
    "difficulty": "beginner",
    "prerequisites": [
      "python-basics",
      "calculus",
      "numpy"
    ],
    "topic_tags": [
      "gradient-descent",
      "BFGS",
      "nelder-mead",
      "numerical-optimization",
      "scipy"
    ],
    "summary": "Academic tutorial covering fundamental optimization algorithms with visual demonstrations of convergence behavior. Explains gradient descent, BFGS, and Nelder-Mead methods through interactive examples and mathematical foundations. Ideal for learning the core optimization techniques used across machine learning and data science.",
    "use_cases": [
      "Implementing custom loss function optimization for machine learning models",
      "Finding optimal parameters for economic models or pricing algorithms"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "gradient descent tutorial with examples",
      "scipy optimization methods comparison",
      "BFGS vs Nelder-Mead algorithm explained",
      "mathematical optimization beginner guide python"
    ]
  },
  {
    "name": "PuLP Official Documentation",
    "description": "Complete LP/MIP documentation with case studies: blending problem, Sudoku, transportation. Multiple solver support.",
    "category": "Linear Programming",
    "url": "https://coin-or.github.io/pulp/",
    "type": "Documentation",
    "level": "Easy",
    "tags": [
      "Optimization",
      "Documentation"
    ],
    "domain": "Optimization & Operations Research",
    "difficulty": "beginner",
    "prerequisites": [
      "python-basics",
      "linear-algebra"
    ],
    "topic_tags": [
      "linear-programming",
      "optimization",
      "python-pulp",
      "mixed-integer-programming",
      "documentation"
    ],
    "summary": "Official documentation for PuLP, a Python library for linear and mixed-integer programming problems. Includes comprehensive tutorials, API reference, and practical case studies like supply chain optimization and resource allocation. Essential reference for anyone implementing optimization solutions in Python.",
    "use_cases": [
      "Optimizing product mix and resource allocation in manufacturing",
      "Solving transportation and logistics routing problems"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "How to use PuLP for linear programming in Python",
      "PuLP documentation and tutorials",
      "Python optimization library examples",
      "Linear programming solver implementation guide"
    ]
  },
  {
    "name": "Google OR-Tools Python Guide",
    "description": "Official documentation with setup and examples. CP-SAT solver won MiniZinc Challenge 2013-2024.",
    "category": "Linear Programming",
    "url": "https://developers.google.com/optimization/introduction/python",
    "type": "Documentation",
    "level": "Medium",
    "tags": [
      "Optimization",
      "Documentation"
    ],
    "domain": "Optimization & Operations Research",
    "difficulty": "beginner",
    "prerequisites": [
      "python-basics",
      "linear-algebra",
      "constraint-programming-concepts"
    ],
    "topic_tags": [
      "constraint-programming",
      "mixed-integer-programming",
      "operations-research",
      "python-library",
      "solver-documentation"
    ],
    "summary": "Official Python documentation for Google OR-Tools, featuring the award-winning CP-SAT constraint solver. Provides setup instructions, API reference, and practical examples for solving optimization problems. Essential starting point for anyone implementing operations research solutions in Python.",
    "use_cases": [
      "Optimizing delivery routes and scheduling for logistics companies",
      "Resource allocation and capacity planning for cloud infrastructure"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "How to get started with Google OR-Tools in Python",
      "CP-SAT solver tutorial and examples",
      "Python optimization library documentation",
      "Google OR-Tools setup guide and best practices"
    ]
  },
  {
    "name": "DoorDash: Next-Generation Dasher Dispatch Optimization",
    "description": "Rare solver benchmarking transparency \u2014 compares CBC, XPress, CPLEX, Gurobi (34x faster than CBC).",
    "category": "Linear Programming",
    "url": "https://careersatdoordash.com/blog/next-generation-optimization-for-dasher-dispatch-at-doordash/",
    "type": "Article",
    "level": "Medium",
    "tags": [
      "Optimization",
      "Blog"
    ],
    "domain": "Optimization & Operations Research",
    "difficulty": "intermediate",
    "prerequisites": [
      "linear-programming",
      "optimization-solvers",
      "python-operations-research"
    ],
    "topic_tags": [
      "solver-benchmarking",
      "dispatch-optimization",
      "gurobi",
      "cplex",
      "vehicle-routing"
    ],
    "summary": "DoorDash's engineering blog post comparing performance of four major optimization solvers (CBC, XPress, CPLEX, Gurobi) for their dasher dispatch system. The post provides rare transparency into commercial solver benchmarking, showing Gurobi achieving 34x speedup over open-source CBC. This is valuable for practitioners choosing optimization tools for large-scale logistics problems.",
    "use_cases": [
      "Selecting optimization solver for delivery routing system",
      "Benchmarking commercial vs open-source solvers for production optimization"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "gurobi vs cplex performance comparison",
      "optimization solver benchmarking results",
      "doordash dispatch optimization technical details",
      "best solver for vehicle routing problems"
    ]
  },
  {
    "name": "Uber: Dynamic Pricing and Matching in Ride-Hailing",
    "description": "Academic rigor with industry application. Joint optimization of pricing and matching.",
    "category": "Case Studies",
    "url": "https://www.uber.com/blog/research/dynamic-pricing-and-matching-in-ride-hailing-platforms/",
    "type": "Article",
    "level": "Hard",
    "tags": [
      "Optimization",
      "Blog"
    ],
    "domain": "Product Sense",
    "difficulty": "intermediate",
    "prerequisites": [
      "linear-programming",
      "matching-algorithms",
      "econometric-modeling"
    ],
    "topic_tags": [
      "dynamic-pricing",
      "matching-algorithms",
      "ride-hailing",
      "optimization",
      "case-study"
    ],
    "summary": "Academic case study examining Uber's joint optimization of pricing and driver-rider matching in real-time markets. Combines rigorous economic theory with practical implementation insights for two-sided marketplace design. Particularly valuable for understanding how platform companies balance supply-demand dynamics through algorithmic pricing.",
    "use_cases": [
      "Designing pricing algorithms for marketplace platforms like delivery services or freelance platforms",
      "Optimizing resource allocation in real-time matching systems for transportation or logistics"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "Uber dynamic pricing algorithm case study",
      "how does ride sharing matching optimization work",
      "academic paper on marketplace pricing algorithms",
      "joint optimization pricing and matching rideshare"
    ]
  },
  {
    "name": "MIT 6.046J Lecture 15: Linear Programming",
    "description": "Video intro from algorithmic perspective. LP formulation, reductions, and simplex method.",
    "category": "Linear Programming",
    "url": "https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/resources/lecture-15-linear-programming-lp-reductions-simplex/",
    "type": "Lectures",
    "level": "Medium",
    "tags": [
      "Optimization",
      "Lectures"
    ],
    "domain": "Optimization & Operations Research",
    "difficulty": "intermediate",
    "prerequisites": [
      "linear-algebra",
      "algorithm-complexity",
      "basic-calculus"
    ],
    "topic_tags": [
      "linear-programming",
      "simplex-method",
      "optimization-algorithms",
      "lectures",
      "MIT"
    ],
    "summary": "MIT's algorithmic introduction to linear programming covering problem formulation, constraint optimization, and the simplex method. Designed for computer science students learning optimization from a computational perspective. Provides foundational understanding of how LP solvers work under the hood.",
    "use_cases": [
      "Learning how recommendation systems solve for optimal content allocation",
      "Understanding resource allocation algorithms in cloud computing platforms"
    ],
    "audience": [
      "Junior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "MIT linear programming lecture",
      "simplex method explained",
      "algorithmic approach to linear programming",
      "how does linear programming work"
    ]
  },
  {
    "name": "GILP: Geometric Interpretation of Linear Programs (Cornell)",
    "description": "Academic-grade visualization (ACM SIGCSE 2023). Shows feasible regions, simplex iterations, branch-and-bound.",
    "category": "Linear Programming",
    "url": "https://gilp.henryrobbins.com/",
    "type": "Tool",
    "level": "Easy",
    "tags": [
      "Optimization",
      "Tool"
    ],
    "domain": "Optimization & Operations Research",
    "difficulty": "beginner",
    "prerequisites": [
      "linear-algebra",
      "basic-optimization",
      "constraint-satisfaction"
    ],
    "topic_tags": [
      "linear-programming",
      "optimization-visualization",
      "simplex-algorithm",
      "educational-tool",
      "geometric-interpretation"
    ],
    "summary": "GILP is an interactive visualization tool that helps users understand linear programming through geometric representation of feasible regions and solution methods. It shows how the simplex algorithm and branch-and-bound work visually, making abstract optimization concepts concrete. Perfect for learning how linear programs are solved and building intuition about optimization geometry.",
    "use_cases": [
      "Teaching linear programming concepts to students or new team members",
      "Building intuition before implementing complex optimization models in production"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "how to visualize linear programming problems",
      "geometric interpretation of simplex algorithm",
      "interactive tool for learning optimization",
      "visual explanation of linear programming feasible regions"
    ]
  },
  {
    "name": "Amazon Science: Operations Research and Optimization",
    "description": "Portal to Amazon's OR research on inventory planning, last-mile delivery, and fulfillment at massive scale.",
    "category": "Routing & Logistics",
    "url": "https://www.amazon.science/research-areas/operations-research-and-optimization",
    "type": "Research Portal",
    "level": "Hard",
    "tags": [
      "Optimization",
      "Research Portal"
    ],
    "domain": "Optimization & Operations Research",
    "difficulty": "intermediate",
    "prerequisites": [
      "linear-programming",
      "graph-algorithms",
      "python-optimization"
    ],
    "topic_tags": [
      "operations-research",
      "supply-chain",
      "inventory-optimization",
      "last-mile-delivery",
      "research-portal"
    ],
    "summary": "Amazon Science's operations research portal showcases cutting-edge optimization methods for inventory planning, delivery routing, and warehouse fulfillment at unprecedented scale. The resource provides access to research papers, case studies, and methodologies developed by Amazon's OR teams. It's particularly valuable for practitioners working on supply chain optimization and logistics problems in tech companies.",
    "use_cases": [
      "Designing inventory allocation algorithms for multi-warehouse e-commerce systems",
      "Optimizing delivery route planning for same-day fulfillment services"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "Amazon operations research inventory optimization",
      "last mile delivery algorithms tech companies",
      "supply chain optimization at scale methods",
      "warehouse fulfillment routing optimization research"
    ]
  },
  {
    "name": "Instacart: Delivering Optimal Shopping Experiences (Gurobi)",
    "description": "Why Instacart chose commercial solvers. Reliability and innovation speed from Gurobi.",
    "category": "Linear Programming",
    "url": "https://www.gurobi.com/case_studies/instacart-delivering-optimal-shopping-experiences/",
    "type": "Case Study",
    "level": "Medium",
    "tags": [
      "Optimization",
      "Case Study"
    ],
    "domain": "Optimization & Operations Research",
    "difficulty": "intermediate",
    "prerequisites": [
      "linear-programming",
      "optimization-modeling",
      "operations-research"
    ],
    "topic_tags": [
      "commercial-solvers",
      "gurobi",
      "instacart",
      "supply-chain",
      "case-study"
    ],
    "summary": "A case study explaining why Instacart selected Gurobi's commercial optimization solver over open-source alternatives. Details their decision criteria around reliability, performance, and development speed for production optimization systems. Provides insights into build-vs-buy decisions for optimization infrastructure at scale.",
    "use_cases": [
      "Evaluating commercial vs open-source solvers for production optimization systems",
      "Understanding vendor selection criteria for mission-critical optimization infrastructure"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "why choose commercial optimization solvers over open source",
      "instacart gurobi case study",
      "build vs buy optimization solvers",
      "commercial linear programming solver benefits"
    ]
  },
  {
    "name": "The Cold Start Problem (Andrew Chen)",
    "description": "Atomic Networks and tipping points of two-sided marketplaces \u2014 why growth stalls",
    "category": "Marketplace Economics",
    "url": "https://www.coldstart.com/",
    "type": "Book",
    "level": "Medium",
    "tags": [
      "Strategy & Analytics",
      "Book"
    ],
    "domain": "Domain Applications",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-economics",
      "product-metrics"
    ],
    "topic_tags": [
      "network-effects",
      "marketplace-strategy",
      "growth-frameworks",
      "two-sided-markets",
      "platform-economics"
    ],
    "summary": "A strategic framework book explaining how network-effect businesses overcome initial user acquisition challenges and reach critical mass. Chen explores atomic network theory and provides practical playbooks for building two-sided marketplaces. Essential reading for understanding why most network businesses fail and how successful platforms achieve sustainable growth.",
    "use_cases": [
      "Designing launch strategy for a new marketplace or platform product",
      "Diagnosing why user growth has stalled in an existing two-sided market"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "how do marketplaces solve chicken and egg problem",
      "network effects strategy for platform growth",
      "why do two sided markets fail to get traction",
      "atomic network theory marketplace launch"
    ]
  },
  {
    "name": "Lenny's Newsletter: How Duolingo Reignited User Growth",
    "description": "Case study on gamification, streaks, and retention mechanics that drove 4.5x growth",
    "category": "Growth & Retention",
    "url": "https://www.lennysnewsletter.com/p/how-duolingo-reignited-user-growth",
    "type": "Article",
    "level": "Medium",
    "tags": [
      "Strategy & Analytics",
      "Article"
    ],
    "domain": "Domain Applications",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-product-metrics",
      "user-retention-analysis"
    ],
    "topic_tags": [
      "gamification",
      "user-retention",
      "product-growth",
      "case-study"
    ],
    "summary": "A detailed case study examining how Duolingo redesigned their product features using gamification mechanics like streaks and social features to achieve 4.5x user growth. The analysis breaks down specific retention strategies and their measurable impact on user engagement. Essential reading for anyone working on consumer product growth.",
    "use_cases": [
      "Designing retention features for a consumer app",
      "Building gamification mechanics to increase user engagement"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "How did Duolingo increase user retention",
      "Gamification strategies for app growth",
      "Product features that drive user engagement",
      "Case study on mobile app retention mechanics"
    ]
  },
  {
    "name": "Growth Accounting & Backtraced Growth Accounting",
    "description": "Standard framework for user lifecycle states (New, Retained, Churned, Stale, Resurrected) with weighted backtrace views",
    "category": "Growth & Retention",
    "url": "https://bytepawn.com/growth-accounting-and-backtraced-growth-accounting.html",
    "type": "Article",
    "level": "Medium",
    "tags": [
      "Strategy & Analytics",
      "Article"
    ],
    "domain": "Domain Applications",
    "difficulty": "intermediate",
    "prerequisites": [
      "SQL-cohort-analysis",
      "user-lifecycle-modeling",
      "retention-metrics"
    ],
    "topic_tags": [
      "growth-accounting",
      "user-lifecycle",
      "retention-analysis",
      "cohort-tracking",
      "product-analytics"
    ],
    "summary": "Growth accounting is a systematic framework for categorizing users into lifecycle states (New, Retained, Churned, Stale, Resurrected) to understand product growth drivers. The backtraced variant provides weighted historical views to better attribute growth contributions over time. This is essential for product teams to diagnose growth issues and prioritize retention vs acquisition efforts.",
    "use_cases": [
      "Diagnosing why monthly active users are declining by decomposing into new user acquisition vs existing user retention",
      "Attributing revenue growth to different user lifecycle segments to inform marketing budget allocation"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "user lifecycle framework for growth analysis",
      "how to measure new vs retained vs churned users",
      "growth accounting methodology for product analytics",
      "backtraced growth accounting implementation"
    ]
  },
  {
    "name": "Guide to Product Metrics",
    "description": "26 metrics across AARRR framework: activation, retention, LTV, NRR, Quick Ratio, PMF Score explained",
    "category": "Growth & Retention",
    "url": "https://www.roarkeclinton.com/posts/product-metrics-guide.html",
    "type": "Article",
    "level": "Medium",
    "tags": [
      "Strategy & Analytics",
      "Article"
    ],
    "domain": "Domain Applications",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-analytics",
      "product-management-fundamentals"
    ],
    "topic_tags": [
      "product-metrics",
      "aarrr-framework",
      "user-retention",
      "customer-lifetime-value",
      "growth-analytics"
    ],
    "summary": "A comprehensive guide covering 26 essential product metrics organized within the AARRR (Acquisition, Activation, Retention, Revenue, Referral) framework. Explains key metrics like activation rates, retention cohorts, customer lifetime value, net revenue retention, and product-market fit scoring. Essential reference for anyone measuring product performance and growth.",
    "use_cases": [
      "Setting up a metrics dashboard for a new product launch to track user engagement and business health",
      "Evaluating product-market fit and growth potential when preparing investor updates or strategic planning"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "what are the most important product metrics to track",
      "AARRR framework metrics explained",
      "how to measure product market fit score",
      "customer lifetime value and retention metrics guide"
    ]
  },
  {
    "name": "Measuring Product Health (Sequoia)",
    "description": "Definitive guide to growth, retention, stickiness & engagement metrics: DAU/MAU, Lness, cohort curves, Quick Ratio",
    "category": "Growth & Retention",
    "url": "https://articles.sequoiacap.com/measuring-product-health",
    "type": "Article",
    "level": "Medium",
    "tags": [
      "Strategy & Analytics",
      "Article"
    ],
    "domain": "Domain Applications",
    "difficulty": "beginner",
    "prerequisites": [
      "SQL-basic-queries",
      "cohort-analysis",
      "funnel-analysis"
    ],
    "topic_tags": [
      "product-metrics",
      "user-engagement",
      "retention-analysis",
      "growth-metrics",
      "DAU-MAU"
    ],
    "summary": "Sequoia's comprehensive guide to essential product health metrics including DAU/MAU ratios, L-ness calculations, cohort retention curves, and Quick Ratio for measuring growth efficiency. Provides practical frameworks for measuring user engagement, stickiness, and sustainable growth patterns. Essential reference for anyone building or analyzing consumer products.",
    "use_cases": [
      "Setting up a product metrics dashboard for a new mobile app to track user engagement and retention",
      "Evaluating the health of an existing SaaS product before a funding round by analyzing cohort retention and growth metrics"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "how to calculate DAU MAU ratio",
      "product health metrics framework",
      "measuring user retention and engagement",
      "cohort analysis for product growth"
    ]
  },
  {
    "name": "Ken Norton: How to Hire a Product Manager",
    "description": "Former Google PM (14+ years) who led Docs, Calendar, Mobile Maps. This essay defines what a PM does by revealing hiring criteria \u2014 the map of competencies to develop.",
    "category": "Frameworks & Strategy",
    "url": "https://www.bringthedonuts.com/essays/productmanager.html",
    "type": "Essay",
    "level": "Easy",
    "tags": [
      "Product Sense",
      "Essay"
    ],
    "domain": "Product Sense",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-business-strategy",
      "cross-functional-collaboration"
    ],
    "topic_tags": [
      "product-management",
      "hiring-frameworks",
      "career-development",
      "tech-industry",
      "google-practices"
    ],
    "summary": "A foundational essay by veteran Google PM Ken Norton that defines product management through the lens of hiring criteria and competencies. The piece breaks down what successful PMs actually do by examining the skills and qualities to look for when hiring them. Essential reading for anyone wanting to understand the PM role or transition into product management.",
    "use_cases": [
      "Data scientists considering a transition to product management roles",
      "Early-career professionals trying to understand what product managers do and how to work with them effectively"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "what does a product manager actually do",
      "how to transition from data science to product management",
      "google product manager hiring criteria",
      "ken norton product management framework"
    ]
  },
  {
    "name": "SVPG: Product Management Start Here",
    "description": "Silicon Valley Product Group's curated entry point distinguishing 'empowered product teams' from 'feature teams' \u2014 exposes why most PM work is 'product management theater'.",
    "category": "Frameworks & Strategy",
    "url": "https://www.svpg.com/product-management-start-here/",
    "type": "Knowledge Base",
    "level": "Easy",
    "tags": [
      "Product Sense",
      "Knowledge Base"
    ],
    "domain": "Product Sense",
    "difficulty": "beginner",
    "prerequisites": [
      "product-roadmap-planning",
      "stakeholder-management"
    ],
    "topic_tags": [
      "product-management",
      "team-structure",
      "empowerment",
      "strategy",
      "framework"
    ],
    "summary": "SVPG's foundational guide distinguishes empowered product teams that solve customer problems from feature teams that just build requirements. It exposes common 'product management theater' where PMs lack real decision-making authority and instead just coordinate delivery timelines.",
    "use_cases": [
      "Transitioning from feature factory delivery model to outcome-driven product development",
      "Diagnosing why your product team feels like order-takers rather than strategic decision-makers"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What's the difference between product management and project management",
      "How to build empowered product teams",
      "Why does my product team feel like a feature factory",
      "Product management best practices for tech companies"
    ]
  },
  {
    "name": "Shreyas Doshi: LNO Framework",
    "description": "Former PM leader at Stripe, Twitter, Google. The LNO Framework (Leverage, Neutral, Overhead tasks) is a breakthrough prioritization model \u2014 distinguishes good from exceptional PM thinking.",
    "category": "Frameworks & Strategy",
    "url": "https://shreyasdoshi.com/",
    "type": "Essays",
    "level": "Medium",
    "tags": [
      "Product Sense",
      "Essays"
    ],
    "domain": "Product Sense",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-project-management",
      "stakeholder-communication"
    ],
    "topic_tags": [
      "task-prioritization",
      "product-management",
      "decision-frameworks",
      "resource-allocation",
      "strategic-planning"
    ],
    "summary": "The LNO Framework categorizes tasks into Leverage (high-impact, multiplicative), Neutral (maintenance, necessary), and Overhead (low-value, time-consuming) buckets. Created by former PM leader Shreyas Doshi, it helps distinguish high-impact work from busy work. This prioritization model is widely adopted by product managers and analysts to optimize time allocation and strategic focus.",
    "use_cases": [
      "Product manager deciding which features to prioritize in next sprint based on potential leverage vs overhead",
      "Data scientist evaluating whether to spend time on model optimization (leverage) vs routine reporting (neutral/overhead)"
    ],
    "audience": [
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "task prioritization framework for product managers",
      "LNO framework Shreyas Doshi",
      "how to distinguish high impact work from busy work",
      "leverage neutral overhead task classification"
    ]
  },
  {
    "name": "Intercom: RICE Prioritization Framework",
    "description": "The RICE framework (Reach, Impact, Confidence, Effort) originated here and is now industry standard \u2014 provides quantitative structure for prioritization.",
    "category": "Frameworks & Strategy",
    "url": "https://www.intercom.com/blog/rice-simple-prioritization-for-product-managers/",
    "type": "Article",
    "level": "Easy",
    "tags": [
      "Product Sense",
      "Blog"
    ],
    "domain": "Product Sense",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-math",
      "product-management-basics"
    ],
    "topic_tags": [
      "product-prioritization",
      "decision-frameworks",
      "quantitative-analysis",
      "product-management",
      "feature-scoring"
    ],
    "summary": "RICE is a quantitative framework for prioritizing product features and initiatives by scoring them across four dimensions: Reach (how many users affected), Impact (how much it matters), Confidence (certainty in estimates), and Effort (resources required). Originally developed at Intercom, it's now widely adopted across tech companies to make data-driven prioritization decisions. The framework helps product teams move beyond gut feelings to systematic evaluation of competing initiatives.",
    "use_cases": [
      "Product manager deciding which features to build next quarter from a backlog of 20+ potential initiatives",
      "Data scientist helping leadership choose between different growth experiments or product improvements"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "RICE framework for product prioritization",
      "how to prioritize product features quantitatively",
      "Intercom RICE scoring model",
      "data-driven product roadmap prioritization methods"
    ]
  },
  {
    "name": "Intercom: On Product Management (Free Book)",
    "description": "Beautifully designed, practical product content from Intercom's product team led by Des Traynor. Free downloadable PDF covering PM fundamentals.",
    "category": "Frameworks & Strategy",
    "url": "https://www.intercom.com/resources/books/intercom-product-management",
    "type": "Online Book",
    "level": "Easy",
    "tags": [
      "Product Sense",
      "Online Book"
    ],
    "domain": "Product Sense",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-product-terminology",
      "user-research-fundamentals"
    ],
    "topic_tags": [
      "product-management",
      "product-strategy",
      "user-experience",
      "free-book",
      "product-fundamentals"
    ],
    "summary": "A comprehensive free book from Intercom covering core product management principles and practices. Written by experienced product leaders, it provides practical frameworks for building products users love. Essential reading for anyone wanting to understand how product decisions impact business outcomes.",
    "use_cases": [
      "Learning product management fundamentals when transitioning from a technical role",
      "Understanding how to work effectively with product teams as a data scientist"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "free product management book",
      "how to learn product management basics",
      "intercom product management guide",
      "product strategy fundamentals for data scientists"
    ]
  },
  {
    "name": "Gibson Biddle: DHM Product Strategy Framework",
    "description": "Former VP Product at Netflix (2005-2010). A 13-essay series walking through exactly how Netflix built its product strategy using DHM (Delight, Hard-to-copy, Margin-enhancing).",
    "category": "Frameworks & Strategy",
    "url": "https://gibsonbiddle.medium.com/intro-to-product-strategy-60bdf72b17e3",
    "type": "Essay Series",
    "level": "Medium",
    "tags": [
      "Product Sense",
      "Essay Series"
    ],
    "domain": "Product Sense",
    "difficulty": "beginner",
    "prerequisites": [
      "product-metrics",
      "basic-business-strategy"
    ],
    "topic_tags": [
      "product-strategy",
      "netflix-case-study",
      "dhm-framework",
      "product-management",
      "essay-series"
    ],
    "summary": "A comprehensive 13-essay series by former Netflix VP Product Gibson Biddle explaining the DHM framework (Delight, Hard-to-copy, Margin-enhancing) used to build Netflix's product strategy from 2005-2010. Provides concrete examples and actionable insights from one of the most successful product transformations in tech history. Essential reading for understanding how to systematically approach product strategy in tech companies.",
    "use_cases": [
      "Learning how to develop product strategy frameworks when transitioning from individual contributor to product leadership roles",
      "Understanding how successful tech companies like Netflix built sustainable competitive advantages through systematic product thinking"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "Netflix product strategy framework",
      "DHM framework Gibson Biddle",
      "how did Netflix build product strategy",
      "product strategy case studies tech companies"
    ]
  },
  {
    "name": "Teresa Torres: Opportunity Solution Trees",
    "description": "Product discovery coach who has trained 17,000+ PMs. The Opportunity Solution Tree framework connects business outcomes \u2192 customer opportunities \u2192 solutions \u2192 experiments.",
    "category": "Frameworks & Strategy",
    "url": "https://www.producttalk.org/opportunity-solution-trees/",
    "type": "Blog + Book",
    "level": "Medium",
    "tags": [
      "Product Sense",
      "Blog + Book"
    ],
    "domain": "Product Sense",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-product-management",
      "customer-research-methods"
    ],
    "topic_tags": [
      "product-discovery",
      "customer-research",
      "product-strategy",
      "framework",
      "product-management"
    ],
    "summary": "A structured framework for product discovery that maps business outcomes to customer opportunities, potential solutions, and testable experiments. Developed by Teresa Torres for product managers to systematically identify and validate product opportunities. Widely used in tech companies to align product decisions with customer needs and business goals.",
    "use_cases": [
      "Product manager trying to prioritize which customer problems to solve next based on business impact",
      "Data scientist working with PMs to design experiments that validate product hypotheses before building features"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "product discovery framework",
      "how to connect customer problems to business outcomes",
      "Teresa Torres opportunity solution tree",
      "structured approach to product experimentation"
    ]
  },
  {
    "name": "Marty Cagan: INSPIRED",
    "description": "THE definitive book on modern product management from SVPG founder. Explains empowered teams, product discovery vs. delivery, and how Amazon, Google, Netflix actually operate.",
    "category": "Frameworks & Strategy",
    "url": "https://www.svpg.com/books/inspired-how-to-create-tech-products-customers-love-2nd-edition/",
    "type": "Book",
    "level": "Easy",
    "tags": [
      "Product Sense",
      "Book"
    ],
    "domain": "Product Sense",
    "difficulty": "beginner",
    "prerequisites": [
      "product-roadmaps",
      "user-interviews",
      "A-B-testing-basics"
    ],
    "topic_tags": [
      "product-management",
      "team-frameworks",
      "product-discovery",
      "business-strategy",
      "book"
    ],
    "summary": "The foundational guide to modern product management by Silicon Valley Product Group founder Marty Cagan. Teaches how to build empowered product teams, distinguish between discovery and delivery phases, and implement practices used by top tech companies. Essential reading for anyone working at the intersection of product, engineering, and data.",
    "use_cases": [
      "Data scientist needs to understand product team dynamics and discovery processes before building models",
      "Junior researcher wants to learn how tech companies actually make product decisions and prioritize features"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "best product management book for data scientists",
      "how do Google Amazon Netflix build products",
      "product discovery vs delivery frameworks",
      "empowered product teams book recommendation"
    ]
  },
  {
    "name": "Atlassian: How to Write Product Requirements",
    "description": "Practical guide to writing PRDs in an agile environment from the makers of Jira and Confluence. Includes templates and explains modern, lightweight documentation.",
    "category": "Metrics & Measurement",
    "url": "https://www.atlassian.com/agile/product-management/requirements",
    "type": "Guide",
    "level": "Easy",
    "tags": [
      "Product Sense",
      "Guide"
    ],
    "domain": "Product Sense",
    "difficulty": "beginner",
    "prerequisites": [
      "agile-methodology",
      "product-development-basics"
    ],
    "topic_tags": [
      "product-requirements",
      "documentation",
      "agile-development",
      "product-management",
      "templates"
    ],
    "summary": "Atlassian's practical guide teaches how to write effective Product Requirements Documents (PRDs) in modern agile environments. The resource provides templates and emphasizes lightweight, collaborative documentation practices used by product teams. It's designed for anyone who needs to communicate product specifications clearly without heavy bureaucratic overhead.",
    "use_cases": [
      "Data scientist needs to document requirements for a new ML feature before implementation",
      "Product manager creating specifications for an A/B testing platform that engineering and data teams can follow"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "how to write product requirements document",
      "PRD template for data science projects",
      "agile product documentation best practices",
      "writing requirements for ML features"
    ]
  },
  {
    "name": "Amplitude: Product Lessons with Shreyas Doshi",
    "description": "Deep conversation on 'The Fundamental Framework of Product Work' (Impact, Execution, Optics levels) \u2014 demonstrates how metrics connect to strategy at sophisticated level.",
    "category": "Frameworks & Strategy",
    "url": "https://amplitude.com/blog/shreyas-doshi-product-lessons",
    "type": "Interview",
    "level": "Medium",
    "tags": [
      "Product Sense",
      "Interview"
    ],
    "domain": "Product Sense",
    "difficulty": "intermediate",
    "prerequisites": [
      "product-metrics",
      "experimentation-design",
      "stakeholder-management"
    ],
    "topic_tags": [
      "product-strategy",
      "metrics-frameworks",
      "impact-measurement",
      "product-management",
      "interview-content"
    ],
    "summary": "In-depth interview with Shreyas Doshi discussing his fundamental framework for product work, covering Impact, Execution, and Optics levels. The conversation demonstrates how to connect product metrics to strategic decision-making and provides a sophisticated approach to evaluating product initiatives.",
    "use_cases": [
      "Learning how to structure product thinking when transitioning from pure analytics to product-focused roles",
      "Understanding how to communicate data insights effectively across different organizational levels and stakeholder groups"
    ],
    "audience": [
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "product strategy frameworks for data scientists",
      "how to connect metrics to business impact",
      "Shreyas Doshi product management interview",
      "impact execution optics framework explained"
    ]
  },
  {
    "name": "Lenny's Newsletter & Podcast",
    "description": "Former Airbnb PM, #1 business newsletter on Substack with 700k+ subscribers. The most comprehensive ongoing resource for developing 'product sense' with concrete, tactical frameworks.",
    "category": "Case Studies",
    "url": "https://www.lennysnewsletter.com/",
    "type": "Newsletter + Podcast",
    "level": "Easy",
    "tags": [
      "Product Sense",
      "Newsletter + Podcast"
    ],
    "domain": "Product Sense",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-product-metrics",
      "user-funnel-analysis"
    ],
    "topic_tags": [
      "product-management",
      "growth-frameworks",
      "business-strategy",
      "user-analytics",
      "case-studies"
    ],
    "summary": "Lenny's Newsletter is the leading product management resource featuring tactical frameworks, case studies, and interviews from top tech companies. It provides concrete, actionable advice for developing product intuition and decision-making skills. The content covers growth strategies, user research, metrics, and real-world examples from companies like Airbnb, Uber, and Netflix.",
    "use_cases": [
      "Learning how to design and analyze product experiments from real company case studies",
      "Understanding how to prioritize features and measure product-market fit using proven frameworks"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "best product management newsletter for data scientists",
      "how to develop product sense as a data scientist",
      "case studies of successful product experiments at tech companies",
      "frameworks for measuring product success and growth"
    ]
  },
  {
    "name": "150 Successful ML Models at Booking.com (KDD 2019)",
    "description": "Reveals that model performance \u2260 business performance. Demonstrates why RCTs are critical for validating ML models in production with framework for hypothesis-driven iteration.",
    "category": "Case Studies",
    "url": "https://dl.acm.org/doi/10.1145/3292500.3330744",
    "type": "Paper",
    "level": "Medium",
    "tags": [
      "Experimentation",
      "Paper"
    ],
    "domain": "Product Sense",
    "difficulty": "intermediate",
    "prerequisites": [
      "randomized-controlled-trials",
      "machine-learning-evaluation",
      "A-B-testing"
    ],
    "topic_tags": [
      "model-validation",
      "business-metrics",
      "production-ml",
      "causal-inference",
      "tech-industry"
    ],
    "summary": "A KDD 2019 case study from Booking.com analyzing 150 machine learning models in production, revealing the critical disconnect between offline model performance and actual business impact. The paper demonstrates why randomized controlled trials are essential for validating ML systems and provides a framework for hypothesis-driven model iteration in real-world settings.",
    "use_cases": [
      "Validating whether a new recommendation system actually increases revenue despite better offline metrics",
      "Designing experimentation frameworks to test ML model business impact before full deployment"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "Why don't better ML models always improve business metrics",
      "How to validate machine learning models in production",
      "Case studies of A/B testing ML systems at scale",
      "Framework for measuring business impact of ML models"
    ]
  },
  {
    "name": "A Quantitative Approach to Product-Market Fit (Tribe Capital)",
    "description": "The foundational text on growth accounting. MAU growth accounting AND revenue growth accounting. Quick Ratio, Gross Retention, Net Churn explained by the team that pioneered it.",
    "category": "Growth & Retention",
    "url": "https://tribecap.co/a-quantitative-approach-to-product-market-fit/",
    "type": "Framework",
    "level": "Hard",
    "tags": [
      "Product Analytics",
      "Framework"
    ],
    "domain": "Domain Applications",
    "difficulty": "beginner",
    "prerequisites": [
      "SQL-basics",
      "cohort-analysis",
      "unit-economics"
    ],
    "topic_tags": [
      "growth-accounting",
      "product-metrics",
      "retention-analysis",
      "churn-modeling",
      "quick-ratio"
    ],
    "summary": "Tribe Capital's foundational framework for measuring product-market fit through quantitative growth accounting methods. Introduces key metrics like Quick Ratio, Gross Retention, and Net Churn for both MAU and revenue analysis. Essential reading for understanding how to systematically measure and improve product growth.",
    "use_cases": [
      "Setting up growth dashboards to track product-market fit metrics for a SaaS platform",
      "Analyzing user retention patterns to identify churn drivers and optimize onboarding flows"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "how to measure product market fit quantitatively",
      "growth accounting framework MAU revenue",
      "quick ratio gross retention net churn explained",
      "tribe capital growth metrics methodology"
    ]
  },
  {
    "name": "The Power User Curve (a16z)",
    "description": "The L30/L28 framework coined by Facebook's growth team. Why Power User Curves beat DAU/MAU: reveals variance, identifies power users, customizable for core actions. Used by a16z to evaluate startups.",
    "category": "Growth & Retention",
    "url": "https://a16z.com/the-power-user-curve-the-best-way-to-understand-your-most-engaged-users/",
    "type": "Framework",
    "level": "Medium",
    "tags": [
      "Product Analytics",
      "Framework"
    ],
    "domain": "Domain Applications",
    "difficulty": "intermediate",
    "prerequisites": [
      "cohort-analysis",
      "product-metrics",
      "SQL-aggregation"
    ],
    "topic_tags": [
      "power-user-curve",
      "L30-L28",
      "user-segmentation",
      "retention-analysis",
      "startup-evaluation"
    ],
    "summary": "The L30/L28 framework developed by Facebook's growth team measures power users by analyzing user activity distribution over 30 vs 28 day periods. Unlike simple DAU/MAU ratios, it reveals engagement variance and helps identify your most valuable user segments. Venture capitalists like a16z use this framework to evaluate startup traction and user engagement quality.",
    "use_cases": [
      "Evaluating whether a startup has genuine power users vs casual browsers for investment decisions",
      "Product teams identifying which user segments drive core engagement to focus retention efforts"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "L30 L28 framework Facebook growth",
      "power user curve vs DAU MAU metrics",
      "how does a16z evaluate startup user engagement",
      "measuring power users instead of daily active users"
    ]
  },
  {
    "name": "How to Measure Cohort Retention (Lenny's Newsletter)",
    "description": "The most comprehensive retention measurement guide. SQL implementations, bounded vs unbounded retention definitions, visualization best practices. When to use X-day vs unbounded retention.",
    "category": "Probability & Inference",
    "url": "https://www.lennysnewsletter.com/p/measuring-cohort-retention",
    "type": "Tutorial",
    "level": "Medium",
    "tags": [
      "Product Analytics",
      "Tutorial"
    ],
    "domain": "Statistics",
    "difficulty": "beginner",
    "prerequisites": [
      "SQL-window-functions",
      "cohort-analysis-basics",
      "data-visualization"
    ],
    "topic_tags": [
      "cohort-retention",
      "product-metrics",
      "SQL-tutorial",
      "retention-analysis",
      "user-analytics"
    ],
    "summary": "A comprehensive guide to measuring and analyzing user retention across cohorts, covering both bounded and unbounded retention definitions. Provides SQL implementations and visualization best practices for product teams. Essential reading for understanding when different retention metrics are appropriate for business decisions.",
    "use_cases": [
      "Product manager needs to track monthly active user retention for a subscription app",
      "Data scientist building automated retention dashboards for different user cohorts"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "how to calculate cohort retention in SQL",
      "difference between bounded and unbounded retention",
      "best practices for retention analysis",
      "cohort retention visualization examples"
    ]
  },
  {
    "name": "Ultimate Guide: Activation (Aakash Gupta)",
    "description": "Traces activation history from Facebook's 2008 growth team, including Chamath's '7 friends in 10 days' discovery. The Setup \u2192 Aha \u2192 Habit framework with data-backed examples.",
    "category": "Growth & Retention",
    "url": "https://www.news.aakashg.com/p/ultimate-guide-activation",
    "type": "Guide",
    "level": "Medium",
    "tags": [
      "Product Analytics",
      "Guide"
    ],
    "domain": "Domain Applications",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-statistics",
      "product-metrics"
    ],
    "topic_tags": [
      "user-activation",
      "growth-hacking",
      "product-analytics",
      "user-onboarding",
      "retention-metrics"
    ],
    "summary": "Comprehensive guide to user activation strategies tracing back to Facebook's early growth team discoveries. Introduces the Setup \u2192 Aha \u2192 Habit framework with historical context and data-driven examples from major tech companies. Essential reading for understanding how leading platforms drive early user engagement and long-term retention.",
    "use_cases": [
      "Designing onboarding flows for a new mobile app to maximize Day 7 retention",
      "Analyzing user journey data to identify activation moments for a SaaS product"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "Facebook 7 friends in 10 days activation metric",
      "user activation framework setup aha habit",
      "how to measure product activation rates",
      "growth team strategies for user onboarding"
    ]
  },
  {
    "name": "How Superhuman Built an Engine to Find PMF (First Round)",
    "description": "Operationalizes Sean Ellis's '40% very disappointed' survey into a systematic process. How Superhuman went from 22% to 58% PMF score using segmentation. Most referenced First Round article.",
    "category": "Probability & Inference",
    "url": "https://review.firstround.com/how-superhuman-built-an-engine-to-find-product-market-fit/",
    "type": "Case Study",
    "level": "Medium",
    "tags": [
      "Product Analytics",
      "Case Study"
    ],
    "domain": "Statistics",
    "difficulty": "beginner",
    "prerequisites": [
      "survey-design",
      "customer-segmentation",
      "basic-statistics"
    ],
    "topic_tags": [
      "product-market-fit",
      "customer-surveys",
      "segmentation-analysis",
      "startup-metrics",
      "case-study"
    ],
    "summary": "A detailed case study showing how Superhuman systematically improved their product-market fit from 22% to 58% using Sean Ellis's survey methodology. The article breaks down their segmentation approach to identify which customer types were most likely to be 'very disappointed' without the product. It's become the most referenced example of operationalizing PMF measurement in practice.",
    "use_cases": [
      "Measuring and improving product-market fit for a new product or feature launch",
      "Segmenting users to identify your core valuable customer base and prioritize product development"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "how to measure product market fit",
      "superhuman pmf case study",
      "sean ellis 40% survey methodology",
      "customer segmentation for product development"
    ]
  },
  {
    "name": "Slack's 2000 Messages Activation Metric",
    "description": "Documents Slack's activation discovery \u2014 after 2,000 messages sent per team, 93% remain active. How they identified this leading indicator. Conversion rate significantly above 5% SaaS average.",
    "category": "Growth & Retention",
    "url": "https://www.growth-letter.com/p/inside-slacks-4-billion-growth-system",
    "type": "Case Study",
    "level": "Easy",
    "tags": [
      "Product Analytics",
      "Case Study"
    ],
    "domain": "Domain Applications",
    "difficulty": "beginner",
    "prerequisites": [
      "cohort-analysis",
      "conversion-funnels",
      "basic-statistics"
    ],
    "topic_tags": [
      "activation-metrics",
      "user-engagement",
      "saas-analytics",
      "product-metrics",
      "retention-analysis"
    ],
    "summary": "Slack's famous case study revealing that teams sending 2,000+ messages have 93% retention rates, far above typical SaaS conversion rates. Shows how to identify and validate activation metrics that predict long-term user success. Classic example of finding leading indicators through cohort analysis and behavioral data.",
    "use_cases": [
      "Finding activation thresholds for your own product by analyzing user behavior patterns",
      "Validating whether engagement metrics actually predict retention in your app"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "Slack 2000 messages activation metric",
      "how to find product activation threshold",
      "user engagement metrics that predict retention",
      "slack activation case study growth"
    ]
  },
  {
    "name": "Coding for practitioners",
    "description": "Built specifically for econ researchers",
    "category": "Python",
    "url": "https://aeturrell.github.io/coding-for-economists/intro.html",
    "type": "Resource",
    "level": "Medium",
    "tags": [
      "Python Fundamentals"
    ],
    "domain": "Programming",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-python-syntax",
      "command-line-basics"
    ],
    "topic_tags": [
      "python-fundamentals",
      "economics-research",
      "coding-tutorial",
      "data-analysis",
      "research-methods"
    ],
    "summary": "A comprehensive Python programming guide tailored specifically for economics researchers and practitioners. This resource covers fundamental programming concepts, data manipulation techniques, and coding best practices within the context of economic analysis and research workflows.",
    "use_cases": [
      "Learning Python syntax and data structures for economic modeling and analysis",
      "Setting up research workflows and reproducible code for academic economics projects"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "Python tutorial for economics research",
      "How to code for economic analysis beginners",
      "Python fundamentals for economists",
      "Learning to program for econ research"
    ]
  },
  {
    "name": "Python Data Science Handbook",
    "description": "Free reference for NumPy, Pandas, Matplotlib",
    "category": "Python",
    "url": "https://jakevdp.github.io/PythonDataScienceHandbook/",
    "type": "Resource",
    "level": "Medium",
    "tags": [
      "Python Fundamentals"
    ],
    "domain": "Programming",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-python-syntax",
      "command-line-basics"
    ],
    "topic_tags": [
      "python-fundamentals",
      "data-manipulation",
      "visualization",
      "reference-guide",
      "numpy-pandas"
    ],
    "summary": "Comprehensive open-access handbook covering essential Python libraries for data science including NumPy for numerical computing, Pandas for data manipulation, and Matplotlib for visualization. Written by Jake VanderPlas, it serves as both a learning resource and practical reference guide for data scientists at all levels. The book provides hands-on examples and clear explanations of core data science workflows in Python.",
    "use_cases": [
      "Learning Python data science fundamentals as a new data scientist or researcher",
      "Quick reference lookup for NumPy array operations or Pandas DataFrame methods during analysis"
    ],
    "audience": [
      "Junior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "free Python data science book",
      "NumPy Pandas tutorial reference",
      "Jake VanderPlas Python handbook",
      "learn Python for data analysis beginner"
    ]
  },
  {
    "name": "StatQuest (Josh Starmer)",
    "description": "Visual intuition for stats \u2014 the 'Bill Nye of Statistics'",
    "category": "Probability & Inference",
    "url": "https://www.youtube.com/@statquest",
    "type": "Resource",
    "level": "Medium",
    "tags": [
      "Statistics"
    ],
    "domain": "Statistics",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-algebra",
      "descriptive-statistics"
    ],
    "topic_tags": [
      "statistical-concepts",
      "data-visualization",
      "educational-videos",
      "hypothesis-testing",
      "regression-analysis"
    ],
    "summary": "StatQuest provides intuitive video explanations of statistical concepts through clear visualizations and step-by-step breakdowns. Josh Starmer makes complex statistical methods accessible by focusing on conceptual understanding rather than mathematical rigor. Perfect for building foundational knowledge before diving into technical implementations.",
    "use_cases": [
      "Understanding what a p-value actually means before running your first A/B test",
      "Grasping the intuition behind linear regression before implementing it in code"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "intuitive explanation of statistics concepts",
      "visual statistics tutorials for beginners",
      "what is a p-value explained simply",
      "beginner friendly statistics videos"
    ]
  },
  {
    "name": "Seeing Theory (Brown)",
    "description": "Beautiful interactive visualizations for building intuition",
    "category": "Probability & Inference",
    "url": "https://seeing-theory.brown.edu/",
    "type": "Resource",
    "level": "Medium",
    "tags": [
      "Statistics"
    ],
    "domain": "Statistics",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-probability",
      "descriptive-statistics"
    ],
    "topic_tags": [
      "interactive-visualization",
      "probability-concepts",
      "statistical-intuition",
      "educational-resource"
    ],
    "summary": "Seeing Theory is an interactive educational platform from Brown University that uses stunning visualizations to build intuitive understanding of probability and statistics concepts. It covers topics from basic probability through advanced inference, making abstract mathematical concepts concrete through hands-on exploration. Perfect for building foundational understanding before diving into technical implementations.",
    "use_cases": [
      "Learning probability distributions and their properties before implementing statistical models",
      "Building intuition for hypothesis testing and confidence intervals before running A/B tests"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "interactive statistics learning",
      "probability visualization tools",
      "intuitive stats education",
      "visual probability theory"
    ]
  },
  {
    "name": "Scipy.stats Documentation",
    "description": "Reference for distributions and tests",
    "category": "Probability & Inference",
    "url": "https://docs.scipy.org/doc/scipy/reference/stats.html",
    "type": "Resource",
    "level": "Medium",
    "tags": [
      "Statistics"
    ],
    "domain": "Statistics",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-basics",
      "numpy-arrays",
      "hypothesis-testing"
    ],
    "topic_tags": [
      "statistical-distributions",
      "hypothesis-testing",
      "python-reference",
      "scipy",
      "documentation"
    ],
    "summary": "Comprehensive documentation for SciPy's statistics module covering probability distributions, statistical tests, and descriptive statistics functions. Essential reference for data scientists implementing statistical methods in Python. Includes detailed API documentation, examples, and mathematical formulations for distributions and statistical procedures.",
    "use_cases": [
      "Looking up parameters and methods for a specific probability distribution like gamma or beta",
      "Finding the right statistical test function and understanding its output format for A/B test analysis"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "scipy stats normal distribution parameters",
      "how to use scipy chi square test",
      "scipy statistical distributions reference",
      "python statistical tests documentation"
    ]
  },
  {
    "name": "SELECT Star SQL",
    "description": "Interactive book teaching SQL through meaningful analysis",
    "category": "SQL",
    "url": "https://selectstarsql.com/",
    "type": "Resource",
    "level": "Medium",
    "tags": [
      "SQL & Databases"
    ],
    "domain": "Programming",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-programming",
      "spreadsheet-software"
    ],
    "topic_tags": [
      "SQL-fundamentals",
      "data-analysis",
      "interactive-learning",
      "query-optimization",
      "database-basics"
    ],
    "summary": "An interactive book that teaches SQL fundamentals through hands-on data analysis exercises and real-world examples. Designed for beginners to learn SQL syntax, query structure, and analytical techniques in an engaging format. Provides step-by-step guidance for writing queries to solve meaningful business and research questions.",
    "use_cases": [
      "Learning SQL basics for first data science role",
      "Teaching SQL concepts to students or team members"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "interactive SQL tutorial for beginners",
      "learn SQL through practice exercises",
      "beginner friendly SQL book with examples",
      "hands-on SQL learning resources"
    ]
  },
  {
    "name": "8 Week SQL Challenge (Danny Ma)",
    "description": "8 business case studies with CTEs and window functions",
    "category": "SQL",
    "url": "https://8weeksqlchallenge.com/",
    "type": "Resource",
    "level": "Medium",
    "tags": [
      "SQL & Databases"
    ],
    "domain": "Programming",
    "difficulty": "intermediate",
    "prerequisites": [
      "SQL-basic-queries",
      "relational-databases",
      "business-analytics"
    ],
    "topic_tags": [
      "SQL-practice",
      "CTEs",
      "window-functions",
      "business-cases",
      "data-analysis"
    ],
    "summary": "A hands-on SQL learning program featuring 8 realistic business case studies that teach advanced SQL concepts through practical applications. Each case study focuses on common data analysis scenarios using Common Table Expressions (CTEs) and window functions. Ideal for data professionals looking to strengthen their SQL skills with real-world business problems.",
    "use_cases": [
      "Preparing for data analyst interviews with hands-on SQL practice",
      "Learning advanced SQL techniques for business intelligence and reporting"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "SQL practice problems with business cases",
      "learn CTEs and window functions",
      "Danny Ma SQL challenge",
      "intermediate SQL exercises for data science"
    ]
  },
  {
    "name": "DataLemur",
    "description": "Real DS interview questions with business context",
    "category": "SQL",
    "url": "https://datalemur.com/",
    "type": "Resource",
    "level": "Medium",
    "tags": [
      "SQL & Databases"
    ],
    "domain": "Programming",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-SQL",
      "relational-databases"
    ],
    "topic_tags": [
      "SQL-practice",
      "interview-preparation",
      "business-analytics",
      "query-optimization"
    ],
    "summary": "DataLemur is a platform offering real data science interview questions focused on SQL with business context and scenarios. It helps practitioners prepare for technical interviews while learning practical SQL applications in business settings. The questions simulate real-world data problems commonly encountered at tech companies.",
    "use_cases": [
      "Preparing for data scientist or analyst interviews at tech companies",
      "Practicing SQL skills with realistic business scenarios and datasets"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "SQL interview practice questions",
      "data science interview prep SQL",
      "business context SQL problems",
      "real world SQL practice exercises"
    ]
  },
  {
    "name": "Problem Solving with Algorithms & Data Structures (Python)",
    "description": "Free interactive textbook \u2014 visualizations and runnable code",
    "category": "Software Engineering",
    "url": "https://runestone.academy/ns/books/published/pythonds/index.html",
    "type": "Resource",
    "level": "Medium",
    "tags": [
      "Computer Science"
    ],
    "domain": "Programming",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-python-syntax",
      "mathematical-reasoning"
    ],
    "topic_tags": [
      "algorithms",
      "data-structures",
      "python-programming",
      "computer-science-fundamentals",
      "interactive-textbook"
    ],
    "summary": "An interactive textbook that teaches fundamental algorithms and data structures using Python with visual demonstrations and executable code examples. Designed for computer science students and practitioners who want to build strong problem-solving foundations. Features hands-on exercises covering sorting, searching, trees, graphs, and algorithmic analysis.",
    "use_cases": [
      "Junior data scientist preparing for technical interviews and needing to understand algorithm complexity",
      "PhD student building foundational computer science knowledge to implement efficient research algorithms"
    ],
    "audience": [
      "Junior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "python algorithms and data structures tutorial",
      "interactive computer science textbook with code examples",
      "learn algorithms with python visualizations",
      "beginner friendly data structures book python"
    ]
  },
  {
    "name": "Real Python: Data Structures",
    "description": "Practical guide with Python-specific implementations",
    "category": "Software Engineering",
    "url": "https://realpython.com/python-data-structures/",
    "type": "Resource",
    "level": "Medium",
    "tags": [
      "Computer Science"
    ],
    "domain": "Programming",
    "difficulty": "beginner",
    "prerequisites": [
      "python-basics",
      "object-oriented-programming"
    ],
    "topic_tags": [
      "data-structures",
      "python-implementation",
      "algorithms",
      "programming-fundamentals"
    ],
    "summary": "A comprehensive guide to implementing and using data structures in Python, covering lists, dictionaries, sets, stacks, queues, and trees with practical examples. Essential for data scientists who need to optimize code performance and choose appropriate data structures for different analytical tasks. Focuses on Python-specific implementations and best practices rather than theoretical computer science.",
    "use_cases": [
      "Optimizing data processing pipelines by choosing efficient data structures for large datasets",
      "Building custom algorithms for A/B testing frameworks that require specific queue or tree implementations"
    ],
    "audience": [
      "Junior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "python data structures for data science",
      "how to implement stacks and queues in python",
      "best data structures for large dataset processing",
      "python dictionaries vs sets performance comparison"
    ]
  },
  {
    "name": "USF Data Structure Visualizations",
    "description": "Interactive animations \u2014 see how trees, heaps, and graphs work",
    "category": "Software Engineering",
    "url": "https://www.cs.usfca.edu/~galles/visualization/Algorithms.html",
    "type": "Resource",
    "level": "Medium",
    "tags": [
      "Computer Science"
    ],
    "domain": "Programming",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-programming",
      "algorithm-concepts"
    ],
    "topic_tags": [
      "data-structures",
      "algorithms",
      "visualization",
      "computer-science",
      "interactive-learning"
    ],
    "summary": "Interactive visual animations that demonstrate how fundamental data structures like trees, heaps, and graphs operate step-by-step. Perfect for learning core computer science concepts through visual exploration rather than just reading code. Helps build intuition for how algorithms manipulate these structures in real-time.",
    "use_cases": [
      "Learning how binary search trees maintain sorted order during insertions and deletions",
      "Understanding how heap data structures work before implementing priority queues in production code"
    ],
    "audience": [
      "Junior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "interactive data structure visualizations",
      "how do binary trees work visually",
      "animated algorithms for learning data structures",
      "visual guide to heaps and graphs"
    ]
  },
  {
    "name": "LeetCode Explore: Data Structures",
    "description": "Structured practice cards with solutions",
    "category": "Software Engineering",
    "url": "https://leetcode.com/explore/learn/",
    "type": "Resource",
    "level": "Medium",
    "tags": [
      "Computer Science"
    ],
    "domain": "Programming",
    "difficulty": "beginner",
    "prerequisites": [
      "python-basics",
      "algorithm-fundamentals"
    ],
    "topic_tags": [
      "data-structures",
      "coding-interviews",
      "algorithm-practice",
      "leetcode",
      "programming-fundamentals"
    ],
    "summary": "Interactive learning modules covering fundamental data structures like arrays, linked lists, trees, and graphs with step-by-step explanations and coding exercises. Designed for structured practice with built-in solutions and explanations to help developers master essential computer science concepts. Particularly useful for interview preparation and strengthening programming foundations.",
    "use_cases": [
      "Preparing for technical interviews at tech companies",
      "Building foundational knowledge before diving into advanced algorithms or machine learning implementations"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "data structures practice problems",
      "leetcode beginner guide",
      "how to learn arrays and linked lists",
      "coding interview preparation data structures"
    ]
  },
  {
    "name": "freeCodeCamp: Algorithms Course",
    "description": "8-hour free video course \u2014 clear explanations",
    "category": "Software Engineering",
    "url": "https://www.youtube.com/watch?v=8hly31xKli0",
    "type": "Resource",
    "level": "Medium",
    "tags": [
      "Computer Science"
    ],
    "domain": "Programming",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-programming-concepts",
      "problem-solving-fundamentals"
    ],
    "topic_tags": [
      "algorithms",
      "data-structures",
      "computer-science",
      "video-course",
      "fundamentals"
    ],
    "summary": "An 8-hour comprehensive video course covering fundamental algorithms and data structures with clear explanations suitable for beginners. This free resource provides a solid foundation in algorithmic thinking and common problem-solving patterns used across software engineering and data science. Perfect for building the computational skills needed for technical interviews and efficient code implementation.",
    "use_cases": [
      "Preparing for technical interviews at tech companies",
      "Building foundational knowledge before diving into machine learning or data analysis roles"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "free algorithms course for beginners",
      "learn data structures and algorithms video",
      "computer science fundamentals for data science",
      "algorithm basics before machine learning"
    ]
  },
  {
    "name": "Abdul Bari (YouTube)",
    "description": "Exceptional whiteboard DSA explanations",
    "category": "Software Engineering",
    "url": "https://www.youtube.com/@abdul_bari",
    "type": "Resource",
    "level": "Medium",
    "tags": [
      "Computer Science"
    ],
    "domain": "Programming",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-programming",
      "mathematical-thinking"
    ],
    "topic_tags": [
      "data-structures",
      "algorithms",
      "video-tutorials",
      "computer-science",
      "coding-interviews"
    ],
    "summary": "Abdul Bari's YouTube channel provides clear, whiteboard-style explanations of data structures and algorithms concepts. His teaching approach breaks down complex algorithmic concepts into digestible visual presentations. This resource is particularly valuable for building foundational computer science knowledge needed for technical interviews and software development.",
    "use_cases": [
      "Preparing for technical coding interviews at tech companies",
      "Learning fundamental data structures and algorithms for software engineering roles"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "best YouTube channel for data structures and algorithms",
      "whiteboard explanations of computer science fundamentals",
      "Abdul Bari algorithms tutorials",
      "visual learning resources for DSA concepts"
    ]
  },
  {
    "name": "VisuAlgo",
    "description": "Animated algorithm visualizations \u2014 sorting, graphs, DP",
    "category": "Software Engineering",
    "url": "https://visualgo.net/",
    "type": "Resource",
    "level": "Medium",
    "tags": [
      "Computer Science"
    ],
    "domain": "Programming",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-data-structures",
      "algorithm-complexity"
    ],
    "topic_tags": [
      "algorithm-visualization",
      "interactive-learning",
      "computer-science",
      "educational-tools"
    ],
    "summary": "VisuAlgo is an interactive platform that provides animated visualizations of fundamental algorithms including sorting, graph traversal, and dynamic programming. It's designed as an educational tool to help students and practitioners understand how algorithms work step-by-step through visual demonstrations. The platform is particularly valuable for learning algorithm mechanics and complexity analysis through hands-on exploration.",
    "use_cases": [
      "Understanding how quicksort partitioning works before implementing it in production code",
      "Learning graph algorithms like Dijkstra's shortest path for network optimization problems"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "interactive algorithm visualizations",
      "learn sorting algorithms visually",
      "animated data structures tutorial",
      "step by step algorithm explanations"
    ]
  },
  {
    "name": "TheAlgorithms/Python",
    "description": "200+ algorithm implementations in Python \u2014 reference code",
    "category": "Software Engineering",
    "url": "https://github.com/TheAlgorithms/Python",
    "type": "Resource",
    "level": "Medium",
    "tags": [
      "Computer Science"
    ],
    "domain": "Programming",
    "difficulty": "beginner",
    "prerequisites": [
      "python-basics",
      "data-structures",
      "big-o-notation"
    ],
    "topic_tags": [
      "algorithms",
      "python-implementation",
      "reference-code",
      "computer-science",
      "educational"
    ],
    "summary": "A comprehensive collection of 200+ classic algorithm implementations in Python, providing clean reference code for fundamental computer science algorithms. The repository serves as both a learning resource for understanding algorithmic concepts and a practical reference for implementing common algorithms. It covers sorting, searching, graph algorithms, dynamic programming, and many other foundational topics with well-documented Python code.",
    "use_cases": [
      "Learning algorithm implementations while preparing for technical interviews",
      "Finding reference code to implement a specific algorithm in a production system"
    ],
    "audience": [
      "Junior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "Python algorithm implementations reference",
      "How to implement quicksort in Python",
      "Computer science algorithms Python code examples",
      "Algorithm reference library for interviews"
    ]
  },
  {
    "name": "Postman Academy",
    "description": "Free API certification path \u2014 often more useful than scraping",
    "category": "Software Engineering",
    "url": "https://academy.postman.com/",
    "type": "Resource",
    "level": "Medium",
    "tags": [
      "Engineering"
    ],
    "domain": "Programming",
    "difficulty": "beginner",
    "prerequisites": [
      "HTTP-requests",
      "JSON-parsing"
    ],
    "topic_tags": [
      "API-integration",
      "data-collection",
      "web-services",
      "certification"
    ],
    "summary": "Postman Academy provides free certification courses for learning API fundamentals, testing, and integration techniques. It's particularly valuable for data scientists who need to collect data from web APIs instead of scraping websites. The structured learning path covers authentication, request handling, and best practices for working with RESTful services.",
    "use_cases": [
      "Collecting social media data from Twitter/LinkedIn APIs for analysis instead of web scraping",
      "Integrating third-party payment or geolocation APIs into data pipelines"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "how to learn APIs for data collection",
      "alternative to web scraping for data gathering",
      "free API certification for data scientists",
      "postman training for beginners"
    ]
  },
  {
    "name": "Playwright for Python",
    "description": "Modern browser automation (faster than Selenium)",
    "category": "Software Engineering",
    "url": "https://playwright.dev/python/",
    "type": "Resource",
    "level": "Medium",
    "tags": [
      "Engineering"
    ],
    "domain": "Programming",
    "difficulty": "beginner",
    "prerequisites": [
      "python-basics",
      "web-scraping-concepts",
      "HTML-CSS-selectors"
    ],
    "topic_tags": [
      "web-automation",
      "browser-testing",
      "data-collection",
      "python-tools",
      "web-scraping"
    ],
    "summary": "Playwright is a modern Python library for automating web browsers that's significantly faster and more reliable than Selenium. It enables programmatic control of Chrome, Firefox, and Safari for web scraping, testing, and data collection tasks. Tech economists use it to gather data from dynamic websites, automate repetitive browser tasks, and build robust data pipelines.",
    "use_cases": [
      "Scraping dynamic pricing data from e-commerce sites that load content via JavaScript",
      "Automating data collection from internal company dashboards and web applications"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "faster alternative to selenium python",
      "how to scrape javascript heavy websites",
      "best python library for browser automation",
      "playwright vs selenium for data collection"
    ]
  },
  {
    "name": "Discrete Optimization (Coursera)",
    "description": "Van Hentenryck's course \u2014 actually makes you implement",
    "category": "Linear Programming",
    "url": "https://www.coursera.org/learn/solving-algorithms-discrete-optimization",
    "type": "Resource",
    "level": "Medium",
    "tags": [
      "Operations Research"
    ],
    "domain": "Optimization & Operations Research",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-programming",
      "linear-algebra",
      "algorithm-design"
    ],
    "topic_tags": [
      "discrete-optimization",
      "constraint-programming",
      "operations-research",
      "implementation-focused",
      "coursera"
    ],
    "summary": "Van Hentenryck's hands-on Coursera course on discrete optimization that emphasizes practical implementation over theory. The course covers constraint programming, local search, and mixed integer programming with real coding assignments. Particularly valuable for learning how optimization algorithms actually work under the hood through direct implementation.",
    "use_cases": [
      "Learning to implement custom optimization solvers for resource allocation problems",
      "Understanding optimization algorithms deeply enough to debug and tune performance in production systems"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "hands-on discrete optimization course",
      "learn constraint programming by coding",
      "Van Hentenryck optimization implementation",
      "practical operations research programming course"
    ]
  },
  {
    "name": "Brady Neal's Introduction to Causal Inference",
    "description": "14-week video course covering potential outcomes, DAGs, do-calculus, and causal discovery. Features guest lectures from Susan Athey, Alberto Abadie, and Yoshua Bengio. Bridges ML and econometric traditions.",
    "category": "Fundamentals",
    "url": "https://www.bradyneal.com/causal-inference-course",
    "type": "Video Course",
    "level": "Medium",
    "tags": [
      "Causal Inference",
      "Video Course"
    ],
    "domain": "Causal Inference",
    "difficulty": "intermediate",
    "prerequisites": [
      "linear-regression",
      "probability-theory",
      "python-basics"
    ],
    "topic_tags": [
      "causal-inference",
      "potential-outcomes",
      "directed-acyclic-graphs",
      "video-lectures",
      "econometrics"
    ],
    "summary": "A comprehensive 14-week video course teaching causal inference fundamentals including potential outcomes framework, DAGs, and do-calculus. Features guest lectures from leading researchers and bridges machine learning and econometric approaches to causal analysis.",
    "use_cases": [
      "Learning rigorous causal inference methods for A/B testing and observational studies",
      "Transitioning from correlational analysis to causal modeling in product analytics"
    ],
    "audience": [
      "Junior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "introduction to causal inference video course",
      "Brady Neal causal inference lectures",
      "learning DAGs and potential outcomes",
      "causal inference for data scientists"
    ]
  },
  {
    "name": "Stanford ML & Causal Inference Short Course",
    "description": "Video lectures from Susan Athey, Jann Spiess, and Stefan Wager covering ML vs. econometrics, ATEs with propensity scores, CATE estimation with causal forests, and loss functions for causal inference.",
    "category": "Fundamentals",
    "url": "https://www.gsb.stanford.edu/faculty-research/labs-initiatives/sil/research/methods/ai-machine-learning/short-course",
    "type": "Video Course",
    "level": "Medium",
    "tags": [
      "Causal Inference",
      "Video Course"
    ],
    "domain": "Causal Inference",
    "difficulty": "intermediate",
    "prerequisites": [
      "linear-regression",
      "python-scikit-learn",
      "randomized-controlled-trials"
    ],
    "topic_tags": [
      "causal-inference",
      "machine-learning",
      "propensity-scores",
      "causal-forests",
      "video-lectures"
    ],
    "summary": "Video course from Stanford faculty covering the intersection of machine learning and causal inference methods. Teaches how to estimate treatment effects using ML techniques like causal forests and propensity score methods. Ideal for data scientists wanting to move beyond correlational analysis to causal questions.",
    "use_cases": [
      "Measuring the causal impact of product features on user engagement using observational data",
      "Estimating heterogeneous treatment effects across customer segments in marketing experiments"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "causal inference course for data scientists",
      "machine learning methods for treatment effects",
      "Stanford causal inference video lectures",
      "propensity scores and causal forests tutorial"
    ]
  },
  {
    "name": "Matteo Courthoud's Causal Inference Blog",
    "description": "PhD economist writing for data scientists in tech. Covers selection bias, Frisch-Waugh-Lovell, weighting vs. matching, DAGs, and CUPED. Every post includes Python code with realistic examples.",
    "category": "Fundamentals",
    "url": "https://matteocourthoud.github.io/post/",
    "type": "Tutorial Blog",
    "level": "Medium",
    "tags": [
      "Causal Inference",
      "Tutorial"
    ],
    "domain": "Causal Inference",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "linear-regression",
      "statistical-significance"
    ],
    "topic_tags": [
      "causal-inference",
      "selection-bias",
      "DAGs",
      "python-tutorials",
      "experimentation"
    ],
    "summary": "A comprehensive blog by PhD economist Matteo Courthoud that translates causal inference methods for data scientists working in tech companies. Each post combines theoretical foundations with practical Python implementations using realistic examples from tech scenarios.",
    "use_cases": [
      "Learning how to properly analyze A/B tests with selection bias using matching and weighting techniques",
      "Understanding when and how to apply CUPED methodology to reduce variance in experiment analysis"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "causal inference tutorial with python code examples",
      "how to handle selection bias in A/B tests",
      "CUPED implementation python data science",
      "practical causal inference for tech companies"
    ]
  },
  {
    "name": "Andrew Heiss's DAG and Backdoor Tutorials",
    "description": "Hands-on tutorials on building DAGs with ggdag, backdoor criterion, confounders/colliders, d-separation, and propensity scores. Uses real variable names with complete R code.",
    "category": "Fundamentals",
    "url": "https://www.andrewheiss.com/blog/2020/02/25/closing-backdoors-dags/",
    "type": "Tutorial",
    "level": "Medium",
    "tags": [
      "Causal Inference",
      "Tutorial"
    ],
    "domain": "Causal Inference",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-R",
      "ggplot2",
      "linear-regression"
    ],
    "topic_tags": [
      "directed-acyclic-graphs",
      "backdoor-criterion",
      "confounders",
      "propensity-scores",
      "r-programming"
    ],
    "summary": "Comprehensive R tutorials teaching how to build and analyze directed acyclic graphs (DAGs) using the ggdag package, with practical examples of identifying confounders, colliders, and applying the backdoor criterion. Covers essential causal inference concepts with complete, runnable code using realistic variable names rather than abstract examples.",
    "use_cases": [
      "Identifying which variables to control for when estimating treatment effects in observational studies",
      "Building causal models to understand relationships between product features and user behavior"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "how to build DAGs in R",
      "backdoor criterion tutorial with code",
      "ggdag package examples",
      "causal inference for beginners R"
    ]
  },
  {
    "name": "Uber Engineering: Causal Inference at Uber",
    "description": "Real industry application showing how PhD-level methods translate to business problems. Covers propensity score matching at scale, RDD for dynamic pricing, and mediation modeling.",
    "category": "Fundamentals",
    "url": "https://www.uber.com/blog/causal-inference-at-uber/",
    "type": "Company Blog",
    "level": "Medium",
    "tags": [
      "Causal Inference",
      "Blog"
    ],
    "domain": "Causal Inference",
    "difficulty": "intermediate",
    "prerequisites": [
      "basic-regression",
      "python-pandas",
      "experimental-design"
    ],
    "topic_tags": [
      "causal-inference",
      "propensity-score-matching",
      "regression-discontinuity",
      "mediation-analysis",
      "industry-application"
    ],
    "summary": "Uber Engineering's comprehensive guide to implementing causal inference methods in production systems. Shows how to scale propensity score matching, apply regression discontinuity design to dynamic pricing, and build mediation models for complex business problems. Bridges the gap between academic theory and real-world tech company applications.",
    "use_cases": [
      "measuring impact of product changes on user behavior at scale",
      "evaluating dynamic pricing strategies using natural experiments"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "how does Uber do causal inference",
      "propensity score matching at scale",
      "regression discontinuity for pricing experiments",
      "causal inference in tech companies"
    ]
  },
  {
    "name": "Asjad Naqvi's DiD Repository",
    "description": "The definitive meta-resource for modern DiD. Covers TWFE failures, Goodman-Bacon decomposition, all major estimators (Callaway-Sant'Anna, Sun-Abraham, etc.) with code in Stata, R, Python, and Julia. Updated quarterly.",
    "category": "Difference-in-Differences",
    "url": "https://asjadnaqvi.github.io/DiD/",
    "type": "Tutorial Repository",
    "level": "Medium",
    "tags": [
      "Causal Inference",
      "DiD"
    ],
    "domain": "Causal Inference",
    "difficulty": "intermediate",
    "prerequisites": [
      "difference-in-differences",
      "panel-data-analysis",
      "causal-inference-fundamentals"
    ],
    "topic_tags": [
      "difference-in-differences",
      "causal-inference",
      "staggered-adoption",
      "TWFE",
      "code-repository"
    ],
    "summary": "A comprehensive repository covering modern difference-in-differences methods, addressing issues with traditional two-way fixed effects estimators. Includes implementation guides for all major estimators across multiple programming languages, making it the go-to resource for practitioners working with staggered treatment adoption designs.",
    "use_cases": [
      "Evaluating policy rollouts with different implementation dates across states or regions",
      "Measuring impact of product feature launches that occurred at different times across user segments"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "modern DiD estimators comparison",
      "Callaway Sant'Anna implementation guide",
      "TWFE problems staggered treatment",
      "difference in differences code repository"
    ]
  },
  {
    "name": "Pedro Sant'Anna's DiD Resources",
    "description": "14 lecture slide decks from the co-creator of Callaway-Sant'Anna. Covers classical DiD, parallel trends, ML for DiD, event studies, TWFE problems, and treatments turning on-and-off.",
    "category": "Difference-in-Differences",
    "url": "https://psantanna.com/did-resources/",
    "type": "Course Materials",
    "level": "Medium",
    "tags": [
      "Causal Inference",
      "DiD"
    ],
    "domain": "Causal Inference",
    "difficulty": "intermediate",
    "prerequisites": [
      "regression-analysis",
      "difference-in-differences",
      "panel-data"
    ],
    "topic_tags": [
      "difference-in-differences",
      "causal-inference",
      "lecture-slides",
      "TWFE",
      "event-studies"
    ],
    "summary": "Comprehensive lecture slides from Pedro Sant'Anna, co-creator of the Callaway-Sant'Anna DiD estimator. Covers modern DiD methods including staggered treatment timing, TWFE problems, and advanced applications with machine learning. Essential resource for understanding cutting-edge developments in difference-in-differences methodology.",
    "use_cases": [
      "Learning modern DiD methods to handle staggered treatment adoption in tech product rollouts",
      "Understanding TWFE bias issues when evaluating policy interventions with heterogeneous treatment timing"
    ],
    "audience": [
      "Early-PhD",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "Pedro Sant'Anna difference in differences lectures",
      "Callaway Sant'Anna DiD slides",
      "modern DiD methods TWFE problems",
      "staggered treatment difference in differences tutorial"
    ]
  },
  {
    "name": "Jonathan Roth's DiD Resources",
    "description": "Course slides and coding exercises focusing on pre-trends testing limitations and HonestDiD sensitivity analysis. Created the HonestDiD and pretrends R packages. Includes practitioner checklists.",
    "category": "Difference-in-Differences",
    "url": "https://www.jonathandroth.com/did-resources/",
    "type": "Course Materials",
    "level": "Hard",
    "tags": [
      "Causal Inference",
      "DiD"
    ],
    "domain": "Causal Inference",
    "difficulty": "intermediate",
    "prerequisites": [
      "difference-in-differences",
      "R-programming",
      "linear-regression"
    ],
    "topic_tags": [
      "difference-in-differences",
      "pre-trends-testing",
      "sensitivity-analysis",
      "causal-inference",
      "R-packages"
    ],
    "summary": "Jonathan Roth's comprehensive DiD resources covering advanced robustness techniques including pre-trends testing and sensitivity analysis. Features the HonestDiD and pretrends R packages with practical implementation guides and checklists for practitioners.",
    "use_cases": [
      "Testing robustness of DiD estimates when parallel trends assumption is questionable",
      "Implementing sensitivity analysis for policy evaluation studies with potential pre-existing trends"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "HonestDiD R package tutorial",
      "pre-trends testing difference in differences",
      "DiD sensitivity analysis methods",
      "Jonathan Roth difference in differences course"
    ]
  },
  {
    "name": "Matteo Courthoud's DiD Tutorial",
    "description": "Industry perspective with full Python code. Covers classic DiD with potential outcomes, parallel trends testing, multiple time periods, Card-Krueger replication, and business applications.",
    "category": "Difference-in-Differences",
    "url": "https://matteocourthoud.github.io/post/diff_in_diffs/",
    "type": "Tutorial",
    "level": "Medium",
    "tags": [
      "Causal Inference",
      "DiD"
    ],
    "domain": "Causal Inference",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "basic-regression",
      "causal-inference-fundamentals"
    ],
    "topic_tags": [
      "difference-in-differences",
      "causal-inference",
      "python-tutorial",
      "business-applications",
      "potential-outcomes"
    ],
    "summary": "A comprehensive Python tutorial on Difference-in-Differences methodology from an industry practitioner's perspective. Covers foundational DiD concepts using potential outcomes framework, parallel trends testing, and extensions to multiple time periods with full code implementation. Includes the classic Card-Krueger minimum wage study replication and practical business applications.",
    "use_cases": [
      "Evaluating impact of policy changes or product launches on business metrics",
      "Measuring causal effects of interventions when randomized experiments aren't feasible"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "difference in differences python tutorial",
      "how to implement DiD in python with code",
      "Card Krueger replication python",
      "parallel trends testing business applications"
    ]
  },
  {
    "name": "Mixtape Sessions GitHub Repository",
    "description": "Free workshop materials from sessions taught at Facebook, eBay, LSE, and Oxford. Covers advanced DiD, staggered timing, PT violations, with coding labs and interactive apps.",
    "category": "Difference-in-Differences",
    "url": "https://github.com/Mixtape-Sessions",
    "type": "Workshop Materials",
    "level": "Medium",
    "tags": [
      "Causal Inference",
      "DiD"
    ],
    "domain": "Causal Inference",
    "difficulty": "intermediate",
    "prerequisites": [
      "difference-in-differences",
      "R-programming",
      "causal-inference-basics"
    ],
    "topic_tags": [
      "difference-in-differences",
      "causal-inference",
      "workshop-materials",
      "staggered-adoption",
      "coding-labs"
    ],
    "summary": "Comprehensive workshop materials from top-tier institutions covering advanced difference-in-differences methods including staggered timing and parallel trends violations. Features hands-on coding labs and interactive applications for practical implementation of modern DiD techniques.",
    "use_cases": [
      "Learning how to handle staggered treatment adoption in product rollout analysis",
      "Implementing robust DiD estimators when parallel trends assumption is violated"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "advanced difference in differences workshop materials",
      "staggered DiD implementation tutorial",
      "parallel trends violations coding examples",
      "Facebook eBay causal inference training materials"
    ]
  },
  {
    "name": "Matteo Courthoud's IV Tutorial",
    "description": "IV in experimental settings with realistic tech examples (newsletter subscription as instrument). Covers LATE/Compliers interpretation, exclusion restriction, weak instruments diagnostics. Complete Python code.",
    "category": "IV & RDD",
    "url": "https://matteocourthoud.github.io/post/instrumental_variables/",
    "type": "Tutorial",
    "level": "Medium",
    "tags": [
      "Causal Inference",
      "IV"
    ],
    "domain": "Causal Inference",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "basic-regression",
      "experimental-design"
    ],
    "topic_tags": [
      "instrumental-variables",
      "LATE",
      "experimental-methods",
      "tech-economics",
      "python-tutorial"
    ],
    "summary": "A comprehensive tutorial on instrumental variables in experimental settings with practical tech industry examples like newsletter subscriptions as instruments. Covers Local Average Treatment Effects (LATE), compliers interpretation, exclusion restriction validation, and weak instruments diagnostics with complete Python implementation.",
    "use_cases": [
      "measuring causal effect of product feature adoption when randomized encouragement doesn't achieve full compliance",
      "evaluating impact of training programs when participation is voluntary despite random assignment to receive invitations"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "instrumental variables tutorial with python code",
      "LATE and compliers in experimental design",
      "how to handle non-compliance in randomized experiments",
      "weak instruments diagnostics python implementation"
    ]
  },
  {
    "name": "Matteo Courthoud's RDD Tutorial",
    "description": "RDD fundamentals, bandwidth selection methods, and replication of Lee, Moretti, Butler (2004). Practical implementation with Python code using statsmodels.",
    "category": "IV & RDD",
    "url": "https://matteocourthoud.github.io/post/regression_discontinuity/",
    "type": "Tutorial",
    "level": "Medium",
    "tags": [
      "Causal Inference",
      "RDD"
    ],
    "domain": "Causal Inference",
    "difficulty": "beginner",
    "prerequisites": [
      "python-pandas",
      "basic-econometrics",
      "statsmodels"
    ],
    "topic_tags": [
      "regression-discontinuity",
      "causal-inference",
      "python-tutorial",
      "bandwidth-selection",
      "replication-study"
    ],
    "summary": "A comprehensive tutorial covering regression discontinuity design fundamentals, including bandwidth selection techniques and hands-on replication of the classic Lee, Moretti, Butler (2004) paper. Provides practical Python implementation using statsmodels with step-by-step code examples for learning RDD methodology.",
    "use_cases": [
      "Evaluating impact of policy changes with eligibility thresholds",
      "Analyzing treatment effects when assignment is based on continuous scoring rules"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "RDD tutorial python statsmodels",
      "regression discontinuity bandwidth selection",
      "Lee Moretti Butler replication code",
      "how to implement RDD in python"
    ]
  },
  {
    "name": "Andrew Heiss's RDD Course Examples",
    "description": "Complete sharp vs. fuzzy RDD comparison with downloadable datasets. Shows rdrobust() usage, 2SLS with iv_robust(), and compliance visualization. Reproducible R code with tidyverse.",
    "category": "IV & RDD",
    "url": "https://evalf20.classes.andrewheiss.com/example/rdd/",
    "type": "Course Materials",
    "level": "Medium",
    "tags": [
      "Causal Inference",
      "RDD"
    ],
    "domain": "Causal Inference",
    "difficulty": "intermediate",
    "prerequisites": [
      "R-programming",
      "regression-analysis",
      "instrumental-variables"
    ],
    "topic_tags": [
      "regression-discontinuity",
      "causal-inference",
      "R-tutorials",
      "econometric-methods",
      "reproducible-research"
    ],
    "summary": "Comprehensive tutorial comparing sharp and fuzzy regression discontinuity designs with hands-on R implementation. Covers rdrobust() package usage and 2SLS estimation with iv_robust(), including practical examples with real datasets. Perfect for learning RDD methodology through reproducible code examples.",
    "use_cases": [
      "Evaluating policy interventions with arbitrary cutoff thresholds",
      "Analyzing treatment effects when compliance varies around discontinuity points"
    ],
    "audience": [
      "Junior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "sharp vs fuzzy RDD tutorial R",
      "rdrobust package examples",
      "regression discontinuity 2SLS implementation",
      "RDD compliance visualization code"
    ]
  },
  {
    "name": "Tilburg Science Hub RDD Tutorials",
    "description": "Based on Cattaneo, Idrobo & Titiunik. Covers ITT vs. LATE, monotonicity, bandwidth selection for fuzzy designs, and multi-dimensional RDD. Includes Colombian education subsidy replication.",
    "category": "IV & RDD",
    "url": "https://tilburgsciencehub.com/topics/analyze/causal-inference/rdd/fuzzy-rdd/",
    "type": "Tutorial",
    "level": "Medium",
    "tags": [
      "Causal Inference",
      "RDD"
    ],
    "domain": "Causal Inference",
    "difficulty": "intermediate",
    "prerequisites": [
      "instrumental-variables",
      "basic-econometrics",
      "R-programming"
    ],
    "topic_tags": [
      "regression-discontinuity",
      "fuzzy-RDD",
      "bandwidth-selection",
      "causal-inference",
      "tutorial"
    ],
    "summary": "Comprehensive tutorial series on regression discontinuity design based on the authoritative Cattaneo, Idrobo & Titiunik framework. Covers advanced topics like fuzzy RDD, bandwidth optimization, and multi-dimensional designs with hands-on replication of Colombian education policy research.",
    "use_cases": [
      "Evaluating impact of scholarship programs with income eligibility thresholds",
      "Analyzing effects of policy interventions with age or score-based cutoffs"
    ],
    "audience": [
      "Early-PhD",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "fuzzy regression discontinuity bandwidth selection tutorial",
      "RDD monotonicity assumption explained",
      "Colombian education subsidy replication study",
      "multi-dimensional regression discontinuity methods"
    ]
  },
  {
    "name": "Teconomics Blog: ML Meets Instrumental Variables",
    "description": "How to reframe past A/B tests as instruments for behaviors you cannot randomize. Covers IV for behavioral effects, Deep IV, and ML for instrument selection. Actionable for data scientists.",
    "category": "IV & RDD",
    "url": "https://medium.com/teconomics-blog/machine-learning-meets-instrumental-variables-c8eecf5cec95",
    "type": "Article",
    "level": "Medium",
    "tags": [
      "Causal Inference",
      "IV"
    ],
    "domain": "Causal Inference",
    "difficulty": "intermediate",
    "prerequisites": [
      "instrumental-variables",
      "python-scikit-learn",
      "A-B-testing"
    ],
    "topic_tags": [
      "instrumental-variables",
      "deep-iv",
      "causal-inference",
      "behavioral-analysis",
      "experiment-design"
    ],
    "summary": "A practical guide for data scientists on using machine learning techniques with instrumental variables to analyze behavioral effects from past A/B tests. Covers reframing experiments as instruments for non-randomizable behaviors and selecting valid instruments using ML methods. Provides actionable frameworks for measuring indirect causal effects in tech settings.",
    "use_cases": [
      "Using past feature rollout experiments as instruments to measure how user engagement affects long-term retention",
      "Leveraging randomized notification experiments to estimate causal effects of app usage on purchasing behavior"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "how to use ML with instrumental variables",
      "reframe A/B tests as instruments for behavior",
      "Deep IV for data scientists",
      "instrument selection with machine learning"
    ]
  },
  {
    "name": "Matteo Courthoud's Synthetic Control Tutorial",
    "description": "SCM for industry practitioners with references to Google, Uber, Facebook use cases. Python implementation with sklearn and cvxpy. Explains SCM as 'transpose of regression' with placebo inference.",
    "category": "Synthetic Control",
    "url": "https://matteocourthoud.github.io/post/synth/",
    "type": "Tutorial",
    "level": "Medium",
    "tags": [
      "Causal Inference",
      "Synthetic Control"
    ],
    "domain": "Causal Inference",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "linear-regression",
      "causal-inference-basics"
    ],
    "topic_tags": [
      "synthetic-control",
      "causal-inference",
      "python-implementation",
      "industry-applications",
      "placebo-testing"
    ],
    "summary": "A practical tutorial on Synthetic Control Methods for industry data scientists, featuring real examples from major tech companies like Google, Uber, and Facebook. Includes Python implementation using sklearn and cvxpy, with emphasis on placebo inference and understanding SCM as the 'transpose of regression'.",
    "use_cases": [
      "Measuring impact of product launches when randomized experiments aren't feasible",
      "Evaluating policy changes or interventions across different markets or regions"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "synthetic control method python tutorial",
      "how to implement synthetic control for A/B testing",
      "causal inference when randomization isn't possible",
      "synthetic control examples from tech companies"
    ]
  },
  {
    "name": "Alberto Abadie's NBER Methods Lecture",
    "description": "Directly from the inventor of synthetic control methods. NBER Summer Institute lecture on foundational theory, best practices, when to use SCM vs. alternatives, and recent developments.",
    "category": "Synthetic Control",
    "url": "https://www.nber.org/research/videos/2021-methods-lecture-alberto-abadie-synthetic-controls-methods-and-practice",
    "type": "Video Lecture",
    "level": "Hard",
    "tags": [
      "Causal Inference",
      "Synthetic Control"
    ],
    "domain": "Causal Inference",
    "difficulty": "intermediate",
    "prerequisites": [
      "difference-in-differences",
      "regression-analysis",
      "panel-data"
    ],
    "topic_tags": [
      "synthetic-control",
      "causal-inference",
      "NBER-lecture",
      "policy-evaluation",
      "counterfactual-analysis"
    ],
    "summary": "NBER Summer Institute lecture by Alberto Abadie, the creator of synthetic control methods, covering foundational theory and practical implementation guidance. Covers when to use synthetic control versus other causal inference methods, best practices for implementation, and recent methodological developments. Essential viewing for anyone looking to understand or apply synthetic control methods properly.",
    "use_cases": [
      "Evaluating the causal impact of a policy change in one state/country using other regions as controls",
      "Measuring the effect of a product launch in one market by creating a synthetic control from similar untreated markets"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "Alberto Abadie synthetic control lecture",
      "NBER synthetic control methods tutorial",
      "how to implement synthetic control analysis",
      "when to use synthetic control vs difference in differences"
    ]
  },
  {
    "name": "Google's CausalImpact Blog Post",
    "description": "Production-grade tool from Google's advertising team. Bayesian structural time-series approach with automatic variable selection and uncertainty quantification. Widely used for marketing impact analysis.",
    "category": "Synthetic Control",
    "url": "https://opensource.googleblog.com/2014/09/causalimpact-new-open-source-package.html",
    "type": "Company Blog",
    "level": "Medium",
    "tags": [
      "Causal Inference",
      "Synthetic Control"
    ],
    "domain": "Causal Inference",
    "difficulty": "intermediate",
    "prerequisites": [
      "R-programming",
      "bayesian-inference",
      "time-series-analysis"
    ],
    "topic_tags": [
      "causal-impact",
      "marketing-attribution",
      "bayesian-structural-time-series",
      "google-tools",
      "blog-post"
    ],
    "summary": "Google's CausalImpact is a production-ready R package that uses Bayesian structural time-series models to estimate causal effects of interventions. Originally developed by Google's advertising team, it automatically handles variable selection and provides uncertainty quantification for impact analysis. The tool is particularly popular in marketing and product analytics for measuring the incremental impact of campaigns or feature launches.",
    "use_cases": [
      "measuring-incremental-lift-from-marketing-campaigns",
      "evaluating-impact-of-product-feature-launches"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "how to measure marketing campaign impact with CausalImpact",
      "Google CausalImpact R package tutorial",
      "bayesian time series for causal inference",
      "production tools for synthetic control methods"
    ]
  },
  {
    "name": "Stitch Fix: Market Matching with CausalImpact",
    "description": "Industry application combining dynamic time warping with CausalImpact for marketing intervention analysis. Shows how synthetic control concepts are adapted for real business problems at scale.",
    "category": "Synthetic Control",
    "url": "https://multithreaded.stitchfix.com/blog/2016/01/13/market-watch/",
    "type": "Company Blog",
    "level": "Medium",
    "tags": [
      "Causal Inference",
      "Synthetic Control"
    ],
    "domain": "Causal Inference",
    "difficulty": "intermediate",
    "prerequisites": [
      "causalimpact-package",
      "dynamic-time-warping",
      "synthetic-control-methods"
    ],
    "topic_tags": [
      "synthetic-control",
      "causal-inference",
      "marketing-analytics",
      "industry-case-study",
      "time-series"
    ],
    "summary": "Real-world case study from Stitch Fix showing how to combine dynamic time warping with Google's CausalImpact package for marketing intervention analysis. Demonstrates practical implementation of synthetic control methods for measuring campaign effectiveness in an e-commerce setting at scale.",
    "use_cases": [
      "measuring impact of marketing campaigns on customer acquisition",
      "evaluating effectiveness of promotional interventions in retail"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "how to measure marketing campaign impact with synthetic control",
      "CausalImpact for marketing analysis case study",
      "dynamic time warping with synthetic control methods",
      "Stitch Fix causal inference marketing experiments"
    ]
  },
  {
    "name": "Matteo Courthoud's Meta-Learners Tutorial",
    "description": "S-learner, T-learner, X-learner with detailed math, causal trees/forests, AIPW estimators. Uses Uber's CausalML package for demos. Complete Jupyter notebooks on GitHub.",
    "category": "Causal ML",
    "url": "https://matteocourthoud.github.io/post/meta_learners/",
    "type": "Tutorial",
    "level": "Medium",
    "tags": [
      "Causal Inference",
      "Causal ML"
    ],
    "domain": "Causal Inference",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-scikit-learn",
      "regression-analysis",
      "treatment-effect-estimation"
    ],
    "topic_tags": [
      "meta-learners",
      "causal-ml",
      "heterogeneous-treatment-effects",
      "jupyter-notebooks",
      "causalml-package"
    ],
    "summary": "Comprehensive tutorial covering meta-learner approaches (S-learner, T-learner, X-learner) for estimating heterogeneous treatment effects in machine learning settings. Includes mathematical foundations, causal trees/forests, and AIPW estimators with hands-on implementation using Uber's CausalML package. Features complete Jupyter notebooks with practical examples for learning and application.",
    "use_cases": [
      "Estimating personalized treatment effects in A/B tests with heterogeneous user populations",
      "Building recommendation systems that account for differential treatment responses across customer segments"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "meta learners causal inference tutorial",
      "S-learner T-learner X-learner comparison",
      "CausalML package tutorial jupyter notebooks",
      "heterogeneous treatment effects machine learning"
    ]
  },
  {
    "name": "KDD 2021 Tutorial: Causal Inference with EconML and CausalML",
    "description": "Industry workshop with 4 case studies from Uber, TripAdvisor, Microsoft. Ready-to-run Google Colab notebooks covering uplift modeling, customer segmentation, and long-term ROI estimation.",
    "category": "Causal ML",
    "url": "https://causal-machine-learning.github.io/kdd2021-tutorial/",
    "type": "Workshop Tutorial",
    "level": "Medium",
    "tags": [
      "Causal Inference",
      "Causal ML"
    ],
    "domain": "Causal Inference",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "regression-analysis",
      "A/B-testing"
    ],
    "topic_tags": [
      "causal-inference",
      "uplift-modeling",
      "econml",
      "causalml",
      "tutorial"
    ],
    "summary": "Hands-on tutorial from KDD 2021 featuring industry practitioners from major tech companies demonstrating causal inference applications. Includes ready-to-run Google Colab notebooks with real-world case studies covering uplift modeling, customer segmentation, and ROI measurement using EconML and CausalML packages.",
    "use_cases": [
      "Measuring incremental impact of marketing campaigns on customer conversion",
      "Estimating long-term revenue effects of product feature changes"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "EconML tutorial with examples",
      "causal inference industry case studies",
      "uplift modeling Google Colab notebooks",
      "CausalML practical applications"
    ]
  },
  {
    "name": "Double/Debiased Machine Learning Guide",
    "description": "From the original DML authors. Explains Neyman orthogonality, cross-fitting, DML with text/complex data. Focuses on practical implementation rather than theory.",
    "category": "Causal ML",
    "url": "https://dmlguide.github.io/",
    "type": "Tutorial Guide",
    "level": "Hard",
    "tags": [
      "Causal Inference",
      "Causal ML"
    ],
    "domain": "Causal Inference",
    "difficulty": "intermediate",
    "prerequisites": [
      "scikit-learn",
      "causal-inference-basics",
      "cross-validation"
    ],
    "topic_tags": [
      "double-machine-learning",
      "neyman-orthogonality",
      "cross-fitting",
      "treatment-effects",
      "implementation-guide"
    ],
    "summary": "A practical guide to Double/Debiased Machine Learning (DML) written by the original method creators. Covers key concepts like Neyman orthogonality and cross-fitting with focus on real-world implementation including text and complex data applications. Emphasizes practical coding and deployment over mathematical theory.",
    "use_cases": [
      "Estimating causal effects of product features using observational data with high-dimensional controls",
      "Measuring impact of marketing campaigns while controlling for complex user behavior patterns"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "double machine learning implementation guide",
      "how to implement DML in practice",
      "neyman orthogonality cross-fitting tutorial",
      "causal ML with high dimensional data"
    ]
  },
  {
    "name": "Mark White's Practical Causal Forest Tutorial",
    "description": "Explains why optimize directly on causal effects, not outcomes. Complete workflow from data prep to interpretation using GRF package. Written for applied researchers transitioning to causal ML.",
    "category": "Causal ML",
    "url": "https://www.markhw.com/blog/causalforestintro",
    "type": "Tutorial",
    "level": "Medium",
    "tags": [
      "Causal Inference",
      "Causal ML"
    ],
    "domain": "Causal Inference",
    "difficulty": "intermediate",
    "prerequisites": [
      "random-forests",
      "causal-inference-basics",
      "R-programming"
    ],
    "topic_tags": [
      "causal-forests",
      "heterogeneous-treatment-effects",
      "GRF-package",
      "tutorial",
      "causal-ML"
    ],
    "summary": "A practical tutorial explaining how to implement causal forests using the GRF package, focusing on optimizing directly on causal effects rather than outcomes. Provides a complete workflow from data preparation to interpretation specifically designed for applied researchers making the transition from traditional causal inference to causal machine learning methods.",
    "use_cases": [
      "estimating heterogeneous treatment effects in A/B tests across user segments",
      "identifying which customers benefit most from a pricing intervention"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "causal forest tutorial GRF package",
      "how to optimize causal effects not outcomes",
      "heterogeneous treatment effects machine learning",
      "causal ML workflow data prep to interpretation"
    ]
  },
  {
    "name": "Uber Engineering: Uplift Modeling for Multiple Treatments",
    "description": "Extending X-Learner and R-Learner to multiple treatments with cost optimization. Production system design for uplift models at scale with cost-aware treatment allocation.",
    "category": "Causal ML",
    "url": "https://www.uber.com/blog/research/uplift-modeling-for-multiple-treatments-with-cost-optimization/",
    "type": "Company Blog",
    "level": "Hard",
    "tags": [
      "Causal Inference",
      "Causal ML"
    ],
    "domain": "Causal Inference",
    "difficulty": "advanced",
    "prerequisites": [
      "causal-inference-fundamentals",
      "x-learner",
      "r-learner"
    ],
    "topic_tags": [
      "uplift-modeling",
      "multiple-treatments",
      "causal-ml",
      "production-systems",
      "cost-optimization"
    ],
    "summary": "Advanced uplift modeling techniques extending X-Learner and R-Learner frameworks to handle multiple treatment scenarios with integrated cost considerations. Covers production-scale implementation strategies for deploying uplift models in real-world systems where treatment allocation decisions must optimize for both effectiveness and cost constraints.",
    "use_cases": [
      "Multi-tier pricing optimization where you need to decide between discount levels, free trials, and premium upgrades",
      "Marketing campaign allocation across email, push notifications, and SMS with different costs per channel"
    ],
    "audience": [
      "Senior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "multiple treatment uplift modeling production",
      "extending x-learner to multiple treatments",
      "cost-aware treatment allocation algorithms",
      "uber uplift modeling implementation"
    ]
  },
  {
    "name": "Evan Miller's A/B Testing Tools",
    "description": "Interactive calculators for sample size, chi-squared, sequential sampling, and t-tests. The companion article 'How Not To Run an A/B Test' is the canonical reference on why repeated significance testing inflates false positives.",
    "category": "A/B Testing Fundamentals",
    "url": "https://www.evanmiller.org/ab-testing/",
    "type": "Interactive Tools",
    "level": "Medium",
    "tags": [
      "Experimentation",
      "Tools"
    ],
    "domain": "Experimentation",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-statistics",
      "hypothesis-testing"
    ],
    "topic_tags": [
      "sample-size-calculation",
      "statistical-significance",
      "online-experimentation",
      "interactive-calculators"
    ],
    "summary": "Collection of interactive web calculators for A/B testing fundamentals including sample size determination, statistical significance testing, and sequential analysis. Paired with essential reading on common A/B testing pitfalls, particularly the dangers of peeking at results before completion.",
    "use_cases": [
      "Calculating required sample size before launching a product feature test",
      "Determining if conversion rate differences between variants are statistically significant"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "sample size calculator A/B test",
      "how to calculate statistical significance A/B testing",
      "A/B testing tools online calculator",
      "why not to peek at A/B test results early"
    ]
  },
  {
    "name": "GrowthBook's Experimentation Fundamentals",
    "description": "Complete single-page reference covering hypothesis formation, statistical significance, Type I/II errors, MDE, power analysis, A/A tests, novelty effects, and experiment interactions. Notes that industry success rates are only ~33%.",
    "category": "A/B Testing Fundamentals",
    "url": "https://docs.growthbook.io/using/fundamentals",
    "type": "Tutorial",
    "level": "Easy",
    "tags": [
      "Experimentation",
      "Tutorial"
    ],
    "domain": "Experimentation",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-statistics",
      "hypothesis-testing"
    ],
    "topic_tags": [
      "experimentation",
      "statistical-significance",
      "power-analysis",
      "reference-guide"
    ],
    "summary": "Comprehensive single-page guide covering all fundamentals of running A/B tests, from hypothesis formation to statistical concepts like power analysis and effect sizes. Essential reference for anyone starting with experimentation, providing both theoretical foundations and practical insights like realistic industry success rates.",
    "use_cases": [
      "Learning how to design and analyze your first A/B test at a tech company",
      "Quick reference when setting up experiment parameters like minimum detectable effect and sample size"
    ],
    "audience": [
      "Junior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "A/B testing fundamentals tutorial",
      "how to calculate statistical significance for experiments",
      "minimum detectable effect and power analysis guide",
      "complete reference for experimentation basics"
    ]
  },
  {
    "name": "Matteo Courthoud's Experimentation Series",
    "description": "Connects experimentation to econometric foundations. Covers CUPED (linking to DiD), group sequential testing, Bayesian A/B testing, and clustered standard errors. Every post includes complete Python code.",
    "category": "A/B Testing Fundamentals",
    "url": "https://matteocourthoud.github.io/post/",
    "type": "Tutorial Blog",
    "level": "Medium",
    "tags": [
      "Experimentation",
      "Tutorial"
    ],
    "domain": "Experimentation",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "hypothesis-testing",
      "regression-analysis"
    ],
    "topic_tags": [
      "experimentation",
      "CUPED",
      "bayesian-ab-testing",
      "clustered-standard-errors",
      "python-tutorials"
    ],
    "summary": "A comprehensive tutorial series that bridges classical econometric methods with modern A/B testing practices. Covers advanced variance reduction techniques like CUPED, sequential testing approaches, and Bayesian methods with full Python implementations. Ideal for data scientists who want to understand the statistical foundations behind experimentation frameworks.",
    "use_cases": [
      "Implementing CUPED to reduce variance in product experiments at a tech company",
      "Setting up group sequential testing for early stopping decisions in marketing campaigns"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "CUPED tutorial with Python code",
      "bayesian A/B testing implementation guide",
      "clustered standard errors in experiments",
      "econometric methods for A/B testing"
    ]
  },
  {
    "name": "Netflix Tech Blog: What is an A/B Test?",
    "description": "Multi-part series covering metric selection, sequential testing at scale, quasi-experimentation when SUTVA is violated, and interleaving for recommendation testing. Published at KDD.",
    "category": "A/B Testing Fundamentals",
    "url": "https://netflixtechblog.com/what-is-an-a-b-test-b08cc1b57962",
    "type": "Engineering Blog",
    "level": "Medium",
    "tags": [
      "Experimentation",
      "Blog"
    ],
    "domain": "Experimentation",
    "difficulty": "intermediate",
    "prerequisites": [
      "hypothesis-testing",
      "basic-statistics",
      "randomized-controlled-trials"
    ],
    "topic_tags": [
      "a-b-testing",
      "experimentation",
      "causal-inference",
      "netflix",
      "tech-blog"
    ],
    "summary": "Netflix's comprehensive guide to A/B testing methodology covering practical implementation challenges at scale. The series addresses advanced topics like metric selection, sequential testing, quasi-experimental methods when randomization assumptions are violated, and interleaving techniques for recommendation systems. Essential reading for practitioners running experiments in tech environments.",
    "use_cases": [
      "designing and analyzing product feature tests at a tech company",
      "implementing recommendation system experiments with proper statistical controls"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "how to design A/B tests for recommendation systems",
      "Netflix approach to sequential testing",
      "A/B testing when randomization assumptions violated",
      "metric selection for product experiments"
    ]
  },
  {
    "name": "Understanding CUPED by Matteo Courthoud",
    "description": "Mathematical derivation from first principles: optimal covariate formula \u03b8 = Cov(X,Y)/Var(X) and variance reduction Var(\u0176_cuped) = Var(\u0232)(1 - \u03c1\u00b2). Compares with DiD and Frisch-Waugh-Lovell theorem. Full Python code.",
    "category": "Variance Reduction",
    "url": "https://matteocourthoud.github.io/post/cuped/",
    "type": "Tutorial",
    "level": "Medium",
    "tags": [
      "Experimentation",
      "CUPED"
    ],
    "domain": "Experimentation",
    "difficulty": "intermediate",
    "prerequisites": [
      "linear-regression",
      "covariance-analysis",
      "python-pandas"
    ],
    "topic_tags": [
      "cuped",
      "variance-reduction",
      "ab-testing",
      "covariate-adjustment",
      "python-tutorial"
    ],
    "summary": "This resource provides a complete mathematical derivation of CUPED (Controlled-experiment Using Pre-Experiment Data) from first principles, showing how pre-experiment covariates reduce variance in A/B tests. It includes the optimal weight formula and connects CUPED to difference-in-differences and regression theory. Complete with Python implementation for practical application.",
    "use_cases": [
      "Reducing variance in A/B test metrics when you have pre-experiment user behavior data",
      "Improving statistical power of experiments by leveraging historical baseline measurements"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "How does CUPED work mathematically",
      "CUPED variance reduction formula derivation",
      "Python implementation of CUPED for A/B testing",
      "What is the optimal covariate weight in CUPED"
    ]
  },
  {
    "name": "Booking.com: Increasing Power with CUPED",
    "description": "Production-ready Hive SQL and Spark/R implementations for big-data scale. Handles missing pre-experiment data gracefully with real A/B test case study showing faster significance achievement.",
    "category": "Variance Reduction",
    "url": "https://booking.ai/how-booking-com-increases-the-power-of-online-experiments-with-cuped-995d186fff1d",
    "type": "Engineering Blog",
    "level": "Medium",
    "tags": [
      "Experimentation",
      "CUPED"
    ],
    "domain": "Experimentation",
    "difficulty": "intermediate",
    "prerequisites": [
      "SQL-window-functions",
      "A-B-testing",
      "hive-sql"
    ],
    "topic_tags": [
      "CUPED",
      "variance-reduction",
      "production-implementation",
      "big-data",
      "spark"
    ],
    "summary": "Production-ready implementation of CUPED (Controlled-experiments Using Pre-Experiment Data) for reducing variance in A/B tests at scale. Includes Hive SQL and Spark/R code that handles real-world data issues like missing pre-experiment values. Shows how to achieve statistical significance faster through variance reduction techniques.",
    "use_cases": [
      "Running A/B tests on large user bases where pre-experiment behavioral data can reduce noise",
      "Implementing CUPED in production data pipelines to speed up experiment decision-making"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "How to implement CUPED in production",
      "CUPED SQL code for big data",
      "Variance reduction A/B testing Spark",
      "CUPED with missing pre-experiment data"
    ]
  },
  {
    "name": "Statsig's CUPED Deep Dive",
    "description": "Outstanding pedagogy using running/weights example. Demonstrates t-test and OLS regression equivalence, shows standard error reduction from 4.73 to 2.13, covers stratification approaches.",
    "category": "Variance Reduction",
    "url": "https://www.statsig.com/blog/cuped",
    "type": "Tutorial",
    "level": "Medium",
    "tags": [
      "Experimentation",
      "CUPED"
    ],
    "domain": "Experimentation",
    "difficulty": "intermediate",
    "prerequisites": [
      "t-tests",
      "linear-regression",
      "experimental-design"
    ],
    "topic_tags": [
      "CUPED",
      "variance-reduction",
      "A-B-testing",
      "causal-inference",
      "tutorial"
    ],
    "summary": "A comprehensive tutorial on CUPED (Controlled-experiments Using Pre-Existing Data) for reducing variance in A/B tests. Uses an intuitive running/weights example to show how incorporating pre-treatment covariates can dramatically reduce standard errors and improve experimental power. Demonstrates the mathematical equivalence between t-test and OLS approaches while covering practical stratification techniques.",
    "use_cases": [
      "Improving power in low-conversion A/B tests by using historical user behavior data",
      "Reducing sample size requirements for experiments by leveraging pre-treatment metrics"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "How to reduce variance in A/B tests using CUPED",
      "CUPED tutorial with practical examples",
      "Using pre-treatment data to improve experiment power",
      "T-test vs regression for controlled experiments"
    ]
  },
  {
    "name": "Eppo: CUPED++ for Extended Variance Reduction",
    "description": "CUPED++ extension using multiple pre-experiment metrics as covariates. Addresses 'new users have no pre-data' limitation. Quantifies impact: experiments can conclude 65% faster.",
    "category": "Variance Reduction",
    "url": "https://www.geteppo.com/blog/cuped-bending-time-in-experimentation",
    "type": "Article",
    "level": "Medium",
    "tags": [
      "Experimentation",
      "CUPED"
    ],
    "domain": "Experimentation",
    "difficulty": "intermediate",
    "prerequisites": [
      "A/B-testing",
      "CUPED",
      "linear-regression"
    ],
    "topic_tags": [
      "CUPED",
      "variance-reduction",
      "A/B-testing",
      "experimentation",
      "covariates"
    ],
    "summary": "CUPED++ extends the classic CUPED variance reduction technique by using multiple pre-experiment metrics as covariates instead of just one. This advancement specifically addresses the limitation where new users lack historical data, enabling experiments to reach statistical significance 65% faster across broader user populations.",
    "use_cases": [
      "Reducing experiment runtime when testing product changes on mixed populations of new and existing users",
      "Improving statistical power in A/B tests for mobile apps or platforms with high user churn rates"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "How to use CUPED with new users who have no historical data",
      "CUPED++ implementation for faster A/B test results",
      "Variance reduction techniques for experiments with mixed user populations",
      "Multiple covariates CUPED extension methods"
    ]
  },
  {
    "name": "DoorDash: CUPAC for ML-Enhanced Variance Reduction",
    "description": "CUPAC (Control Using Predictions As Covariate) - ML-based CUPED extension for when standard CUPED fails. Achieved 25%+ reduction in switchback test duration.",
    "category": "Variance Reduction",
    "url": "https://careersatdoordash.com/blog/improving-experimental-power-through-control-using-predictions-as-covariate-cupac/",
    "type": "Engineering Blog",
    "level": "Hard",
    "tags": [
      "Experimentation",
      "CUPED"
    ],
    "domain": "Experimentation",
    "difficulty": "intermediate",
    "prerequisites": [
      "CUPED",
      "control-variates",
      "regression-analysis"
    ],
    "topic_tags": [
      "CUPAC",
      "variance-reduction",
      "experimentation",
      "A/B-testing",
      "machine-learning"
    ],
    "summary": "CUPAC extends CUPED by using machine learning predictions as control variates when standard CUPED assumptions fail. DoorDash developed this method to achieve 25%+ reduction in switchback test duration by leveraging ML models to create better variance reduction controls.",
    "use_cases": [
      "Reducing variance in marketplace experiments where traditional pre-period controls don't satisfy CUPED assumptions",
      "Shortening switchback experiment duration by using ML predictions to control for confounding factors"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "CUPAC variance reduction method",
      "ML-enhanced CUPED for experiments",
      "DoorDash CUPAC implementation",
      "when CUPED fails use machine learning"
    ]
  },
  {
    "name": "Spotify: Choosing a Sequential Testing Framework",
    "description": "The definitive industry comparison of five frameworks: GST, mSPRT, GAVI, Corrected-Alpha, Bonferroni. Monte Carlo simulations comparing power. Maps methods to companies: GST (Spotify), mSPRT (Optimizely, Uber, Netflix).",
    "category": "Sequential Testing",
    "url": "https://engineering.atspotify.com/2023/03/choosing-sequential-testing-framework-comparisons-and-discussions",
    "type": "Engineering Blog",
    "level": "Hard",
    "tags": [
      "Experimentation",
      "Sequential Testing"
    ],
    "domain": "Experimentation",
    "difficulty": "intermediate",
    "prerequisites": [
      "hypothesis-testing",
      "statistical-power-analysis",
      "monte-carlo-simulation"
    ],
    "topic_tags": [
      "sequential-testing",
      "a-b-testing",
      "experimental-design",
      "statistical-power",
      "industry-comparison"
    ],
    "summary": "A comprehensive industry comparison of five sequential testing frameworks used by major tech companies, evaluated through Monte Carlo simulations for statistical power. Maps specific methods to real companies: GST at Spotify, mSPRT at Optimizely/Uber/Netflix, providing practical guidance for choosing frameworks. Essential reference for data scientists implementing continuous experimentation systems.",
    "use_cases": [
      "Selecting the right sequential testing framework for your company's A/B testing platform",
      "Benchmarking statistical power trade-offs when implementing early stopping rules for experiments"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "comparison of sequential testing frameworks",
      "which sequential testing method does Spotify use",
      "mSPRT vs GST statistical power comparison",
      "how to choose sequential testing framework for experiments"
    ]
  },
  {
    "name": "Evan Miller: Simple Sequential A/B Testing",
    "description": "Derives a simple sequential test using gambler's ruin: stop when T-C reaches 2\u221aN. Elegant and implementable with basic arithmetic. Includes interactive calculator.",
    "category": "Sequential Testing",
    "url": "https://www.evanmiller.org/sequential-ab-testing.html",
    "type": "Tutorial",
    "level": "Medium",
    "tags": [
      "Experimentation",
      "Sequential Testing"
    ],
    "domain": "Experimentation",
    "difficulty": "intermediate",
    "prerequisites": [
      "hypothesis-testing",
      "probability-theory",
      "basic-calculus"
    ],
    "topic_tags": [
      "sequential-testing",
      "ab-testing",
      "stopping-rules",
      "experimentation",
      "gamblers-ruin"
    ],
    "summary": "Evan Miller presents an elegant sequential A/B testing method derived from gambler's ruin theory, with a simple stopping rule: stop when T-C reaches 2\u221aN. This approach allows practitioners to monitor experiments continuously and stop early when significance is reached, using only basic arithmetic rather than complex statistical software.",
    "use_cases": [
      "Running product experiments where you want to stop as soon as statistical significance is achieved to save time and resources",
      "Implementing sequential testing in production systems where complex statistical libraries aren't available but basic math operations are feasible"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "simple sequential A/B test implementation",
      "how to do sequential testing without complex statistics",
      "early stopping rules for A/B tests",
      "gambler's ruin approach to experimentation"
    ]
  },
  {
    "name": "Matteo Courthoud: Group Sequential Testing",
    "description": "Pedagogical progression from peeking problem through Bonferroni, Pocock, O'Brien-Fleming to Lan-DeMets alpha-spending. Simulates 10,000 experiments showing Type I error rates. Full Python code.",
    "category": "Sequential Testing",
    "url": "https://matteocourthoud.github.io/post/group_sequential_testing/",
    "type": "Tutorial",
    "level": "Medium",
    "tags": [
      "Experimentation",
      "Sequential Testing"
    ],
    "domain": "Experimentation",
    "difficulty": "intermediate",
    "prerequisites": [
      "hypothesis-testing",
      "python-scipy",
      "monte-carlo-simulation"
    ],
    "topic_tags": [
      "group-sequential-testing",
      "alpha-spending",
      "bonferroni-correction",
      "experimentation-methods",
      "python-tutorial"
    ],
    "summary": "A comprehensive tutorial on group sequential testing methods that addresses the multiple testing problem when analyzing experiments before completion. Covers classical approaches like Bonferroni and Pocock boundaries, modern alpha-spending functions, and includes Monte Carlo simulations to demonstrate Type I error control. Essential for data scientists running experiments who need to peek at results while maintaining statistical validity.",
    "use_cases": [
      "A/B testing at a tech company where business stakeholders want weekly updates on experiment progress",
      "Clinical trial analysis where interim monitoring is required for safety and efficacy decisions"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "how to peek at A/B test results without inflating Type 1 error",
      "group sequential testing python implementation",
      "alpha spending functions for early stopping experiments",
      "Pocock O'Brien Fleming boundaries tutorial"
    ]
  },
  {
    "name": "Netflix: Sequential A/B Testing Keeps the World Streaming",
    "description": "Anytime-valid inference at production scale. Real case study: detecting play-delay issues that would have prevented 60% of devices from streaming. Covers time-uniform confidence bands.",
    "category": "Sequential Testing",
    "url": "https://netflixtechblog.com/sequential-a-b-testing-keeps-the-world-streaming-netflix-part-1-continuous-data-cba6c7ed49df",
    "type": "Engineering Blog",
    "level": "Hard",
    "tags": [
      "Experimentation",
      "Sequential Testing"
    ],
    "domain": "Experimentation",
    "difficulty": "intermediate",
    "prerequisites": [
      "hypothesis-testing",
      "confidence-intervals",
      "A-B-testing-fundamentals"
    ],
    "topic_tags": [
      "sequential-testing",
      "anytime-valid-inference",
      "production-experimentation",
      "netflix-case-study"
    ],
    "summary": "Netflix's approach to sequential A/B testing using anytime-valid inference methods for production-scale experimentation. Demonstrates how time-uniform confidence bands enabled detection of critical streaming issues affecting majority of devices. Shows practical implementation of sequential testing to make decisions before traditional fixed-horizon experiments conclude.",
    "use_cases": [
      "Detecting critical product issues early in A/B tests before waiting for full experiment duration",
      "Running continuous monitoring of product changes with valid statistical inference at any time point"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "Netflix sequential A/B testing methodology",
      "anytime valid inference production experiments",
      "how to stop A/B tests early with valid results",
      "sequential testing real world case studies"
    ]
  },
  {
    "name": "DoorDash: Switchback Tests Under Network Effects",
    "description": "Why traditional A/B tests fail in three-sided marketplaces and how switchback testing with region-time randomization solves interference. Uses 30-minute time windows.",
    "category": "Interference & Switchback",
    "url": "https://careersatdoordash.com/blog/switchback-tests-and-randomized-experimentation-under-network-effects-at-doordash/",
    "type": "Engineering Blog",
    "level": "Medium",
    "tags": [
      "Experimentation",
      "Switchback"
    ],
    "domain": "Experimentation",
    "difficulty": "intermediate",
    "prerequisites": [
      "randomized-controlled-trials",
      "statistical-inference",
      "causal-inference"
    ],
    "topic_tags": [
      "switchback-testing",
      "network-effects",
      "marketplace-experimentation",
      "causal-inference",
      "doordash"
    ],
    "summary": "DoorDash's approach to experimentation in three-sided marketplaces where traditional A/B tests fail due to interference between users, drivers, and merchants. Uses switchback methodology with geographic and temporal randomization in 30-minute windows to isolate treatment effects and handle network spillovers.",
    "use_cases": [
      "Testing pricing changes in rideshare/delivery platforms where driver supply affects all users",
      "Evaluating marketplace features where merchant behavior impacts customer experience across regions"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "switchback testing marketplace interference",
      "A/B testing network effects three sided markets",
      "DoorDash experimentation methodology",
      "how to test when users affect each other"
    ]
  },
  {
    "name": "DoorDash: Statistical Analysis for Switchback Experiments",
    "description": "Deep methodology comparing OLS, Multi-Level Modeling, and Cluster Robust Standard Errors for switchback analysis. Addresses small independent units problem. Achieved 30% faster iterations.",
    "category": "Interference & Switchback",
    "url": "https://doordash.engineering/2019/02/20/experiment-rigor-for-switchback-experiment-analysis/",
    "type": "Engineering Blog",
    "level": "Hard",
    "tags": [
      "Experimentation",
      "Switchback"
    ],
    "domain": "Experimentation",
    "difficulty": "advanced",
    "prerequisites": [
      "linear-regression",
      "clustered-standard-errors",
      "multilevel-modeling"
    ],
    "topic_tags": [
      "switchback-experiments",
      "statistical-inference",
      "causal-analysis",
      "marketplace-experiments",
      "methodology-comparison"
    ],
    "summary": "DoorDash's comprehensive statistical methodology guide for analyzing switchback experiments, comparing three key approaches: OLS, Multi-Level Modeling, and Cluster Robust Standard Errors. Specifically addresses the challenge of small independent units in marketplace settings and demonstrates how proper statistical methods can accelerate experiment iteration cycles by 30%.",
    "use_cases": [
      "Analyzing marketplace experiments where users switch between treatment and control over time",
      "Choosing appropriate statistical methods when you have limited independent clusters in your switchback design"
    ],
    "audience": [
      "Senior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "How to analyze switchback experiments with few independent units",
      "OLS vs multilevel modeling for switchback analysis",
      "DoorDash switchback experiment statistical methods",
      "Cluster robust standard errors for marketplace experiments"
    ]
  },
  {
    "name": "Lyft: Experimentation in a Ridesharing Marketplace",
    "description": "Foundational article on SUTVA violations through potential outcomes framework. The bias-variance tradeoff table for randomization schemes (user to city level) is highly cited.",
    "category": "Interference & Switchback",
    "url": "https://eng.lyft.com/experimentation-in-a-ridesharing-marketplace-b39db027a66e",
    "type": "Engineering Blog",
    "level": "Medium",
    "tags": [
      "Experimentation",
      "Marketplace"
    ],
    "domain": "Experimentation",
    "difficulty": "intermediate",
    "prerequisites": [
      "randomized-controlled-trials",
      "potential-outcomes-framework",
      "statistical-inference"
    ],
    "topic_tags": [
      "SUTVA-violations",
      "marketplace-experiments",
      "randomization-schemes",
      "bias-variance-tradeoff",
      "network-effects"
    ],
    "summary": "This foundational article explains how standard experimental assumptions (SUTVA) break down in ridesharing marketplaces due to network effects and interference between users. It provides a framework for understanding different randomization schemes from user-level to city-level and their associated bias-variance tradeoffs. The resource is essential for anyone designing experiments in two-sided marketplaces or networked environments.",
    "use_cases": [
      "Designing A/B tests for marketplace platforms where treatment of some users affects others",
      "Choosing appropriate randomization units when network effects violate standard experimental assumptions"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "SUTVA violations in marketplace experiments",
      "randomization schemes bias variance tradeoff",
      "experimentation network effects ridesharing",
      "marketplace A/B testing interference problems"
    ]
  },
  {
    "name": "Statsig: Switchback Experiments Overview",
    "description": "Best introductory resource with clear visual diagrams showing traditional A/B vs. switchback designs. Covers burn-in and burn-out periods to prevent cross-contamination.",
    "category": "Interference & Switchback",
    "url": "https://www.statsig.com/blog/switchback-experiments",
    "type": "Tutorial",
    "level": "Easy",
    "tags": [
      "Experimentation",
      "Switchback"
    ],
    "domain": "Experimentation",
    "difficulty": "beginner",
    "prerequisites": [
      "A-B-testing-basics",
      "experimental-design-fundamentals"
    ],
    "topic_tags": [
      "switchback-experiments",
      "interference-mitigation",
      "experimentation-design",
      "A-B-testing",
      "visual-guide"
    ],
    "summary": "An introductory guide to switchback experiments that addresses interference problems in traditional A/B tests. Uses clear visual diagrams to explain how switchback designs alternate treatments over time and includes burn-in/burn-out periods to prevent contamination. Essential reading for anyone dealing with network effects or spillover in experiments.",
    "use_cases": [
      "Testing ride-sharing pricing changes where driver behavior affects both treatment and control users",
      "Evaluating marketplace features where seller actions impact buyers across experimental groups"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "what are switchback experiments",
      "A/B testing with interference problems",
      "switchback vs traditional AB testing explained",
      "burn-in burn-out periods experiments"
    ]
  },
  {
    "name": "Wayfair: Geo Experiments for Incrementality",
    "description": "Convex optimization for treatment assignment when simple randomization won't work. Covers synthetic control matching and practical constraints like maximum geo share limits.",
    "category": "Interference & Switchback",
    "url": "https://www.aboutwayfair.com/careers/tech-blog/how-wayfair-uses-geo-experiments-to-measure-incrementality",
    "type": "Engineering Blog",
    "level": "Medium",
    "tags": [
      "Experimentation",
      "Geo Experiments"
    ],
    "domain": "Experimentation",
    "difficulty": "intermediate",
    "prerequisites": [
      "A/B-testing-fundamentals",
      "convex-optimization",
      "synthetic-control-methods"
    ],
    "topic_tags": [
      "geo-experiments",
      "incrementality-measurement",
      "synthetic-control",
      "treatment-assignment",
      "spatial-interference"
    ],
    "summary": "A guide to using convex optimization for geographic experiment design when standard randomization fails due to spillover effects or business constraints. Covers synthetic control matching techniques and practical implementation considerations like geographic share limits for treatment assignment.",
    "use_cases": [
      "Measuring incrementality of marketing campaigns across cities when users can easily cross geographic boundaries",
      "Testing supply chain or fulfillment changes where spillover effects between regions invalidate standard A/B tests"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "How to run geo experiments with spillover effects",
      "Convex optimization for geographic treatment assignment",
      "Synthetic control matching for incrementality testing",
      "Geographic A/B testing when randomization doesn't work"
    ]
  },
  {
    "name": "Eugene Yan: Bandits for Recommender Systems",
    "description": "The definitive practitioner's guide synthesizing implementations from 12+ tech companies (Spotify, Netflix, Yahoo, DoorDash, Twitter, Alibaba, Amazon). Covers \u03b5-greedy, UCB, Thompson Sampling.",
    "category": "Bandits & Adaptive",
    "url": "https://eugeneyan.com/writing/bandits/",
    "type": "Tutorial",
    "level": "Medium",
    "tags": [
      "Experimentation",
      "Bandits"
    ],
    "domain": "Experimentation",
    "difficulty": "intermediate",
    "prerequisites": [
      "A-B-testing",
      "python-scikit-learn",
      "reinforcement-learning-basics"
    ],
    "topic_tags": [
      "multi-armed-bandits",
      "recommendation-systems",
      "online-learning",
      "tech-industry",
      "practitioner-guide"
    ],
    "summary": "A comprehensive practitioner's guide to implementing bandit algorithms for recommendation systems, synthesizing real-world approaches from major tech companies. Covers core algorithms like \u03b5-greedy, UCB, and Thompson Sampling with production implementation details. Essential reading for data scientists building adaptive recommendation systems at scale.",
    "use_cases": [
      "Building a content recommendation system that learns user preferences in real-time",
      "Optimizing product recommendations on an e-commerce platform while balancing exploration vs exploitation"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "bandit algorithms for recommendation systems",
      "how to implement Thompson sampling for recommendations",
      "multi-armed bandits in production at tech companies",
      "epsilon greedy vs UCB for recommender systems"
    ]
  },
  {
    "name": "Eppo: Bandit vs. Experiment Testing Decision Guide",
    "description": "The single best resource for when to use bandits vs. experiments. Covers perishable decisions, impact estimation challenges, why A/B tests win for complex multi-metric decisions.",
    "category": "Bandits & Adaptive",
    "url": "https://www.geteppo.com/blog/bandit-or-experiment",
    "type": "Guide",
    "level": "Medium",
    "tags": [
      "Experimentation",
      "Bandits"
    ],
    "domain": "Experimentation",
    "difficulty": "intermediate",
    "prerequisites": [
      "a-b-testing",
      "multi-armed-bandits",
      "causal-inference"
    ],
    "topic_tags": [
      "bandits",
      "experimentation",
      "decision-framework",
      "adaptive-testing",
      "resource-guide"
    ],
    "summary": "A comprehensive decision framework for choosing between bandit algorithms and traditional A/B tests. Provides clear guidance on when each approach is optimal based on decision characteristics, measurement complexity, and business constraints. Essential reading for practitioners designing adaptive experimentation strategies.",
    "use_cases": [
      "Deciding whether to use a bandit or A/B test for a new product feature launch with multiple variants",
      "Choosing the right experimentation approach for ad optimization where conversion tracking has delays"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "when to use bandits vs A/B tests",
      "bandit vs experiment decision framework",
      "should I use multi armed bandit or A/B test",
      "bandits vs experiments comparison guide"
    ]
  },
  {
    "name": "Stitch Fix: Multi-Armed Bandits Experimentation Platform",
    "description": "Inside look at building bandit infrastructure. Covers Thompson Sampling convergence, deterministic allocation via hashing, and reward services architecture with feedback loop diagrams.",
    "category": "Bandits & Adaptive",
    "url": "https://multithreaded.stitchfix.com/blog/2020/08/05/bandits/",
    "type": "Engineering Blog",
    "level": "Hard",
    "tags": [
      "Experimentation",
      "Bandits"
    ],
    "domain": "Experimentation",
    "difficulty": "intermediate",
    "prerequisites": [
      "A-B-testing",
      "python-programming",
      "bayesian-statistics"
    ],
    "topic_tags": [
      "multi-armed-bandits",
      "thompson-sampling",
      "experimentation-platform",
      "infrastructure",
      "case-study"
    ],
    "summary": "Detailed case study of Stitch Fix's production multi-armed bandit platform implementation. Covers practical engineering considerations including Thompson Sampling algorithms, deterministic user allocation, and reward feedback architecture. Provides real-world insights into building scalable experimentation infrastructure at a major tech company.",
    "use_cases": [
      "Building experimentation infrastructure for dynamic content recommendation",
      "Implementing adaptive testing systems that optimize while collecting data"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "How to build multi-armed bandit platform in production",
      "Thompson sampling implementation at scale",
      "Stitch Fix experimentation infrastructure",
      "Real-world bandit algorithm deployment"
    ]
  },
  {
    "name": "Eppo: How Netflix, Lyft, and Yahoo Use Contextual Bandits",
    "description": "Case studies: Netflix artwork personalization, Lyft pricing optimization, Yahoo news with LinUCB. Explains why contextual bandits beat full recommenders for smaller action spaces.",
    "category": "Bandits & Adaptive",
    "url": "https://www.geteppo.com/blog/netflix-lyft-yahoo-contextual-bandits",
    "type": "Case Study",
    "level": "Medium",
    "tags": [
      "Experimentation",
      "Bandits"
    ],
    "domain": "Experimentation",
    "difficulty": "intermediate",
    "prerequisites": [
      "multi-armed-bandits",
      "A/B-testing",
      "reinforcement-learning-basics"
    ],
    "topic_tags": [
      "contextual-bandits",
      "personalization",
      "real-time-optimization",
      "case-studies"
    ],
    "summary": "This resource showcases how major tech companies implement contextual bandits for real-world optimization problems. It covers Netflix's artwork personalization, Lyft's pricing strategies, and Yahoo's news recommendations using LinUCB algorithm. The content explains when contextual bandits are preferable to full recommendation systems, particularly for scenarios with limited action spaces.",
    "use_cases": [
      "Optimizing content personalization with limited creative variants",
      "Dynamic pricing strategies that adapt to user context and market conditions"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "contextual bandits real world examples",
      "Netflix personalization algorithm case study",
      "when to use contextual bandits vs recommender systems",
      "LinUCB implementation at tech companies"
    ]
  },
  {
    "name": "Google Machine Learning Crash Course",
    "description": "15-hour interactive course originally for Google engineers, refreshed 2024 with LLMs/AutoML. Covers supervised learning, feature engineering, and production ML with Colab exercises. Teaches exact mental models Google engineers use.",
    "category": "Fundamentals",
    "url": "https://developers.google.com/machine-learning/crash-course",
    "type": "Course",
    "level": "Easy",
    "tags": [
      "Machine Learning",
      "Course"
    ],
    "domain": "Machine Learning",
    "difficulty": "beginner",
    "prerequisites": [
      "python-basics",
      "linear-algebra",
      "pandas-dataframes"
    ],
    "topic_tags": [
      "machine-learning",
      "supervised-learning",
      "feature-engineering",
      "google-ml",
      "interactive-course"
    ],
    "summary": "Google's internal 15-hour machine learning course designed to teach engineers production ML fundamentals. Covers supervised learning, feature engineering, and model deployment with hands-on Colab exercises and recently updated with LLM content. Provides the exact mental models and best practices used by Google engineers in production systems.",
    "use_cases": [
      "New data scientist at tech company needs to understand ML fundamentals used in production",
      "Software engineer transitioning to ML role wants industry-standard practices and frameworks"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "Google machine learning course for beginners",
      "interactive ML tutorial with hands-on exercises",
      "production machine learning best practices course",
      "learn ML the way Google engineers do"
    ]
  },
  {
    "name": "Andrew Ng's Machine Learning Specialization",
    "description": "Comprehensive theoretical grounding redesigned 2022 with modern Python. Three-course sequence on supervised/unsupervised learning and recommender systems. 4.9/5 from 37,000+ reviews. Free to audit.",
    "category": "Fundamentals",
    "url": "https://www.coursera.org/specializations/machine-learning-introduction",
    "type": "Course",
    "level": "Medium",
    "tags": [
      "Machine Learning",
      "Course"
    ],
    "domain": "Machine Learning",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-python",
      "linear-algebra",
      "calculus"
    ],
    "topic_tags": [
      "supervised-learning",
      "unsupervised-learning",
      "recommender-systems",
      "online-course",
      "python"
    ],
    "summary": "Andrew Ng's updated machine learning specialization providing comprehensive theoretical foundations with modern Python implementations. Covers supervised learning, unsupervised learning, and recommender systems across three courses. Highly rated foundational course sequence ideal for building core ML understanding from scratch.",
    "use_cases": [
      "Building foundational ML knowledge before starting first data science role",
      "Academic preparation for ML-focused PhD coursework or thesis research"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "Andrew Ng machine learning course 2022",
      "best beginner machine learning course with Python",
      "foundational ML course for data science career",
      "comprehensive machine learning specialization online"
    ]
  },
  {
    "name": "StatQuest with Josh Starmer",
    "description": "Visual explanations of cross-validation, regularization, gradient boosting, PCA, and bias-variance tradeoff. 675,000+ subscribers. Fills conceptual gaps that course-based learning misses.",
    "category": "Fundamentals",
    "url": "https://www.youtube.com/@statquest",
    "type": "Video Series",
    "level": "Easy",
    "tags": [
      "Machine Learning",
      "Video"
    ],
    "domain": "Machine Learning",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-linear-algebra",
      "python-numpy"
    ],
    "topic_tags": [
      "cross-validation",
      "regularization",
      "gradient-boosting",
      "PCA",
      "bias-variance"
    ],
    "summary": "StatQuest provides intuitive visual explanations of core machine learning concepts through animated videos and clear examples. Josh Starmer breaks down complex statistical methods into digestible segments that help bridge the gap between theory and practical understanding. The channel is particularly valuable for solidifying conceptual understanding of methods you'll implement in practice.",
    "use_cases": [
      "Understanding why your regularized model performs better before presenting results to stakeholders",
      "Grasping the intuition behind PCA before applying dimensionality reduction to high-dimensional user data"
    ],
    "audience": [
      "Junior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "visual explanation of cross validation",
      "intuitive understanding of gradient boosting",
      "bias variance tradeoff explained simply",
      "PCA concepts for beginners"
    ]
  },
  {
    "name": "Made With ML",
    "description": "Implementation-first approach: build models from scratch with NumPy before PyTorch. Emphasizes clean, production-quality code with proper software engineering practices. By Goku Mohandas (ex-Apple ML).",
    "category": "Fundamentals",
    "url": "https://madewithml.com/courses/foundations/",
    "type": "Course",
    "level": "Medium",
    "tags": [
      "Machine Learning",
      "Course"
    ],
    "domain": "Machine Learning",
    "difficulty": "beginner",
    "prerequisites": [
      "python-basics",
      "linear-algebra",
      "numpy-arrays"
    ],
    "topic_tags": [
      "machine-learning",
      "implementation-from-scratch",
      "production-code",
      "software-engineering",
      "pytorch"
    ],
    "summary": "A comprehensive machine learning course that teaches implementation fundamentals by building models from scratch using NumPy before moving to PyTorch. Emphasizes production-quality code with proper software engineering practices, making it ideal for practitioners who want to understand ML algorithms deeply while learning industry best practices.",
    "use_cases": [
      "Junior data scientists wanting to understand ML algorithms beyond black-box usage",
      "Bootcamp graduates preparing for technical interviews requiring ML implementation knowledge"
    ],
    "audience": [
      "Junior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "learn machine learning from scratch with numpy",
      "ML course with clean code practices",
      "build neural networks from first principles",
      "production ML code best practices course"
    ]
  },
  {
    "name": "Kaggle's Intermediate Machine Learning",
    "description": "Hands-on XGBoost with graded exercises. Covers missing values, categorical encoding, pipelines, cross-validation, then XGBoost tuning (n_estimators, early_stopping, learning_rate). Free certificate.",
    "category": "Gradient Boosting",
    "url": "https://www.kaggle.com/learn/intermediate-machine-learning",
    "type": "Course",
    "level": "Medium",
    "tags": [
      "Machine Learning",
      "XGBoost"
    ],
    "domain": "Machine Learning",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn-basics",
      "train-test-split"
    ],
    "topic_tags": [
      "xgboost",
      "gradient-boosting",
      "hyperparameter-tuning",
      "cross-validation",
      "hands-on-tutorial"
    ],
    "summary": "Kaggle's hands-on course teaching XGBoost implementation with practical exercises and automatic grading. Covers data preprocessing fundamentals like handling missing values and categorical encoding, then dives into XGBoost parameter tuning including n_estimators, early stopping, and learning rate optimization. Includes free certificate upon completion.",
    "use_cases": [
      "Building your first production-ready gradient boosting model for tabular prediction problems",
      "Learning XGBoost hyperparameter optimization techniques for Kaggle competitions"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "XGBoost tutorial with hands-on exercises",
      "how to tune XGBoost hyperparameters",
      "Kaggle machine learning course with certificate",
      "beginner XGBoost implementation guide"
    ]
  },
  {
    "name": "Neptune.ai: When to Choose CatBoost Over XGBoost",
    "description": "Algorithm selection with benchmark comparisons. Explains CatBoost's ordered boosting (preventing target leakage), symmetric vs. asymmetric trees. Decision framework practitioners need.",
    "category": "Gradient Boosting",
    "url": "https://neptune.ai/blog/when-to-choose-catboost-over-xgboost-or-lightgbm",
    "type": "Guide",
    "level": "Medium",
    "tags": [
      "Machine Learning",
      "XGBoost"
    ],
    "domain": "Machine Learning",
    "difficulty": "intermediate",
    "prerequisites": [
      "gradient-boosting-basics",
      "python-sklearn",
      "hyperparameter-tuning"
    ],
    "topic_tags": [
      "catboost",
      "xgboost",
      "algorithm-selection",
      "benchmark-comparison",
      "ensemble-methods"
    ],
    "summary": "Practical guide comparing CatBoost and XGBoost gradient boosting algorithms with performance benchmarks. Explains CatBoost's unique ordered boosting approach that prevents target leakage and compares symmetric vs asymmetric tree structures. Provides decision framework for practitioners to choose the right algorithm for their specific use case.",
    "use_cases": [
      "Selecting optimal boosting algorithm for tabular prediction tasks with categorical features",
      "Comparing model performance when dealing with datasets prone to overfitting or target leakage"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "catboost vs xgboost performance comparison",
      "when to use catboost over xgboost",
      "catboost ordered boosting explained",
      "gradient boosting algorithm selection guide"
    ]
  },
  {
    "name": "MLJAR: Feature Importance with XGBoost",
    "description": "Definitive guide covering three importance methods: gain, weight, and SHAP. Complete Colab code comparing built-in importance vs. permutation vs. SHAP values. Essential for model interpretation.",
    "category": "Gradient Boosting",
    "url": "https://mljar.com/blog/feature-importance-xgboost/",
    "type": "Tutorial",
    "level": "Medium",
    "tags": [
      "Machine Learning",
      "SHAP"
    ],
    "domain": "Machine Learning",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "xgboost",
      "scikit-learn"
    ],
    "topic_tags": [
      "feature-importance",
      "model-interpretation",
      "xgboost",
      "shap-values",
      "tutorial"
    ],
    "summary": "Comprehensive tutorial comparing three XGBoost feature importance methods: gain, weight, and SHAP values. Includes complete code examples and practical comparisons to help practitioners choose the right interpretation method for their models. Essential for understanding which features drive XGBoost predictions.",
    "use_cases": [
      "Explaining model predictions to business stakeholders",
      "Feature selection and engineering for improved model performance"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "how to interpret xgboost feature importance",
      "xgboost gain vs weight importance",
      "SHAP values for xgboost tutorial",
      "feature importance methods comparison xgboost"
    ]
  },
  {
    "name": "Analytics Vidhya: Hyperparameter Tuning Guide",
    "description": "Systematic tuning methodology from Kaggle winners. Sequential approach: fix tree params, tune learning rate/iterations, add regularization. Key insight: 10\u00d7 decrease in learning_rate needs ~10\u00d7 increase in n_estimators.",
    "category": "Gradient Boosting",
    "url": "https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/",
    "type": "Guide",
    "level": "Medium",
    "tags": [
      "Machine Learning",
      "Tuning"
    ],
    "domain": "Machine Learning",
    "difficulty": "intermediate",
    "prerequisites": [
      "gradient-boosting-algorithms",
      "python-scikit-learn",
      "cross-validation"
    ],
    "topic_tags": [
      "hyperparameter-tuning",
      "gradient-boosting",
      "model-optimization",
      "kaggle-methods",
      "practical-guide"
    ],
    "summary": "A systematic methodology for tuning gradient boosting models based on proven Kaggle competition strategies. Provides a sequential approach to parameter optimization with practical insights about the trade-offs between learning rate and iteration count. Essential reading for data scientists looking to improve their model tuning workflow beyond random search.",
    "use_cases": [
      "Optimizing XGBoost/LightGBM models for a machine learning competition",
      "Systematically tuning gradient boosting parameters for a production recommendation system"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "how to tune hyperparameters for gradient boosting",
      "systematic approach to XGBoost parameter tuning",
      "kaggle winners hyperparameter optimization strategy",
      "learning rate vs n_estimators trade-off gradient boosting"
    ]
  },
  {
    "name": "Fast.ai Practical Deep Learning for Coders",
    "description": "Top-down approach: deploying models by lesson 2, then progressively revealing mechanics. Part 1: vision, NLP, tabular, collaborative filtering. Part 2: backprop to Stable Diffusion. Alumni at Google Brain, OpenAI, Tesla.",
    "category": "Deep Learning",
    "url": "https://course.fast.ai",
    "type": "Course",
    "level": "Medium",
    "tags": [
      "Machine Learning",
      "Deep Learning"
    ],
    "domain": "Machine Learning",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-python",
      "high-school-math",
      "jupyter-notebooks"
    ],
    "topic_tags": [
      "practical-deep-learning",
      "computer-vision",
      "natural-language-processing",
      "hands-on-tutorial",
      "pytorch"
    ],
    "summary": "A hands-on deep learning course that gets you building and deploying real models from day one, then teaches the underlying theory. Covers practical applications across vision, NLP, tabular data, and recommender systems with a focus on implementation over mathematics. Alumni have gone on to work at top AI companies, making this a proven pathway from beginner to practitioner.",
    "use_cases": [
      "Junior developer wanting to add deep learning skills to build recommendation systems or image classifiers",
      "Career changer from traditional analytics looking to transition into modern ML roles at tech companies"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "best beginner deep learning course for practitioners",
      "how to learn deep learning without PhD in math",
      "practical deep learning tutorial that actually works",
      "fast.ai course review for career change to ML"
    ]
  },
  {
    "name": "Andrej Karpathy's Neural Networks: Zero to Hero",
    "description": "Build understanding through implementation. From backprop in 100 lines to building GPT from scratch. By OpenAI founding member and former Tesla AI Director. PyTorch naming conventions for production code.",
    "category": "Deep Learning",
    "url": "https://karpathy.ai/zero-to-hero.html",
    "type": "Video Series",
    "level": "Medium",
    "tags": [
      "Machine Learning",
      "Deep Learning"
    ],
    "domain": "Machine Learning",
    "difficulty": "beginner",
    "prerequisites": [
      "python-basics",
      "linear-algebra",
      "calculus-derivatives"
    ],
    "topic_tags": [
      "neural-networks",
      "pytorch",
      "backpropagation",
      "GPT",
      "hands-on-coding"
    ],
    "summary": "A practical video course that teaches neural networks through hands-on implementation, starting with basic backpropagation and progressing to building GPT from scratch. Created by Andrej Karpathy, this series emphasizes understanding through coding rather than theory, using PyTorch conventions suitable for production environments.",
    "use_cases": [
      "Learning neural network fundamentals by implementing backprop and basic architectures from scratch",
      "Building a GPT model step-by-step to understand transformer architecture and modern language models"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "learn neural networks from scratch coding",
      "build GPT transformer from zero implementation",
      "Andrej Karpathy neural network tutorial",
      "hands-on deep learning backpropagation course"
    ]
  },
  {
    "name": "Andrew Ng's Deep Learning Specialization",
    "description": "5 courses: neural network foundations, optimization/regularization, ML projects, CNNs, sequence models including transformers. 120,000+ five-star reviews. Free to audit. Balance of intuition, math, and application.",
    "category": "Deep Learning",
    "url": "https://www.coursera.org/specializations/deep-learning",
    "type": "Course",
    "level": "Medium",
    "tags": [
      "Machine Learning",
      "Deep Learning"
    ],
    "domain": "Machine Learning",
    "difficulty": "beginner",
    "prerequisites": [
      "python-basics",
      "linear-algebra",
      "calculus"
    ],
    "topic_tags": [
      "neural-networks",
      "computer-vision",
      "sequence-modeling",
      "online-course",
      "transformers"
    ],
    "summary": "Comprehensive 5-course specialization covering deep learning fundamentals from neural network basics to advanced architectures like CNNs and transformers. Designed by Andrew Ng with balance of theory and practical implementation, suitable for beginners with strong math background. Over 120,000 students have completed this highly-rated program that's free to audit.",
    "use_cases": [
      "Junior data scientist learning deep learning fundamentals for first ML role",
      "PhD student building foundation in neural networks before specializing in computer vision research"
    ],
    "audience": [
      "Junior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "Andrew Ng deep learning course",
      "best beginner deep learning specialization",
      "how to learn neural networks from scratch",
      "free deep learning course with good reviews"
    ]
  },
  {
    "name": "3Blue1Brown Neural Network Series",
    "description": "Unparalleled mathematical visualization. Grant Sanderson's custom animations make backpropagation and gradient descent genuinely intuitive. Newer transformer and LLM explainer videos particularly valuable.",
    "category": "Deep Learning",
    "url": "https://www.3blue1brown.com/topics/neural-networks",
    "type": "Video Series",
    "level": "Easy",
    "tags": [
      "Machine Learning",
      "Deep Learning"
    ],
    "domain": "Machine Learning",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-calculus",
      "linear-algebra",
      "python-basics"
    ],
    "topic_tags": [
      "neural-networks",
      "backpropagation",
      "gradient-descent",
      "transformers",
      "visual-learning"
    ],
    "summary": "Grant Sanderson's animated video series that makes neural network concepts visually intuitive through custom mathematical visualizations. Covers fundamentals like backpropagation and gradient descent, plus modern topics like transformers and LLMs. Perfect for building conceptual understanding before diving into implementation.",
    "use_cases": [
      "Getting intuitive understanding before implementing first neural network",
      "Explaining ML concepts to non-technical stakeholders or team members"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "visual explanation of how neural networks work",
      "beginner friendly neural network tutorial",
      "how does backpropagation actually work",
      "animated explanation of gradient descent"
    ]
  },
  {
    "name": "Jay Alammar's Illustrated Transformer",
    "description": "Definitive visual guide to attention mechanisms, referenced at Stanford, Harvard, MIT, Princeton, CMU. Step-by-step illustrations of self-attention, multi-head attention, positional encoding. Covers BERT, GPT-2, retrieval transformers.",
    "category": "Deep Learning",
    "url": "https://jalammar.github.io/illustrated-transformer/",
    "type": "Tutorial",
    "level": "Medium",
    "tags": [
      "Machine Learning",
      "Transformers"
    ],
    "domain": "Machine Learning",
    "difficulty": "beginner",
    "prerequisites": [
      "neural-networks",
      "python-numpy",
      "backpropagation"
    ],
    "topic_tags": [
      "transformers",
      "attention-mechanisms",
      "visual-guide",
      "deep-learning",
      "NLP"
    ],
    "summary": "Jay Alammar's visual tutorial breaks down transformer architecture using clear illustrations and animations. It explains self-attention, multi-head attention, and positional encoding in an accessible way that's become the go-to resource at top universities. Perfect for understanding the foundation behind BERT, GPT, and modern language models.",
    "use_cases": [
      "Understanding how ChatGPT and GPT models process text",
      "Learning transformer architecture before implementing BERT for text classification"
    ],
    "audience": [
      "Junior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "how do transformers work visual explanation",
      "self attention mechanism tutorial",
      "illustrated guide to BERT architecture",
      "transformer attention heads explained"
    ]
  },
  {
    "name": "Google's Recommendation Systems Course",
    "description": "Industry-standard architecture: candidate retrieval \u2192 scoring \u2192 re-ranking. Built by YouTube RecSys engineers. 4-hour course on collaborative filtering, matrix factorization, embeddings, deep approaches. YouTube case study at 2B+ user scale.",
    "category": "Recommender Systems",
    "url": "https://developers.google.com/machine-learning/recommendation",
    "type": "Course",
    "level": "Medium",
    "tags": [
      "Machine Learning",
      "RecSys"
    ],
    "domain": "Machine Learning",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-scikit-learn",
      "matrix-multiplication",
      "tensorflow-basics"
    ],
    "topic_tags": [
      "collaborative-filtering",
      "matrix-factorization",
      "deep-learning",
      "embeddings",
      "youtube-case-study"
    ],
    "summary": "Google's comprehensive course on building production recommendation systems, covering the three-stage pipeline from candidate retrieval through scoring to re-ranking. Taught by YouTube engineers who built systems serving billions of users, with hands-on examples of collaborative filtering, matrix factorization, and deep learning approaches. Ideal for practitioners who need to understand both the theory and real-world implementation challenges of large-scale recommendation systems.",
    "use_cases": [
      "Building product recommendation engine for e-commerce platform",
      "Implementing content recommendation system for streaming service"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "Google recommendation systems course",
      "YouTube RecSys engineering tutorial",
      "collaborative filtering matrix factorization course",
      "production recommendation system architecture"
    ]
  },
  {
    "name": "Eugene Yan: System Design for Recommendations",
    "description": "Production patterns from Alibaba, Facebook, DoorDash, LinkedIn in a 2\u00d72 framework (offline/online \u00d7 retrieval/ranking). By Amazon Principal Applied Scientist. Referenced by NVIDIA as canonical industry reading.",
    "category": "Recommender Systems",
    "url": "https://eugeneyan.com/writing/system-design-for-discovery/",
    "type": "Article",
    "level": "Medium",
    "tags": [
      "Machine Learning",
      "RecSys"
    ],
    "domain": "Machine Learning",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-scikit-learn",
      "A-B-testing",
      "SQL-joins"
    ],
    "topic_tags": [
      "system-design",
      "production-ml",
      "recommendation-engines",
      "industry-patterns",
      "ml-architecture"
    ],
    "summary": "A comprehensive guide to building production recommendation systems using a 2\u00d72 framework that organizes approaches by offline/online processing and retrieval/ranking stages. Features real implementation patterns from major tech companies like Alibaba, Facebook, and LinkedIn. Essential reading for data scientists moving from prototype models to scalable recommendation infrastructure.",
    "use_cases": [
      "Designing a new recommendation system architecture for an e-commerce platform",
      "Scaling an existing ML recommendation model from prototype to production serving millions of users"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "how to build production recommendation systems",
      "recommendation system architecture patterns",
      "scaling recommender systems to production",
      "industry best practices for recommendation engines"
    ]
  },
  {
    "name": "Netflix Technology Blog: Recommendation Systems",
    "description": "How Netflix Prize pioneers continue innovating. Foundation models with transformers, multi-task learning across surfaces, RecSysOps for production monitoring at 200M+ user scale. Lessons unavailable elsewhere.",
    "category": "Recommender Systems",
    "url": "https://netflixtechblog.com/tagged/recommendation-system",
    "type": "Engineering Blog",
    "level": "Hard",
    "tags": [
      "Machine Learning",
      "RecSys"
    ],
    "domain": "Machine Learning",
    "difficulty": "advanced",
    "prerequisites": [
      "deep-learning",
      "transformer-architectures",
      "production-ml-systems"
    ],
    "topic_tags": [
      "recommender-systems",
      "netflix",
      "foundation-models",
      "multi-task-learning",
      "production-ml"
    ],
    "summary": "Netflix's technical blog documenting their evolution from collaborative filtering to modern foundation models and transformers for recommendations. Covers multi-task learning across different product surfaces and RecSysOps practices for monitoring recommendation systems at massive scale. Provides rare insights into production challenges and solutions from one of the world's largest recommendation platforms.",
    "use_cases": [
      "Building multi-surface recommendation systems that work across web, mobile, and TV interfaces",
      "Implementing production monitoring and ops practices for large-scale recommendation engines"
    ],
    "audience": [
      "Senior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "Netflix recommendation system architecture",
      "How does Netflix use transformers for recommendations",
      "Multi-task learning for recommendation systems",
      "Production monitoring for recommendation engines at scale"
    ]
  },
  {
    "name": "TensorFlow Recommenders Tutorials",
    "description": "Executable code for two-tower architecture used at Google, YouTube, Pinterest. MovieLens examples: user/item embeddings, retrieval models, ranking layers, serving with approximate nearest neighbors. Concept to deployment.",
    "category": "Recommender Systems",
    "url": "https://www.tensorflow.org/recommenders",
    "type": "Tutorial",
    "level": "Medium",
    "tags": [
      "Machine Learning",
      "RecSys"
    ],
    "domain": "Machine Learning",
    "difficulty": "intermediate",
    "prerequisites": [
      "tensorflow",
      "python-pandas",
      "neural-networks"
    ],
    "topic_tags": [
      "two-tower-architecture",
      "embedding-models",
      "tensorflow-recommenders",
      "retrieval-ranking",
      "tutorial"
    ],
    "summary": "Hands-on tutorials for building production-scale recommender systems using TensorFlow Recommenders, featuring the two-tower architecture deployed at major tech companies. Covers the full pipeline from user/item embeddings through retrieval and ranking to model serving with approximate nearest neighbors. Uses MovieLens dataset to demonstrate real-world implementation patterns.",
    "use_cases": [
      "Building a movie/content recommendation system with separate user and item towers",
      "Implementing large-scale retrieval and ranking pipeline for e-commerce product recommendations"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "tensorflow recommenders two tower tutorial",
      "how to build recommender system like youtube",
      "movielens embedding model tensorflow",
      "retrieval ranking recommender system implementation"
    ]
  },
  {
    "name": "Eugene Yan: Position Bias in Search",
    "description": "Measurement and mitigation techniques: RandPair, FairPairs, propensity scoring. Essential for production ranking systems where position corrupts training data.",
    "category": "Search & Ranking",
    "url": "https://eugeneyan.com/writing/position-bias/",
    "type": "Article",
    "level": "Medium",
    "tags": [
      "Machine Learning",
      "Search"
    ],
    "domain": "Machine Learning",
    "difficulty": "intermediate",
    "prerequisites": [
      "A/B-testing",
      "machine-learning-evaluation",
      "ranking-systems"
    ],
    "topic_tags": [
      "position-bias",
      "search-ranking",
      "bias-mitigation",
      "production-ML",
      "evaluation-metrics"
    ],
    "summary": "Position bias occurs in search and recommendation systems when user clicks are influenced by item position rather than true relevance, corrupting training data. This resource covers measurement techniques like RandPair and FairPairs, plus propensity scoring methods to correct for positional effects. Essential knowledge for anyone building or evaluating production ranking systems where click data drives model training.",
    "use_cases": [
      "Improving search result quality by debiasing click-through rate data used for model training",
      "Evaluating recommendation system performance while accounting for position-dependent user behavior"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "how to handle position bias in search rankings",
      "RandPair FairPairs position bias techniques",
      "propensity scoring for ranking systems",
      "click data bias in recommendation systems"
    ]
  },
  {
    "name": "Airbnb: ML-Powered Search Ranking",
    "description": "Masterclass in production search evolution. 4-stage journey from baseline to personalized GBDT ranking with A/B test results (+13%, +7.9%, +5.1% booking improvements). Feature engineering for two-sided marketplaces.",
    "category": "Search & Ranking",
    "url": "https://medium.com/airbnb-engineering/machine-learning-powered-search-ranking-of-airbnb-experiences-110b4b1a0789",
    "type": "Engineering Blog",
    "level": "Medium",
    "tags": [
      "Machine Learning",
      "Search"
    ],
    "domain": "Machine Learning",
    "difficulty": "intermediate",
    "prerequisites": [
      "gradient-boosting",
      "A-B-testing",
      "feature-engineering"
    ],
    "topic_tags": [
      "search-ranking",
      "two-sided-marketplaces",
      "GBDT",
      "personalization",
      "production-ML"
    ],
    "summary": "Detailed case study of how Airbnb evolved their search ranking system through four stages, from basic scoring to personalized machine learning models. Shows real A/B test results and practical feature engineering techniques for matching guests with hosts in a two-sided marketplace.",
    "use_cases": [
      "Building ML-powered search ranking for marketplace platforms",
      "Implementing personalized recommendation systems with measurable business impact"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "airbnb search ranking machine learning case study",
      "how to build personalized search for marketplaces",
      "GBDT ranking system A/B test results",
      "two-sided marketplace feature engineering examples"
    ]
  },
  {
    "name": "OLX Engineering: From RankNet to LambdaMART",
    "description": "Clearest learning-to-rank explanation with code. Why ranking differs from classification, pointwise vs. pairwise vs. listwise approaches. Implementing RankNet and LambdaMART with XGBoost rank:pairwise and rank:ndcg.",
    "category": "Search & Ranking",
    "url": "https://tech.olx.com/from-ranknet-to-lambdamart-leveraging-xgboost-for-enhanced-ranking-models-cf21f33350fb",
    "type": "Engineering Blog",
    "level": "Hard",
    "tags": [
      "Machine Learning",
      "LTR"
    ],
    "domain": "Machine Learning",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-scikit-learn",
      "gradient-boosting",
      "xgboost"
    ],
    "topic_tags": [
      "learning-to-rank",
      "ranknet",
      "lambdamart",
      "search-algorithms",
      "xgboost-ranking"
    ],
    "summary": "Comprehensive tutorial explaining learning-to-rank fundamentals with practical implementations. Covers the progression from basic RankNet neural networks to advanced LambdaMART algorithms using XGBoost. Includes working code examples and clear explanations of why ranking problems require different approaches than standard classification.",
    "use_cases": [
      "Building search result ranking systems for e-commerce platforms",
      "Ranking job recommendations or content feeds based on user preferences"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "learning to rank tutorial with code examples",
      "difference between ranknet and lambdamart",
      "how to implement xgboost ranking algorithms",
      "learning to rank vs classification machine learning"
    ]
  },
  {
    "name": "LinkedIn: AI Behind Recruiter Search",
    "description": "Enterprise-scale search: multi-layer ranking (L1 retrieval \u2192 L2 ranking), evolution from linear to GBDT to neural, GLMix personalization. Among the largest learning-to-rank systems in production.",
    "category": "Search & Ranking",
    "url": "https://www.linkedin.com/blog/engineering/recommendations/ai-behind-linkedin-recruiter-search-and-recommendation-systems",
    "type": "Engineering Blog",
    "level": "Hard",
    "tags": [
      "Machine Learning",
      "Search"
    ],
    "domain": "Machine Learning",
    "difficulty": "intermediate",
    "prerequisites": [
      "learning-to-rank",
      "gradient-boosting",
      "neural-networks"
    ],
    "topic_tags": [
      "learning-to-rank",
      "enterprise-search",
      "multi-stage-ranking",
      "personalization",
      "production-systems"
    ],
    "summary": "LinkedIn's enterprise search system demonstrates multi-layer ranking architecture at massive scale, progressing from linear models to gradient boosting to neural approaches. The system showcases GLMix personalization framework and represents one of the largest learning-to-rank deployments in production. Essential reading for understanding how to scale search ranking systems beyond academic toy problems.",
    "use_cases": [
      "Designing multi-stage ranking pipeline for large-scale search or recommendation system",
      "Implementing personalized ranking at enterprise scale with millions of users"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "multi-stage ranking system design",
      "enterprise search ranking architecture",
      "learning to rank at LinkedIn scale",
      "GLMix personalization framework implementation"
    ]
  },
  {
    "name": "OpenAI Cookbook: Semantic Search with Embeddings",
    "description": "Modern embedding-based retrieval end-to-end. Embedding generation with OpenAI API, Pinecone vector database, cosine similarity search. Foundation for semantic search and RAG systems.",
    "category": "Search & Ranking",
    "url": "https://cookbook.openai.com/examples/vector_databases/pinecone/semantic_search",
    "type": "Tutorial",
    "level": "Medium",
    "tags": [
      "Machine Learning",
      "Search"
    ],
    "domain": "Machine Learning",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-requests",
      "vector-databases",
      "cosine-similarity"
    ],
    "topic_tags": [
      "semantic-search",
      "embeddings",
      "vector-databases",
      "openai-api",
      "pinecone"
    ],
    "summary": "A comprehensive guide to building semantic search systems using OpenAI's embedding models and Pinecone vector database. Shows the complete pipeline from text preprocessing to similarity-based retrieval using cosine distance. Essential foundation for building RAG applications and modern search experiences.",
    "use_cases": [
      "Building a document search system for internal company knowledge base",
      "Creating semantic product search for e-commerce recommendations"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "how to build semantic search with embeddings",
      "openai embeddings tutorial with vector database",
      "pinecone cosine similarity search implementation",
      "semantic search vs keyword search comparison"
    ]
  },
  {
    "name": "LangChain Academy: Intro to LangGraph",
    "description": "Most comprehensive free agent-building course. 6-hour, 55-lesson course on state management, memory, human-in-the-loop, parallelization, deployment. Used in production at Klarna, LinkedIn, Elastic.",
    "category": "LLMs & Agents",
    "url": "https://academy.langchain.com/courses/intro-to-langgraph",
    "type": "Course",
    "level": "Medium",
    "tags": [
      "Machine Learning",
      "LLMs"
    ],
    "domain": "Machine Learning",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-programming",
      "langchain-basics",
      "REST-APIs"
    ],
    "topic_tags": [
      "agent-frameworks",
      "state-management",
      "production-deployment",
      "human-in-the-loop",
      "llm-orchestration"
    ],
    "summary": "A comprehensive 55-lesson course teaching LangGraph for building production-ready AI agents with advanced features like state management, memory, and human oversight. Used by major companies like Klarna and LinkedIn, this free course covers the full pipeline from development to deployment. Essential for data scientists wanting to move beyond simple LLM calls to sophisticated agent systems.",
    "use_cases": [
      "Building customer service chatbots that maintain conversation context and escalate to humans when needed",
      "Creating data analysis agents that can run experiments, remember results, and coordinate multiple AI models in parallel"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "How to build production AI agents with memory",
      "LangGraph tutorial for state management",
      "Best course for learning agent frameworks",
      "How to deploy LLM agents at scale"
    ]
  },
  {
    "name": "Anthropic's Prompt Engineering Tutorial",
    "description": "Definitive prompting from Claude's creators. 26,000+ GitHub stars. Interactive notebooks on direct prompting, chain-of-thought, output formatting, hallucination avoidance, tool use. 'Best LLM vendor documentation' - Simon Willison.",
    "category": "LLMs & Agents",
    "url": "https://github.com/anthropics/prompt-eng-interactive-tutorial",
    "type": "Tutorial",
    "level": "Easy",
    "tags": [
      "Machine Learning",
      "LLMs"
    ],
    "domain": "Machine Learning",
    "difficulty": "beginner",
    "prerequisites": [
      "python-basics",
      "API-requests"
    ],
    "topic_tags": [
      "prompt-engineering",
      "claude-api",
      "interactive-tutorials",
      "llm-optimization",
      "output-formatting"
    ],
    "summary": "Comprehensive tutorial from Anthropic covering prompt engineering techniques for Claude and other LLMs. Features interactive Jupyter notebooks demonstrating direct prompting, chain-of-thought reasoning, output formatting, and tool integration. Highly regarded as the gold standard for learning systematic prompt optimization.",
    "use_cases": [
      "Building reliable AI chatbots with consistent output formats",
      "Creating LLM-powered data analysis workflows that minimize hallucinations"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "best prompt engineering tutorial for beginners",
      "how to reduce LLM hallucinations in production",
      "Claude API prompting techniques and examples",
      "interactive notebooks for learning prompt engineering"
    ]
  },
  {
    "name": "Eugene Yan: Patterns for Building LLM-based Systems",
    "description": "7 production patterns: Evals, RAG, Fine-tuning, Caching, Guardrails, Defensive UX, User Feedback. 66-minute read with evaluation metrics (BLEU, ROUGE, BERTScore), RAG patterns, fine-tuning decisions. From Amazon experience.",
    "category": "LLMs & Agents",
    "url": "https://eugeneyan.com/writing/llm-patterns/",
    "type": "Article",
    "level": "Medium",
    "tags": [
      "Machine Learning",
      "LLMs"
    ],
    "domain": "Machine Learning",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-programming",
      "transformer-architectures",
      "REST-APIs"
    ],
    "topic_tags": [
      "LLM-systems",
      "production-ML",
      "evaluation-metrics",
      "retrieval-augmented-generation",
      "model-fine-tuning"
    ],
    "summary": "Comprehensive guide covering 7 essential patterns for building production LLM systems, including evaluation frameworks, RAG implementation, and fine-tuning decisions. Based on real Amazon experience, provides practical metrics like BLEU and ROUGE for system assessment. Essential reading for data scientists moving from experimentation to production LLM deployment.",
    "use_cases": [
      "Building a customer service chatbot that needs reliable evaluation and guardrails",
      "Implementing a document Q&A system with RAG and proper caching strategies"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "How to evaluate LLM systems in production",
      "RAG implementation patterns and best practices",
      "LLM fine-tuning vs prompt engineering decision framework",
      "Production LLM system architecture patterns"
    ]
  },
  {
    "name": "DeepLearning.AI Short Courses",
    "description": "Rapid skill-building in 1-2 hours. Key free courses: LangChain for LLM Apps (Harrison Chase), Building Systems with ChatGPT, Functions/Tools/Agents. Interactive Jupyter notebooks, zero setup.",
    "category": "LLMs & Agents",
    "url": "https://learn.deeplearning.ai",
    "type": "Course",
    "level": "Easy",
    "tags": [
      "Machine Learning",
      "LLMs"
    ],
    "domain": "Machine Learning",
    "difficulty": "beginner",
    "prerequisites": [
      "python-basics",
      "jupyter-notebooks",
      "API-calls"
    ],
    "topic_tags": [
      "langchain",
      "chatgpt-api",
      "llm-applications",
      "interactive-tutorials",
      "hands-on-learning"
    ],
    "summary": "DeepLearning.AI offers bite-sized interactive courses that teach practical LLM application development in 1-2 hours. These hands-on tutorials use pre-configured Jupyter notebooks to walk through building real systems with LangChain, ChatGPT APIs, and agent frameworks. Perfect for quickly getting up to speed on LLM tooling without environment setup hassles.",
    "use_cases": [
      "Building a customer service chatbot using LangChain and OpenAI APIs",
      "Creating an AI assistant that can call external tools and APIs to answer complex queries"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "quick tutorial for building LLM apps",
      "beginner friendly LangChain course",
      "how to use ChatGPT API in applications",
      "interactive LLM development tutorial"
    ]
  },
  {
    "name": "FreeCodeCamp: RAG from Scratch",
    "description": "Deep RAG understanding by Lance Martin (LangChain engineer, Stanford PhD). 2.5-hour video on advanced techniques: Multi-Query, RAG Fusion, Decomposition, Step Back, HyDE, Corrective RAG, Self-RAG patterns.",
    "category": "LLMs & Agents",
    "url": "https://www.freecodecamp.org/news/mastering-rag-from-scratch/",
    "type": "Video Tutorial",
    "level": "Medium",
    "tags": [
      "Machine Learning",
      "RAG"
    ],
    "domain": "Machine Learning",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-langchain",
      "vector-databases",
      "transformer-embeddings"
    ],
    "topic_tags": [
      "retrieval-augmented-generation",
      "langchain",
      "vector-search",
      "llm-engineering",
      "video-tutorial"
    ],
    "summary": "Comprehensive 2.5-hour tutorial by LangChain engineer Lance Martin covering advanced RAG patterns from first principles. Goes beyond basic RAG to explore sophisticated techniques like Multi-Query, RAG Fusion, and Self-RAG for production applications. Perfect for practitioners wanting deep understanding of modern retrieval-augmented generation systems.",
    "use_cases": [
      "Building production RAG systems for enterprise knowledge bases",
      "Improving retrieval quality in customer support chatbots using advanced RAG patterns"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "advanced RAG techniques tutorial",
      "RAG from scratch video course",
      "LangChain RAG patterns implementation",
      "how to build production RAG systems"
    ]
  },
  {
    "name": "Lyft Engineering",
    "description": "Rideshare economics, forecasting, and marketplace efficiency. Technical deep-dives on pricing, dispatch, and causal inference.",
    "category": "Marketplace Economics",
    "url": "https://eng.lyft.com/",
    "type": "Blog",
    "level": "Medium",
    "tags": [
      "lyft",
      "rideshare",
      "forecasting"
    ],
    "domain": "Domain Applications",
    "image_url": "https://miro.medium.com/v2/resize:fit:1200/1*jEy_mYdgaUo8QHPZ4O8Ijw.png",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "A-B-testing",
      "basic-econometrics"
    ],
    "topic_tags": [
      "marketplace-design",
      "pricing-algorithms",
      "causal-inference",
      "demand-forecasting",
      "rideshare"
    ],
    "summary": "Lyft's engineering blog features technical posts on marketplace economics, dynamic pricing, and causal inference methods used in ridesharing. The content covers real-world applications of econometric methods, machine learning for demand forecasting, and optimization algorithms for driver-rider matching. Posts typically include code examples and detailed methodology explanations from Lyft's data science and economics teams.",
    "use_cases": [
      "Learning how to implement dynamic pricing algorithms for marketplace platforms",
      "Understanding causal inference techniques for measuring the impact of marketplace interventions"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "How does Lyft do dynamic pricing",
      "rideshare marketplace economics blog",
      "causal inference at tech companies",
      "demand forecasting for ride sharing"
    ]
  },
  {
    "name": "Netflix Tech Blog",
    "description": "Streaming personalization, A/B testing at scale, recommendations. How Netflix builds data products for 200M+ subscribers.",
    "category": "Search & Ranking",
    "url": "https://netflixtechblog.com/",
    "type": "Blog",
    "level": "Medium",
    "tags": [
      "netflix",
      "personalization",
      "experimentation"
    ],
    "domain": "Machine Learning",
    "image_url": "https://miro.medium.com/v2/resize:fit:1200/1*ty4NvNrGg4ReETxqU2N3Og.png",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "statistical-hypothesis-testing",
      "collaborative-filtering"
    ],
    "topic_tags": [
      "recommendation-systems",
      "ab-testing",
      "personalization",
      "streaming-platforms",
      "tech-blog"
    ],
    "summary": "Netflix's technical blog documenting their approach to building recommendation systems and running experiments at massive scale. Covers real-world implementations of personalization algorithms, A/B testing infrastructure, and data products serving 200M+ users. Essential reading for understanding how streaming platforms optimize user experience through data science.",
    "use_cases": [
      "Building recommendation systems for content platforms or e-commerce sites",
      "Designing A/B testing infrastructure for high-traffic applications"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "How does Netflix build recommendation systems",
      "A/B testing at Netflix scale",
      "Streaming platform personalization algorithms",
      "Netflix data science blog posts"
    ]
  },
  {
    "name": "LinkedIn Engineering",
    "description": "Professional network data science, feed ranking, economic graph insights. ML and economics at scale.",
    "category": "Search & Ranking",
    "url": "https://engineering.linkedin.com/blog",
    "type": "Blog",
    "level": "Medium",
    "tags": [
      "linkedin",
      "ranking",
      "networks"
    ],
    "domain": "Machine Learning",
    "image_url": "https://media.licdn.com/dms/image/v2/D4D08AQFhfZ29NAMysw/croft-frontend-shrinkToFit1024/croft-frontend-shrinkToFit1024/0/1704725126858?e=2147483647&v=beta&t=V0MYfZWEy1ih4igUZgbIg8XIkxlycNP4mGXA8_GFF0Q",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-scikit-learn",
      "network-analysis",
      "ranking-algorithms"
    ],
    "topic_tags": [
      "social-networks",
      "recommendation-systems",
      "feed-ranking",
      "graph-algorithms",
      "network-effects"
    ],
    "summary": "LinkedIn Engineering's technical blog and resources covering large-scale data science applications in professional networking. Focus on feed ranking algorithms, economic graph analysis, and machine learning systems that power LinkedIn's platform. Provides insights into real-world implementation of network analysis and recommendation systems at massive scale.",
    "use_cases": [
      "Building recommendation systems for social or professional networks",
      "Implementing feed ranking algorithms for content platforms"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "How does LinkedIn rank posts in news feed",
      "Network effects measurement in social platforms",
      "Large scale recommendation system architecture",
      "Economic graph analysis techniques"
    ]
  },
  {
    "name": "Google AI Blog",
    "description": "Research publications from Google AI. Covers ML, NLP, computer vision, and applied AI research.",
    "category": "LLMs & Agents",
    "url": "https://ai.googleblog.com/",
    "type": "Blog",
    "level": "Hard",
    "tags": [
      "google",
      "research",
      "ai"
    ],
    "domain": "Machine Learning",
    "image_url": "https://lh3.googleusercontent.com/IYTYPvbsFNvTsrF2vFC4OqKbPgjVVLpBPYoTMvdvCgxXlL1wN5_DwqAk7fWLVs_LgEYJVPz-zBqLLGDMJsAUAkM_tEdZ7YE=w1200",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-programming",
      "linear-algebra",
      "neural-networks"
    ],
    "topic_tags": [
      "machine-learning",
      "deep-learning",
      "research-papers",
      "google-ai",
      "blog-posts"
    ],
    "summary": "Google AI Blog is the official research publication platform where Google's AI teams share their latest findings in machine learning, natural language processing, computer vision, and applied AI. The blog covers both theoretical breakthroughs and practical applications, making cutting-edge research accessible to practitioners. Posts typically include detailed explanations, code examples, and links to full research papers.",
    "use_cases": [
      "Staying current with state-of-the-art ML techniques and Google's latest AI developments",
      "Finding implementation details and practical insights for reproducing research results in production systems"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the latest AI research papers from Google",
      "Google AI blog machine learning breakthroughs",
      "How does Google implement large language models",
      "State of the art computer vision techniques from Google"
    ]
  },
  {
    "name": "Tim Roughgarden's CS269I: Incentives in Computer Science",
    "description": "20+ hours of video with publication-quality notes. Covers Gale-Shapley, NRMP matching, deferred acceptance, strategyproofness proofs, cryptocurrency incentives. Uniquely bridges classical stable matching with modern applications.",
    "category": "Market Design & Matching",
    "url": "https://timroughgarden.org/f16/f16.html",
    "type": "Video Course",
    "level": "Hard",
    "tags": [
      "Economics",
      "Market Design"
    ],
    "domain": "Economics",
    "difficulty": "intermediate",
    "prerequisites": [
      "game-theory-basics",
      "mathematical-proofs",
      "algorithm-analysis"
    ],
    "topic_tags": [
      "stable-matching",
      "mechanism-design",
      "strategyproof-algorithms",
      "video-lectures",
      "academic-course"
    ],
    "summary": "Tim Roughgarden's comprehensive Stanford course bridging classical matching theory with modern applications in tech and crypto. Features rigorous treatment of Gale-Shapley algorithm, medical residency matching, and strategic behavior in algorithmic systems. Combines theoretical foundations with practical implementation insights through high-quality video lectures and detailed notes.",
    "use_cases": [
      "Designing matching systems for two-sided markets like job platforms or dating apps",
      "Understanding incentive structures in cryptocurrency protocols and blockchain mechanisms"
    ],
    "audience": [
      "Early-PhD",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "stable matching algorithms course",
      "Gale-Shapley algorithm video lectures",
      "mechanism design for computer scientists",
      "strategyproof matching theory Stanford"
    ]
  },
  {
    "name": "Tim Roughgarden's CS364A: Kidney Exchange",
    "description": "Definitive algorithmic treatment of kidney exchange. Covers Top Trading Cycles, cycle packing, incentive-compatible organ allocation. The actual algorithms used by the Alliance for Paired Kidney Donation.",
    "category": "Market Design & Matching",
    "url": "https://timroughgarden.org/f13/l/l10.pdf",
    "type": "Lecture Notes",
    "level": "Hard",
    "tags": [
      "Economics",
      "Market Design"
    ],
    "domain": "Economics",
    "difficulty": "intermediate",
    "prerequisites": [
      "graph-theory",
      "linear-programming",
      "game-theory-basics"
    ],
    "topic_tags": [
      "kidney-exchange",
      "matching-algorithms",
      "top-trading-cycles",
      "mechanism-design",
      "healthcare-economics"
    ],
    "summary": "Tim Roughgarden's comprehensive course on the algorithms powering real kidney exchange systems. Covers the theoretical foundations and practical implementations of matching algorithms like Top Trading Cycles used by organ donation networks. Essential resource for understanding how market design theory translates into life-saving healthcare applications.",
    "use_cases": [
      "Designing matching systems for healthcare resource allocation",
      "Understanding algorithmic approaches to two-sided markets with compatibility constraints"
    ],
    "audience": [
      "Early-PhD",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "kidney exchange algorithms",
      "top trading cycles implementation",
      "organ allocation matching theory",
      "Tim Roughgarden kidney exchange course"
    ]
  },
  {
    "name": "Uber Engineering: Marketplace Matching",
    "description": "How classical two-sided matching translates to 30M+ predictions/minute. MDP framework, batch vs. greedy matching, bipartite graph algorithms, RL for supply-demand balance. Quantitative production results.",
    "category": "Market Design & Matching",
    "url": "https://www.uber.com/blog/research/dynamic-pricing-and-matching-in-ride-hailing-platforms/",
    "type": "Engineering Blog",
    "level": "Medium",
    "tags": [
      "Economics",
      "Marketplace"
    ],
    "domain": "Economics",
    "difficulty": "intermediate",
    "prerequisites": [
      "graph-algorithms",
      "reinforcement-learning",
      "markov-decision-processes"
    ],
    "topic_tags": [
      "two-sided-matching",
      "marketplace-optimization",
      "bipartite-graphs",
      "real-time-systems",
      "production-engineering"
    ],
    "summary": "Uber's engineering approach to scaling classical two-sided matching theory to handle 30+ million predictions per minute in their rider-driver marketplace. Covers the transition from theoretical matching algorithms to production systems using MDP frameworks, batch vs greedy matching strategies, and reinforcement learning for supply-demand optimization. Includes quantitative results and performance metrics from Uber's live marketplace.",
    "use_cases": [
      "Building matching systems for ride-sharing or delivery platforms",
      "Optimizing real-time marketplace allocation with millions of participants"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "How does Uber match drivers and riders at scale",
      "Two-sided matching algorithms for marketplaces",
      "Production implementation of bipartite matching",
      "Real-time marketplace optimization techniques"
    ]
  },
  {
    "name": "Airbnb Engineering: Two-Sided Marketplace Matching",
    "description": "Unique focus on 'both sides must accept' constraint. Host acceptance prediction, listing embeddings, cold start solutions. Shows how to infer host preferences from behavior\u20143.75% booking improvement.",
    "category": "Market Design & Matching",
    "url": "https://medium.com/airbnb-engineering/how-airbnb-uses-machine-learning-to-detect-host-preferences-18ce07150fa3",
    "type": "Engineering Blog",
    "level": "Medium",
    "tags": [
      "Economics",
      "Marketplace"
    ],
    "domain": "Economics",
    "difficulty": "intermediate",
    "prerequisites": [
      "machine-learning-embeddings",
      "logistic-regression",
      "python-scikit-learn"
    ],
    "topic_tags": [
      "two-sided-markets",
      "recommendation-systems",
      "host-guest-matching",
      "marketplace-optimization",
      "behavioral-inference"
    ],
    "summary": "Airbnb's approach to solving the unique challenge of two-sided marketplace matching where both hosts and guests must accept each other. The resource details how to predict host acceptance behavior, create listing embeddings, and handle cold start problems in marketplace settings. Shows practical implementation that achieved measurable business impact through behavioral inference techniques.",
    "use_cases": [
      "Building recommendation systems for platforms where both parties must approve the match (dating apps, freelance marketplaces, rental platforms)",
      "Improving conversion rates in marketplaces by predicting and optimizing for mutual acceptance probability"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "How to handle two-sided matching in marketplaces",
      "Airbnb host acceptance prediction model",
      "Cold start problem solutions for marketplace recommendations",
      "Behavioral inference techniques for marketplace optimization"
    ]
  },
  {
    "name": "Tim Roughgarden's CS364A: Mechanism Design",
    "description": "The definitive free resource from a G\u00f6del Prize winner. 20 video lectures (~75 min each) covering Vickrey auctions, Myerson's Lemma, VCG, sponsored search, combinatorial auctions, revenue-maximizing mechanisms.",
    "category": "Auction Theory",
    "url": "https://timroughgarden.org/f13/f13.html",
    "type": "Video Course",
    "level": "Hard",
    "tags": [
      "Economics",
      "Auctions"
    ],
    "domain": "Economics",
    "difficulty": "advanced",
    "prerequisites": [
      "microeconomics-theory",
      "game-theory",
      "mathematical-optimization"
    ],
    "topic_tags": [
      "mechanism-design",
      "auction-theory",
      "game-theory",
      "video-lectures",
      "sponsored-search"
    ],
    "summary": "Comprehensive video lecture series on mechanism design by G\u00f6del Prize winner Tim Roughgarden, covering foundational auction theory from Vickrey auctions to modern applications. Essential resource for understanding how to design systems where strategic agents truthfully reveal private information. Covers both theoretical foundations and practical applications like sponsored search auctions.",
    "use_cases": [
      "Designing auction mechanisms for ad platforms or marketplace bidding systems",
      "Understanding revenue optimization strategies for multi-item auctions in e-commerce"
    ],
    "audience": [
      "Early-PhD",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "mechanism design course video lectures",
      "Vickrey auction theory explained",
      "sponsored search auction algorithms",
      "combinatorial auction mechanism design"
    ]
  },
  {
    "name": "Easley & Kleinberg: Sponsored Search Markets",
    "description": "Clearest pedagogical treatment of online ad auctions. VCG from 'harm principle,' GSP mechanics, GSP vs VCG comparison with worked examples, why truth-telling isn't dominant in GSP. Perfect for understanding Google/Facebook ads.",
    "category": "Auction Theory",
    "url": "https://www.cs.cornell.edu/home/kleinber/networks-book/networks-book-ch15.pdf",
    "type": "Chapter",
    "level": "Medium",
    "tags": [
      "Economics",
      "Auctions"
    ],
    "domain": "Economics",
    "difficulty": "intermediate",
    "prerequisites": [
      "game-theory-basics",
      "microeconomics-fundamentals",
      "mechanism-design"
    ],
    "topic_tags": [
      "auction-theory",
      "sponsored-search",
      "vcg-auctions",
      "ad-markets",
      "mechanism-design"
    ],
    "summary": "Comprehensive textbook chapter explaining how online advertising auctions work, covering both theoretical VCG auctions and practical Generalized Second Price (GSP) mechanisms used by Google and Facebook. Provides clear mathematical foundations with worked examples showing why advertisers don't always bid truthfully in real ad platforms.",
    "use_cases": [
      "Understanding how to optimize bidding strategies for Google Ads or Facebook advertising campaigns",
      "Designing auction mechanisms for marketplace platforms or analyzing competitive dynamics in digital advertising markets"
    ],
    "audience": [
      "Early-PhD",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "how do Google ads auctions work",
      "VCG vs GSP auction comparison",
      "why don't advertisers bid truthfully in sponsored search",
      "mechanism design for online advertising"
    ]
  },
  {
    "name": "Jonathan Levin's Revenue Equivalence Notes",
    "description": "Concise, rigorous proof from leading auction theorist (Susan Athey co-author). Revenue Equivalence Theorem, first-price vs. second-price, Dutch/English equivalence. Essential for understanding when auction format matters.",
    "category": "Auction Theory",
    "url": "https://economics.utoronto.ca/damiano/ps426/RET-Levin-Notes.pdf",
    "type": "Lecture Notes",
    "level": "Hard",
    "tags": [
      "Economics",
      "Auctions"
    ],
    "domain": "Economics",
    "difficulty": "intermediate",
    "prerequisites": [
      "game-theory-basics",
      "probability-theory",
      "mathematical-proofs"
    ],
    "topic_tags": [
      "revenue-equivalence",
      "auction-design",
      "mechanism-design",
      "economic-theory",
      "mathematical-economics"
    ],
    "summary": "Rigorous mathematical notes proving the Revenue Equivalence Theorem by Jonathan Levin, demonstrating when different auction formats yield identical expected revenue. Essential reading for understanding auction mechanism design and when format choice matters for revenue optimization. Provides clear proofs connecting first-price, second-price, Dutch, and English auction equivalences.",
    "use_cases": [
      "Designing optimal auction mechanisms for marketplace platforms",
      "Understanding when to use different bidding formats in advertising auctions"
    ],
    "audience": [
      "Early-PhD",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "revenue equivalence theorem proof",
      "first price vs second price auction revenue",
      "when do auction formats give same revenue",
      "Jonathan Levin auction theory notes"
    ]
  },
  {
    "name": "Kevin Leyton-Brown's VCG Mechanism Lectures",
    "description": "Structured theorem-proof format with worked examples. VCG formal definition, DSIC proofs, Clarke pivot rule, budget balance, shortest path auctions. Shows exactly how second-price sealed-bid is VCG special case.",
    "category": "Auction Theory",
    "url": "https://www.cs.ubc.ca/~kevinlb/teaching/cs532l%20-%202007-8/lectures/lect16.pdf",
    "type": "Lecture Notes",
    "level": "Hard",
    "tags": [
      "Economics",
      "Auctions"
    ],
    "domain": "Economics",
    "difficulty": "intermediate",
    "prerequisites": [
      "game-theory-basics",
      "mathematical-proofs",
      "optimization-theory"
    ],
    "topic_tags": [
      "VCG-mechanism",
      "auction-design",
      "mechanism-design",
      "dominant-strategy",
      "lecture-series"
    ],
    "summary": "Comprehensive lecture series on Vickrey-Clarke-Groves (VCG) mechanisms covering formal definitions, dominant strategy incentive compatibility proofs, and practical implementations. Uses structured theorem-proof format with worked examples including Clarke pivot rule and shortest path auctions. Essential foundational material for understanding modern auction theory and mechanism design.",
    "use_cases": [
      "Designing online advertising auctions with truthful bidding incentives",
      "Creating fair allocation mechanisms for computational resources in cloud platforms"
    ],
    "audience": [
      "Early-PhD",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "VCG mechanism tutorial with proofs",
      "how does second price auction relate to VCG",
      "dominant strategy incentive compatible auctions explained",
      "Clarke pivot rule mechanism design"
    ]
  },
  {
    "name": "Google Research: Market Algorithms Team",
    "description": "Direct from engineers designing Google's auction systems. Ad exchange design, budget-constrained mechanisms, autobidding formulas, Price of Anarchy. Collaboration between Roughgarden, Tardos, and Google engineers.",
    "category": "Auction Theory",
    "url": "https://research.google/teams/market-algorithms/",
    "type": "Research Blog",
    "level": "Hard",
    "tags": [
      "Economics",
      "Auctions"
    ],
    "domain": "Economics",
    "difficulty": "advanced",
    "prerequisites": [
      "game-theory",
      "mechanism-design",
      "convex-optimization"
    ],
    "topic_tags": [
      "auction-mechanisms",
      "ad-exchanges",
      "algorithmic-game-theory",
      "price-of-anarchy",
      "autobidding"
    ],
    "summary": "Research collaboration between Google engineers and academic economists on designing auction systems for ad exchanges. Covers budget-constrained mechanisms, autobidding algorithms, and theoretical analysis of market efficiency through Price of Anarchy metrics. Provides insights into real-world implementation of auction theory at massive scale.",
    "use_cases": [
      "designing auction mechanisms for two-sided marketplaces",
      "implementing autobidding systems for advertising platforms"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "how does Google design ad auction mechanisms",
      "autobidding algorithms implementation",
      "price of anarchy in ad exchanges",
      "budget constrained auction mechanisms"
    ]
  },
  {
    "name": "NFX Network Effects Bible",
    "description": "The definitive practitioner reference. Sarnoff's/Metcalfe's/Reed's Laws, critical mass, same-side vs. cross-side effects, chicken-and-egg solutions, switching costs. Continuously updated with visual diagrams.",
    "category": "Platform Economics",
    "url": "https://www.nfx.com/post/network-effects-bible",
    "type": "Guide",
    "level": "Medium",
    "tags": [
      "Economics",
      "Network Effects"
    ],
    "domain": "Economics",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-microeconomics",
      "business-strategy-fundamentals"
    ],
    "topic_tags": [
      "network-effects",
      "platform-strategy",
      "market-dynamics",
      "business-models",
      "competitive-moats"
    ],
    "summary": "A comprehensive practitioner's guide to understanding and applying network effects in business strategy. Covers foundational laws like Metcalfe's and Reed's, practical frameworks for identifying network effects, and strategies for overcoming chicken-and-egg problems. Essential reading for anyone working on platforms, marketplaces, or products where user value increases with network size.",
    "use_cases": [
      "Designing growth strategies for a two-sided marketplace platform",
      "Analyzing competitive advantages and switching costs for social media products"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "what are network effects in platform business",
      "metcalfe's law vs reed's law differences",
      "how to solve chicken and egg problem marketplace",
      "network effects types same side cross side"
    ]
  },
  {
    "name": "NFX Network Effects Manual: 16 Types",
    "description": "Most granular taxonomy: Physical, Protocol, Personal Utility, Marketplace, Platform, Asymptotic, Data, Tech Performance, Language, Belief, Bandwagon, Tribal effects. Explains why Uber/Lyft face asymptotic effects.",
    "category": "Platform Economics",
    "url": "https://www.nfx.com/post/network-effects-manual",
    "type": "Guide",
    "level": "Medium",
    "tags": [
      "Economics",
      "Network Effects"
    ],
    "domain": "Economics",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-microeconomics",
      "business-strategy-frameworks"
    ],
    "topic_tags": [
      "network-effects",
      "platform-strategy",
      "competitive-moats",
      "business-models"
    ],
    "summary": "A comprehensive taxonomy breaking down network effects into 16 distinct types, from physical networks to psychological effects like bandwagon and tribal behaviors. This manual provides frameworks for understanding how different platforms create value through user connections and explains strategic implications like why ride-sharing companies face diminishing returns at scale.",
    "use_cases": [
      "Evaluating competitive advantages when analyzing platform companies like marketplaces or social networks",
      "Designing growth strategies for two-sided platforms by identifying which network effects to prioritize"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "types of network effects classification",
      "why do network effects stop working",
      "platform economics network effects taxonomy",
      "uber lyft network effects analysis"
    ]
  },
  {
    "name": "NFX Network Effects Masterclass",
    "description": "3+ hour video course from operators who built 10+ companies with $10B+ in exits. 16 network effect types, case studies (Uber, Facebook, Bitcoin), Network Bonding Theory, cold start, Web3 applications.",
    "category": "Platform Economics",
    "url": "https://www.nfx.com/masterclass",
    "type": "Video Course",
    "level": "Medium",
    "tags": [
      "Economics",
      "Network Effects"
    ],
    "domain": "Economics",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-microeconomics",
      "business-strategy-fundamentals"
    ],
    "topic_tags": [
      "network-effects",
      "platform-strategy",
      "cold-start-problem",
      "web3-economics",
      "video-course"
    ],
    "summary": "A comprehensive video masterclass teaching 16 types of network effects through real case studies from companies like Uber, Facebook, and Bitcoin. Created by experienced operators, it covers Network Bonding Theory, cold start strategies, and Web3 applications for building network-driven businesses.",
    "use_cases": [
      "Understanding how to build network effects into a new platform or marketplace product",
      "Analyzing why certain tech companies achieved massive scale and defensibility through network dynamics"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "how do network effects work in tech companies",
      "network effects masterclass for beginners",
      "cold start problem solutions for platforms",
      "Web3 network effects examples"
    ]
  },
  {
    "name": "Stratechery Aggregation Theory",
    "description": "Most cited framework for understanding internet platform dominance. Zero distribution/marginal/transaction costs, aggregator virtuous cycle, winner-take-all dynamics, platform vs. aggregator distinction.",
    "category": "Platform Economics",
    "url": "https://stratechery.com/aggregation-theory/",
    "type": "Blog Series",
    "level": "Medium",
    "tags": [
      "Economics",
      "Platforms"
    ],
    "domain": "Economics",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-microeconomics",
      "network-effects-concepts"
    ],
    "topic_tags": [
      "aggregation-theory",
      "platform-economics",
      "network-effects",
      "business-strategy",
      "internet-platforms"
    ],
    "summary": "Stratechery's Aggregation Theory explains how internet platforms achieve dominance through zero marginal costs and direct customer relationships. The framework describes the virtuous cycle where platforms attract users, then suppliers, creating winner-take-all dynamics. It's essential reading for understanding modern tech company strategy and market structure.",
    "use_cases": [
      "Analyzing why Google/Facebook dominate their markets and how new platforms might compete",
      "Designing product strategy for a two-sided marketplace or platform business"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is aggregation theory",
      "How do internet platforms achieve dominance",
      "Platform vs aggregator difference",
      "Why do tech companies become monopolies"
    ]
  },
  {
    "name": "a16z: Measuring Network Effects",
    "description": "Quantitative measurement frameworks. Network effects vs. virality vs. scale, multi-tenanting impact, practical KPIs (DAU/MAU by density, organic vs. paid ratios, market-by-market unit economics).",
    "category": "Platform Economics",
    "url": "https://a16z.com/tag/all-about-network-effects/",
    "type": "Blog Series",
    "level": "Medium",
    "tags": [
      "Economics",
      "Network Effects"
    ],
    "domain": "Economics",
    "difficulty": "intermediate",
    "prerequisites": [
      "cohort-analysis",
      "funnel-metrics",
      "basic-econometrics"
    ],
    "topic_tags": [
      "network-effects",
      "platform-metrics",
      "growth-measurement",
      "marketplace-economics",
      "kpi-frameworks"
    ],
    "summary": "A16z's framework for quantitatively measuring network effects in platform businesses, distinguishing them from virality and scale effects. Provides practical KPI methodologies including density-based DAU/MAU ratios, organic vs paid user acquisition metrics, and market-by-market unit economics for multi-sided platforms.",
    "use_cases": [
      "Product manager at a marketplace needs to prove network effects are driving growth vs just viral marketing",
      "Data scientist building dashboards to track platform health across different geographic markets"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "how to measure network effects quantitatively",
      "difference between network effects and virality metrics",
      "KPIs for marketplace platform growth",
      "measuring multi-sided platform economics"
    ]
  },
  {
    "name": "QuantEcon: Discrete State Dynamic Programming",
    "description": "Gold standard for DP in economics. Bellman equation, value/policy iteration, contraction mapping proofs, stochastic optimal growth. Runnable Jupyter notebooks implement DiscreteDP class.",
    "category": "Computational Economics",
    "url": "https://python-advanced.quantecon.org/discrete_dp.html",
    "type": "Tutorial",
    "level": "Hard",
    "tags": [
      "Economics",
      "Dynamic Programming"
    ],
    "domain": "Economics",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-programming",
      "linear-algebra",
      "optimization-theory"
    ],
    "topic_tags": [
      "dynamic-programming",
      "bellman-equation",
      "value-iteration",
      "jupyter-notebooks",
      "computational-economics"
    ],
    "summary": "Comprehensive tutorial on discrete state dynamic programming with rigorous mathematical foundations and practical implementation. Covers the Bellman equation, value and policy iteration algorithms, and contraction mapping theory with runnable Python code. Essential resource for economists and data scientists working on sequential decision problems.",
    "use_cases": [
      "Modeling optimal investment decisions over time with uncertainty",
      "Solving inventory management problems with stochastic demand"
    ],
    "audience": [
      "Early-PhD",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "dynamic programming bellman equation tutorial",
      "value iteration policy iteration python implementation",
      "discrete state DP economics jupyter notebook",
      "stochastic optimal growth model code"
    ]
  },
  {
    "name": "Matteo Courthoud's BLP Demand Estimation",
    "description": "Exceptionally clear BLP from first principles. Share inversion, nested fixed-point step-by-step, instrument selection (BLP, Hausman, cost shifters), GMM estimation. Python implementation included.",
    "category": "Computational Economics",
    "url": "https://matteocourthoud.github.io/course/empirical-io/02_demand_estimation/",
    "type": "Course Notes",
    "level": "Hard",
    "tags": [
      "Economics",
      "IO"
    ],
    "domain": "Economics",
    "difficulty": "advanced",
    "prerequisites": [
      "industrial-organization",
      "python-numpy",
      "gmm-estimation"
    ],
    "topic_tags": [
      "blp-estimation",
      "demand-modeling",
      "industrial-organization",
      "python-implementation",
      "gmm"
    ],
    "summary": "A comprehensive tutorial on Berry-Levinsohn-Pakes (BLP) demand estimation methodology, covering the complete workflow from theoretical foundations to practical implementation. Explains share inversion, nested fixed-point algorithms, and instrument selection strategies with working Python code. Essential for researchers analyzing product markets and estimating demand elasticities in differentiated product industries.",
    "use_cases": [
      "Estimating demand elasticities for products in differentiated markets like automobiles or cereals",
      "Analyzing market power and competitive effects in merger simulations for antitrust policy"
    ],
    "audience": [
      "Early-PhD",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "BLP demand estimation tutorial Python",
      "how to implement Berry Levinsohn Pakes model",
      "nested fixed point algorithm BLP step by step",
      "instrument selection for demand estimation BLP"
    ]
  },
  {
    "name": "Frank Pinter's Demand Estimation Notes",
    "description": "Builds intuition from multinomial logit \u2192 Berry (1994) \u2192 full BLP. MPEC vs. nested fixed-point, micro BLP with second-choice data. Written for PhD field exam prep with red bus-blue bus example.",
    "category": "Computational Economics",
    "url": "https://frankpinter.com/notes/Demand_Estimation_Notes.pdf",
    "type": "Course Notes",
    "level": "Hard",
    "tags": [
      "Economics",
      "IO"
    ],
    "domain": "Economics",
    "difficulty": "intermediate",
    "prerequisites": [
      "multinomial-logit",
      "maximum-likelihood-estimation",
      "numerical-optimization"
    ],
    "topic_tags": [
      "demand-estimation",
      "blp-model",
      "industrial-organization",
      "structural-economics",
      "mpec"
    ],
    "summary": "Comprehensive notes on demand estimation methods progressing from basic multinomial logit to the full Berry-Levinsohn-Pakes (BLP) model. Covers both mathematical foundation and computational implementation, including MPEC vs nested fixed-point approaches and extensions to micro BLP with second-choice data. Written as PhD field exam preparation material with intuitive examples.",
    "use_cases": [
      "PhD student preparing for industrial organization comprehensive exams",
      "Researcher implementing BLP demand estimation for market structure analysis"
    ],
    "audience": [
      "Early-PhD",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "BLP demand estimation tutorial",
      "multinomial logit to BLP progression",
      "MPEC vs nested fixed point BLP",
      "red bus blue bus example demand estimation"
    ]
  },
  {
    "name": "AEA: Machine Learning and Econometrics (Athey/Imbens)",
    "description": "9 hours from two of the most influential computational economists. ML vs. causal inference, heterogeneous treatment effects, LASSO/random forests, causal forests, policy learning. Athey pioneered ML in economics; Imbens won 2021 Nobel.",
    "category": "Computational Economics",
    "url": "https://www.aeaweb.org/conference/cont-ed/2018-webcasts",
    "type": "Video Course",
    "level": "Hard",
    "tags": [
      "Economics",
      "Causal ML"
    ],
    "domain": "Economics",
    "difficulty": "intermediate",
    "prerequisites": [
      "linear-regression",
      "python-scikit-learn",
      "difference-in-differences"
    ],
    "topic_tags": [
      "causal-inference",
      "machine-learning",
      "econometrics",
      "treatment-effects",
      "video-lectures"
    ],
    "summary": "Comprehensive 9-hour lecture series from Nobel laureate Guido Imbens and Susan Athey covering the intersection of machine learning and causal inference. Covers heterogeneous treatment effects, causal forests, policy learning, and when to use ML vs traditional econometric methods. Essential viewing for anyone applying ML methods to causal questions in economics or industry.",
    "use_cases": [
      "Estimating personalized treatment effects in A/B tests with machine learning",
      "Learning policy optimization and causal forest implementation for heterogeneous effects"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "machine learning causal inference lectures",
      "Athey Imbens econometrics course",
      "causal forests heterogeneous treatment effects",
      "ML vs econometrics when to use which"
    ]
  },
  {
    "name": "MIT OCW: Dynamic Programming (Bertsekas)",
    "description": "6 advanced lectures (~12 hours) from the definitive DP authority. Approximate DP, large-scale infinite horizon problems, policy iteration with function approximation, temporal difference, neuro-dynamic programming.",
    "category": "Computational Economics",
    "url": "https://ocw.mit.edu/courses/6-231-dynamic-programming-and-stochastic-control-fall-2015/pages/related-video-lectures/",
    "type": "Video Course",
    "level": "Hard",
    "tags": [
      "Economics",
      "Dynamic Programming"
    ],
    "domain": "Economics",
    "difficulty": "advanced",
    "prerequisites": [
      "markov-decision-processes",
      "linear-algebra",
      "optimization-theory"
    ],
    "topic_tags": [
      "dynamic-programming",
      "reinforcement-learning",
      "function-approximation",
      "neuro-dynamic-programming",
      "lectures"
    ],
    "summary": "Advanced lecture series from Dimitri Bertsekas covering cutting-edge dynamic programming methods for large-scale problems. Focuses on approximate solutions, policy iteration with function approximation, and neuro-dynamic programming techniques. Essential for researchers working on reinforcement learning, optimal control, and complex sequential decision problems.",
    "use_cases": [
      "Building reinforcement learning models for large state spaces where exact DP is computationally infeasible",
      "Developing optimal bidding strategies in ad auctions with complex state dynamics and function approximation"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "Bertsekas dynamic programming lectures",
      "function approximation in reinforcement learning",
      "neuro dynamic programming course",
      "advanced DP methods for large problems"
    ]
  },
  {
    "name": "Open Source Economics: Structural Estimation",
    "description": "From UChicago's Masters in Computational Social Science. Structural vs. reduced-form, MLE, GMM, Simulated Method of Moments. Complete GitHub repositories with Python/Jupyter implementations.",
    "category": "Computational Economics",
    "url": "https://opensourceecon.github.io/CompMethods/struct_est/intro.html",
    "type": "Online Course",
    "level": "Hard",
    "tags": [
      "Economics",
      "Structural"
    ],
    "domain": "Economics",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "maximum-likelihood-estimation",
      "microeconomics-theory"
    ],
    "topic_tags": [
      "structural-estimation",
      "computational-economics",
      "maximum-likelihood",
      "jupyter-notebooks",
      "econometrics"
    ],
    "summary": "University of Chicago course materials teaching structural estimation methods for economists, contrasting with reduced-form approaches. Covers maximum likelihood estimation, GMM, and Simulated Method of Moments with hands-on Python implementations. Provides complete GitHub repositories with Jupyter notebooks for practical learning of advanced econometric techniques.",
    "use_cases": [
      "Estimating consumer demand models to understand price elasticity and market structure",
      "Building dynamic models of firm investment decisions using structural parameters"
    ],
    "audience": [
      "Early-PhD",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "structural estimation python tutorial",
      "MLE vs GMM econometrics course",
      "computational economics jupyter notebooks",
      "structural vs reduced form methods comparison"
    ]
  },
  {
    "name": "ritvikmath Time Series YouTube + GitHub",
    "description": "Hand-drawn diagrams build intuition before code. Covers AR, MA, ARMA, ARIMA, SARIMA, stationarity, ACF/PACF, GARCH. GitHub repo (700+ stars) with complete Jupyter notebooks. Explains why not just how.",
    "category": "Classical Methods",
    "url": "https://www.youtube.com/@ritvikmath",
    "type": "Video Series",
    "level": "Easy",
    "tags": [
      "Forecasting",
      "Time Series"
    ],
    "domain": "Forecasting & Time Series",
    "difficulty": "beginner",
    "prerequisites": [
      "python-basics",
      "pandas-dataframes",
      "basic-statistics"
    ],
    "topic_tags": [
      "time-series-analysis",
      "forecasting-models",
      "youtube-tutorials",
      "jupyter-notebooks",
      "visual-learning"
    ],
    "summary": "Visual YouTube series teaching time series fundamentals through hand-drawn diagrams before diving into code implementation. Covers classical forecasting methods like ARIMA and GARCH with accompanying GitHub notebooks that explain the intuition behind each technique.",
    "use_cases": [
      "Learning time series forecasting for first data science project involving sales or demand prediction",
      "Understanding stationarity concepts before implementing ARIMA models for financial data analysis"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "beginner time series analysis tutorial with code",
      "ARIMA SARIMA explained visually with examples",
      "time series forecasting YouTube course with notebooks",
      "learn ACF PACF for time series analysis"
    ]
  },
  {
    "name": "Interpreting ACF and PACF Plots",
    "description": "Uses synthetic data with known parameters to demonstrate what patterns indicate which model types. Clear decision rules for AR/MA order selection. Visual approach builds pattern recognition skill.",
    "category": "Classical Methods",
    "url": "https://towardsdatascience.com/interpreting-acf-and-pacf-plots-for-time-series-forecasting-af0d6db4061c/",
    "type": "Tutorial",
    "level": "Medium",
    "tags": [
      "Forecasting",
      "Time Series"
    ],
    "domain": "Forecasting & Time Series",
    "difficulty": "beginner",
    "prerequisites": [
      "time-series-basics",
      "autoregression-concepts",
      "moving-averages"
    ],
    "topic_tags": [
      "ACF-PACF",
      "ARIMA-modeling",
      "pattern-recognition",
      "model-selection",
      "visual-diagnostics"
    ],
    "summary": "A tutorial using synthetic datasets to teach pattern recognition in ACF and PACF plots for time series model identification. Provides clear decision rules for determining AR and MA orders through visual inspection. Essential skill for classical time series analysis before fitting ARIMA models.",
    "use_cases": [
      "Identifying appropriate ARIMA model order before forecasting sales or demand data",
      "Diagnosing time series structure in A/B test metrics with temporal dependencies"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "how to read ACF PACF plots",
      "ARIMA model selection using correlograms",
      "interpreting autocorrelation plots for time series",
      "AR MA order identification visual guide"
    ]
  },
  {
    "name": "MSTL Multi-Seasonal Decomposition in Python",
    "description": "Written by the engineer who contributed MSTL to statsmodels. STL algorithm internals, LOESS smoothing foundations, comparison to Prophet/TBATS. Electricity demand example with step-by-step algorithm walkthrough.",
    "category": "Classical Methods",
    "url": "https://www.blog.trainindata.com/multi-seasonal-time-series-decomposition-using-mstl-in-python/",
    "type": "Tutorial",
    "level": "Medium",
    "tags": [
      "Forecasting",
      "Decomposition"
    ],
    "domain": "Forecasting & Time Series",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "time-series-analysis",
      "statsmodels"
    ],
    "topic_tags": [
      "seasonal-decomposition",
      "MSTL",
      "LOESS-smoothing",
      "time-series",
      "electricity-demand"
    ],
    "summary": "Deep dive into MSTL (Multiple Seasonal-Trend decomposition using Loess) written by the original contributor to statsmodels. Covers STL algorithm internals, LOESS smoothing foundations, and practical comparisons to Prophet and TBATS. Includes step-by-step implementation with electricity demand forecasting example.",
    "use_cases": [
      "Decomposing electricity demand with daily and weekly seasonality",
      "Analyzing retail sales data with multiple seasonal patterns"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "MSTL implementation Python statsmodels",
      "multiple seasonal decomposition electricity demand",
      "STL vs Prophet vs TBATS comparison",
      "LOESS smoothing time series decomposition"
    ]
  },
  {
    "name": "Time Series Handbook: LightGBM for M5",
    "description": "Complete Jupyter Book with runnable code. LightGBM MAE (200.5) vs. naive baseline (698.0). Feature engineering (lags, rolling windows), recursive vs. direct forecasting, hyperparameter tuning. Free via GitHub with Binder.",
    "category": "Machine Learning",
    "url": "https://phdinds-aim.github.io/time_series_handbook/08_WinningestMethods/lightgbm_m5_forecasting.html",
    "type": "Interactive Tutorial",
    "level": "Medium",
    "tags": [
      "Forecasting",
      "LightGBM"
    ],
    "domain": "Forecasting & Time Series",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "gradient-boosting",
      "time-series-fundamentals"
    ],
    "topic_tags": [
      "lightgbm",
      "m5-competition",
      "time-series-forecasting",
      "feature-engineering",
      "jupyter-book"
    ],
    "summary": "A comprehensive Jupyter Book demonstrating LightGBM for time series forecasting using the M5 competition dataset. Covers feature engineering techniques like lags and rolling windows, compares recursive vs. direct forecasting approaches, and includes hyperparameter optimization. Achieves MAE of 200.5 compared to 698.0 naive baseline with fully runnable code examples.",
    "use_cases": [
      "Building demand forecasting models for retail inventory planning",
      "Creating sales prediction systems with gradient boosting methods"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "lightgbm time series forecasting tutorial",
      "m5 competition lightgbm solution",
      "feature engineering for time series with gradient boosting",
      "recursive vs direct forecasting lightgbm"
    ]
  },
  {
    "name": "Blocked Time Series Cross Validation",
    "description": "Addresses critical issue: expanding window CV produces overly optimistic estimates. Drop-in sklearn-compatible code. Explains why blocked CV gives realistic production performance estimates.",
    "category": "Machine Learning",
    "url": "https://towardsdatascience.com/reduce-bias-in-time-series-cross-validation-with-blocked-split-4ecbfc88f5a4/",
    "type": "Tutorial",
    "level": "Medium",
    "tags": [
      "Forecasting",
      "Cross-Validation"
    ],
    "domain": "Forecasting & Time Series",
    "difficulty": "intermediate",
    "prerequisites": [
      "scikit-learn",
      "time-series-analysis",
      "python-pandas"
    ],
    "topic_tags": [
      "time-series",
      "cross-validation",
      "forecasting",
      "model-evaluation",
      "sklearn"
    ],
    "summary": "A cross-validation technique specifically designed for time series data that prevents data leakage by creating time-based blocks instead of random splits. Provides more realistic performance estimates for production forecasting models compared to standard expanding window approaches. Essential for anyone building time-dependent predictive models in tech environments.",
    "use_cases": [
      "Evaluating demand forecasting models for e-commerce inventory management",
      "Validating user engagement prediction models with sequential behavioral data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "time series cross validation sklearn",
      "blocked cv forecasting models",
      "how to evaluate time series models properly",
      "cross validation temporal data leakage"
    ]
  },
  {
    "name": "M5 Competition Analysis: Learnings and Winning Solutions",
    "description": "Synthesizes learnings from 5,558 teams on 42,840 time series. Key finding: ML beats statistical when you have many correlated series, exogenous variables, hierarchical structure. LightGBM vs. N-BEATS vs. seq2seq comparison.",
    "category": "Machine Learning",
    "url": "https://medium.com/analytics-vidhya/predicting-the-future-with-learnings-from-the-m5-competition-d54e84ca3d0d",
    "type": "Analysis",
    "level": "Medium",
    "tags": [
      "Forecasting",
      "Competition"
    ],
    "domain": "Forecasting & Time Series",
    "difficulty": "intermediate",
    "prerequisites": [
      "time-series-analysis",
      "lightgbm",
      "neural-networks"
    ],
    "topic_tags": [
      "forecasting",
      "competition-analysis",
      "lightgbm",
      "neural-networks",
      "hierarchical-forecasting"
    ],
    "summary": "Comprehensive analysis of the M5 forecasting competition results from over 5,500 teams predicting 42,840 time series. Shows when machine learning methods outperform statistical approaches and compares popular models like LightGBM, N-BEATS, and seq2seq. Provides practical guidance for choosing forecasting methods based on data characteristics.",
    "use_cases": [
      "Selecting appropriate forecasting methods for retail demand prediction with hierarchical product categories",
      "Benchmarking ML vs statistical approaches for multi-series forecasting problems"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "when does machine learning beat statistical forecasting",
      "M5 competition winning solutions comparison",
      "lightgbm vs n-beats for time series forecasting",
      "hierarchical forecasting best practices from competitions"
    ]
  },
  {
    "name": "TensorFlow Time Series Forecasting Tutorial",
    "description": "Official Google documentation with production-quality code. Builds models incrementally: linear \u2192 dense \u2192 CNN \u2192 LSTM. Includes baseline comparisons so you can assess if DL is worth the complexity. Runnable in Colab.",
    "category": "Deep Learning",
    "url": "https://www.tensorflow.org/tutorials/structured_data/time_series",
    "type": "Interactive Tutorial",
    "level": "Medium",
    "tags": [
      "Forecasting",
      "Deep Learning"
    ],
    "domain": "Forecasting & Time Series",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-tensorflow",
      "neural-networks",
      "time-series-basics"
    ],
    "topic_tags": [
      "time-series-forecasting",
      "tensorflow",
      "LSTM",
      "CNN",
      "tutorial"
    ],
    "summary": "A comprehensive TensorFlow tutorial that teaches time series forecasting by building increasingly complex models from linear regression to LSTMs. Includes production-ready code, baseline comparisons, and guidance on when deep learning approaches are justified over simpler methods.",
    "use_cases": [
      "Forecasting product demand or user engagement metrics at a tech company",
      "Building predictive models for financial time series or sensor data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "tensorflow time series forecasting tutorial",
      "how to build LSTM models for forecasting",
      "deep learning vs linear models time series",
      "production tensorflow forecasting code"
    ]
  },
  {
    "name": "Temporal Fusion Transformer: Complete Tutorial",
    "description": "End-to-end TFT with PyTorch Forecasting. Handles heterogeneous features (static, time-varying known/unknown). Interpretability via variable importance and attention. Shows when TFT outperforms simpler methods.",
    "category": "Deep Learning",
    "url": "https://towardsdatascience.com/temporal-fusion-transformer-time-series-forecasting-with-deep-learning-complete-tutorial-d32c1e51cd91/",
    "type": "Tutorial",
    "level": "Hard",
    "tags": [
      "Forecasting",
      "Transformers"
    ],
    "domain": "Forecasting & Time Series",
    "difficulty": "intermediate",
    "prerequisites": [
      "pytorch-basics",
      "time-series-analysis",
      "transformer-architecture"
    ],
    "topic_tags": [
      "temporal-fusion-transformer",
      "time-series-forecasting",
      "pytorch-forecasting",
      "attention-mechanism",
      "interpretable-ml"
    ],
    "summary": "Complete tutorial on Temporal Fusion Transformer (TFT), a state-of-the-art deep learning model for time series forecasting that handles multiple types of features and provides interpretability. Uses PyTorch Forecasting for implementation and shows how to leverage attention mechanisms and variable importance for explainable predictions. Includes guidance on when TFT is worth the complexity over simpler forecasting methods.",
    "use_cases": [
      "demand-forecasting-with-promotional-calendar-and-weather-data",
      "financial-time-series-prediction-with-mixed-categorical-and-numerical-features"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "temporal fusion transformer tutorial pytorch",
      "TFT time series forecasting implementation",
      "interpretable deep learning forecasting with attention",
      "when to use temporal fusion transformer vs simpler models"
    ]
  },
  {
    "name": "Time Series Forecasting with Lag-Llama",
    "description": "Foundation models landscape (Lag-Llama, TimesFM, Moirai, TimeGPT-1). Zero-shot vs. fine-tuning decision framework. Probabilistic forecasts with uncertainty quantification. Complete Python with GluonTS.",
    "category": "Deep Learning",
    "url": "https://www.ibm.com/think/tutorials/lag-llama",
    "type": "Tutorial",
    "level": "Hard",
    "tags": [
      "Forecasting",
      "Foundation Models"
    ],
    "domain": "Forecasting & Time Series",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "time-series-analysis",
      "pytorch-basics"
    ],
    "topic_tags": [
      "time-series-forecasting",
      "foundation-models",
      "probabilistic-forecasting",
      "gluonts",
      "python-tutorial"
    ],
    "summary": "A comprehensive guide to modern foundation models for time series forecasting, comparing Lag-Llama, TimesFM, Moirai, and TimeGPT-1. Provides a decision framework for choosing between zero-shot predictions and fine-tuning approaches. Includes hands-on Python implementation using GluonTS for probabilistic forecasting with uncertainty quantification.",
    "use_cases": [
      "Forecasting product demand across multiple SKUs without historical model training",
      "Predicting user engagement metrics with confidence intervals for A/B test planning"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "foundation models for time series forecasting",
      "lag llama vs timesgpt comparison",
      "zero shot time series forecasting python",
      "probabilistic forecasting with uncertainty gluonts"
    ]
  },
  {
    "name": "Chronos-Bolt: Fast Zero-Shot Forecasting (AWS)",
    "description": "T5 architecture with patching. Quantifies efficiency-accuracy tradeoff: 250x faster, 20x more memory efficient than original Chronos. Benchmarked on 27 datasets. Shows combining univariate foundation models with exogenous features.",
    "category": "Deep Learning",
    "url": "https://aws.amazon.com/blogs/machine-learning/fast-and-accurate-zero-shot-forecasting-with-chronos-bolt-and-autogluon/",
    "type": "Engineering Blog",
    "level": "Hard",
    "tags": [
      "Forecasting",
      "Foundation Models"
    ],
    "domain": "Forecasting & Time Series",
    "difficulty": "intermediate",
    "prerequisites": [
      "pytorch-transformers",
      "time-series-analysis",
      "transformer-architectures"
    ],
    "topic_tags": [
      "zero-shot-forecasting",
      "foundation-models",
      "time-series",
      "model-efficiency",
      "aws"
    ],
    "summary": "Chronos-Bolt is AWS's optimized version of the Chronos forecasting foundation model, built on T5 architecture with patching techniques. It delivers 250x speed improvement and 20x memory efficiency while maintaining competitive accuracy across 27 benchmark datasets. The model demonstrates how to effectively combine univariate foundation models with external features for zero-shot time series forecasting.",
    "use_cases": [
      "Deploying fast forecasting models in production environments where latency and memory constraints are critical",
      "Running zero-shot forecasting experiments across multiple time series without domain-specific training data"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "fast zero shot time series forecasting",
      "efficient transformer models for forecasting",
      "chronos bolt vs original chronos performance",
      "foundation models for time series with external features"
    ]
  },
  {
    "name": "Uber: Forecasting Introduction",
    "description": "Written by M4 Competition winner team. Covers 15 million trips/day across 600+ cities. Explicitly addresses ML vs. statistical methods decision. Use cases: marketplace, capacity planning, marketing.",
    "category": "Production Systems",
    "url": "https://www.uber.com/blog/forecasting-introduction/",
    "type": "Engineering Blog",
    "level": "Medium",
    "tags": [
      "Forecasting",
      "Production"
    ],
    "domain": "Forecasting & Time Series",
    "difficulty": "intermediate",
    "prerequisites": [
      "time-series-analysis",
      "python-pandas",
      "statistical-modeling"
    ],
    "topic_tags": [
      "time-series-forecasting",
      "production-systems",
      "statistical-methods",
      "marketplace-optimization",
      "capacity-planning"
    ],
    "summary": "Production-scale forecasting guide from Uber's M4 Competition winning team covering 15 million daily trips across 600+ cities. Provides practical framework for choosing between ML and statistical approaches in high-volume forecasting scenarios. Includes real-world applications for marketplace dynamics, capacity planning, and marketing optimization.",
    "use_cases": [
      "Predicting demand surge patterns for ride-sharing platforms to optimize driver allocation",
      "Forecasting customer acquisition volumes for marketing budget planning in two-sided marketplaces"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "How to choose between ML and statistical methods for forecasting",
      "Production forecasting at scale like Uber",
      "Time series forecasting for marketplace capacity planning",
      "Real-world forecasting implementation best practices"
    ]
  },
  {
    "name": "Uber: Backtesting at Scale",
    "description": "Architecture for ~10 million backtests. Four backtesting vectors (cities, windows, variants, granularity). Go/Cadence workflows. Evolution from Omphalos framework to handle exponential growth.",
    "category": "Production Systems",
    "url": "https://www.uber.com/blog/backtesting-at-scale/",
    "type": "Engineering Blog",
    "level": "Hard",
    "tags": [
      "Forecasting",
      "Production"
    ],
    "domain": "Forecasting & Time Series",
    "difficulty": "intermediate",
    "prerequisites": [
      "time-series-forecasting",
      "distributed-systems",
      "workflow-orchestration"
    ],
    "topic_tags": [
      "backtesting",
      "scalable-architecture",
      "forecasting-systems",
      "production-engineering",
      "workflow-management"
    ],
    "summary": "Uber's architecture for running ~10 million backtests across cities, time windows, model variants, and granularities. Uses Go and Cadence workflows to handle exponential growth beyond their original Omphalos framework. Shows how to scale forecasting validation from prototype to production at massive scale.",
    "use_cases": [
      "Building scalable backtesting infrastructure for demand forecasting across multiple markets",
      "Designing distributed systems to validate time series models across thousands of parameter combinations"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "how to scale backtesting for time series forecasting",
      "uber backtesting architecture distributed systems",
      "running millions of backtests production system",
      "cadence workflows for model validation at scale"
    ]
  },
  {
    "name": "DoorDash: ELITE Ensemble Learning",
    "description": "ELITE (Ensemble Learning for Improved Time-series Estimation). Addresses accuracy vs. speed/cost tradeoffs. Scales to tens of thousands of targets. Practical engineering decisions for when perfect is enemy of good.",
    "category": "Production Systems",
    "url": "https://doordash.engineering/2023/06/20/how-doordash-built-an-ensemble-learning-model-for-time-series-forecasting/",
    "type": "Engineering Blog",
    "level": "Hard",
    "tags": [
      "Forecasting",
      "Production"
    ],
    "domain": "Forecasting & Time Series",
    "difficulty": "intermediate",
    "prerequisites": [
      "time-series-forecasting",
      "ensemble-methods",
      "python-scikit-learn"
    ],
    "topic_tags": [
      "ensemble-learning",
      "time-series",
      "production-forecasting",
      "scalability",
      "doordash"
    ],
    "summary": "DoorDash's ELITE framework combines multiple forecasting models to improve prediction accuracy while maintaining computational efficiency at scale. The system handles tens of thousands of forecasting targets simultaneously, making practical engineering tradeoffs between model complexity and operational requirements. It demonstrates how to operationalize ensemble methods for real-world time-series problems where perfect accuracy must be balanced against speed and cost constraints.",
    "use_cases": [
      "Forecasting delivery demand across thousands of restaurant locations with varying traffic patterns",
      "Predicting inventory needs for multiple product categories while managing computational costs in production"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "ensemble learning for time series forecasting at scale",
      "DoorDash forecasting production system",
      "how to balance accuracy vs speed in forecasting models",
      "ensemble methods for thousands of time series targets"
    ]
  },
  {
    "name": "Instacart Anytime: Data Science Paradigm",
    "description": "End-to-end system: forecasting integrates with supply planning and capacity decisions. Key metrics: Availability, Idleness, Unmet Demand. Multi-horizon forecasting (weeks ahead for acquisition, hourly for store-level).",
    "category": "Production Systems",
    "url": "https://tech.instacart.com/instacart-anytime-a-data-science-paradigm-33eb25a5c32d",
    "type": "Engineering Blog",
    "level": "Medium",
    "tags": [
      "Forecasting",
      "Production"
    ],
    "domain": "Forecasting & Time Series",
    "difficulty": "intermediate",
    "prerequisites": [
      "time-series-forecasting",
      "python-pandas",
      "supply-chain-basics"
    ],
    "topic_tags": [
      "demand-forecasting",
      "supply-chain",
      "multi-horizon-prediction",
      "production-systems",
      "instacart"
    ],
    "summary": "Instacart's end-to-end forecasting system that integrates demand prediction with supply planning and capacity decisions. The system handles multiple time horizons from hourly store-level forecasts to weeks-ahead acquisition planning. Key focus on balancing availability, shopper idleness, and unmet demand through coordinated forecasting.",
    "use_cases": [
      "Building integrated forecasting systems for marketplace platforms with supply and demand matching",
      "Designing multi-horizon prediction pipelines for retail operations with capacity constraints"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "instacart forecasting system architecture",
      "multi-horizon demand forecasting production",
      "supply chain forecasting integration",
      "marketplace capacity planning forecasting"
    ]
  },
  {
    "name": "Causal Inference for the Brave and True: Time Series",
    "description": "By economist at Nubank. Chapters 13-15, 24-25 address panel data/time series causal analysis. DiD, synthetic controls, RDD with time dimension. Bridges econometrics and ML with executable notebooks.",
    "category": "Specialized Methods",
    "url": "https://matheusfacure.github.io/python-causality-handbook/landing-page.html",
    "type": "Interactive Course",
    "level": "Hard",
    "tags": [
      "Forecasting",
      "Causal"
    ],
    "domain": "Forecasting & Time Series",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "basic-regression",
      "difference-in-differences"
    ],
    "topic_tags": [
      "time-series-causal-inference",
      "synthetic-control",
      "regression-discontinuity",
      "panel-data",
      "jupyter-notebooks"
    ],
    "summary": "A practical guide to causal inference methods specifically for time series and panel data, written by a Nubank economist. Covers difference-in-differences, synthetic controls, and regression discontinuity with time dimensions through executable Python notebooks. Bridges traditional econometric methods with modern machine learning implementation approaches.",
    "use_cases": [
      "Measuring impact of product feature launch on user engagement over time",
      "Evaluating policy interventions using regional panel data with synthetic control groups"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "time series causal inference python tutorial",
      "synthetic control method implementation guide",
      "difference in differences panel data python",
      "causal analysis time series data practical examples"
    ]
  },
  {
    "name": "Conformal Prediction Intervals for Time Series",
    "description": "Distribution-free uncertainty quantification without Gaussian assumptions. Model-agnostic approach works with any forecasting method. Addresses limitation of bootstrap (only captures data uncertainty). MAPIE implementation.",
    "category": "Specialized Methods",
    "url": "https://towardsdatascience.com/time-series-forecasting-with-conformal-prediction-intervals-scikit-learn-is-all-you-need-4b68143a027a/",
    "type": "Tutorial",
    "level": "Hard",
    "tags": [
      "Forecasting",
      "Uncertainty"
    ],
    "domain": "Forecasting & Time Series",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-time-series",
      "quantile-regression",
      "cross-validation"
    ],
    "topic_tags": [
      "conformal-prediction",
      "uncertainty-quantification",
      "time-series",
      "forecasting",
      "model-agnostic"
    ],
    "summary": "Conformal prediction provides distribution-free uncertainty intervals for time series forecasts without assuming Gaussian errors. This model-agnostic approach works with any forecasting method and captures both model and data uncertainty. The MAPIE library provides practical implementation for Python users.",
    "use_cases": [
      "Creating prediction intervals for revenue forecasts when unsure about error distribution assumptions",
      "Quantifying uncertainty in demand forecasting models for inventory planning decisions"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "How to create prediction intervals without Gaussian assumptions",
      "Model agnostic uncertainty quantification time series",
      "Conformal prediction vs bootstrap for forecasting",
      "MAPIE conformal prediction tutorial"
    ]
  },
  {
    "name": "Anomaly Detection in Time Series",
    "description": "Systematic coverage: point outliers, subsequence outliers. Methods from simple to complex: STL-based, Isolation Forest, ARIMA/Prophet-based, autoencoders with PyOD. Critical for pre-forecasting data cleaning.",
    "category": "Specialized Methods",
    "url": "https://neptune.ai/blog/anomaly-detection-in-time-series",
    "type": "Tutorial",
    "level": "Medium",
    "tags": [
      "Forecasting",
      "Anomaly Detection"
    ],
    "domain": "Forecasting & Time Series",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn",
      "time-series-basics"
    ],
    "topic_tags": [
      "anomaly-detection",
      "time-series",
      "outlier-detection",
      "data-preprocessing",
      "forecasting-prep"
    ],
    "summary": "A comprehensive guide to detecting anomalies in time series data, covering both point and subsequence outliers. Progresses from simple statistical methods like STL decomposition to advanced techniques including Isolation Forest and neural autoencoders, with practical PyOD implementations for data scientists preparing forecasting pipelines.",
    "use_cases": [
      "Cleaning historical sales data before building demand forecasting models",
      "Identifying unusual user behavior patterns in app usage metrics before A/B testing"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "how to detect outliers in time series before forecasting",
      "anomaly detection methods for cleaning time series data",
      "pyod time series outlier detection tutorial",
      "preprocessing time series data remove anomalies forecasting"
    ]
  },
  {
    "name": "Change Point Detection in Time Series",
    "description": "Six algorithms via ruptures library (PELT, Dynamic Programming, Binary Segmentation, Window-based, Bottom-up, Kernel CPD). Real Google Search Console application. Discusses computational complexity tradeoffs.",
    "category": "Specialized Methods",
    "url": "https://forecastegy.com/posts/change-point-detection-time-series-python/",
    "type": "Tutorial",
    "level": "Medium",
    "tags": [
      "Forecasting",
      "Change Point"
    ],
    "domain": "Forecasting & Time Series",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "time-series-analysis",
      "scikit-learn"
    ],
    "topic_tags": [
      "change-point-detection",
      "time-series",
      "ruptures-library",
      "anomaly-detection",
      "structural-breaks"
    ],
    "summary": "A comprehensive guide to detecting structural breaks in time series using six different algorithms implemented in the ruptures Python library. Compares computational complexity and performance tradeoffs between methods like PELT, Dynamic Programming, and Binary Segmentation. Includes a practical application using Google Search Console data to demonstrate real-world implementation.",
    "use_cases": [
      "Detecting when user engagement patterns change after product launches or marketing campaigns",
      "Identifying structural breaks in business metrics like revenue or conversion rates during economic shifts"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "how to detect change points in time series python",
      "ruptures library tutorial for structural breaks",
      "best algorithm for change point detection",
      "detecting when metrics behavior changes over time"
    ]
  }
]
